<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Lecture 4 Linear algebra | Computational Math Camp</title>
  <meta name="description" content="Contains lecture notes for the 2022 Computational Math Camp." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="Lecture 4 Linear algebra | Computational Math Camp" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Contains lecture notes for the 2022 Computational Math Camp." />
  <meta name="github-repo" content="math-camp/notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Lecture 4 Linear algebra | Computational Math Camp" />
  
  <meta name="twitter:description" content="Contains lecture notes for the 2022 Computational Math Camp." />
  

<meta name="author" content="Yanyan Sheng" />


<meta name="date" content="2022-08-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="critical-points.html"/>
<link rel="next" href="multivariable-differentiation.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script>
\[
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\se}{\text{se}}
\newcommand{\sd}{\text{sd}}
\newcommand{\Cor}{\mathrm{Cor}}
\newcommand{\Lagr}{\mathcal{L}}
\newcommand{\lagr}{\mathcal{l}}
\]


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Overview</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#meeting-information"><i class="fa fa-check"></i>Meeting information</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#instructional-staff"><i class="fa fa-check"></i>Instructional staff</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#teaching-assistants"><i class="fa fa-check"></i>Teaching assistants</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-description"><i class="fa fa-check"></i>Course description</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-should-take-this-course"><i class="fa fa-check"></i>Who should take this course</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#grades"><i class="fa fa-check"></i>Grades</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#disability-services"><i class="fa fa-check"></i>Disability services</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#core-texts"><i class="fa fa-check"></i>Core texts</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-format"><i class="fa fa-check"></i>Course format</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#problem-sets"><i class="fa fa-check"></i>Problem sets</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-schedule"><i class="fa fa-check"></i>Course schedule</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="sets-functions.html"><a href="sets-functions.html"><i class="fa fa-check"></i><b>1</b> Linear equations, inequalities, sets and functions, quadratics, and logarithms</a>
<ul>
<li class="chapter" data-level="" data-path="sets-functions.html"><a href="sets-functions.html#learning-objectives"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="sets-functions.html"><a href="sets-functions.html#supplemental-readings"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="1.1" data-path="sets-functions.html"><a href="sets-functions.html#what-is-computational-social-science"><i class="fa fa-check"></i><b>1.1</b> What is computational social science?</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="sets-functions.html"><a href="sets-functions.html#disciplines-within-social-science"><i class="fa fa-check"></i><b>1.1.1</b> Disciplines within social science</a></li>
<li class="chapter" data-level="1.1.2" data-path="sets-functions.html"><a href="sets-functions.html#computational-social-science"><i class="fa fa-check"></i><b>1.1.2</b> Computational social science</a></li>
<li class="chapter" data-level="1.1.3" data-path="sets-functions.html"><a href="sets-functions.html#acquiring-css-skills"><i class="fa fa-check"></i><b>1.1.3</b> Acquiring CSS skills</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="sets-functions.html"><a href="sets-functions.html#difference-between-math-probability-and-statistics"><i class="fa fa-check"></i><b>1.2</b> Difference between math, probability, and statistics</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="sets-functions.html"><a href="sets-functions.html#mathematics"><i class="fa fa-check"></i><b>1.2.1</b> Mathematics</a></li>
<li class="chapter" data-level="1.2.2" data-path="sets-functions.html"><a href="sets-functions.html#probability"><i class="fa fa-check"></i><b>1.2.2</b> Probability</a></li>
<li class="chapter" data-level="1.2.3" data-path="sets-functions.html"><a href="sets-functions.html#statistics"><i class="fa fa-check"></i><b>1.2.3</b> Statistics</a></li>
<li class="chapter" data-level="1.2.4" data-path="sets-functions.html"><a href="sets-functions.html#their-uses"><i class="fa fa-check"></i><b>1.2.4</b> Their uses</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="sets-functions.html"><a href="sets-functions.html#goals-for-this-camp"><i class="fa fa-check"></i><b>1.3</b> Goals for this camp</a></li>
<li class="chapter" data-level="1.4" data-path="sets-functions.html"><a href="sets-functions.html#course-logistics"><i class="fa fa-check"></i><b>1.4</b> Course logistics</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="sets-functions.html"><a href="sets-functions.html#course-staff"><i class="fa fa-check"></i><b>1.4.1</b> Course staff</a></li>
<li class="chapter" data-level="1.4.2" data-path="sets-functions.html"><a href="sets-functions.html#teaching-assistants-1"><i class="fa fa-check"></i><b>1.4.2</b> Teaching assistants</a></li>
<li class="chapter" data-level="1.4.3" data-path="sets-functions.html"><a href="sets-functions.html#prerequisites-for-the-math-camp"><i class="fa fa-check"></i><b>1.4.3</b> Prerequisites for the math camp</a></li>
<li class="chapter" data-level="1.4.4" data-path="sets-functions.html"><a href="sets-functions.html#alternatives-to-this-camp"><i class="fa fa-check"></i><b>1.4.4</b> Alternatives to this camp</a></li>
<li class="chapter" data-level="1.4.5" data-path="sets-functions.html"><a href="sets-functions.html#evaluation"><i class="fa fa-check"></i><b>1.4.5</b> Evaluation</a></li>
<li class="chapter" data-level="1.4.6" data-path="sets-functions.html"><a href="sets-functions.html#why-are-we-doing-this"><i class="fa fa-check"></i><b>1.4.6</b> Why are we doing this</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="sets-functions.html"><a href="sets-functions.html#mathematical-notation"><i class="fa fa-check"></i><b>1.5</b> Mathematical notation</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="sets-functions.html"><a href="sets-functions.html#why-math-is-important-to-social-science"><i class="fa fa-check"></i><b>1.5.1</b> Why math is important to social science</a></li>
<li class="chapter" data-level="1.5.2" data-path="sets-functions.html"><a href="sets-functions.html#example-paradox-of-voting"><i class="fa fa-check"></i><b>1.5.2</b> Example: Paradox of voting</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="sets-functions.html"><a href="sets-functions.html#sets"><i class="fa fa-check"></i><b>1.6</b> Sets</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="sets-functions.html"><a href="sets-functions.html#set-builder-notation"><i class="fa fa-check"></i><b>1.6.1</b> Set builder notation</a></li>
<li class="chapter" data-level="1.6.2" data-path="sets-functions.html"><a href="sets-functions.html#set-operations"><i class="fa fa-check"></i><b>1.6.2</b> Set operations</a></li>
<li class="chapter" data-level="1.6.3" data-path="sets-functions.html"><a href="sets-functions.html#some-facts-about-sets"><i class="fa fa-check"></i><b>1.6.3</b> Some facts about sets</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="sets-functions.html"><a href="sets-functions.html#functions"><i class="fa fa-check"></i><b>1.7</b> Functions</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="sets-functions.html"><a href="sets-functions.html#ordered-pairs"><i class="fa fa-check"></i><b>1.7.1</b> Ordered pairs</a></li>
<li class="chapter" data-level="1.7.2" data-path="sets-functions.html"><a href="sets-functions.html#relation"><i class="fa fa-check"></i><b>1.7.2</b> Relation</a></li>
<li class="chapter" data-level="1.7.3" data-path="sets-functions.html"><a href="sets-functions.html#relation-vs.-function"><i class="fa fa-check"></i><b>1.7.3</b> Relation vs.Â function</a></li>
<li class="chapter" data-level="1.7.4" data-path="sets-functions.html"><a href="sets-functions.html#two-major-properties-of-functions"><i class="fa fa-check"></i><b>1.7.4</b> Two major properties of functions</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="sets-functions.html"><a href="sets-functions.html#quadratic-functions"><i class="fa fa-check"></i><b>1.8</b> Quadratic functions</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="sets-functions.html"><a href="sets-functions.html#quadratic-equation"><i class="fa fa-check"></i><b>1.8.1</b> Quadratic equation</a></li>
<li class="chapter" data-level="1.8.2" data-path="sets-functions.html"><a href="sets-functions.html#quadratic-formula"><i class="fa fa-check"></i><b>1.8.2</b> Quadratic formula</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="sets-functions.html"><a href="sets-functions.html#systems-of-linear-equations"><i class="fa fa-check"></i><b>1.9</b> Systems of linear equations</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="sets-functions.html"><a href="sets-functions.html#one-solution"><i class="fa fa-check"></i><b>1.9.1</b> One solution</a></li>
<li class="chapter" data-level="1.9.2" data-path="sets-functions.html"><a href="sets-functions.html#no-solution"><i class="fa fa-check"></i><b>1.9.2</b> No solution</a></li>
<li class="chapter" data-level="1.9.3" data-path="sets-functions.html"><a href="sets-functions.html#infinite-solutions"><i class="fa fa-check"></i><b>1.9.3</b> Infinite solutions</a></li>
<li class="chapter" data-level="1.9.4" data-path="sets-functions.html"><a href="sets-functions.html#three-equations-in-three-unknowns"><i class="fa fa-check"></i><b>1.9.4</b> Three equations in three unknowns</a></li>
<li class="chapter" data-level="1.9.5" data-path="sets-functions.html"><a href="sets-functions.html#gaussian-elimination"><i class="fa fa-check"></i><b>1.9.5</b> Gaussian elimination</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="sets-functions.html"><a href="sets-functions.html#logarithms-and-exponential-functions"><i class="fa fa-check"></i><b>1.10</b> Logarithms and exponential functions</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="sets-functions.html"><a href="sets-functions.html#functions-with-exponents"><i class="fa fa-check"></i><b>1.10.1</b> Functions with exponents</a></li>
<li class="chapter" data-level="1.10.2" data-path="sets-functions.html"><a href="sets-functions.html#common-rules-of-exponents"><i class="fa fa-check"></i><b>1.10.2</b> Common rules of exponents</a></li>
<li class="chapter" data-level="1.10.3" data-path="sets-functions.html"><a href="sets-functions.html#logarithms"><i class="fa fa-check"></i><b>1.10.3</b> Logarithms</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="sets-functions.html"><a href="sets-functions.html#bonus-content-computational-tools-for-the-future"><i class="fa fa-check"></i><b>1.11</b> Bonus content: Computational tools for the future</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="sets-functions.html"><a href="sets-functions.html#programming-languages-for-statistical-learning"><i class="fa fa-check"></i><b>1.11.1</b> Programming languages for statistical learning</a></li>
<li class="chapter" data-level="1.11.2" data-path="sets-functions.html"><a href="sets-functions.html#version-control-git"><i class="fa fa-check"></i><b>1.11.2</b> Version control (Git)</a></li>
<li class="chapter" data-level="1.11.3" data-path="sets-functions.html"><a href="sets-functions.html#publishing"><i class="fa fa-check"></i><b>1.11.3</b> Publishing</a></li>
<li class="chapter" data-level="1.11.4" data-path="sets-functions.html"><a href="sets-functions.html#how-will-you-acquire-these-skills"><i class="fa fa-check"></i><b>1.11.4</b> How will you acquire these skills?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html"><i class="fa fa-check"></i><b>2</b> Sequences, limits, continuity, and derivatives</a>
<ul>
<li class="chapter" data-level="" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#learning-objectives-1"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#supplemental-readings-1"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="2.1" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#sequence"><i class="fa fa-check"></i><b>2.1</b> Sequence</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#definition"><i class="fa fa-check"></i><b>2.1.1</b> Definition</a></li>
<li class="chapter" data-level="2.1.2" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#examples"><i class="fa fa-check"></i><b>2.1.2</b> Examples</a></li>
<li class="chapter" data-level="2.1.3" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#arithmetic-and-geometric-progressions"><i class="fa fa-check"></i><b>2.1.3</b> Arithmetic and geometric progressions</a></li>
<li class="chapter" data-level="2.1.4" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#convergence"><i class="fa fa-check"></i><b>2.1.4</b> Convergence</a></li>
<li class="chapter" data-level="2.1.5" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#algebra-of-sequences"><i class="fa fa-check"></i><b>2.1.5</b> Algebra of sequences</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#limits"><i class="fa fa-check"></i><b>2.2</b> Limits</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#sequences-leadsto-limits-of-functions"><i class="fa fa-check"></i><b>2.2.1</b> Sequences <span class="math inline">\(\leadsto\)</span> limits of functions</a></li>
<li class="chapter" data-level="2.2.2" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#limits-of-functions"><i class="fa fa-check"></i><b>2.2.2</b> Limits of functions</a></li>
<li class="chapter" data-level="2.2.3" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#examples-of-limits"><i class="fa fa-check"></i><b>2.2.3</b> Examples of limits</a></li>
<li class="chapter" data-level="2.2.4" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#not-all-functions-have-limits"><i class="fa fa-check"></i><b>2.2.4</b> Not all functions have limits</a></li>
<li class="chapter" data-level="2.2.5" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#intuitive-definition-of-a-limit"><i class="fa fa-check"></i><b>2.2.5</b> Intuitive definition of a limit</a></li>
<li class="chapter" data-level="2.2.6" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#algebra-of-limits"><i class="fa fa-check"></i><b>2.2.6</b> Algebra of limits</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#continuity"><i class="fa fa-check"></i><b>2.3</b> Continuity</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#defining-continuity"><i class="fa fa-check"></i><b>2.3.1</b> Defining continuity</a></li>
<li class="chapter" data-level="2.3.2" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#a-real-world-example-of-limits-measuring-incumbency-advantage"><i class="fa fa-check"></i><b>2.3.2</b> A real-world example of limits: Measuring incumbency advantage</a></li>
<li class="chapter" data-level="2.3.3" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#continuity-and-limits"><i class="fa fa-check"></i><b>2.3.3</b> Continuity and limits</a></li>
<li class="chapter" data-level="2.3.4" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#algebra-of-continuous-functions"><i class="fa fa-check"></i><b>2.3.4</b> Algebra of continuous functions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#what-is-calculus"><i class="fa fa-check"></i><b>2.4</b> What is calculus?</a></li>
<li class="chapter" data-level="2.5" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#derivatives"><i class="fa fa-check"></i><b>2.5</b> Derivatives</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#how-functions-change"><i class="fa fa-check"></i><b>2.5.1</b> How functions change</a></li>
<li class="chapter" data-level="2.5.2" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#the-tangent-as-a-limit"><i class="fa fa-check"></i><b>2.5.2</b> The tangent as a limit</a></li>
<li class="chapter" data-level="2.5.3" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#derivative"><i class="fa fa-check"></i><b>2.5.3</b> Derivative</a></li>
<li class="chapter" data-level="2.5.4" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#rates-of-change-in-a-function"><i class="fa fa-check"></i><b>2.5.4</b> Rates of change in a function</a></li>
<li class="chapter" data-level="2.5.5" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#examples-of-derivatives"><i class="fa fa-check"></i><b>2.5.5</b> Examples of derivatives</a></li>
<li class="chapter" data-level="2.5.6" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#continuity-and-derivatives"><i class="fa fa-check"></i><b>2.5.6</b> Continuity and derivatives</a></li>
<li class="chapter" data-level="2.5.7" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#what-goes-wrong"><i class="fa fa-check"></i><b>2.5.7</b> What goes wrong?</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#calculating-derivatives"><i class="fa fa-check"></i><b>2.6</b> Calculating derivatives</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#derivative-rules"><i class="fa fa-check"></i><b>2.6.1</b> Derivative rules</a></li>
<li class="chapter" data-level="2.6.2" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#challenge-problems"><i class="fa fa-check"></i><b>2.6.2</b> Challenge problems</a></li>
<li class="chapter" data-level="2.6.3" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#composite-functions"><i class="fa fa-check"></i><b>2.6.3</b> Composite functions</a></li>
<li class="chapter" data-level="2.6.4" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#chain-rule"><i class="fa fa-check"></i><b>2.6.4</b> Chain rule</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#derivatives-for-the-exponential-function-and-natural-logarithms"><i class="fa fa-check"></i><b>2.7</b> Derivatives for the exponential function and natural logarithms</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#derivative-of-exponential-function"><i class="fa fa-check"></i><b>2.7.1</b> Derivative of exponential function</a></li>
<li class="chapter" data-level="2.7.2" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#derivative-of-the-natural-logarithm"><i class="fa fa-check"></i><b>2.7.2</b> Derivative of the natural logarithm</a></li>
<li class="chapter" data-level="2.7.3" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#relevance-of-exponential-functions-and-natural-logarithm"><i class="fa fa-check"></i><b>2.7.3</b> Relevance of exponential functions and natural logarithm</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#derivatives-and-properties-of-functions"><i class="fa fa-check"></i><b>2.8</b> Derivatives and properties of functions</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#relative-maxima-minima-and-derivatives"><i class="fa fa-check"></i><b>2.8.1</b> Relative maxima, minima and derivatives</a></li>
<li class="chapter" data-level="2.8.2" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#mean-value-theorem"><i class="fa fa-check"></i><b>2.8.2</b> Mean value theorem</a></li>
<li class="chapter" data-level="2.8.3" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#applications-of-the-mean-value-theorem"><i class="fa fa-check"></i><b>2.8.3</b> Applications of the mean value theorem</a></li>
<li class="chapter" data-level="2.8.4" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#extension-to-indeterminate-form-limits"><i class="fa fa-check"></i><b>2.8.4</b> Extension to indeterminate form limits</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="critical-points.html"><a href="critical-points.html"><i class="fa fa-check"></i><b>3</b> Critical points and approximation</a>
<ul>
<li class="chapter" data-level="" data-path="critical-points.html"><a href="critical-points.html#learning-objectives-2"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="critical-points.html"><a href="critical-points.html#supplemental-readings-2"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="3.1" data-path="critical-points.html"><a href="critical-points.html#intuition"><i class="fa fa-check"></i><b>3.1</b> Intuition</a></li>
<li class="chapter" data-level="3.2" data-path="critical-points.html"><a href="critical-points.html#higher-order-derivatives"><i class="fa fa-check"></i><b>3.2</b> Higher order derivatives</a></li>
<li class="chapter" data-level="3.3" data-path="critical-points.html"><a href="critical-points.html#critical-points-1"><i class="fa fa-check"></i><b>3.3</b> Critical points</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="critical-points.html"><a href="critical-points.html#inflection-point"><i class="fa fa-check"></i><b>3.3.1</b> Inflection point</a></li>
<li class="chapter" data-level="3.3.2" data-path="critical-points.html"><a href="critical-points.html#concavity"><i class="fa fa-check"></i><b>3.3.2</b> Concavity</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="critical-points.html"><a href="critical-points.html#extrema"><i class="fa fa-check"></i><b>3.4</b> Extrema</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="critical-points.html"><a href="critical-points.html#minimum-and-maximum-on-the-interval-05-are-located-at-the-endpoints"><i class="fa fa-check"></i><b>3.4.1</b> Minimum and maximum on the interval <span class="math inline">\([0,5]\)</span> are located at the endpoints</a></li>
<li class="chapter" data-level="3.4.2" data-path="critical-points.html"><a href="critical-points.html#global-maximum-is-located-at-x0"><i class="fa fa-check"></i><b>3.4.2</b> Global maximum is located at <span class="math inline">\(x=0\)</span></a></li>
<li class="chapter" data-level="3.4.3" data-path="critical-points.html"><a href="critical-points.html#global-minimum-is-located-at-x---frac92"><i class="fa fa-check"></i><b>3.4.3</b> Global minimum is located at <span class="math inline">\(x= - \frac{9}{2}\)</span></a></li>
<li class="chapter" data-level="3.4.4" data-path="critical-points.html"><a href="critical-points.html#a-bunch-of-local-minima-and-maxima"><i class="fa fa-check"></i><b>3.4.4</b> A bunch of local minima and maxima</a></li>
<li class="chapter" data-level="3.4.5" data-path="critical-points.html"><a href="critical-points.html#x0-is-an-inflection-point-that-is-neither-a-minimum-nor-a-maximum-fx-0"><i class="fa fa-check"></i><b>3.4.5</b> <span class="math inline">\(x=0\)</span> is an inflection point that is neither a minimum nor a maximum (<span class="math inline">\(f&#39;&#39;(x) = 0\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="critical-points.html"><a href="critical-points.html#framework-for-analytical-optimization"><i class="fa fa-check"></i><b>3.5</b> Framework for analytical optimization</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="critical-points.html"><a href="critical-points.html#example-fx--x2-x-in--3-3"><i class="fa fa-check"></i><b>3.5.1</b> Example: <span class="math inline">\(f(x) = -x^2\)</span>, <span class="math inline">\(x \in [-3, 3]\)</span></a></li>
<li class="chapter" data-level="3.5.2" data-path="critical-points.html"><a href="critical-points.html#example-fx-x3-x-in--3-3"><i class="fa fa-check"></i><b>3.5.2</b> Example: <span class="math inline">\(f(x) = x^3\)</span>, <span class="math inline">\(x \in [-3, 3]\)</span></a></li>
<li class="chapter" data-level="3.5.3" data-path="critical-points.html"><a href="critical-points.html#example-spatial-model"><i class="fa fa-check"></i><b>3.5.3</b> Example: spatial model</a></li>
<li class="chapter" data-level="3.5.4" data-path="critical-points.html"><a href="critical-points.html#example-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>3.5.4</b> Example: Maximum likelihood estimation</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="critical-points.html"><a href="critical-points.html#computational-optimization-procedures"><i class="fa fa-check"></i><b>3.6</b> Computational optimization procedures</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="critical-points.html"><a href="critical-points.html#newton-raphson-root-finding"><i class="fa fa-check"></i><b>3.6.1</b> Newton-Raphson root finding</a></li>
<li class="chapter" data-level="3.6.2" data-path="critical-points.html"><a href="critical-points.html#grid-search"><i class="fa fa-check"></i><b>3.6.2</b> Grid search</a></li>
<li class="chapter" data-level="3.6.3" data-path="critical-points.html"><a href="critical-points.html#gradient-descent"><i class="fa fa-check"></i><b>3.6.3</b> Gradient descent</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-algebra.html"><a href="linear-algebra.html"><i class="fa fa-check"></i><b>4</b> Linear algebra</a>
<ul>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#learning-objectives-3"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#supplemental-readings-3"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="4.1" data-path="linear-algebra.html"><a href="linear-algebra.html#linear-algebra-1"><i class="fa fa-check"></i><b>4.1</b> Linear algebra</a></li>
<li class="chapter" data-level="4.2" data-path="linear-algebra.html"><a href="linear-algebra.html#points-and-vectors"><i class="fa fa-check"></i><b>4.2</b> Points and vectors</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="linear-algebra.html"><a href="linear-algebra.html#points"><i class="fa fa-check"></i><b>4.2.1</b> Points</a></li>
<li class="chapter" data-level="4.2.2" data-path="linear-algebra.html"><a href="linear-algebra.html#vectors"><i class="fa fa-check"></i><b>4.2.2</b> Vectors</a></li>
<li class="chapter" data-level="4.2.3" data-path="linear-algebra.html"><a href="linear-algebra.html#one-dimensional-example"><i class="fa fa-check"></i><b>4.2.3</b> One dimensional example</a></li>
<li class="chapter" data-level="4.2.4" data-path="linear-algebra.html"><a href="linear-algebra.html#two-dimensional-example"><i class="fa fa-check"></i><b>4.2.4</b> Two dimensional example</a></li>
<li class="chapter" data-level="4.2.5" data-path="linear-algebra.html"><a href="linear-algebra.html#three-dimensional-example"><i class="fa fa-check"></i><b>4.2.5</b> Three dimensional example</a></li>
<li class="chapter" data-level="4.2.6" data-path="linear-algebra.html"><a href="linear-algebra.html#n-dimensional-example"><i class="fa fa-check"></i><b>4.2.6</b> <span class="math inline">\(N\)</span>-dimensional example</a></li>
<li class="chapter" data-level="4.2.7" data-path="linear-algebra.html"><a href="linear-algebra.html#examples-of-some-basic-arithmetic"><i class="fa fa-check"></i><b>4.2.7</b> Examples of some basic arithmetic</a></li>
<li class="chapter" data-level="4.2.8" data-path="linear-algebra.html"><a href="linear-algebra.html#linear-dependence"><i class="fa fa-check"></i><b>4.2.8</b> Linear dependence</a></li>
<li class="chapter" data-level="4.2.9" data-path="linear-algebra.html"><a href="linear-algebra.html#inner-product"><i class="fa fa-check"></i><b>4.2.9</b> Inner product</a></li>
<li class="chapter" data-level="4.2.10" data-path="linear-algebra.html"><a href="linear-algebra.html#calculating-vector-length"><i class="fa fa-check"></i><b>4.2.10</b> Calculating vector length</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="linear-algebra.html"><a href="linear-algebra.html#example-text-analysis"><i class="fa fa-check"></i><b>4.3</b> Example: text analysis</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="linear-algebra.html"><a href="linear-algebra.html#measure-1-inner-product"><i class="fa fa-check"></i><b>4.3.1</b> Measure 1: inner product</a></li>
<li class="chapter" data-level="4.3.2" data-path="linear-algebra.html"><a href="linear-algebra.html#measure-2-cosine-similarity"><i class="fa fa-check"></i><b>4.3.2</b> Measure 2: cosine similarity</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="linear-algebra.html"><a href="linear-algebra.html#matricies"><i class="fa fa-check"></i><b>4.4</b> Matricies</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="linear-algebra.html"><a href="linear-algebra.html#basic-arithmetic"><i class="fa fa-check"></i><b>4.4.1</b> Basic arithmetic</a></li>
<li class="chapter" data-level="4.4.2" data-path="linear-algebra.html"><a href="linear-algebra.html#transposition"><i class="fa fa-check"></i><b>4.4.2</b> Transposition</a></li>
<li class="chapter" data-level="4.4.3" data-path="linear-algebra.html"><a href="linear-algebra.html#multiplication"><i class="fa fa-check"></i><b>4.4.3</b> Multiplication</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="linear-algebra.html"><a href="linear-algebra.html#example-neural-networks"><i class="fa fa-check"></i><b>4.5</b> Example: neural networks</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="linear-algebra.html"><a href="linear-algebra.html#how-are-neural-networks-used"><i class="fa fa-check"></i><b>4.5.1</b> How are neural networks used</a></li>
<li class="chapter" data-level="4.5.2" data-path="linear-algebra.html"><a href="linear-algebra.html#how-are-neural-networks-related-to-linear-algebra"><i class="fa fa-check"></i><b>4.5.2</b> How are neural networks related to linear algebra?</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="linear-algebra.html"><a href="linear-algebra.html#matrix-inversion"><i class="fa fa-check"></i><b>4.6</b> Matrix inversion</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="linear-algebra.html"><a href="linear-algebra.html#calculating-matrix-inversions"><i class="fa fa-check"></i><b>4.6.1</b> Calculating matrix inversions</a></li>
<li class="chapter" data-level="4.6.2" data-path="linear-algebra.html"><a href="linear-algebra.html#when-do-inverses-exist"><i class="fa fa-check"></i><b>4.6.2</b> When do inverses exist</a></li>
<li class="chapter" data-level="4.6.3" data-path="linear-algebra.html"><a href="linear-algebra.html#inverting-a-2-times-2-matrix"><i class="fa fa-check"></i><b>4.6.3</b> Inverting a <span class="math inline">\(2 \times 2\)</span> matrix</a></li>
<li class="chapter" data-level="4.6.4" data-path="linear-algebra.html"><a href="linear-algebra.html#inverting-an-n-times-n-matrix"><i class="fa fa-check"></i><b>4.6.4</b> Inverting an <span class="math inline">\(n \times n\)</span> matrix</a></li>
<li class="chapter" data-level="4.6.5" data-path="linear-algebra.html"><a href="linear-algebra.html#application-to-regression-analysis"><i class="fa fa-check"></i><b>4.6.5</b> Application to regression analysis</a></li>
<li class="chapter" data-level="4.6.6" data-path="linear-algebra.html"><a href="linear-algebra.html#application-to-solving-systems-of-equations-tax-benefits-of-charitable-contributions"><i class="fa fa-check"></i><b>4.6.6</b> Application to solving systems of equations: tax benefits of charitable contributions</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="linear-algebra.html"><a href="linear-algebra.html#determinant"><i class="fa fa-check"></i><b>4.7</b> Determinant</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="linear-algebra.html"><a href="linear-algebra.html#relevance-of-the-determinant"><i class="fa fa-check"></i><b>4.7.1</b> Relevance of the determinant</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="linear-algebra.html"><a href="linear-algebra.html#matrix-decomposition"><i class="fa fa-check"></i><b>4.8</b> Matrix decomposition</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="linear-algebra.html"><a href="linear-algebra.html#dimension-reduction"><i class="fa fa-check"></i><b>4.8.1</b> Dimension reduction</a></li>
<li class="chapter" data-level="4.8.2" data-path="linear-algebra.html"><a href="linear-algebra.html#singular-value-decomposition"><i class="fa fa-check"></i><b>4.8.2</b> Singular value decomposition</a></li>
<li class="chapter" data-level="4.8.3" data-path="linear-algebra.html"><a href="linear-algebra.html#principal-components-analysis"><i class="fa fa-check"></i><b>4.8.3</b> Principal components analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html"><i class="fa fa-check"></i><b>5</b> Functions of several variables and optimization with several variables</a>
<ul>
<li class="chapter" data-level="" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#learning-objectives-4"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#supplemental-readings-4"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="5.1" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#higher-order-derivatives-1"><i class="fa fa-check"></i><b>5.1</b> Higher order derivatives</a></li>
<li class="chapter" data-level="5.2" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#multivariate-function"><i class="fa fa-check"></i><b>5.2</b> Multivariate function</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#definition-2"><i class="fa fa-check"></i><b>5.2.1</b> Definition</a></li>
<li class="chapter" data-level="5.2.2" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#evaluating-multivariate-functions"><i class="fa fa-check"></i><b>5.2.2</b> Evaluating multivariate functions</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#multivariate-derivatives"><i class="fa fa-check"></i><b>5.3</b> Multivariate derivatives</a></li>
<li class="chapter" data-level="5.4" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#multivariate-optimization"><i class="fa fa-check"></i><b>5.4</b> Multivariate optimization</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#differences-from-single-variable-optimization-procedure"><i class="fa fa-check"></i><b>5.4.1</b> Differences from single variable optimization procedure</a></li>
<li class="chapter" data-level="5.4.2" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#first-derivative-test-gradient"><i class="fa fa-check"></i><b>5.4.2</b> First derivative test: Gradient</a></li>
<li class="chapter" data-level="5.4.3" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#second-derivative-test-hessian"><i class="fa fa-check"></i><b>5.4.3</b> Second derivative test: Hessian</a></li>
<li class="chapter" data-level="5.4.4" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#basic-procedure-summarized"><i class="fa fa-check"></i><b>5.4.4</b> Basic procedure summarized</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#a-simple-optimization-example"><i class="fa fa-check"></i><b>5.5</b> A simple optimization example</a></li>
<li class="chapter" data-level="5.6" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#maximum-likelihood-estimation-for-a-normal-distribution"><i class="fa fa-check"></i><b>5.6</b> Maximum likelihood estimation for a normal distribution</a></li>
<li class="chapter" data-level="5.7" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#computational-optimization-procedures-1"><i class="fa fa-check"></i><b>5.7</b> Computational optimization procedures</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#multivariate-newton-raphson"><i class="fa fa-check"></i><b>5.7.1</b> Multivariate Newton-Raphson</a></li>
<li class="chapter" data-level="5.7.2" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#grid-search-1"><i class="fa fa-check"></i><b>5.7.2</b> Grid search</a></li>
<li class="chapter" data-level="5.7.3" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#gradient-descent-1"><i class="fa fa-check"></i><b>5.7.3</b> Gradient descent</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="integral-calculus.html"><a href="integral-calculus.html"><i class="fa fa-check"></i><b>6</b> Integration and integral calculus</a>
<ul>
<li class="chapter" data-level="" data-path="integral-calculus.html"><a href="integral-calculus.html#learning-objectives-5"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="integral-calculus.html"><a href="integral-calculus.html#supplemental-readings-5"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="6.1" data-path="integral-calculus.html"><a href="integral-calculus.html#prepare-for-the-journey"><i class="fa fa-check"></i><b>6.1</b> Prepare for the journey</a></li>
<li class="chapter" data-level="6.2" data-path="integral-calculus.html"><a href="integral-calculus.html#indefinite-integration"><i class="fa fa-check"></i><b>6.2</b> Indefinite integration</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="integral-calculus.html"><a href="integral-calculus.html#many-possible-antiderivatives"><i class="fa fa-check"></i><b>6.2.1</b> Many possible antiderivatives</a></li>
<li class="chapter" data-level="6.2.2" data-path="integral-calculus.html"><a href="integral-calculus.html#common-rules-of-integration"><i class="fa fa-check"></i><b>6.2.2</b> Common rules of integration</a></li>
<li class="chapter" data-level="6.2.3" data-path="integral-calculus.html"><a href="integral-calculus.html#practice-integrating-functions"><i class="fa fa-check"></i><b>6.2.3</b> Practice integrating functions</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="integral-calculus.html"><a href="integral-calculus.html#the-definite-integral-area-under-the-curve"><i class="fa fa-check"></i><b>6.3</b> The definite integral: area under the curve</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="integral-calculus.html"><a href="integral-calculus.html#the-definite-integral-riemann"><i class="fa fa-check"></i><b>6.3.1</b> The definite integral (Riemann)</a></li>
<li class="chapter" data-level="6.3.2" data-path="integral-calculus.html"><a href="integral-calculus.html#counterexamples"><i class="fa fa-check"></i><b>6.3.2</b> Counterexamples</a></li>
<li class="chapter" data-level="6.3.3" data-path="integral-calculus.html"><a href="integral-calculus.html#fundamental-theorem-of-calculus"><i class="fa fa-check"></i><b>6.3.3</b> Fundamental theorem of calculus</a></li>
<li class="chapter" data-level="6.3.4" data-path="integral-calculus.html"><a href="integral-calculus.html#common-rules-for-definite-integrals"><i class="fa fa-check"></i><b>6.3.4</b> Common rules for definite integrals</a></li>
<li class="chapter" data-level="6.3.5" data-path="integral-calculus.html"><a href="integral-calculus.html#practice-solving-definite-integrals"><i class="fa fa-check"></i><b>6.3.5</b> Practice solving definite integrals</a></li>
<li class="chapter" data-level="6.3.6" data-path="integral-calculus.html"><a href="integral-calculus.html#integration-by-substitution"><i class="fa fa-check"></i><b>6.3.6</b> Integration by substitution</a></li>
<li class="chapter" data-level="6.3.7" data-path="integral-calculus.html"><a href="integral-calculus.html#integration-by-parts"><i class="fa fa-check"></i><b>6.3.7</b> Integration by parts</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="integral-calculus.html"><a href="integral-calculus.html#infinite-integrals"><i class="fa fa-check"></i><b>6.4</b> Infinite integrals</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="integral-calculus.html"><a href="integral-calculus.html#two-sided-infinite-integrals"><i class="fa fa-check"></i><b>6.4.1</b> Two-sided infinite integrals</a></li>
<li class="chapter" data-level="6.4.2" data-path="integral-calculus.html"><a href="integral-calculus.html#improper-integrals"><i class="fa fa-check"></i><b>6.4.2</b> Improper integrals</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="integral-calculus.html"><a href="integral-calculus.html#monte-carlo-and-integration"><i class="fa fa-check"></i><b>6.5</b> Monte Carlo and integration</a></li>
<li class="chapter" data-level="6.6" data-path="integral-calculus.html"><a href="integral-calculus.html#multivariate-integration"><i class="fa fa-check"></i><b>6.6</b> Multivariate integration</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="integral-calculus.html"><a href="integral-calculus.html#more-complicated-bounds-of-integration"><i class="fa fa-check"></i><b>6.6.1</b> More complicated bounds of integration</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="integral-calculus.html"><a href="integral-calculus.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="sample-space-probability.html"><a href="sample-space-probability.html"><i class="fa fa-check"></i><b>7</b> Sample space and probability</a>
<ul>
<li class="chapter" data-level="" data-path="sample-space-probability.html"><a href="sample-space-probability.html#learning-objectives-6"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="sample-space-probability.html"><a href="sample-space-probability.html#supplemental-readings-6"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="7.1" data-path="sample-space-probability.html"><a href="sample-space-probability.html#model-of-probability"><i class="fa fa-check"></i><b>7.1</b> Model of probability</a></li>
<li class="chapter" data-level="7.2" data-path="sample-space-probability.html"><a href="sample-space-probability.html#sample-space"><i class="fa fa-check"></i><b>7.2</b> Sample space</a></li>
<li class="chapter" data-level="7.3" data-path="sample-space-probability.html"><a href="sample-space-probability.html#events"><i class="fa fa-check"></i><b>7.3</b> Events</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="sample-space-probability.html"><a href="sample-space-probability.html#event-operations"><i class="fa fa-check"></i><b>7.3.1</b> Event operations</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="sample-space-probability.html"><a href="sample-space-probability.html#probability-1"><i class="fa fa-check"></i><b>7.4</b> Probability</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="sample-space-probability.html"><a href="sample-space-probability.html#three-axioms"><i class="fa fa-check"></i><b>7.4.1</b> Three axioms</a></li>
<li class="chapter" data-level="7.4.2" data-path="sample-space-probability.html"><a href="sample-space-probability.html#basic-examples"><i class="fa fa-check"></i><b>7.4.2</b> Basic examples</a></li>
<li class="chapter" data-level="7.4.3" data-path="sample-space-probability.html"><a href="sample-space-probability.html#surprising-probability-facts"><i class="fa fa-check"></i><b>7.4.3</b> Surprising probability facts</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="sample-space-probability.html"><a href="sample-space-probability.html#conditional-probability"><i class="fa fa-check"></i><b>7.5</b> Conditional probability</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="sample-space-probability.html"><a href="sample-space-probability.html#examples-1"><i class="fa fa-check"></i><b>7.5.1</b> Examples</a></li>
<li class="chapter" data-level="7.5.2" data-path="sample-space-probability.html"><a href="sample-space-probability.html#difference-between-prab-and-prba"><i class="fa fa-check"></i><b>7.5.2</b> Difference between <span class="math inline">\(\Pr(A|B)\)</span> and <span class="math inline">\(\Pr(B|A)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="sample-space-probability.html"><a href="sample-space-probability.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.6</b> Law of total probability</a></li>
<li class="chapter" data-level="7.7" data-path="sample-space-probability.html"><a href="sample-space-probability.html#bayes-rule"><i class="fa fa-check"></i><b>7.7</b> Bayesâ Rule</a></li>
<li class="chapter" data-level="7.8" data-path="sample-space-probability.html"><a href="sample-space-probability.html#independence-of-probabilities"><i class="fa fa-check"></i><b>7.8</b> Independence of probabilities</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="sample-space-probability.html"><a href="sample-space-probability.html#rolling-a-4-sided-die"><i class="fa fa-check"></i><b>7.8.1</b> Rolling a 4-sided die</a></li>
<li class="chapter" data-level="7.8.2" data-path="sample-space-probability.html"><a href="sample-space-probability.html#independence-and-causal-inference"><i class="fa fa-check"></i><b>7.8.2</b> Independence and causal inference</a></li>
<li class="chapter" data-level="7.8.3" data-path="sample-space-probability.html"><a href="sample-space-probability.html#independence-of-a-collection-of-events"><i class="fa fa-check"></i><b>7.8.3</b> Independence of a collection of events</a></li>
<li class="chapter" data-level="7.8.4" data-path="sample-space-probability.html"><a href="sample-space-probability.html#independent-trials-and-the-binomial-probabilities"><i class="fa fa-check"></i><b>7.8.4</b> Independent trials and the binomial probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="sample-space-probability.html"><a href="sample-space-probability.html#counting"><i class="fa fa-check"></i><b>7.9</b> Counting</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="sample-space-probability.html"><a href="sample-space-probability.html#counting-principle"><i class="fa fa-check"></i><b>7.9.1</b> Counting principle</a></li>
<li class="chapter" data-level="7.9.2" data-path="sample-space-probability.html"><a href="sample-space-probability.html#permutations"><i class="fa fa-check"></i><b>7.9.2</b> Permutations</a></li>
<li class="chapter" data-level="7.9.3" data-path="sample-space-probability.html"><a href="sample-space-probability.html#combinations"><i class="fa fa-check"></i><b>7.9.3</b> Combinations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html"><i class="fa fa-check"></i><b>8</b> Discrete random variables</a>
<ul>
<li class="chapter" data-level="" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#learning-objectives-7"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#supplemental-readings-7"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="8.1" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#random-variable"><i class="fa fa-check"></i><b>8.1</b> Random variable</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#discrete-random-variables-1"><i class="fa fa-check"></i><b>8.1.1</b> Discrete random variables</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#probability-mass-functions"><i class="fa fa-check"></i><b>8.2</b> Probability mass functions</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#intuition-1"><i class="fa fa-check"></i><b>8.2.1</b> Intuition</a></li>
<li class="chapter" data-level="8.2.2" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#definition-3"><i class="fa fa-check"></i><b>8.2.2</b> Definition</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#cumulative-mass-function"><i class="fa fa-check"></i><b>8.3</b> Cumulative mass function</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#three-person-experiment"><i class="fa fa-check"></i><b>8.3.1</b> Three person experiment</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#famous-discrete-random-variables"><i class="fa fa-check"></i><b>8.4</b> Famous discrete random variables</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#bernoulli"><i class="fa fa-check"></i><b>8.4.1</b> Bernoulli</a></li>
<li class="chapter" data-level="8.4.2" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#binomial"><i class="fa fa-check"></i><b>8.4.2</b> Binomial</a></li>
<li class="chapter" data-level="8.4.3" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#geometric"><i class="fa fa-check"></i><b>8.4.3</b> Geometric</a></li>
<li class="chapter" data-level="8.4.4" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#poisson"><i class="fa fa-check"></i><b>8.4.4</b> Poisson</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#functions-of-random-variables"><i class="fa fa-check"></i><b>8.5</b> Functions of random variables</a></li>
<li class="chapter" data-level="8.6" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#expectation-mean-and-variance"><i class="fa fa-check"></i><b>8.6</b> Expectation, mean, and variance</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#motivation"><i class="fa fa-check"></i><b>8.6.1</b> Motivation</a></li>
<li class="chapter" data-level="8.6.2" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#expectation"><i class="fa fa-check"></i><b>8.6.2</b> Expectation</a></li>
<li class="chapter" data-level="8.6.3" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#variance-moments-and-the-expected-value-rule"><i class="fa fa-check"></i><b>8.6.3</b> Variance, moments, and the expected value rule</a></li>
<li class="chapter" data-level="8.6.4" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#practice-calculating-expectation-and-variance"><i class="fa fa-check"></i><b>8.6.4</b> Practice calculating expectation and variance</a></li>
<li class="chapter" data-level="8.6.5" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#decision-making-using-expected-values"><i class="fa fa-check"></i><b>8.6.5</b> Decision making using expected values</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#cumulative-mass-function-redux"><i class="fa fa-check"></i><b>8.7</b> Cumulative mass function, redux</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#common-cmfs"><i class="fa fa-check"></i><b>8.7.1</b> Common CMFs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="general-random-variables.html"><a href="general-random-variables.html"><i class="fa fa-check"></i><b>9</b> General random variables</a>
<ul>
<li class="chapter" data-level="" data-path="general-random-variables.html"><a href="general-random-variables.html#learning-objectives-8"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="general-random-variables.html"><a href="general-random-variables.html#supplemental-readings-8"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="9.1" data-path="general-random-variables.html"><a href="general-random-variables.html#continuous-random-variables"><i class="fa fa-check"></i><b>9.1</b> Continuous random variables</a></li>
<li class="chapter" data-level="9.2" data-path="general-random-variables.html"><a href="general-random-variables.html#probability-density-function"><i class="fa fa-check"></i><b>9.2</b> Probability density function</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="general-random-variables.html"><a href="general-random-variables.html#definition-4"><i class="fa fa-check"></i><b>9.2.1</b> Definition</a></li>
<li class="chapter" data-level="9.2.2" data-path="general-random-variables.html"><a href="general-random-variables.html#example-uniform-random-variable"><i class="fa fa-check"></i><b>9.2.2</b> Example: Uniform Random Variable</a></li>
<li class="chapter" data-level="9.2.3" data-path="general-random-variables.html"><a href="general-random-variables.html#expectation-continuous"><i class="fa fa-check"></i><b>9.2.3</b> Expectation</a></li>
<li class="chapter" data-level="9.2.4" data-path="general-random-variables.html"><a href="general-random-variables.html#exponential-random-variable"><i class="fa fa-check"></i><b>9.2.4</b> Exponential random variable</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="general-random-variables.html"><a href="general-random-variables.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>9.3</b> Cumulative distribution function</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="general-random-variables.html"><a href="general-random-variables.html#properties-of-cdfs"><i class="fa fa-check"></i><b>9.3.1</b> Properties of CDFs</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="general-random-variables.html"><a href="general-random-variables.html#normal-distribution"><i class="fa fa-check"></i><b>9.4</b> Normal distribution</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="general-random-variables.html"><a href="general-random-variables.html#expected-valuevariance-of-normal-distribution"><i class="fa fa-check"></i><b>9.4.1</b> Expected value/variance of normal distribution</a></li>
<li class="chapter" data-level="9.4.2" data-path="general-random-variables.html"><a href="general-random-variables.html#why-rely-on-the-standard-normal-distribution"><i class="fa fa-check"></i><b>9.4.2</b> Why rely on the standard normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="general-random-variables.html"><a href="general-random-variables.html#gamma-distribution"><i class="fa fa-check"></i><b>9.5</b> Gamma distribution</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="general-random-variables.html"><a href="general-random-variables.html#properties-of-gamma-distributions"><i class="fa fa-check"></i><b>9.5.1</b> Properties of Gamma distributions</a></li>
<li class="chapter" data-level="9.5.2" data-path="general-random-variables.html"><a href="general-random-variables.html#importance-of-the-gamma-distribution"><i class="fa fa-check"></i><b>9.5.2</b> Importance of the Gamma distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="general-random-variables.html"><a href="general-random-variables.html#chi2-distribution"><i class="fa fa-check"></i><b>9.6</b> <span class="math inline">\(\chi^2\)</span> distribution</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="general-random-variables.html"><a href="general-random-variables.html#chi2-properties"><i class="fa fa-check"></i><b>9.6.1</b> <span class="math inline">\(\chi^2\)</span> properties</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="general-random-variables.html"><a href="general-random-variables.html#students-t-distribution"><i class="fa fa-check"></i><b>9.7</b> Studentâs <span class="math inline">\(t\)</span> distribution</a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="general-random-variables.html"><a href="general-random-variables.html#history-of-students-t"><i class="fa fa-check"></i><b>9.7.1</b> History of Studentâs <span class="math inline">\(t\)</span></a></li>
<li class="chapter" data-level="9.7.2" data-path="general-random-variables.html"><a href="general-random-variables.html#differences-from-the-normal-distribution"><i class="fa fa-check"></i><b>9.7.2</b> Differences from the Normal Distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="multivar-distribs.html"><a href="multivar-distribs.html"><i class="fa fa-check"></i><b>10</b> Multivariate distributions</a>
<ul>
<li class="chapter" data-level="" data-path="multivar-distribs.html"><a href="multivar-distribs.html#learning-objectives-9"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="multivar-distribs.html"><a href="multivar-distribs.html#supplemental-readings-9"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="10.1" data-path="multivar-distribs.html"><a href="multivar-distribs.html#multivariate-distribution"><i class="fa fa-check"></i><b>10.1</b> Multivariate distribution</a></li>
<li class="chapter" data-level="10.2" data-path="multivar-distribs.html"><a href="multivar-distribs.html#examples-of-joint-pdfs"><i class="fa fa-check"></i><b>10.2</b> Examples of joint PDFs</a></li>
<li class="chapter" data-level="10.3" data-path="multivar-distribs.html"><a href="multivar-distribs.html#multivariate-cumulative-density-function"><i class="fa fa-check"></i><b>10.3</b> Multivariate cumulative density function</a></li>
<li class="chapter" data-level="10.4" data-path="multivar-distribs.html"><a href="multivar-distribs.html#marginalization"><i class="fa fa-check"></i><b>10.4</b> Marginalization</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="multivar-distribs.html"><a href="multivar-distribs.html#joint-vs.-conditional-pdf"><i class="fa fa-check"></i><b>10.4.1</b> Joint vs.Â conditional PDF</a></li>
<li class="chapter" data-level="10.4.2" data-path="multivar-distribs.html"><a href="multivar-distribs.html#why-does-marginalization-work"><i class="fa fa-check"></i><b>10.4.2</b> Why does marginalization work?</a></li>
<li class="chapter" data-level="10.4.3" data-path="multivar-distribs.html"><a href="multivar-distribs.html#move-to-the-continuous-case"><i class="fa fa-check"></i><b>10.4.3</b> Move to the continuous case</a></li>
<li class="chapter" data-level="10.4.4" data-path="multivar-distribs.html"><a href="multivar-distribs.html#a-simple-example"><i class="fa fa-check"></i><b>10.4.4</b> A (simple) example</a></li>
<li class="chapter" data-level="10.4.5" data-path="multivar-distribs.html"><a href="multivar-distribs.html#more-complex-example"><i class="fa fa-check"></i><b>10.4.5</b> More complex example</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="multivar-distribs.html"><a href="multivar-distribs.html#conditional-distribution"><i class="fa fa-check"></i><b>10.5</b> Conditional distribution</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="multivar-distribs.html"><a href="multivar-distribs.html#a-simple-example-of-dependence"><i class="fa fa-check"></i><b>10.5.1</b> A (simple) example of dependence</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="multivar-distribs.html"><a href="multivar-distribs.html#expectation-1"><i class="fa fa-check"></i><b>10.6</b> Expectation</a></li>
<li class="chapter" data-level="10.7" data-path="multivar-distribs.html"><a href="multivar-distribs.html#covariance-and-correlation"><i class="fa fa-check"></i><b>10.7</b> Covariance and correlation</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="multivar-distribs.html"><a href="multivar-distribs.html#some-observations"><i class="fa fa-check"></i><b>10.7.1</b> Some observations</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="multivar-distribs.html"><a href="multivar-distribs.html#sums-of-random-variables"><i class="fa fa-check"></i><b>10.8</b> Sums of random variables</a></li>
<li class="chapter" data-level="10.9" data-path="multivar-distribs.html"><a href="multivar-distribs.html#multivariate-normal-distribution-1"><i class="fa fa-check"></i><b>10.9</b> Multivariate normal distribution</a>
<ul>
<li class="chapter" data-level="10.9.1" data-path="multivar-distribs.html"><a href="multivar-distribs.html#bivariate-example"><i class="fa fa-check"></i><b>10.9.1</b> Bivariate example</a></li>
<li class="chapter" data-level="10.9.2" data-path="multivar-distribs.html"><a href="multivar-distribs.html#properties-of-the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>10.9.2</b> Properties of the multivariate normal distribution</a></li>
<li class="chapter" data-level="10.9.3" data-path="multivar-distribs.html"><a href="multivar-distribs.html#independence-and-multivariate-normal"><i class="fa fa-check"></i><b>10.9.3</b> Independence and multivariate normal</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#limits"><i class="fa fa-check"></i><b>11</b> Properties of random variables and limit theorems</a>
<ul>
<li class="chapter" data-level="" data-path="limits.html"><a href="limits.html"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="limits.html"><a href="limits.html#supplemental-readings-10"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="11.1" data-path="limits.html"><a href="limits.html#iterated-expectations"><i class="fa fa-check"></i><b>11.1</b> Iterated Expectations</a></li>
<li class="chapter" data-level="11.2" data-path="limits.html"><a href="limits.html#change-of-coordinates"><i class="fa fa-check"></i><b>11.2</b> Change of coordinates</a></li>
<li class="chapter" data-level="11.3" data-path="limits.html"><a href="limits.html#moment-generating-functions"><i class="fa fa-check"></i><b>11.3</b> Moment generating functions</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="limits.html"><a href="limits.html#the-moments-of-the-normal-distribution"><i class="fa fa-check"></i><b>11.3.1</b> The moments of the normal distribution</a></li>
<li class="chapter" data-level="11.3.2" data-path="limits.html"><a href="limits.html#extracting-moments-of-the-normal-distribution"><i class="fa fa-check"></i><b>11.3.2</b> Extracting moments of the normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="limits.html"><a href="limits.html#sequences-of-independent-random-variables"><i class="fa fa-check"></i><b>11.4</b> Sequences of independent random variables</a></li>
<li class="chapter" data-level="11.5" data-path="limits.html"><a href="limits.html#inequalities-and-limit-theorems"><i class="fa fa-check"></i><b>11.5</b> Inequalities and limit theorems</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="limits.html"><a href="limits.html#limit-theorems"><i class="fa fa-check"></i><b>11.5.1</b> Limit theorems</a></li>
<li class="chapter" data-level="11.5.2" data-path="limits.html"><a href="limits.html#weak-law-of-large-numbers"><i class="fa fa-check"></i><b>11.5.2</b> Weak law of large numbers</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="limits.html"><a href="limits.html#sequence-of-random-variables"><i class="fa fa-check"></i><b>11.6</b> Sequence of random variables</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="limits.html"><a href="limits.html#meanvariance-of-sample-mean"><i class="fa fa-check"></i><b>11.6.1</b> Mean/variance of sample mean</a></li>
<li class="chapter" data-level="11.6.2" data-path="limits.html"><a href="limits.html#weak-law-of-large-numbers-1"><i class="fa fa-check"></i><b>11.6.2</b> Weak law of large numbers</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="limits.html"><a href="limits.html#sequences-and-convergence"><i class="fa fa-check"></i><b>11.7</b> Sequences and convergence</a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="limits.html"><a href="limits.html#convergence-definitions"><i class="fa fa-check"></i><b>11.7.1</b> Convergence definitions</a></li>
<li class="chapter" data-level="11.7.2" data-path="limits.html"><a href="limits.html#convergence-in-probability"><i class="fa fa-check"></i><b>11.7.2</b> Convergence in probability</a></li>
<li class="chapter" data-level="11.7.3" data-path="limits.html"><a href="limits.html#almost-sure-convergence"><i class="fa fa-check"></i><b>11.7.3</b> Almost sure convergence</a></li>
<li class="chapter" data-level="11.7.4" data-path="limits.html"><a href="limits.html#convergence-in-distribution"><i class="fa fa-check"></i><b>11.7.4</b> Convergence in distribution</a></li>
<li class="chapter" data-level="11.7.5" data-path="limits.html"><a href="limits.html#convergence-in-distribution-not-rightarrow-convergence-in-probability"><i class="fa fa-check"></i><b>11.7.5</b> Convergence in distribution <span class="math inline">\(\not \Rightarrow\)</span> convergence in probability</a></li>
<li class="chapter" data-level="11.7.6" data-path="limits.html"><a href="limits.html#central-limit-theorem"><i class="fa fa-check"></i><b>11.7.6</b> Central limit theorem</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="classic-inference.html"><a href="classic-inference.html"><i class="fa fa-check"></i><b>12</b> Classical statistical inference</a>
<ul>
<li class="chapter" data-level="" data-path="classic-inference.html"><a href="classic-inference.html#learning-objectives-11"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="classic-inference.html"><a href="classic-inference.html#supplemental-readings-11"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="12.1" data-path="classic-inference.html"><a href="classic-inference.html#statistical-inference"><i class="fa fa-check"></i><b>12.1</b> Statistical inference</a></li>
<li class="chapter" data-level="12.2" data-path="classic-inference.html"><a href="classic-inference.html#parametric-models"><i class="fa fa-check"></i><b>12.2</b> Parametric models</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="classic-inference.html"><a href="classic-inference.html#examples-of-parametric-models"><i class="fa fa-check"></i><b>12.2.1</b> Examples of parametric models</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="classic-inference.html"><a href="classic-inference.html#point-estimates"><i class="fa fa-check"></i><b>12.3</b> Point estimates</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="classic-inference.html"><a href="classic-inference.html#properties-of-point-estimates"><i class="fa fa-check"></i><b>12.3.1</b> Properties of point estimates</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="classic-inference.html"><a href="classic-inference.html#confidence-sets"><i class="fa fa-check"></i><b>12.4</b> Confidence sets</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="classic-inference.html"><a href="classic-inference.html#caution-interpreting-confidence-intervals"><i class="fa fa-check"></i><b>12.4.1</b> Caution interpreting confidence intervals</a></li>
<li class="chapter" data-level="12.4.2" data-path="classic-inference.html"><a href="classic-inference.html#constructing-confidence-intervals"><i class="fa fa-check"></i><b>12.4.2</b> Constructing confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="classic-inference.html"><a href="classic-inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>12.5</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="classic-inference.html"><a href="classic-inference.html#types-of-errors"><i class="fa fa-check"></i><b>12.5.1</b> Types of errors</a></li>
<li class="chapter" data-level="12.5.2" data-path="classic-inference.html"><a href="classic-inference.html#power-function"><i class="fa fa-check"></i><b>12.5.2</b> Power function</a></li>
<li class="chapter" data-level="12.5.3" data-path="classic-inference.html"><a href="classic-inference.html#sided-tests"><i class="fa fa-check"></i><b>12.5.3</b> Sided tests</a></li>
<li class="chapter" data-level="12.5.4" data-path="classic-inference.html"><a href="classic-inference.html#example-hypothesis-test"><i class="fa fa-check"></i><b>12.5.4</b> Example hypothesis test</a></li>
<li class="chapter" data-level="12.5.5" data-path="classic-inference.html"><a href="classic-inference.html#wald-test"><i class="fa fa-check"></i><b>12.5.5</b> Wald test</a></li>
<li class="chapter" data-level="12.5.6" data-path="classic-inference.html"><a href="classic-inference.html#wald-or-t-test"><i class="fa fa-check"></i><b>12.5.6</b> Wald or <span class="math inline">\(t\)</span>-test?</a></li>
<li class="chapter" data-level="12.5.7" data-path="classic-inference.html"><a href="classic-inference.html#relationship-to-confidence-intervals"><i class="fa fa-check"></i><b>12.5.7</b> Relationship to confidence intervals</a></li>
<li class="chapter" data-level="12.5.8" data-path="classic-inference.html"><a href="classic-inference.html#statistical-vs.-scientific-significance"><i class="fa fa-check"></i><b>12.5.8</b> Statistical vs.Â scientific significance</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="classic-inference.html"><a href="classic-inference.html#p-values"><i class="fa fa-check"></i><b>12.6</b> <span class="math inline">\(p\)</span>-values</a>
<ul>
<li class="chapter" data-level="12.6.1" data-path="classic-inference.html"><a href="classic-inference.html#interpreting-p-values"><i class="fa fa-check"></i><b>12.6.1</b> Interpreting <span class="math inline">\(p\)</span>-values</a></li>
<li class="chapter" data-level="12.6.2" data-path="classic-inference.html"><a href="classic-inference.html#calculating-p-values"><i class="fa fa-check"></i><b>12.6.2</b> Calculating <span class="math inline">\(p\)</span>-values</a></li>
<li class="chapter" data-level="12.6.3" data-path="classic-inference.html"><a href="classic-inference.html#pearsons-chi2-test-for-multinomial-data"><i class="fa fa-check"></i><b>12.6.3</b> Pearsonâs <span class="math inline">\(\chi^2\)</span> test for multinomial data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="mle-ols.html"><a href="mle-ols.html"><i class="fa fa-check"></i><b>13</b> Maximum likelihood estimation and linear regression</a>
<ul>
<li class="chapter" data-level="" data-path="mle-ols.html"><a href="mle-ols.html#learning-objectives-12"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="mle-ols.html"><a href="mle-ols.html#supplemental-readings-12"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="13.1" data-path="mle-ols.html"><a href="mle-ols.html#maximum-likelihood"><i class="fa fa-check"></i><b>13.1</b> Maximum likelihood</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="mle-ols.html"><a href="mle-ols.html#properties-of-maximum-likelihood-estimators"><i class="fa fa-check"></i><b>13.1.1</b> Properties of maximum likelihood estimators</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="mle-ols.html"><a href="mle-ols.html#least-squares-regression"><i class="fa fa-check"></i><b>13.2</b> Least squares regression</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="mle-ols.html"><a href="mle-ols.html#simple-linear-regression"><i class="fa fa-check"></i><b>13.2.1</b> Simple linear regression</a></li>
<li class="chapter" data-level="13.2.2" data-path="mle-ols.html"><a href="mle-ols.html#estimation-strategy"><i class="fa fa-check"></i><b>13.2.2</b> Estimation strategy</a></li>
<li class="chapter" data-level="13.2.3" data-path="mle-ols.html"><a href="mle-ols.html#least-squares-estimator"><i class="fa fa-check"></i><b>13.2.3</b> Least squares estimator</a></li>
<li class="chapter" data-level="13.2.4" data-path="mle-ols.html"><a href="mle-ols.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>13.2.4</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="13.2.5" data-path="mle-ols.html"><a href="mle-ols.html#properties-of-the-least-squares-estimator"><i class="fa fa-check"></i><b>13.2.5</b> Properties of the least squares estimator</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="mle-ols.html"><a href="mle-ols.html#assumptions-of-linear-regression-models"><i class="fa fa-check"></i><b>13.3</b> Assumptions of linear regression models</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="mle-ols.html"><a href="mle-ols.html#linearity"><i class="fa fa-check"></i><b>13.3.1</b> Linearity</a></li>
<li class="chapter" data-level="13.3.2" data-path="mle-ols.html"><a href="mle-ols.html#constant-variance"><i class="fa fa-check"></i><b>13.3.2</b> Constant variance</a></li>
<li class="chapter" data-level="13.3.3" data-path="mle-ols.html"><a href="mle-ols.html#normality"><i class="fa fa-check"></i><b>13.3.3</b> Normality</a></li>
<li class="chapter" data-level="13.3.4" data-path="mle-ols.html"><a href="mle-ols.html#independence"><i class="fa fa-check"></i><b>13.3.4</b> Independence</a></li>
<li class="chapter" data-level="13.3.5" data-path="mle-ols.html"><a href="mle-ols.html#fixed-x-or-x-measured-without-error-and-independent-of-the-error"><i class="fa fa-check"></i><b>13.3.5</b> Fixed <span class="math inline">\(X\)</span>, or <span class="math inline">\(X\)</span> measured without error and independent of the error</a></li>
<li class="chapter" data-level="13.3.6" data-path="mle-ols.html"><a href="mle-ols.html#x-is-not-invariant"><i class="fa fa-check"></i><b>13.3.6</b> <span class="math inline">\(X\)</span> is not invariant</a></li>
<li class="chapter" data-level="13.3.7" data-path="mle-ols.html"><a href="mle-ols.html#handling-violations-of-assumptions"><i class="fa fa-check"></i><b>13.3.7</b> Handling violations of assumptions</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="mle-ols.html"><a href="mle-ols.html#unusual-and-influential-data"><i class="fa fa-check"></i><b>13.4</b> Unusual and influential data</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="mle-ols.html"><a href="mle-ols.html#terms"><i class="fa fa-check"></i><b>13.4.1</b> Terms</a></li>
<li class="chapter" data-level="13.4.2" data-path="mle-ols.html"><a href="mle-ols.html#measuring-leverage"><i class="fa fa-check"></i><b>13.4.2</b> Measuring leverage</a></li>
<li class="chapter" data-level="13.4.3" data-path="mle-ols.html"><a href="mle-ols.html#measuring-discrepancy"><i class="fa fa-check"></i><b>13.4.3</b> Measuring discrepancy</a></li>
<li class="chapter" data-level="13.4.4" data-path="mle-ols.html"><a href="mle-ols.html#measuring-influence"><i class="fa fa-check"></i><b>13.4.4</b> Measuring influence</a></li>
<li class="chapter" data-level="13.4.5" data-path="mle-ols.html"><a href="mle-ols.html#visualizing-leverage-discrepancy-and-influence"><i class="fa fa-check"></i><b>13.4.5</b> Visualizing leverage, discrepancy, and influence</a></li>
<li class="chapter" data-level="13.4.6" data-path="mle-ols.html"><a href="mle-ols.html#numerical-rules-of-thumb"><i class="fa fa-check"></i><b>13.4.6</b> Numerical rules of thumb</a></li>
<li class="chapter" data-level="13.4.7" data-path="mle-ols.html"><a href="mle-ols.html#how-to-treat-unusual-observations"><i class="fa fa-check"></i><b>13.4.7</b> How to treat unusual observations</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="mle-ols.html"><a href="mle-ols.html#non-normally-distributed-errors"><i class="fa fa-check"></i><b>13.5</b> Non-normally distributed errors</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="mle-ols.html"><a href="mle-ols.html#detecting-non-normally-distributed-errors"><i class="fa fa-check"></i><b>13.5.1</b> Detecting non-normally distributed errors</a></li>
<li class="chapter" data-level="13.5.2" data-path="mle-ols.html"><a href="mle-ols.html#fixing-non-normally-distributed-errors"><i class="fa fa-check"></i><b>13.5.2</b> Fixing non-normally distributed errors</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="mle-ols.html"><a href="mle-ols.html#non-constant-error-variance"><i class="fa fa-check"></i><b>13.6</b> Non-constant error variance</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="mle-ols.html"><a href="mle-ols.html#detecting-heteroscedasticity"><i class="fa fa-check"></i><b>13.6.1</b> Detecting heteroscedasticity</a></li>
<li class="chapter" data-level="13.6.2" data-path="mle-ols.html"><a href="mle-ols.html#accounting-for-heteroscedasticity"><i class="fa fa-check"></i><b>13.6.2</b> Accounting for heteroscedasticity</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="mle-ols.html"><a href="mle-ols.html#non-linearity-in-the-data"><i class="fa fa-check"></i><b>13.7</b> Non-linearity in the data</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="mle-ols.html"><a href="mle-ols.html#partial-residual-plots"><i class="fa fa-check"></i><b>13.7.1</b> Partial residual plots</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="mle-ols.html"><a href="mle-ols.html#collinearity"><i class="fa fa-check"></i><b>13.8</b> Collinearity</a>
<ul>
<li class="chapter" data-level="13.8.1" data-path="mle-ols.html"><a href="mle-ols.html#perfect-collinearity"><i class="fa fa-check"></i><b>13.8.1</b> Perfect collinearity</a></li>
<li class="chapter" data-level="13.8.2" data-path="mle-ols.html"><a href="mle-ols.html#less-than-perfect-collinearity"><i class="fa fa-check"></i><b>13.8.2</b> Less-than-perfect collinearity</a></li>
<li class="chapter" data-level="13.8.3" data-path="mle-ols.html"><a href="mle-ols.html#fixing-multicollinearity"><i class="fa fa-check"></i><b>13.8.3</b> Fixing multicollinearity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>14</b> Bayesian inference</a>
<ul>
<li class="chapter" data-level="" data-path="bayesian-inference.html"><a href="bayesian-inference.html#learning-objectives-13"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="bayesian-inference.html"><a href="bayesian-inference.html#supplemental-readings-13"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="14.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bayesian-philosophy"><i class="fa fa-check"></i><b>14.1</b> Bayesian philosophy</a></li>
<li class="chapter" data-level="14.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bayes-theorem"><i class="fa fa-check"></i><b>14.2</b> Bayesâ theorem</a></li>
<li class="chapter" data-level="14.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bayesian-method"><i class="fa fa-check"></i><b>14.3</b> Bayesian method</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#example-coin-tossing"><i class="fa fa-check"></i><b>14.3.1</b> Example: coin tossing</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#updating-your-prior-beliefs"><i class="fa fa-check"></i><b>14.4</b> Updating your prior beliefs</a></li>
<li class="chapter" data-level="14.5" data-path="bayesian-inference.html"><a href="bayesian-inference.html#simulation"><i class="fa fa-check"></i><b>14.5</b> Simulation</a></li>
<li class="chapter" data-level="14.6" data-path="bayesian-inference.html"><a href="bayesian-inference.html#priors"><i class="fa fa-check"></i><b>14.6</b> Priors</a>
<ul>
<li class="chapter" data-level="14.6.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#improper-priors"><i class="fa fa-check"></i><b>14.6.1</b> Improper priors</a></li>
<li class="chapter" data-level="14.6.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#flat-priors-are-not-invariant"><i class="fa fa-check"></i><b>14.6.2</b> Flat priors are not invariant</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="bayesian-inference.html"><a href="bayesian-inference.html#multiparameter-problems"><i class="fa fa-check"></i><b>14.7</b> Multiparameter problems</a></li>
<li class="chapter" data-level="14.8" data-path="bayesian-inference.html"><a href="bayesian-inference.html#critiques-and-defenses-of-bayesian-inference"><i class="fa fa-check"></i><b>14.8</b> Critiques and defenses of Bayesian inference</a>
<ul>
<li class="chapter" data-level="14.8.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#critique-of-bayesian-inference"><i class="fa fa-check"></i><b>14.8.1</b> Critique of Bayesian inference</a></li>
<li class="chapter" data-level="14.8.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#defense-of-bayesian-inference"><i class="fa fa-check"></i><b>14.8.2</b> Defense of Bayesian inference</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bayesian-inference.html"><a href="bayesian-inference.html#acknowledgements-1"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Math Camp</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-algebra" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Lecture 4</span> Linear algebra<a href="linear-algebra.html#linear-algebra" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="learning-objectives-3" class="section level2 unnumbered hasAnchor">
<h2>Learning objectives<a href="linear-algebra.html#learning-objectives-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Define vector and matrix</li>
<li>Visualize vectors in multiple dimensions</li>
<li>Demonstrate applicability of linear algebra to text analysis and cosine similarity</li>
<li>Perform basic algebraic operations on vectors and matricies</li>
<li>Generalize linear algebra to tensors and neural networks</li>
<li>Define matrix inversion</li>
<li>Demonstrate how to solve systems of linear equations using matrix inversion</li>
<li>Define the determinant of a matrix</li>
<li>Define matrix decomposition</li>
<li>Explain singular value decomposition and demonstrate the applicability of matrix algebra to real-world problems</li>
</ul>
</div>
<div id="supplemental-readings-3" class="section level2 unnumbered hasAnchor">
<h2>Supplemental readings<a href="linear-algebra.html#supplemental-readings-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Chapters 11-12, 13.1-.3, <span class="citation">Pemberton and Rau (<a href="#ref-pemberton2015" role="doc-biblioref">2011</a>)</span></li>
<li><a href="https://openstax.org/details/books/calculus-volume-3">OpenStax Calculus: Volume 3, ch 2</a></li>
<li><a href="https://openstax.org/details/books/college-algebra">OpenStax College Algebra, ch 7.5-.8</a></li>
</ul>
</div>
<div id="linear-algebra-1" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Linear algebra<a href="linear-algebra.html#linear-algebra-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A <strong>matrix</strong> is a rectangular array of numbers arranged in rows and columns.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:neo"></span>
<img src="images/neo.jpeg" alt="Not that matrix." width="90%" />
<p class="caption">
Figure 4.1: Not that matrix.
</p>
</div>
<p>Many common statistical methods in the social sciences rely on data structured as matricies (e.g.Â ordinary least squares regression). As computational social science expands and data sources explode in complexity and scope, big data needs to be stored in processed in many higher dimensional spaces.</p>
<p><strong>Linear algebra</strong> is the algebra of matricies. It allows us to examine the geometry of high dimensional space, and expand calculus into functions with multiple variables. It is very important for regression/machine learning/deep learning methods you will encounter in your coursework and research.</p>
</div>
<div id="points-and-vectors" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Points and vectors<a href="linear-algebra.html#points-and-vectors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="points" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Points<a href="linear-algebra.html#points" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>A point exists in a single dimension (in <span class="math inline">\(\Re^1\)</span>)
<ul>
<li><span class="math inline">\(1\)</span></li>
<li><span class="math inline">\(\pi\)</span></li>
<li><span class="math inline">\(e\)</span></li>
</ul></li>
<li>An ordered pair exists in two dimensions (<span class="math inline">\(\Re^2 = \Re \times \Re\)</span>)
<ul>
<li><span class="math inline">\((1,2)\)</span></li>
<li><span class="math inline">\((0,0)\)</span></li>
<li><span class="math inline">\((\pi, e)\)</span></li>
</ul></li>
<li>An ordered triple in three dimensions (<span class="math inline">\(\Re^3 = \Re \times \Re \times \Re\)</span>)
<ul>
<li><span class="math inline">\((3.1, 4.5, 6.1132)\)</span></li>
</ul></li>
<li>An ordered <span class="math inline">\(n\)</span>-tuple in <span class="math inline">\(n\)</span>-dimensions <span class="math inline">\(R^n = \Re \times \Re \times \ldots \times \Re\)</span>
<ul>
<li><span class="math inline">\((a_{1}, a_{2}, \ldots, a_{n})\)</span></li>
</ul></li>
</ul>
</div>
<div id="vectors" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Vectors<a href="linear-algebra.html#vectors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A point <span class="math inline">\(\mathbf{x} \in \Re^{n}\)</span> is an ordered n-tuple, <span class="math inline">\((x_{1}, x_{2}, \ldots, x_{n})\)</span>. The <strong>vector</strong> <span class="math inline">\(\mathbf{x} \in \Re^{n}\)</span> is the arrow pointing from the origin <span class="math inline">\((0, 0, \ldots, 0)\)</span> to <span class="math inline">\(\mathbf{x}\)</span>.</p>
</div>
<div id="one-dimensional-example" class="section level3 hasAnchor" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> One dimensional example<a href="linear-algebra.html#one-dimensional-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><img src="04-linear-algebra_files/figure-html/one-d-1.png" width="90%" style="display: block; margin: auto;" /><img src="04-linear-algebra_files/figure-html/one-d-2.png" width="90%" style="display: block; margin: auto;" /><img src="04-linear-algebra_files/figure-html/one-d-3.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="two-dimensional-example" class="section level3 hasAnchor" number="4.2.4">
<h3><span class="header-section-number">4.2.4</span> Two dimensional example<a href="linear-algebra.html#two-dimensional-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><img src="04-linear-algebra_files/figure-html/two-d-1.png" width="90%" style="display: block; margin: auto;" /><img src="04-linear-algebra_files/figure-html/two-d-2.png" width="90%" style="display: block; margin: auto;" /><img src="04-linear-algebra_files/figure-html/two-d-3.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="three-dimensional-example" class="section level3 hasAnchor" number="4.2.5">
<h3><span class="header-section-number">4.2.5</span> Three dimensional example<a href="linear-algebra.html#three-dimensional-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>(Latitude, Longitude, Elevation)</li>
<li><span class="math inline">\((1,2,3)\)</span></li>
<li><span class="math inline">\((0,1,2)\)</span></li>
</ul>
</div>
<div id="n-dimensional-example" class="section level3 hasAnchor" number="4.2.6">
<h3><span class="header-section-number">4.2.6</span> <span class="math inline">\(N\)</span>-dimensional example<a href="linear-algebra.html#n-dimensional-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Individual campaign donation records
<span class="math display">\[\mathbf{x} = (1000, 0, 10, 50, 15, 4, 0, 0, 0, \ldots, 2400000000)\]</span></li>
<li>U.S. countiesâ proportion of vote for Donald Trump
<span class="math display">\[\mathbf{y} = (0.8, 0.5, 0.6, \ldots, 0.2)\]</span></li>
<li>Run experiment, assess feeling thermometer of elected official
<span class="math display">\[\mathbf{t} = (0, 100, 50, 70, 80, \ldots, 100)\]</span></li>
</ul>
</div>
<div id="examples-of-some-basic-arithmetic" class="section level3 hasAnchor" number="4.2.7">
<h3><span class="header-section-number">4.2.7</span> Examples of some basic arithmetic<a href="linear-algebra.html#examples-of-some-basic-arithmetic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="vectorscalar-additionmultiplication" class="section level4 hasAnchor" number="4.2.7.1">
<h4><span class="header-section-number">4.2.7.1</span> Vector/scalar addition/multiplication<a href="linear-algebra.html#vectorscalar-additionmultiplication" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Suppose:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{u} &amp; =  (1, 2, 3, 4, 5)  \\
\mathbf{v} &amp; =  (1, 1, 1, 1, 1)  \\
k &amp; =  2
\end{aligned}
\]</span></p>
<p>Then,</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{u}  + \mathbf{v} &amp; = (1 + 1, 2 + 1, 3+ 1, 4 + 1, 5+ 1)  = (2, 3, 4, 5, 6) \\
k \mathbf{u} &amp; = (2 \times 1, 2 \times 2, 2 \times 3, 2 \times 4, 2 \times 5) = (2, 4, 6, 8, 10)  \\
k \mathbf{v} &amp; = (2 \times 1,2 \times 1,2 \times 1,2 \times 1,2 \times 1) = (2, 2, 2, 2, 2)
\end{aligned}
\]</span></p>
</div>
</div>
<div id="linear-dependence" class="section level3 hasAnchor" number="4.2.8">
<h3><span class="header-section-number">4.2.8</span> Linear dependence<a href="linear-algebra.html#linear-dependence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Expressions such as <span class="math inline">\(\mathbf{a} + \mathbf{b}\)</span> and <span class="math inline">\(2\mathbf{a} - 3\mathbf{b}\)</span> are examples of <strong>linear combinations</strong> of vectors <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span>. Generally, a linear combination of two vectors <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span> is a vector of the form <span class="math inline">\(\alpha \mathbf{a} + \beta\mathbf{b}\)</span>, where <span class="math inline">\(\alpha, \beta\)</span> are scalars. This form extends to linear combinations of more than two vectors</p>
<p><span class="math display">\[\alpha \mathbf{a} + \beta\mathbf{b} + \gamma\mathbf{c} + \delta\mathbf{d} + \ldots\]</span></p>
<p>Suppose we have a set of <span class="math inline">\(k\)</span> <span class="math inline">\(n\)</span>-vectors <span class="math inline">\(\mathbf{b}^1, \mathbf{b}^2, \ldots \mathbf{b}^k\)</span>. We say that the vectors <span class="math inline">\(\mathbf{b}^1, \mathbf{b}^2, \ldots \mathbf{b}^k\)</span> are <strong>linearly independent</strong> if they are not linearly dependent; thus, <span class="math inline">\(\mathbf{b}^1, \mathbf{b}^2, \ldots \mathbf{b}^k\)</span> are linearly independent if none of the vectors can be expressed as a linear combination of the others.</p>
<div id="example-of-linear-dependence" class="section level4 hasAnchor" number="4.2.8.1">
<h4><span class="header-section-number">4.2.8.1</span> Example of linear dependence<a href="linear-algebra.html#example-of-linear-dependence" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[\mathbf{a} = \begin{bmatrix}
3 \\
1
\end{bmatrix}, \quad \mathbf{b} = \begin{bmatrix}
2 \\
2
\end{bmatrix}, \quad \mathbf{c} = \begin{bmatrix}
1 \\
3
\end{bmatrix}\]</span></p>
<p>It is obvious that <span class="math inline">\(\mathbf{b} = \frac{1}{2} (\mathbf{a} + \mathbf{c})\)</span>. Therefore the vectors <span class="math inline">\(\mathbf{a}, \mathbf{b}, \mathbf{c}\)</span> are linearly dependent.</p>
</div>
<div id="detecting-linear-dependence" class="section level4 hasAnchor" number="4.2.8.2">
<h4><span class="header-section-number">4.2.8.2</span> Detecting linear dependence<a href="linear-algebra.html#detecting-linear-dependence" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A simple criterion for linear dependence is the following: <span class="math inline">\(\mathbf{b}^1, \mathbf{b}^2, \ldots \mathbf{b}^k\)</span> are linearly dependent if and only if there exist scalars <span class="math inline">\(\alpha_1, \alpha_2, \ldots, \alpha_k\)</span> <em>not all zero</em> such that</p>
<p><span class="math display">\[\alpha_1 \mathbf{b}^1 + \alpha_2 \mathbf{b}^2 + \ldots + \alpha_k \mathbf{b}^k = \mathbf{0}\]</span></p>
<div id="example-1" class="section level5 hasAnchor" number="4.2.8.2.1">
<h5><span class="header-section-number">4.2.8.2.1</span> Example 1<a href="linear-algebra.html#example-1" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Consider the vectors</p>
<p><span class="math display">\[\mathbf{a} = \begin{bmatrix}
2 \\
1 \\
2
\end{bmatrix}, \quad \mathbf{b} = \begin{bmatrix}
4 \\
1 \\
3
\end{bmatrix}, \quad \mathbf{c} = \begin{bmatrix}
1 \\
1 \\
2
\end{bmatrix}\]</span></p>
<p>Suppose there are scalars <span class="math inline">\(\alpha, \beta, \gamma\)</span> such that</p>
<p><span class="math display">\[\alpha\mathbf{a} + \beta\mathbf{b} + \gamma\mathbf{c} = \mathbf{0}\]</span></p>
<p>We can express this as a <strong>system of equations</strong>:</p>
<p><span class="math display">\[
\begin{aligned}
2\alpha &amp;+ 4\beta &amp;+ \gamma &amp;= 0 \\
\alpha &amp;+ \beta &amp;+ \gamma &amp;= 0 \\
2\alpha &amp;+ 3\beta &amp;+ 2\gamma &amp;= 0
\end{aligned}
\]</span></p>
<p>We can solve this system of equations by reducing it down until one equation has just a single term remaining. For instance, eliminate <span class="math inline">\(\alpha\)</span> from the second and third equations by subtracting from those equations suitable multiples of the first equation:</p>
<p><span class="math display">\[
\begin{aligned}
2\alpha &amp;+ 4\beta &amp;+ \gamma &amp;= 0 \\
&amp;- \beta &amp;+ \frac{1}{2}\gamma &amp;= 0 \\
&amp;- \beta &amp;+ \gamma &amp;= 0
\end{aligned}
\]</span></p>
<p>Next, eliminate <span class="math inline">\(\beta\)</span> from the third equation by subtracting a suitable multiple of the second:</p>
<p><span class="math display">\[
\begin{aligned}
2\alpha &amp;+ 4\beta &amp;+ \gamma &amp;= 0 \\
&amp;- \beta &amp;+ \frac{1}{2}\gamma &amp;= 0 \\
&amp; &amp;+ \frac{1}{2} \gamma &amp;= 0
\end{aligned}
\]</span></p>
<p>We can now use <strong>back-substitution</strong> to solve for <span class="math inline">\(\gamma\)</span>, then plug in that value to solve for <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\alpha\)</span>. Since the output of each equation is <span class="math inline">\(0\)</span>, this reduces down to <span class="math inline">\(\gamma = \beta = \alpha = 0\)</span>, so the vectors <span class="math inline">\(\mathbf{a}, \mathbf{b}, \mathbf{c}\)</span> are linearly independent.</p>
</div>
<div id="example-2" class="section level5 hasAnchor" number="4.2.8.2.2">
<h5><span class="header-section-number">4.2.8.2.2</span> Example 2<a href="linear-algebra.html#example-2" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Consider the vectors</p>
<p><span class="math display">\[\mathbf{a} = \begin{bmatrix}
2 \\
1 \\
2
\end{bmatrix}, \quad \mathbf{b} = \begin{bmatrix}
4 \\
1 \\
3
\end{bmatrix}, \quad \mathbf{c} = \begin{bmatrix}
2 \\
2 \\
3
\end{bmatrix}\]</span></p>
<p>Suppose there are scalars <span class="math inline">\(\alpha, \beta, \gamma\)</span> such that</p>
<p><span class="math display">\[\alpha\mathbf{a} + \beta\mathbf{b} + \gamma\mathbf{c} = \mathbf{0}\]</span></p>
<p>Express this as a system of equations:</p>
<p><span class="math display">\[
\begin{aligned}
2\alpha &amp;+ 4\beta &amp;+ 2\gamma &amp;= 0 \\
\alpha &amp;+ \beta &amp;+ 2\gamma &amp;= 0 \\
2\alpha &amp;+ 3\beta &amp;+ 3\gamma &amp;= 0
\end{aligned}
\]</span></p>
<p>Letâs try to eliminate <span class="math inline">\(\alpha\)</span> in the last two equations by subtracting multiples of the first equation:</p>
<p><span class="math display">\[
\begin{aligned}
2\alpha &amp;+ 4\beta &amp;+ 2\gamma &amp;= 0 \\
&amp;- \beta &amp;+ \gamma &amp;= 0 \\
&amp;- \beta &amp;+ \gamma &amp;= 0
\end{aligned}
\]</span></p>
<p>We can cancel out the third equation to be <span class="math inline">\(0 = 0\)</span>, leaving us with</p>
<p><span class="math display">\[
\begin{aligned}
2\alpha &amp;+ 4\beta &amp;+ 2\gamma &amp;= 0 \\
&amp;- \beta &amp;+ \gamma &amp;= 0
\end{aligned}
\]</span></p>
<p>We could find the complete solution by assigning an arbitrary value to <span class="math inline">\(\gamma\)</span> and then finding <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\alpha\)</span>. However, simply putting <span class="math inline">\(\gamma = 1\)</span> leads to <span class="math inline">\(\beta = 1\)</span> and <span class="math inline">\(\alpha = -3\)</span>. We have thus found non-zero values for <span class="math inline">\(\alpha, \beta, \gamma\)</span>, not all zero, such that <span class="math inline">\(\alpha\mathbf{a} + \beta\mathbf{b} + \gamma\mathbf{c} = \mathbf{0}\)</span>. Therefore the vectors <span class="math inline">\(\mathbf{a}, \mathbf{b}, \mathbf{c}\)</span> are linearly dependent.</p>
</div>
</div>
</div>
<div id="inner-product" class="section level3 hasAnchor" number="4.2.9">
<h3><span class="header-section-number">4.2.9</span> Inner product<a href="linear-algebra.html#inner-product" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>inner product</strong>, also known as the <strong>dot product</strong>, of two vectors results in a <strong>scalar</strong> (a single value). Suppose <span class="math inline">\(\mathbf{u} \in \Re^{n}\)</span> and <span class="math inline">\(\mathbf{v} \in \Re^{n}\)</span>. The inner product <span class="math inline">\(\mathbf{u} \cdot \mathbf{v}\)</span> is the sum of the item-by-item products:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{u} \cdot \mathbf{v} &amp;=  u_{1} v_{1} + u_{2}v_{2} + \ldots + u_{n} v_{n}  \\
                                                        &amp; =  \sum_{i=1}^{N} u_{i} v_{i}
\end{aligned}
\]</span></p>
<p>Suppose <span class="math inline">\(\mathbf{u} = (1, 2, 3)\)</span> and <span class="math inline">\(\mathbf{v} = (2, 3, 1)\)</span>. Then:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{u} \cdot \mathbf{v} &amp; =  1 \times 2 +  2 \times 3 +  3 \times 1 \\
                &amp; = 2+ 6 + 3 \\
                &amp; = 11              
\end{aligned}
\]</span></p>
</div>
<div id="calculating-vector-length" class="section level3 hasAnchor" number="4.2.10">
<h3><span class="header-section-number">4.2.10</span> Calculating vector length<a href="linear-algebra.html#calculating-vector-length" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The standard measurement for length of a vector is known as the <strong>vector norm</strong>. To illustrate, consider a vector in <span class="math inline">\(\Re^2\)</span>:</p>
<p><img src="04-linear-algebra_files/figure-html/pythagorean-theorem-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>The <strong>Pythagorean Theorem</strong> states that for a right triangle with sides length <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, the length of the hypotenuse <span class="math inline">\(c = \sqrt{a^2 + b^2}\)</span>. So if <span class="math inline">\(c\)</span> is a vector in <span class="math inline">\(\Re^2\)</span>, we can directly apply the Pythagorean Theorem to calculate its length from the origin <span class="math inline">\((0,0)\)</span>. We are guaranteed a right triangle because <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> simply correspond to the first and second scalar values contained in <span class="math inline">\(c\)</span> (i.e.Â <span class="math inline">\((3, 4)\)</span>).</p>
<p>This generalizes to <span class="math inline">\(\Re^n\)</span> as given by:</p>
<p><span class="math display">\[
\begin{aligned}
\| \mathbf{v}\| &amp; = (\mathbf{v} \cdot \mathbf{v} )^{1/2} \\
                           &amp; = (v_{1}^2 + v_{2}^{2} + v_{3}^{2} + \ldots + v_{n}^{2} )^{1/2}
\end{aligned}
\]</span></p>
<p>So to calculate the vector norm of a three-dimensional vector <span class="math inline">\(\mathbf{x} = (1,1,1)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\| \mathbf{x}\| &amp; = (\mathbf{x} \cdot \mathbf{x} )^{1/2} \\
                           &amp; = (x_{1}^2 + x_{2}^{2} + x_{3}^{2})^{1/2} \\
                           &amp; = (1 + 1 + 1)^{1/2} \\
                           &amp;= \sqrt{3}
\end{aligned}
\]</span></p>
</div>
</div>
<div id="example-text-analysis" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Example: text analysis<a href="linear-algebra.html#example-text-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Text analysis is a common research method in the social sciences. In order to computationally analyze documents, we need to store them in a meaningful format. This does not mean the format is human-readable, simply interpretable by a computer. In the most simplistic form, documents are represented as vectors, and each value counts the frequency a word appears in a given document. While we throw away information such as word order, we can represent the information in a mathematical fashion using a vector.</p>
<pre><code> a abandoned abc ability able about above abroad absorbed absorbing abstract
43         0   0       0    0    10     0      0        0         0        1</code></pre>
<p>Mathematically, this is represented by the vector <span class="math inline">\((43,0,0,0,0,10,\dots)\)</span></p>
<p>Consider two hypothetical documents:</p>
<p><span class="math display">\[
\begin{aligned}
\text{Doc1} &amp; = (1, 1, 3, \ldots, 5) \\
\text{Doc2} &amp; = (2, 0, 0, \ldots, 1) \\
\textbf{Doc1}, \textbf{Doc2} &amp; \in \Re^{M}
\end{aligned}
\]</span></p>
<p>where each vector is length <span class="math inline">\(m\)</span>. Linear algebra provides many potentially useful operations. For example, the <strong>inner product</strong> between documents is:</p>
<p><span class="math display">\[
\begin{aligned}
\textbf{Doc1} \cdot \textbf{Doc2}  &amp;  =  (1, 1, 3, \ldots, 5) (2, 0, 0, \ldots, 1)&#39;  \\
&amp; =  1 \times 2 + 1 \times 0 + 3 \times 0 + \ldots + 5 \times 1 \\
&amp; = 7
\end{aligned}
\]</span></p>
<p>The <strong>length</strong> of a document is:</p>
<p><span class="math display">\[
\begin{aligned}
\| \textbf{Doc1} \| &amp; \equiv  \sqrt{ \textbf{Doc1} \cdot \textbf{Doc1} } \\
&amp; =  \sqrt{(1, 1, 3, \ldots , 5) (1, 1, 3, \ldots, 5)&#39; } \\
  &amp; =  \sqrt{1^{2} +1^{2} + 3^{2} + 5^{2} } \\
   &amp; =   6
\end{aligned}
\]</span></p>
<p>The <strong>cosine</strong> of the angle between the documents:</p>
<p><span class="math display">\[
\begin{aligned}
\cos (\theta) &amp; \equiv  \left(\frac{\textbf{Doc1} \cdot \textbf{Doc2}}{\| \textbf{Doc1}\| \|\textbf{Doc2} \|} \right) \\
&amp; = \frac{7} { 6 \times  2.24} \\
  &amp; = 0.52
\end{aligned}
\]</span></p>

<div class="rmdnote">
Remember <span class="math inline">\(\cos(\theta)\)</span> is the ratio of the length of the sides of a right triangle <span class="math inline">\(\dfrac{a}{c}\)</span>.
</div>
<p>What is the purpose of this property? What does this value tell us? It tells us the <strong>similarity</strong> in vector space between the documents.</p>
<p>Measuring similarity between documents is very useful. These methods have been used for:</p>
<ul>
<li>Assessing potential plagiarism</li>
<li>Comparing similarity of legislative texts</li>
</ul>
<p>What properties should a similarity measure have?</p>
<ul>
<li>The <strong>maximum</strong> should be the document with itself</li>
<li>The <strong>minimum</strong> should be documents which have no words in common (i.e.Â <strong>orthogonal</strong>)</li>
<li>Increasing when more of the same words are used</li>
<li>Normalized for document length</li>
</ul>
<div id="measure-1-inner-product" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Measure 1: inner product<a href="linear-algebra.html#measure-1-inner-product" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider if we used the inner product to measure similarity.</p>
<p><span class="math display">\[(2,1) \cdot (1,4) = 6\]</span></p>
<p><img src="04-linear-algebra_files/figure-html/inner-product-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>The problem is that the inner product is length dependent. Consider the same method calculated for the vector in purple:</p>
<p><img src="04-linear-algebra_files/figure-html/inner-product-not-same-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p><span class="math display">\[(4,2) \cdot (1,4) = 12\]</span></p>
<p>We get different measures of similarity, yet is the new document really that different? We want something more consistent that accounts for varying overall document length.</p>
</div>
<div id="measure-2-cosine-similarity" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Measure 2: cosine similarity<a href="linear-algebra.html#measure-2-cosine-similarity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><img src="04-linear-algebra_files/figure-html/cosine-sim-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p><span class="math display">\[
\begin{aligned}
(4,2) \cdot (1,4) &amp;= 12 \\
\mathbf{a} \cdot \mathbf{b} &amp;= \|\mathbf{a} \| \times \|\mathbf{b} \| \times \cos(\theta) \\
\frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a} \| \times \|\mathbf{b} \|}  &amp;= \cos(\theta)
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\cos(\theta)\)</span> removes document length from the similarity measure.</p>
<p><span class="math display">\[
\begin{aligned}
\cos (\theta) &amp; \equiv  \left(\frac{\textbf{Doc1} \cdot \textbf{Doc2}}{\| \textbf{Doc1}\| \|\textbf{Doc2} \|} \right) \\
&amp; = \frac{(2, 1) \cdot (1, 4)} {\| (2,1)\| \| (1,4) \|} \\
&amp; = \frac{6} {(\sqrt{2^2 + 1^2}) (\sqrt{1^2 + 4^2})} \\
&amp; = \frac{6} {(\sqrt{5}) (\sqrt{17})} \\
  &amp; \approx 0.65
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\cos (\theta) &amp; \equiv  \left(\frac{\textbf{Doc3} \cdot \textbf{Doc2}}{\| \textbf{Doc3}\| \|\textbf{Doc2} \|} \right) \\
&amp; = \frac{(4,2) \cdot (1, 4)} {\| (24,2)\| \| (1,4) \|} \\
&amp; = \frac{12} {(\sqrt{4^2 + 2^2}) (\sqrt{1^2 + 4^2})} \\
&amp; = \frac{12} {(\sqrt{20}) (\sqrt{17})} \\
  &amp; \approx 0.65
\end{aligned}
\]</span></p>
<p>This measure works for documents of any length with any unique number of words.</p>
</div>
</div>
<div id="matricies" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Matricies<a href="linear-algebra.html#matricies" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A <strong>matrix</strong> is a rectangular arrangement (array) of numbers defined by two <strong>axes</strong> (colloquially known as dimensions):</p>
<ol style="list-style-type: decimal">
<li>Rows</li>
<li>Columns</li>
</ol>

<div class="rmdnote">
<strong>Dimension</strong> is sometimes used interchangably to describe the length of a vector and the number of axes in an array. It can be somewhat confusing. A dimension could refer to the number of entries along a specific axis or the number of axes in the array (tensor).
</div>
<p><span class="math display">\[
\mathbf{A} =
\begin{array}{rrrr}
a_{11} &amp; a_{12} &amp; \ldots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \ldots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \ldots &amp; a_{mn} \\
\end{array}
\]</span></p>
<p>If <span class="math inline">\(\mathbf{A}\)</span> has <span class="math inline">\(m\)</span> rows <span class="math inline">\(n\)</span> columns we will say that <span class="math inline">\(\mathbf{A}\)</span> is an <span class="math inline">\(m \times n\)</span> matrix. Suppose <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span> are <span class="math inline">\(m \times n\)</span> matrices. Then <span class="math inline">\(\mathbf{X} = \mathbf{Y}\)</span> if <span class="math inline">\(x_{ij} = y_{ij}\)</span> for all <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>.</p>
<div id="simple-examples" class="section level4 hasAnchor" number="4.4.0.1">
<h4><span class="header-section-number">4.4.0.1</span> Simple examples<a href="linear-algebra.html#simple-examples" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[
\mathbf{X} = \left[ \begin{array}{rrr}
1 &amp; 2 &amp; 3 \\
2 &amp; 1 &amp; 4 \\
\end{array} \right]
\]</span></p>
<p><span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(2 \times 3\)</span> matrix.</p>
<p><span class="math display">\[
\mathbf{Y} = \left[ \begin{array}{rr}
1 &amp; 2 \\
3 &amp; 2 \\
1 &amp; 4 \\
\end{array} \right]
\]</span></p>
<p><span class="math inline">\(\mathbf{Y}\)</span> is an <span class="math inline">\(3 \times 2\)</span> matrix. <span class="math inline">\(\mathbf{X} \neq \mathbf{Y}\)</span> because the dimensions are different.</p>
</div>
<div id="basic-arithmetic" class="section level3 hasAnchor" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Basic arithmetic<a href="linear-algebra.html#basic-arithmetic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="addition" class="section level4 hasAnchor" number="4.4.1.1">
<h4><span class="header-section-number">4.4.1.1</span> Addition<a href="linear-algebra.html#addition" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="matrix" class="section level5 hasAnchor" number="4.4.1.1.1">
<h5><span class="header-section-number">4.4.1.1.1</span> Matrix<a href="linear-algebra.html#matrix" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Suppose <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span> are <span class="math inline">\(m \times n\)</span> matrices. Then:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{X} + \mathbf{Y} &amp; =  \begin{bmatrix}
x_{11} &amp; x_{12} &amp; \ldots &amp; x_{1n} \\
x_{21} &amp; x_{22} &amp; \ldots &amp; x_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{m1} &amp; x_{m2} &amp; \ldots &amp; x_{mn} \\
\end{bmatrix} +
\begin{bmatrix}
y_{11} &amp; y_{12} &amp; \ldots &amp; y_{1n} \\
y_{21} &amp; y_{22} &amp; \ldots &amp; y_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
y_{m1} &amp; y_{m2} &amp; \ldots &amp; y_{mn} \\
\end{bmatrix}
\\
&amp; = \begin{bmatrix}
x_{11} + y_{11} &amp; x_{12} + y_{12} &amp; \ldots &amp; x_{1n} + y_{1n} \\
x_{21} + y_{21} &amp; x_{22} + y_{22} &amp; \ldots &amp; x_{2n} + y_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
x_{m1} + y_{m1} &amp; x_{m2} + y_{m2} &amp; \ldots &amp; x_{mn} + y_{mn} \\
\end{bmatrix}
\end{aligned}
\]</span></p>
</div>
<div id="scalar" class="section level5 hasAnchor" number="4.4.1.1.2">
<h5><span class="header-section-number">4.4.1.1.2</span> Scalar<a href="linear-algebra.html#scalar" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Suppose <span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(m \times n\)</span> matrix and <span class="math inline">\(k \in \Re\)</span>. Then:</p>
<p><span class="math display">\[
\begin{aligned}
k \mathbf{X} &amp; =  \begin{bmatrix}
k x_{11} &amp; k x_{12} &amp; \ldots &amp;  k x_{1n} \\
k x_{21} &amp; k x_{22} &amp; \ldots &amp; k x_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
k x_{m1} &amp; k x_{m2} &amp; \ldots &amp; k x_{mn} \\
\end{bmatrix}
\end{aligned}
\]</span></p>
</div>
</div>
</div>
<div id="transposition" class="section level3 hasAnchor" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> Transposition<a href="linear-algebra.html#transposition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Matricies must be conformable in order to perform certain operations. For example, matrix addition requires matricies to possess the same number of rows <span class="math inline">\(m\)</span> and columns <span class="math inline">\(n\)</span> in order to add each element together. If the second matrix instead has <span class="math inline">\(n\)</span> rows and <span class="math inline">\(m\)</span> columns, we can <strong>transpose</strong> it to flip the dimensionality of the matrix.</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{X} &amp; = \begin{bmatrix}
x_{11} &amp; x_{12} &amp; \ldots &amp; x_{1n} \\
x_{21} &amp; x_{22} &amp; \ldots &amp; x_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{m1} &amp; x_{m2} &amp; \ldots &amp; x_{mn} \\
\end{bmatrix} \\
\mathbf{X}&#39; &amp; =  \begin{bmatrix}
x_{11} &amp; x_{21} &amp; \ldots &amp; x_{m1} \\
x_{12} &amp; x_{22} &amp; \ldots &amp; x_{m2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{1n} &amp; x_{2n} &amp; \ldots &amp; x_{mn}
\end{bmatrix}
\end{aligned}
\]</span></p>
<ul>
<li>If <span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(m \times n\)</span> then <span class="math inline">\(\mathbf{X}&#39;\)</span> is <span class="math inline">\(n \times m\)</span></li>
<li>If <span class="math inline">\(\mathbf{X} = \mathbf{X}&#39;\)</span> then we say <span class="math inline">\(\mathbf{X}\)</span> is symmetric
<ul>
<li>Only square matricies can be symmetric</li>
</ul></li>
</ul>
</div>
<div id="multiplication" class="section level3 hasAnchor" number="4.4.3">
<h3><span class="header-section-number">4.4.3</span> Multiplication<a href="linear-algebra.html#multiplication" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Matrix multiplication</strong> is extraordinarily useful for many computational problems, though somewhat tedious to calculate by hand. Suppose we have two matrices:</p>
<p><span class="math display">\[\mathbf{X} = \begin{bmatrix} 1 &amp; 1 \\ 1&amp; 1 \\ \end{bmatrix} , \quad \mathbf{Y} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{bmatrix} \]</span></p>
<p>We will create a new matrix <span class="math inline">\(\mathbf{A}\)</span> by matrix multiplication:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{A} &amp; = \mathbf{X} \mathbf{Y} \\
&amp; = \begin{bmatrix}
1 &amp; 1 \\
1 &amp; 1 \\
\end{bmatrix}
\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4 \\
\end{bmatrix} \\
&amp;= \begin{bmatrix}
1 \times 1 + 1 \times 3 &amp; 1 \times 2 + 1 \times 4 \\
1 \times 1 + 1 \times 3 &amp; 1 \times 2 + 1 \times 4\\
\end{bmatrix} \\
&amp;= \begin{bmatrix}
4 &amp; 6 \\
4 &amp; 6
\end{bmatrix}
\end{aligned}
\]</span></p>
<div id="algebraic-properties" class="section level4 hasAnchor" number="4.4.3.1">
<h4><span class="header-section-number">4.4.3.1</span> Algebraic properties<a href="linear-algebra.html#algebraic-properties" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Matricies must be <strong>conformable</strong> - that is, the number of columns in <span class="math inline">\(\mathbf{A}\)</span> must equal the number of rows in <span class="math inline">\(\mathbf{B}\)</span>. If <span class="math inline">\(\mathbf{AB}\)</span> exists then we call the matricies conformable.</li>
<li>Associative property: <span class="math inline">\((\mathbf{XY})\mathbf{Z} = \mathbf{X}(\mathbf{YZ})\)</span></li>
<li>Additive distributive property: <span class="math inline">\((\mathbf{X} + \mathbf{Y})\mathbf{Z} = \mathbf{XZ} + \mathbf{YZ}\)</span></li>
<li>Zero property: <span class="math inline">\(\mathbf{X0} = 0\)</span></li>
<li>Order matters: <span class="math inline">\(\mathbf{XY} \neq \mathbf{YX}\)</span>
<ul>
<li>Different from scalar multiplication: <span class="math inline">\(xy = yx\)</span></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="example-neural-networks" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Example: neural networks<a href="linear-algebra.html#example-neural-networks" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Why should we care about vectors/matricies and arithmetic operations? What use are they to computational social science? Iâm glad you asked! Vectors/matricies and linear algebra form the heart of a <strong>neural network</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:neural-network"></span>
<img src="https://upload.wikimedia.org/wikipedia/commons/9/99/Neural_network_example.svg" alt="Source: [Wikipedia](https://en.wikipedia.org/wiki/Neural_network)" width="60%" />
<p class="caption">
Figure 4.2: Source: <a href="https://en.wikipedia.org/wiki/Neural_network">Wikipedia</a>
</p>
</div>
<p><strong>Deep learning</strong> is the name we use for <strong>stacked neural networks</strong>; that is, networks composed of several layers. The layers are made of <strong>nodes.</strong> A node is just a place where computation happens, loosely patterned on a neuron in the human brain, which fires when it encounters sufficient stimuli.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a></p>
<p>A node combines input from the data with a set of coefficients, or weights, that either amplify or dampen that input, thereby assigning significance to inputs with regard to the task the algorithm is trying to learn; e.g.Â which input is most helpful is classifying data without error? These input-weight products are summed and then the sum is passed through a nodeâs so-called activation function, to determine whether and to what extent that signal should progress further through the network to affect the ultimate outcome, say, an act of classification. If the signals passes through, the neuron has been âactivated.â</p>
<div id="how-are-neural-networks-used" class="section level3 hasAnchor" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> How are neural networks used<a href="linear-algebra.html#how-are-neural-networks-used" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Self-driving cars</li>
<li>Voice activated assistants</li>
<li>Automatic machine translation</li>
<li>Image recognition</li>
<li>Detection of diseases</li>
</ul>
</div>
<div id="how-are-neural-networks-related-to-linear-algebra" class="section level3 hasAnchor" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> How are neural networks related to linear algebra?<a href="linear-algebra.html#how-are-neural-networks-related-to-linear-algebra" class="anchor-section" aria-label="Anchor link to header"></a></h3>

<div class="rmdnote">
This section draws heavily from <span class="citation">Francois (<a href="#ref-francois2017" role="doc-biblioref">2017</a>)</span>. I highly encourage this text (especially chapter 2) for an intuitive introduction to deep learning and its mathematical building blocks.
</div>
<p>A <strong>tensor</strong> is the core unit of data in deep learning. It is a generalization of vectors and matricies to higher-dimensions.</p>
<ul>
<li>Scalars (0D tensors) - a tensor containing a single number</li>
<li>Vectors (1D tensors) - a tensor with a one-dimensional array of numbers</li>
<li>Matricies (2D tensors) - a tensor with a two-dimensional array of numbers</li>
<li>3D tensors and higher-dimensional tensors - array of matricies</li>
</ul>
<div id="tensor-operations" class="section level4 hasAnchor" number="4.5.2.1">
<h4><span class="header-section-number">4.5.2.1</span> Tensor operations<a href="linear-algebra.html#tensor-operations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>All transformations inside of a deep neural network are reduced to a handful of <strong>tensor operations</strong>, or generalizations of matrix operations (e.g.Â addition, multiplication). The key point is if you can do it with a matrix, you can do it with a tensor.</p>
</div>
<div id="geometric-interpretation" class="section level4 hasAnchor" number="4.5.2.2">
<h4><span class="header-section-number">4.5.2.2</span> Geometric interpretation<a href="linear-algebra.html#geometric-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Deep learning is a chain of small tensor operations which are geometric transformations of the input data</li>
<li>We could interpret it as a very complex geometric transformation in a high-dimensional space, implemented via a long series of simple steps</li>
</ul>
<p><img src="images/fig-2-9.png" width="90%" style="display: block; margin: auto;" /></p>
<ul>
<li>Consider two pieces of paper - one red and one blue</li>
<li>Stick them together and crumple them into a small ball
<ul>
<li>The crumpled paper ball is the input data</li>
<li>Each sheet of paper is a class of data</li>
<li>We want to classify points on the paper as red or blue</li>
</ul></li>
<li>Deep learning is then the series of steps necessary to uncrumple the ball so that the two classes are cleanly separable again</li>
</ul>
</div>
<div id="here-is-the-linear-algebra" class="section level4 hasAnchor" number="4.5.2.3">
<h4><span class="header-section-number">4.5.2.3</span> Here is the linear algebra<a href="linear-algebra.html#here-is-the-linear-algebra" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Mathematically, each layer in the network example transforms input data as follows:</p>
<p><span class="math display">\[\mathbf{Y} = \text{activation}(\mathbf{W} \cdot \mathbf{X} + \mathbf{B})\]</span></p>
<ul>
<li><span class="math inline">\(\mathbf{X}\)</span> - input tensor</li>
<li><span class="math inline">\(\mathbf{Y}\)</span> - output tensor</li>
<li><span class="math inline">\(\mathbf{W}, \mathbf{B}\)</span> - weight/parameter tensors (attributes of the layer, determined through the optimization process)</li>
<li><span class="math inline">\(\text{activation}()\)</span>
<ul>
<li>Modifies the input in a non-linear fashion to allow for non-linear relationships between features/independent variables in the data</li>
<li>Key traits
<ul>
<li>Non-linear</li>
<li>Continuously differentiable</li>
<li>Fixed range</li>
</ul></li>
<li>Common activation functions
<ul>
<li><p>Rectified Linear Units (RELU)</p>
<p><span class="math display">\[R(z) = \max(0, z)\]</span></p></li>
<li><p>Sigmoid function (aka logistic regression)</p>
<p><span class="math display">\[S(z) = \frac{1}{1 + e^{-z}}\]</span></p></li>
</ul></li>
</ul></li>
<li>How do we determine the values for the weights and parameters? Come back next week when we discuss optimization.</li>
</ul>
<p>Each layer adjusts the tensor to create new, and ideally more clear, structures of the underlying data. A basic neural network may contain thousands, if not millions, or tensor operations. While you will not complete these by hand, it is important to understand the underlying principles when you seek to implement neural networks.</p>
</div>
</div>
</div>
<div id="matrix-inversion" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Matrix inversion<a href="linear-algebra.html#matrix-inversion" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose <span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(n \times n\)</span> matrix. We want to find the matrix <span class="math inline">\(\mathbf{X}^{-1}\)</span> such that</p>
<p><span class="math display">\[
\mathbf{X}^{-1} \mathbf{X} = \mathbf{X} \mathbf{X}^{-1} = \mathbf{I}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{I}\)</span> is the <span class="math inline">\(n \times n\)</span> identity matrix.</p>
<p>Why is this useful? Matrix inversion is necessary to:</p>
<ul>
<li>Solve systems of equations</li>
<li>Perform linear regression</li>
<li>Provide intuition about <strong>colinearity</strong>, <strong>fixed effects</strong>, and other relevant design matricies for social scientists.</li>
</ul>
<div id="calculating-matrix-inversions" class="section level3 hasAnchor" number="4.6.1">
<h3><span class="header-section-number">4.6.1</span> Calculating matrix inversions<a href="linear-algebra.html#calculating-matrix-inversions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the following equations:</p>
<p><span class="math display">\[
\begin{aligned}
x_{1} + x_{2} + x_{3} &amp;= 0 \\
0x_{1}  +   5x_{2} + 0x_{3}  &amp; = 5 \\
0 x_{1} + 0 x_{2} + 3 x_{3} &amp; =  6 \\
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{A}  &amp;= \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 5 &amp; 0 \\ 0 &amp; 0 &amp; 3 \end{bmatrix} \\
\mathbf{x} &amp;= (x_{1} , x_{2}, x_{3} ) \\
\mathbf{b} &amp;= (0, 5, 6)
\end{aligned}
\]</span></p>
<p>The system of equations are now,</p>
<p><span class="math display">\[\mathbf{A}\mathbf{x} =\mathbf{b}\]</span></p>
<p><span class="math inline">\(\mathbf{A}^{-1}\)</span> exists <strong>if and only if</strong> <span class="math inline">\(\mathbf{A}\mathbf{x} = \mathbf{b}\)</span> has only one solution.</p>
<div id="definition-1" class="section level4 hasAnchor" number="4.6.1.1">
<h4><span class="header-section-number">4.6.1.1</span> Definition<a href="linear-algebra.html#definition-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Suppose <span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(n \times n\)</span> matrix. We will call <span class="math inline">\(\mathbf{X}^{-1}\)</span> the <strong>inverse</strong> of <span class="math inline">\(\mathbf{X}\)</span> if</p>
<p><span class="math display">\[
\mathbf{X}^{-1} \mathbf{X} = \mathbf{X} \mathbf{X}^{-1} = \mathbf{I}
\]</span></p>
<p>If <span class="math inline">\(\mathbf{X}^{-1}\)</span> exists then <span class="math inline">\(\mathbf{X}\)</span> is invertible. If <span class="math inline">\(\mathbf{X}^{-1}\)</span> does not exist, then we will say <span class="math inline">\(\mathbf{X}\)</span> is <strong>singular</strong>.</p>

<div class="rmdnote">
Note the key requirement: only square matricies are invertible.
</div>
<p>Solved via R:</p>
<pre><code>##      [,1] [,2]   [,3]
## [1,]    1 -0.2 -0.333
## [2,]    0  0.2  0.000
## [3,]    0  0.0  0.333</code></pre>
</div>
</div>
<div id="when-do-inverses-exist" class="section level3 hasAnchor" number="4.6.2">
<h3><span class="header-section-number">4.6.2</span> When do inverses exist<a href="linear-algebra.html#when-do-inverses-exist" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Inverses exist when the columns and rows are <strong>linearly independent</strong> - that is, there is not repeated information in the matrix.</p>
<div id="example-1-1" class="section level4 hasAnchor" number="4.6.2.1">
<h4><span class="header-section-number">4.6.2.1</span> Example 1<a href="linear-algebra.html#example-1-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Consider <span class="math inline">\(\mathbf{v}_{1} = (1, 0, 0)\)</span>, <span class="math inline">\(\mathbf{v}_{2} = (0,1,0)\)</span>, <span class="math inline">\(\mathbf{v}_{3} = (0,0,1)\)</span></p>
<p>Can we write any of these vectors as a combination of the other vectors? <strong>No.</strong></p>
</div>
<div id="example-2-1" class="section level4 hasAnchor" number="4.6.2.2">
<h4><span class="header-section-number">4.6.2.2</span> Example 2<a href="linear-algebra.html#example-2-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Consider <span class="math inline">\(\mathbf{v}_{1} = (1, 0, 0)\)</span>, <span class="math inline">\(\mathbf{v}_{2} = (0,1,0)\)</span>, <span class="math inline">\(\mathbf{v}_{3} = (0,0,1)\)</span>, <span class="math inline">\(\mathbf{v}_{4} = (1, 2, 3)\)</span>.</p>
<p>Can we write this as a combination of other vectors?</p>
<p><span class="math display">\[\mathbf{v}_{4} = \mathbf{v}_{1} + 2 \mathbf{v}_{2} + 3\mathbf{v}_{3}\]</span></p>
<p>So a matrix <span class="math inline">\(\mathbf{V}\)</span> containing these vectors as columns is not linearly independent, and therefore is noninvertible (also known as <strong>singular</strong>).</p>
</div>
</div>
<div id="inverting-a-2-times-2-matrix" class="section level3 hasAnchor" number="4.6.3">
<h3><span class="header-section-number">4.6.3</span> Inverting a <span class="math inline">\(2 \times 2\)</span> matrix<a href="linear-algebra.html#inverting-a-2-times-2-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If <span class="math inline">\(\mathbf{A} = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\)</span> and <span class="math inline">\(ad \neq bc\)</span>, then <span class="math inline">\(\mathbf{A}\)</span> is invertible and</p>
<p><span class="math display">\[\mathbf{A}^{-1} = \frac{1}{ad - bc} \begin{bmatrix}
d &amp; -b \\
-c &amp; a
\end{bmatrix}\]</span></p>
<p>For example</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{A} &amp;= \begin{bmatrix}
9 &amp; 7 \\
2 &amp; 1
\end{bmatrix} \\
\mathbf{A}^{-1} &amp;= \frac{1}{(-5)} \begin{bmatrix}
1 &amp; -7 \\
-2 &amp; 9
\end{bmatrix} = \begin{bmatrix}
-0.2 &amp; 1.4 \\
0.4 &amp; -1.8
\end{bmatrix}
\end{aligned}
\]</span></p>
<p>We can verify by</p>
<p><span class="math display">\[\mathbf{A}^{-1} \mathbf{A} = \begin{bmatrix}
9 &amp; 7 \\
2 &amp; 1
\end{bmatrix} \begin{bmatrix}
-0.2 &amp; 1.4 \\
0.4 &amp; -1.8
\end{bmatrix} = \begin{bmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{bmatrix} = \mathbf{I}\]</span></p>
</div>
<div id="inverting-an-n-times-n-matrix" class="section level3 hasAnchor" number="4.6.4">
<h3><span class="header-section-number">4.6.4</span> Inverting an <span class="math inline">\(n \times n\)</span> matrix<a href="linear-algebra.html#inverting-an-n-times-n-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can use the process of <strong>Gauss-Jordan elimination</strong> to calculate the inverse of an <span class="math inline">\(n \times n\)</span> matrix. Gauss-Jordan elimination is a procedure to solve a system of equations. Using an <strong>augmented matrix</strong>, we apply a series of <strong>elementary row operations</strong> to modify the augmented matrix until we have a <strong>diagonal matrix</strong> on the left-hand side. Elementary row operations include:</p>
<ol style="list-style-type: decimal">
<li>Exchanging two rows in the matrix</li>
<li>Subtracting a multiple of one row from another row</li>
</ol>
<p>First we setup an <strong>augmented matrix</strong> <span class="math inline">\([\mathbf{A} \; \mathbf{I}]\)</span> which we reduce to the form <span class="math inline">\([\mathbf{D} \; \mathbf{B}]\)</span>, where <span class="math inline">\(\mathbf{D}\)</span> is a diagonal matrix with no diagonal entry equal to <span class="math inline">\(0\)</span>. <span class="math inline">\(\mathbf{A}^{-1}\)</span> is then found by dividing each row of <span class="math inline">\(\mathbf{B}\)</span> by the corresponding diagonal entry of <span class="math inline">\(\mathbf{D}\)</span>.</p>
<div id="example-1-2" class="section level4 hasAnchor" number="4.6.4.1">
<h4><span class="header-section-number">4.6.4.1</span> Example 1<a href="linear-algebra.html#example-1-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For example, let us invert</p>
<p><span class="math display">\[\mathbf{A} = \begin{bmatrix}
2 &amp; 1 &amp; 2 \\
3 &amp; 1 &amp; 1 \\
3 &amp; 1 &amp; 2
\end{bmatrix}\]</span></p>
<p>First setup the augmented matrix:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr|rrr}
2 &amp; 1 &amp; 2 &amp; 1 &amp; 0 &amp; 0 \\
3 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\
3 &amp; 1 &amp; 2 &amp; 0 &amp; 0 &amp; 1
\end{array}
\right]
\]</span></p>
<p>Next we substract <span class="math inline">\(3/2\)</span> times the first row from each of the other rows to get:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr|rrr}
2 &amp; 1 &amp; 2 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; -1/2 &amp; -2 &amp; -3/2 &amp; 1 &amp; 0 \\
0 &amp; -1/2 &amp; -1 &amp; -3/2 &amp; 0 &amp; 1
\end{array}
\right]
\]</span></p>
<p>Our next step is to add twice the second row to the first row, and to subtract the second row from the third row. We obtain:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr|rrr}
2 &amp; 0 &amp; -2 &amp; -2 &amp; 2 &amp; 0 \\
0 &amp; -1/2 &amp; -2 &amp; -3/2 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; -1 &amp; 1
\end{array}
\right]
\]</span></p>
<p>Finally we add twice the third row to the first and second rows:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr|rrr}
2 &amp; 0 &amp; 0 &amp; -2 &amp; 0 &amp; 2 \\
0 &amp; -1/2 &amp; 0 &amp; -3/2 &amp; -1 &amp; 2 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; -1 &amp; 1
\end{array}
\right]
\]</span></p>
<p>At this point <span class="math inline">\(\mathbf{B}\)</span> is a diagonal matrix with all non-zero elements on the diagonal. We obtain <span class="math inline">\(\mathbf{A}^{-1}\)</span> by dividing the first row of <span class="math inline">\(\mathbf{B}\)</span> by <span class="math inline">\(2\)</span>, the second row by <span class="math inline">\(-\frac{1}{2}\)</span>, and the third row by <span class="math inline">\(1\)</span>:</p>
<p><span class="math display">\[\mathbf{A}^{-1} = \begin{bmatrix}
-1 &amp; 0 &amp; 1 \\
3 &amp; 2 &amp; -4 \\
0 &amp; -1 &amp; 1
\end{bmatrix}\]</span></p>
</div>
<div id="example-2-2" class="section level4 hasAnchor" number="4.6.4.2">
<h4><span class="header-section-number">4.6.4.2</span> Example 2<a href="linear-algebra.html#example-2-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Invert the matrix</p>
<p><span class="math display">\[\mathbf{A} = \begin{bmatrix}
1 &amp; 3 &amp; 5 \\
1 &amp; 7 &amp; 5 \\
5 &amp; 10 &amp; 15
\end{bmatrix}\]</span></p>
<ol style="list-style-type: decimal">
<li><p>First setup the augmented matrix:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr|rrr}
1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 7 &amp; 5 &amp; 0 &amp; 1 &amp; 0 \\
5 &amp; 10 &amp; 15 &amp; 0 &amp; 0 &amp; 1
\end{array}
\right]
\]</span></p></li>
<li><p>Subtract row 1 from row 2:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr|rrr}
1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 4 &amp; 0 &amp; -1 &amp; 1 &amp; 0 \\
5 &amp; 10 &amp; 15 &amp; 0 &amp; 0 &amp; 1
\end{array}
\right]
\]</span></p></li>
<li><p>Subtract 5 Ã (row 1) from row 3:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr|rrr}
1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 4 &amp; 0 &amp; -1 &amp; 1 &amp; 0 \\
0 &amp; -5 &amp; -10 &amp; -5 &amp; 0 &amp; 1
\end{array}
\right]
\]</span></p></li>
<li><p>Swap row 2 with row 3:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr|rrr}
1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; -5 &amp; -10 &amp; -5 &amp; 0 &amp; 1 \\
0 &amp; 4 &amp; 0 &amp; -1 &amp; 1 &amp; 0 \\
\end{array}
\right]
\]</span></p></li>
<li><p>Add 4/5 Ã (row 2) to row 3:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr|rrr}
1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; -5 &amp; -10 &amp; -5 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; -8 &amp; -5 &amp; 1 &amp; \frac{4}{5} \\
\end{array}
\right]
\]</span></p></li>
<li><p>Divide row 3 by -8:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr|rrr}
1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; -5 &amp; -10 &amp; -5 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1 &amp; \frac{5}{8} &amp; -\frac{1}{8} &amp; -\frac{1}{10} \\
\end{array}
\right]
\]</span></p></li>
<li><p>Add 10 Ã (row 3) to row 2:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr|rrr}
1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; -5 &amp; 0 &amp; \frac{5}{4} &amp; -\frac{5}{4} &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; \frac{5}{8} &amp; -\frac{1}{8} &amp; -\frac{1}{10} \\
\end{array}
\right]
\]</span></p></li>
<li><p>Subtract 5 Ã (row 3) from row 1:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr|rrr}
1 &amp; 3 &amp; 0 &amp; -\frac{17}{8} &amp; \frac{5}{8} &amp; \frac{1}{2} \\
0 &amp; -5 &amp; 0 &amp; \frac{5}{4} &amp; -\frac{5}{4} &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; \frac{5}{8} &amp; -\frac{1}{8} &amp; -\frac{1}{10} \\
\end{array}
\right]
\]</span></p></li>
<li><p>Divide row 2 by -5:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr|rrr}
1 &amp; 3 &amp; 0 &amp; -\frac{17}{8} &amp; \frac{5}{8} &amp; \frac{1}{2} \\
0 &amp; 1 &amp; 0 &amp; -\frac{1}{4} &amp; \frac{1}{4} &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; \frac{5}{8} &amp; -\frac{1}{8} &amp; -\frac{1}{10} \\
\end{array}
\right]
\]</span></p></li>
<li><p>Subtract 3 Ã (row 2) from row 1:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr|rrr}
1 &amp; 0 &amp; 0 &amp; -\frac{11}{8} &amp; -\frac{1}{8} &amp; \frac{1}{2} \\
0 &amp; 1 &amp; 0 &amp; -\frac{1}{4} &amp; \frac{1}{4} &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; \frac{5}{8} &amp; -\frac{1}{8} &amp; -\frac{1}{10} \\
\end{array}
\right]
\]</span></p></li>
</ol>
<p><span class="math display">\[\mathbf{A}^{-1} = \begin{bmatrix}
-\frac{11}{8} &amp; -\frac{1}{8} &amp; \frac{1}{2} \\
-\frac{1}{4} &amp; \frac{1}{4} &amp; 0 \\
\frac{5}{8} &amp; -\frac{1}{8} &amp; -\frac{1}{10}
\end{bmatrix}\]</span></p>
<p>We can simplify by factoring out an appropriate term:</p>
<p><span class="math display">\[\mathbf{A}^{-1} = \frac{1}{40} \begin{bmatrix}
-55 &amp; -5 &amp; 20 \\
-10 &amp; 10 &amp; 0 \\
25 &amp; -5 &amp; -4
\end{bmatrix}\]</span></p>
</div>
</div>
<div id="application-to-regression-analysis" class="section level3 hasAnchor" number="4.6.5">
<h3><span class="header-section-number">4.6.5</span> Application to regression analysis<a href="linear-algebra.html#application-to-regression-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In methods classes you learn about linear regression. For each <span class="math inline">\(i\)</span> (individual) we observe covariates <span class="math inline">\(x_{i1}, x_{i2}, \ldots, x_{ik}\)</span> and dependent variable <span class="math inline">\(Y_{i}\)</span>. Then,</p>
<p><span class="math display">\[
\begin{aligned}
Y_{1} &amp; = \beta_{0} + \beta_{1} x_{11} + \beta_{2} x_{12} + \ldots + \beta_{k} x_{1k} \\
Y_{2} &amp; = \beta_{0} + \beta_{1} x_{21} + \beta_{2} x_{22} + \ldots + \beta_{k} x_{2k} \\
\vdots &amp; \vdots &amp; \vdots \\
Y_{i} &amp; = \beta_{0} + \beta_{1} x_{i1} + \beta_{2} x_{i2} + \ldots + \beta_{k} x_{ik} \\
\vdots &amp; \vdots &amp; \vdots \\
Y_{n} &amp; = \beta_{0} + \beta_{1} x_{n1} + \beta_{2} x_{n2} + \ldots + \beta_{k} x_{nk}
\end{aligned}
\]</span></p>
<ul>
<li><span class="math inline">\(\mathbf{x}_{i} = (1, x_{i1}, x_{i2}, \ldots, x_{ik})\)</span></li>
<li><span class="math inline">\(\mathbf{X} = \begin{bmatrix} \mathbf{x}_{1}\\\mathbf{x}_{2}\\ \vdots \\ \mathbf{x}_{n} \end{bmatrix}\)</span></li>
<li><span class="math inline">\(\boldsymbol{\beta} = (\beta_{0}, \beta_{1}, \ldots, \beta_{k} )\)</span></li>
<li><span class="math inline">\(\mathbf{Y} = (Y_{1}, Y_{2}, \ldots, Y_{n})\)</span></li>
</ul>
<p>Then we can write</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{Y} &amp;= \mathbf{X}\mathbf{\beta} \\
\mathbf{X}^{&#39;} \mathbf{Y} &amp;= \mathbf{X}^{&#39;} \mathbf{X} \mathbf{\beta} \\
(\mathbf{X}^{&#39;}\mathbf{X})^{-1} \mathbf{X}^{&#39;} \mathbf{Y} &amp;= (\mathbf{X}^{&#39;}\mathbf{X})^{-1}\mathbf{X}^{&#39;} \mathbf{X} \mathbf{\beta} \\
(\mathbf{X}^{&#39;}\mathbf{X})^{-1} \mathbf{X}^{&#39;} \mathbf{Y} &amp;=\mathbf{\beta}
\end{aligned}
\]</span></p>
<ol style="list-style-type: decimal">
<li>Pre-multiply both sides by <span class="math inline">\(\mathbf{X}&#39;\)</span></li>
<li>Pre-multiply both sides by <span class="math inline">\((\mathbf{X}^{&#39;}\mathbf{X})^{-1}\)</span></li>
<li><span class="math inline">\((\mathbf{X}^{&#39;}\mathbf{X})^{-1}\mathbf{X}^{&#39;} \mathbf{X} = \mathbf{I}\)</span></li>
</ol>
<p>This depends on <span class="math inline">\((\mathbf{X}^{&#39;}\mathbf{X})^{-1}\)</span> being invertible. If this is true, we can calculate the values for <span class="math inline">\(\boldsymbol{\beta}\)</span>. If not, then we cannot. When might this occur? Weâll see some occurences of this in todayâs and future problem sets.</p>
</div>
<div id="application-to-solving-systems-of-equations-tax-benefits-of-charitable-contributions" class="section level3 hasAnchor" number="4.6.6">
<h3><span class="header-section-number">4.6.6</span> Application to solving systems of equations: tax benefits of charitable contributions<a href="linear-algebra.html#application-to-solving-systems-of-equations-tax-benefits-of-charitable-contributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose a company earns before-tax profits of $100,000.<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> It has agreed to contribute 10% of its after-tax profits to the Red Cross Relief Fund. It must pay a state tax of 5% of its profits (after the Red Cross donation) and a federal tax of 40% of its profits (after the donation and state taxes are paid). How much does the company pay in state taxes, federal taxes, and Red Cross donation?</p>
<p>Without a model, this is a difficult problem because each payment takes into consideration the other payments. However, if we write out the <strong>linear equations</strong> which describe these deductions and payments, then we can understand the relationships between these payments and solve it in a straightforward manner.</p>
<p>Let <span class="math inline">\(C\)</span>, <span class="math inline">\(S\)</span>, and <span class="math inline">\(F\)</span> represent the amounts of the charitable contributin, state tax, and federal tax, respectively. After-profits are <span class="math inline">\(\$100{,}000 - (S + F)\)</span>, so <span class="math inline">\(C = 0.10 \times (100{,}000 - (S + F))\)</span>. We can write this as</p>
<p><span class="math display">\[C + 0.1S + 0.1F = 10{,}000\]</span></p>
<p>putting all the variables on one side. The statement that state tax is 5% of the profits net of the donation becomes <span class="math inline">\(S = 0.05 \times (100{,}000 - C)\)</span>, which is</p>
<p><span class="math display">\[0.05C + S = 5{,}000\]</span></p>
<p>Federal taxes are 40% of the profit after deducting <span class="math inline">\(C\)</span> and <span class="math inline">\(S\)</span>, this relationship is expressed as <span class="math inline">\(F = 0.40 \times [100{,}000 - (C+S)]\)</span>, or</p>
<p><span class="math display">\[0.4C + 0.4S + F = 40{,}000\]</span></p>
<p>We can summarize these payments in a single system of linear equations:</p>
<p><span class="math display">\[
\begin{aligned}
C &amp; + &amp; 0.1S &amp; + &amp; 0.1F &amp;= 10{,}000 \\
0.05C &amp; + &amp; S &amp; &amp;&amp;= 5{,}000 \\
0.4C &amp; + &amp; 0.4S &amp; + &amp; F &amp;= 40{,}000
\end{aligned}
\]</span></p>
<p>We could substitute the middle equation for <span class="math inline">\(S\)</span> in terms of <span class="math inline">\(C\)</span> and solve the resulting system. Or, we can use matrix inversion:</p>
<pre><code>## [1]  5956  4702 35737</code></pre>
</div>
</div>
<div id="determinant" class="section level2 hasAnchor" number="4.7">
<h2><span class="header-section-number">4.7</span> Determinant<a href="linear-algebra.html#determinant" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <strong>determinant</strong> of a square matrix is a single number summary. The determinant uses all of the values of a square matrix to provide a summary of structure. Unfortunately it is rather complicated to calculate for larger matricies. First letâs consider how to calculate the determinant of a <span class="math inline">\(2 \times 2\)</span> matrix, which is the difference in diagonal products.</p>
<p><span class="math display">\[\det(\mathbf{X}) = \mid \mathbf{X} \mid = \left| \begin{matrix}
x_{11} &amp; x_{12} \\
x_{21} &amp; x_{22}
\end{matrix} \right| = x_{11}x_{22} - x_{12}x_{21}\]</span></p>
<p>Some simple examples include</p>
<p><span class="math display">\[\left| \begin{matrix}
1 &amp; 2 \\
3 &amp; 4
\end{matrix} \right| = (1)(4) - (2)(3) = 4 - 6 = -2\]</span></p>
<p><span class="math display">\[\left| \begin{matrix}
10 &amp; \frac{1}{2} \\
4 &amp; 1
\end{matrix} \right| = (10)(1) - \left( \frac{1}{2} \right)(4) = 10 - 2 = 8\]</span></p>
<p><span class="math display">\[\left| \begin{matrix}
2 &amp; 3 \\
6 &amp; 9
\end{matrix} \right| = (2)(9) - (3)(6) = 18 - 18 = 0\]</span></p>
<p>The last case, where the determinant is <span class="math inline">\(0\)</span>, is an important case which we shall see shortly.</p>
<p>Unfortunately calculating determinants gets much more involved with square matricies larger than <span class="math inline">\(2 \times 2\)</span>. First we need to define a <strong>submatrix</strong>. The submatrix is simply a form achieved by deleting rows and/or columns of a matrix, leaving the remaining elements in their respective places. So for the matrix <span class="math inline">\(\mathbf{X}\)</span>, notice the following submatricies:</p>
<p><span class="math display">\[
\mathbf{X} = \begin{bmatrix}
x_{11} &amp; x_{12} &amp; x_{13} &amp; x_{14} \\
x_{21} &amp; x_{22} &amp; x_{23} &amp; x_{24} \\
x_{31} &amp; x_{32} &amp; x_{33} &amp; x_{34} \\
x_{41} &amp; x_{42} &amp; x_{43} &amp; x_{44} \\
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\mathbf{X}_{[11]} = \begin{bmatrix}
x_{22} &amp; x_{23} &amp; x_{24} \\
x_{32} &amp; x_{33} &amp; x_{34} \\
x_{42} &amp; x_{43} &amp; x_{44} \\
\end{bmatrix},
\mathbf{X}_{[24]} = \begin{bmatrix}
x_{11} &amp; x_{12} &amp; x_{13}  \\
x_{31} &amp; x_{32} &amp; x_{33}  \\
x_{41} &amp; x_{42} &amp; x_{43}  \\
\end{bmatrix}
\]</span></p>
<p>To generalize for further <span class="math inline">\(n \times n\)</span> matricies, the determinant can be calculated as</p>
<p><span class="math display">\[\mid \mathbf{X} \mid = \sum_{j=1}^n (-1)^{i+j} x_{ij} \mid\mathbf{X}_{[ij]}\mid\]</span></p>
<p>where the <span class="math inline">\(ij\)</span>th <strong>minor</strong> of <span class="math inline">\(\mathbf{X}\)</span> for <span class="math inline">\(x_{ij}\)</span>, <span class="math inline">\(\mid\mathbf{X}_{[ij]}\mid\)</span>, is the determinant of the <span class="math inline">\((n - 1) \times (n - 1)\)</span> submatrix that results from taking the <span class="math inline">\(i\)</span>th row and <span class="math inline">\(j\)</span>th column out. The <strong>cofactor</strong> of <span class="math inline">\(\mathbf{X}\)</span> is the minor signed as <span class="math inline">\((-1)^{i+j} x_{ij} \mid\mathbf{X}_{[ij]}\mid\)</span>. To calculate the determinant we cycle recursively through the columns and take sums with a formula that multiplies the cofactor by the determining value.</p>
<p>For instance, here is the method applied to a <span class="math inline">\(3 \times 3\)</span> matrix:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{X} &amp;= \begin{bmatrix}
x_{11} &amp; x_{12} &amp; x_{13} \\
x_{21} &amp; x_{22} &amp; x_{23} \\
x_{31} &amp; x_{32} &amp; x_{33} \\
\end{bmatrix} \\
\det(\mathbf{X}) &amp;= (+1)x_{11} \left| \begin{matrix}
x_{22} &amp; x_{23} \\
x_{32} &amp; x_{33} \\
\end{matrix} \right| +(-1)x_{12} \left| \begin{matrix}
x_{21} &amp; x_{23} \\
x_{31} &amp; x_{33} \\
\end{matrix} \right| + (+1)x_{13} \left| \begin{matrix}
x_{21} &amp; x_{22} \\
x_{31} &amp; x_{32} \\
\end{matrix} \right|
\end{aligned}
\]</span></p>
<p>Now the problem is simplified because the subsequent three determinant calculations are on <span class="math inline">\(2 \times 2\)</span> matricies.</p>
<div id="relevance-of-the-determinant" class="section level3 hasAnchor" number="4.7.1">
<h3><span class="header-section-number">4.7.1</span> Relevance of the determinant<a href="linear-algebra.html#relevance-of-the-determinant" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Remember how we wanted to invert a <span class="math inline">\(2 \times 2\)</span> matrix previously?</p>
<p><span class="math display">\[\mathbf{A}^{-1} = \frac{1}{ad - bc} \begin{bmatrix}
d &amp; -b \\
-c &amp; a
\end{bmatrix}\]</span></p>
<p><span class="math inline">\(\frac{1}{ad - bc}\)</span> is the formula for the determinant of a <span class="math inline">\(2 \times 2\)</span> matrix! Recall that non-invertible (singular) matricies are square matricies which have columns or rows that are linearly dependent. Well would it surprise you to know that singular matricies also have the unique property whereby their determinant is <span class="math inline">\(0\)</span>. This is also important as we move into eigenvectors and diagonalization.</p>
</div>
</div>
<div id="matrix-decomposition" class="section level2 hasAnchor" number="4.8">
<h2><span class="header-section-number">4.8</span> Matrix decomposition<a href="linear-algebra.html#matrix-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Matrix decomposition</strong> is a factorization of a matrix into a product of matricies. That is, a matrix can be <strong>decomposed</strong> into more efficient matricies depending on the calculations needing to be performed. <strong>LU decomposition</strong> applies to square matricies:</p>
<p><span class="math display">\[\mathbf{A} = \mathbf{L}\mathbf{U}\]</span></p>
<p>where <span class="math inline">\(\mathbf{L}\)</span> is a lower triangular matrix and <span class="math inline">\(\mathbf{U}\)</span> is an upper triangular matrix. The benefit of this decomposition is for solving a system of linear equations <span class="math inline">\(\mathbf{A}\mathbf{x} =\mathbf{b}\)</span> because they reduce the number of steps necessary in Gauss-Jordan elimination to invert the matrix. Hence, more computationally efficient.</p>
<p>LU decomposition only works on square matricies. But there are many other forms of decomposition used for solving systems of linear equations, or more commonly in the social sciences for <strong>dimension reduction</strong>.</p>
<div id="dimension-reduction" class="section level3 hasAnchor" number="4.8.1">
<h3><span class="header-section-number">4.8.1</span> Dimension reduction<a href="linear-algebra.html#dimension-reduction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Dimension reduction</strong> refers to decreasing the number of dimensions in your dataset. There are a couple reasons you might do this:</p>
<ol style="list-style-type: decimal">
<li>You want to visualize the data but you have a lot of variables. You could generate something like a scatterplot matrix, but once you have more than a handful of variables even these become difficult to interpret.</li>
<li>You want to use the variables in a supervised learning framework, but reduce the total number of predictors to make the estimation more efficient.</li>
</ol>
<p>In either case, the goal is to reduce the dimensionality of the data by identifying a smaller number of representative variables/vectors/columns that collectively explain most of the variability in the original dataset. There are several methods available for performing such a task. First we will examine an example of applying dimension reduction techniques to summarize roll-call voting in the United States.</p>
<div id="application-dw-nominate" class="section level4 hasAnchor" number="4.8.1.1">
<h4><span class="header-section-number">4.8.1.1</span> Application: DW-NOMINATE<a href="linear-algebra.html#application-dw-nominate" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In the 1990s, dimension reduction techniques revolutionized the study of U.S. legislative politics. Measuring the ideology of legislators prior to this point was difficult because there was no method for locating legislators along an ideological spectrum (liberal-conservative) in a manner that allowed comparisons over time. That is, how liberal was a Democrat in 1870 compared to a Democrat in 1995? Additionally, supposed you wanted to predict how a legislator would vote on a given bill. Roll-call votes record individual legislator behavior, so you could use past votes to predict future ones. But there have been tens of thousands of recorded votes over the course of the U.S. Congress. Even in a given term of Congress, the Senate may cast hundreds of recorded votes. But there are only 100 senators (at present), and you cannot estimate a regression model when your number of predictors <span class="math inline">\(p\)</span> is larger than your number of observations <span class="math inline">\(n\)</span>. We need some method for reducing the dimensionality of this data to a handful of variables which explain as much of the variation in roll-call voting as possible.</p>
<p><strong>Multidimensional scaling techniques</strong> can be used to perform this feat. The technical details of this specific application are beyond the scope of this class, but Keith Poole and Howard Rosenthal developed a specific procedure called <a href="http://Voteview.com/">NOMINATE</a> to reduce the dimensionality of the data. Rather than using <span class="math inline">\(p\)</span> predictors to explain or predict individual legislatorâs roll-call votes, where <span class="math inline">\(p\)</span> is the total number of roll-call votes in the recorded history of the U.S. Congress, Poole and Rosenthal examined the similarity of legislatorsâ votes in a given session of Congress and over time to identify two major dimensions to roll-call voting in the U.S. Congress. That is, roll-call votes in Congress can generally be explained by two variables that can be estimated for every past and present member of Congress. The two dimensions do not have any inherent substantive interpretation, but by graphically examining the two dimensions, it becomes clear that they represent two specific factors in legislative voting:</p>
<ol style="list-style-type: decimal">
<li>First dimension - political ideology. This dimension appears to represent political ideology on the liberal-conservative spectrum. Positive values on this dimension refer to increasingly conservative voting patterns, and negative values refer to increasingly liberal voting patterns.</li>
<li>Second dimension - âissue of the dayâ. This dimension appears to pick up on attitudes that are salient at different points in the nationâs history. They could be regional differences (Southern vs.Â non-Southern states), or attitudes towards specific policy issues (i.e.Â slavery).</li>
</ol>
<p>This data can be used for a wide range of research questions. For example, we could use it to assess the degree of polarization in the U.S. Congress over time:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:polarize-house"></span>
<img src="https://voteview.com/static/articles/party_polarization/voteview_house_party_means.png" alt="Source: [Polarization in Congress](https://voteview.com/articles/party_polarization)" width="90%" />
<p class="caption">
Figure 4.3: Source: <a href="https://voteview.com/articles/party_polarization">Polarization in Congress</a>
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:polarize-senate"></span>
<img src="https://voteview.com/static/articles/party_polarization/voteview_senate_party_means.png" alt="Source: [Polarization in Congress](https://voteview.com/articles/party_polarization)" width="90%" />
<p class="caption">
Figure 4.4: Source: <a href="https://voteview.com/articles/party_polarization">Polarization in Congress</a>
</p>
</div>
</div>
</div>
<div id="singular-value-decomposition" class="section level3 hasAnchor" number="4.8.2">
<h3><span class="header-section-number">4.8.2</span> Singular value decomposition<a href="linear-algebra.html#singular-value-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Singular-Value Decomposition</strong>, or SVD, is a matrix decomposition method for reducing a matrix to its constitutent parts in order to make subsequent matrix calculations simpler. Unlike LU decomposition, SVD works with any rectangular matrix (not just square matricies). Suppose <span class="math inline">\(\mathbf{M}\)</span> is an <span class="math inline">\(m \times n\)</span> matrix. There exists a factorization of the form</p>
<p><span class="math display">\[\mathbf{M} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^{*}\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\mathbf{U}\)</span> is an <span class="math inline">\(m \times n\)</span> matrix</li>
<li><span class="math inline">\(\boldsymbol{\Sigma}\)</span> is an <span class="math inline">\(n \times n\)</span> diagonal matrix</li>
<li><span class="math inline">\(\mathbf{V}^{*}\)</span> is the transpose of an <span class="math inline">\(n \times n\)</span> matrix</li>
</ul>
<p>The diagonal entries <span class="math inline">\(\sigma_i\)</span> of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> are known as the <strong>singular values</strong> of <span class="math inline">\(\mathbf{M}\)</span>. The columns of <span class="math inline">\(\mathbf{U}\)</span> are called the <strong>left-singular vectors</strong> of <span class="math inline">\(\mathbf{M}\)</span>, and the columns of V are called the <strong>right-singular vectors</strong> of <span class="math inline">\(\mathbf{M}\)</span>.</p>
<div id="image-compression" class="section level4 hasAnchor" number="4.8.2.1">
<h4><span class="header-section-number">4.8.2.1</span> Image compression<a href="linear-algebra.html#image-compression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Digital images can be compressed using this technique. The image is treated as a matrix of pixels with corresponding color values and is decomposed into smaller <strong>ranks</strong> (i.e.Â columns) that retain only the essential information that comprises the image.<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></p>
<p><img src="04-linear-algebra_files/figure-html/read-image-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>The picture of the lion can be stored as an 600 by 337 matrix, where each value is a number between 0 and 1 that indicates how white or black the pixel should appear.</p>
<pre><code>##       [,1]  [,2]  [,3]  [,4]  [,5]
## [1,] 0.361 0.369 0.381 0.393 0.403
## [2,] 0.365 0.373 0.385 0.397 0.407
## [3,] 0.369 0.377 0.389 0.399 0.411
## [4,] 0.377 0.385 0.395 0.407 0.420
## [5,] 0.388 0.391 0.403 0.416 0.424</code></pre>
<p>SVD of this matrix results in 3 new matricies:<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a></p>
<ul>
<li><p><span class="math inline">\(\mathbf{U}\)</span></p>
<pre><code>##         [,1]    [,2]     [,3]     [,4]     [,5]
## [1,] -0.0398 -0.0291 -0.02032 0.019709 -0.01329
## [2,] -0.0405 -0.0150 -0.00198 0.000273 -0.00208
## [3,] -0.0396 -0.0186 -0.01972 0.020905  0.01126
## [4,] -0.0390 -0.0264 -0.02890 0.039385  0.01012
## [5,] -0.0398 -0.0300 -0.03199 0.037500  0.00553</code></pre>
<p>Matrix size: <span class="math inline">\((600, 337)\)</span></p></li>
<li><p><span class="math inline">\(\boldsymbol{\Sigma}\)</span></p>
<pre><code>##      [,1] [,2] [,3] [,4] [,5]
## [1,]  193  0.0  0.0    0  0.0
## [2,]    0 29.2  0.0    0  0.0
## [3,]    0  0.0 16.2    0  0.0
## [4,]    0  0.0  0.0   15  0.0
## [5,]    0  0.0  0.0    0 12.2</code></pre>
<p>Length: <span class="math inline">\(337\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{V}^{*}\)</span></p>
<pre><code>##         [,1]    [,2]   [,3]   [,4]    [,5]
## [1,] -0.0556 0.00838 0.0211 0.0377 -0.0119
## [2,] -0.0558 0.00848 0.0179 0.0391 -0.0131
## [3,] -0.0560 0.00874 0.0138 0.0405 -0.0146
## [4,] -0.0561 0.00888 0.0114 0.0405 -0.0159
## [5,] -0.0561 0.00874 0.0102 0.0394 -0.0159</code></pre>
<p>Matrix size: <span class="math inline">\((337, 337)\)</span></p></li>
</ul>
</div>
<div id="interesting-properties-of-svd" class="section level4 hasAnchor" number="4.8.2.2">
<h4><span class="header-section-number">4.8.2.2</span> Interesting properties of SVD<a href="linear-algebra.html#interesting-properties-of-svd" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="recovering-the-data" class="section level5 hasAnchor" number="4.8.2.2.1">
<h5><span class="header-section-number">4.8.2.2.1</span> Recovering the data<a href="linear-algebra.html#recovering-the-data" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>We can recover the original matrix by multiplying the matricies back together:</p>
<p><span class="math display">\[\mathbf{M} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^{*}\]</span></p>
<pre><code>##       [,1]  [,2]  [,3]  [,4]  [,5]
## [1,] 0.361 0.369 0.381 0.393 0.403
## [2,] 0.365 0.373 0.385 0.397 0.407
## [3,] 0.369 0.377 0.389 0.399 0.411
## [4,] 0.377 0.385 0.395 0.407 0.420
## [5,] 0.388 0.391 0.403 0.416 0.424</code></pre>
</div>
<div id="reducing-the-data" class="section level5 hasAnchor" number="4.8.2.2.2">
<h5><span class="header-section-number">4.8.2.2.2</span> Reducing the data<a href="linear-algebra.html#reducing-the-data" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The next useful property of SVD is the values in the diagonal matrix <span class="math inline">\(\Sigma\)</span>. Notice how they are sorted in descending order.</p>
<pre><code>##   [1] 193.4417  29.1733  16.1600  14.9806  12.1708  11.3756  10.5788   8.9693
##   [9]   8.3404   7.6359   7.4752   6.8798   6.1244   5.9575   5.5327   5.3978
##  [17]   5.1953   4.8511   4.6521   4.6020   4.2501   4.1820   4.0820   4.0382
##  [25]   3.8938   3.8375   3.7173   3.5563   3.5273   3.4986   3.4396   3.4027
##  [33]   3.3417   3.2681   3.2025   3.1409   3.0671   3.0221   3.0124   2.9543
##  [41]   2.8912   2.8365   2.8076   2.7306   2.6768   2.6547   2.6008   2.5562
##  [49]   2.5353   2.5186   2.4892   2.4669   2.3997   2.3361   2.3274   2.2823
##  [57]   2.2424   2.2378   2.1923   2.1692   2.1122   2.0840   2.0704   2.0510
##  [65]   2.0241   2.0196   1.9849   1.9568   1.9305   1.9237   1.9052   1.8737
##  [73]   1.8433   1.8222   1.8107   1.7891   1.7699   1.7554   1.7195   1.7039
##  [81]   1.6870   1.6695   1.6453   1.6310   1.6101   1.5815   1.5727   1.5373
##  [89]   1.5198   1.5105   1.4861   1.4748   1.4609   1.4378   1.4321   1.4016
##  [97]   1.4001   1.3788   1.3624   1.3386   1.3301   1.3169   1.3057   1.2704
## [105]   1.2593   1.2419   1.2376   1.2065   1.1922   1.1825   1.1741   1.1584
## [113]   1.1405   1.1314   1.1157   1.1003   1.0921   1.0705   1.0602   1.0480
## [121]   1.0406   1.0314   1.0191   0.9983   0.9939   0.9919   0.9634   0.9500
## [129]   0.9434   0.9337   0.9213   0.9153   0.9044   0.8910   0.8777   0.8528
## [137]   0.8458   0.8419   0.8246   0.8196   0.8005   0.7967   0.7924   0.7866
## [145]   0.7734   0.7591   0.7564   0.7469   0.7365   0.7283   0.7198   0.7159
## [153]   0.7118   0.7009   0.6926   0.6874   0.6817   0.6634   0.6552   0.6517
## [161]   0.6493   0.6352   0.6184   0.6127   0.6073   0.6039   0.6014   0.5949
## [169]   0.5915   0.5810   0.5767   0.5627   0.5547   0.5456   0.5381   0.5351
## [177]   0.5310   0.5247   0.5211   0.5139   0.5025   0.4998   0.4966   0.4808
## [185]   0.4763   0.4725   0.4613   0.4552   0.4529   0.4471   0.4411   0.4374
## [193]   0.4326   0.4309   0.4232   0.4178   0.4152   0.4047   0.4005   0.3970
## [201]   0.3884   0.3795   0.3790   0.3770   0.3705   0.3690   0.3597   0.3535
## [209]   0.3506   0.3465   0.3434   0.3387   0.3341   0.3243   0.3201   0.3183
## [217]   0.3099   0.3073   0.3020   0.2980   0.2972   0.2953   0.2911   0.2826
## [225]   0.2787   0.2738   0.2705   0.2644   0.2584   0.2542   0.2533   0.2472
## [233]   0.2424   0.2397   0.2356   0.2320   0.2300   0.2268   0.2205   0.2187
## [241]   0.2160   0.2096   0.2077   0.1980   0.1961   0.1930   0.1895   0.1891
## [249]   0.1853   0.1814   0.1798   0.1772   0.1720   0.1704   0.1681   0.1658
## [257]   0.1650   0.1617   0.1539   0.1523   0.1483   0.1457   0.1436   0.1424
## [265]   0.1367   0.1360   0.1332   0.1304   0.1276   0.1265   0.1259   0.1232
## [273]   0.1201   0.1158   0.1119   0.1112   0.1079   0.1069   0.1044   0.1010
## [281]   0.0993   0.0980   0.0934   0.0905   0.0900   0.0878   0.0868   0.0847
## [289]   0.0838   0.0796   0.0763   0.0744   0.0733   0.0710   0.0682   0.0674
## [297]   0.0671   0.0637   0.0612   0.0595   0.0570   0.0556   0.0537   0.0501
## [305]   0.0485   0.0446   0.0435   0.0426   0.0401   0.0361   0.0354   0.0336
## [313]   0.0311   0.0295   0.0286   0.0257   0.0248   0.0238   0.0235   0.0233
## [321]   0.0224   0.0221   0.0218   0.0208   0.0203   0.0200   0.0195   0.0191
## [329]   0.0184   0.0181   0.0175   0.0174   0.0170   0.0162   0.0157   0.0155
## [337]   0.0152</code></pre>
<p>These tell us the relative importance of each column in <span class="math inline">\(\mathbf{U}\)</span> and <span class="math inline">\(\mathbf{V}^{*}\)</span>. When their values are multiplied by really small numbers (or even 0), then they do not contribute much information. In the original image we have 337 columns. What if we want to represent as much of the original information as possible, in a more compact form?</p>
<p><img src="04-linear-algebra_files/figure-html/svd-sigma-prop-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>The first column alone explains over 92.967% of the variation in the original matrix. What if we just used the first two columns to redraw the picture?</p>
<pre><code>##       [,1]  [,2]  [,3]  [,4]  [,5]
## [1,] 0.421 0.422 0.424 0.424 0.424
## [2,] 0.432 0.434 0.435 0.436 0.436
## [3,] 0.421 0.423 0.424 0.425 0.425
## [4,] 0.413 0.414 0.416 0.416 0.416
## [5,] 0.421 0.423 0.424 0.425 0.425</code></pre>
<p><img src="04-linear-algebra_files/figure-html/lions-rank1-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>Okay, that doesnât appear to be enough. What is the fewest we could get away with?</p>
<p><img src="04-linear-algebra_files/figure-html/lions-rank-all-1.gif" width="90%" style="display: block; margin: auto;" /></p>

<div class="rmdnote">
âRank 150â means retaining only the first 150 columns from all of matricies in the SVD.
</div>
<p>Rank 173 doesnât look too bad, and Rank 215 looks pretty indistinguishable from the original. The original matrix contained 202,200 different values in the matrix. If we used SVD to compress the image and only use the first 215 columns of each individual matrix, we would shrink the size of the image by 28%.</p>
</div>
</div>
</div>
<div id="principal-components-analysis" class="section level3 hasAnchor" number="4.8.3">
<h3><span class="header-section-number">4.8.3</span> Principal components analysis<a href="linear-algebra.html#principal-components-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Principal components analysis</strong> (PCA) is a basic technique for dimension reduction. The goal is to find a low-dimensional representation of the data that contains as much as possible of the variation. So for example, say your original dataset had 30 columns (i.e.Â variables, dimensions). We want to reduce the number of columns while still maintaining the overall structure of the matrix. This can be helpful for many reasons, including</p>
<ol style="list-style-type: decimal">
<li>Exploratory data analysis - visualize <span class="math inline">\(p\)</span> dimensions in a simple <span class="math inline">\(2\)</span> dimensional plot</li>
<li>Statistical learning - reduce the number of features/independent variables in a statistical learning model to improve efficiency or remove multicollinearity</li>
</ol>
<p>The PCA algorithm is implemented as:</p>
<ol style="list-style-type: decimal">
<li>Rescale each column to be mean 0 and standard deviation 1. This prevents variables with larger values and variances from dominating the projection.</li>
<li>Compute the covariance matrix <span class="math inline">\(\mathbf{S}\)</span>. Here <span class="math inline">\(\mathbf{X}\)</span> is a data matrix:
<span class="math display">\[\mathbf{S} = \dfrac{1}{N} \mathbf{X}&#39; \mathbf{X}\]</span></li>
<li>Compute the <span class="math inline">\(K\)</span> largest <strong>eigenvectors</strong> of <span class="math inline">\(\mathbf{S}\)</span>. These eigenvectors are the principal components of the dataset. Remember that every eigenvector has a corresponding <strong>eigenvalue</strong>. The eigenvector defines the direction of the line, and the eigenvalue tells you how much variance there is in the data in that direction (essentially how spread out the data is on that line).</li>
</ol>
<p>Computing the covariance matrix is expensive when <span class="math inline">\(\mathbf{X}\)</span> is very large or when <span class="math inline">\(\mathbf{X}\)</span> is very small. SVD can be used to make this process more efficient by computing SVD on the original matrix. <span class="math inline">\(\mathbf{V}^{*}\)</span> contains the <strong>principal directions</strong> (or the eigenvectors), the columns of <span class="math inline">\(\mathbf{U} \boldsymbol{\Sigma}\)</span> are <strong>principal components</strong> (scores) for each observation, and the values of the diagonal elements of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> are equivalent to the eigenvalues computed from <span class="math inline">\(\mathbf{S}\)</span> (amount of variance explained by the principal components).</p>
<p>The total number of principal components for a given <span class="math inline">\(n \times p\)</span> data set is <span class="math inline">\(\min(n,p)\)</span>, either the number of observations in the data or the number of variables in the data (whichever is smaller). Once we estimate the principal components, we can plot them against each other in order to produce a low-dimensional visualization of the data.</p>
<div id="example-usarrests" class="section level4 hasAnchor" number="4.8.3.1">
<h4><span class="header-section-number">4.8.3.1</span> Example: <code>USArrests</code><a href="linear-algebra.html#example-usarrests" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Letâs look at the use of PCA on the <code>USArrests</code> dataset, reproduced from <strong>An Introduction to Statistical Learning</strong>.</p>
<p><img src="04-linear-algebra_files/figure-html/usarrests-pc-scores-plot-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p><img src="04-linear-algebra_files/figure-html/usarrests-pc-scores-biplot-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>The principal component score vectors have length <span class="math inline">\(n=50\)</span> and the principal component loading vectors have length <span class="math inline">\(p=4\)</span> (in this data set, <span class="math inline">\(p &lt; n\)</span>). The biplot visualizes the relationship between the first two principal components for the dataset, including both the scores and the loading vectors. The first principal component places approximately equal weight on murder, assault, and rape. We can tell this because these vectorsâ length on the first principal component dimension are roughly the same, whereas the length for urban population is smaller. Conversely, the second principal component (the vertical axis) places more emphasis on urban population. Intuitively this makes sense because murder, assault, and rape are all measures of violent crime, and it makes sense that they should be correlated with one another (i.e.Â states with high murder rates are likely to have high rates of rape as well).</p>
<p>We can also interpret the plot for individual states based on their positions along the two dimensions. States with large positive values on the first principal component have high crime rates while states with large negative values have low crime rates; states with large positive values on the second principal component have high levels of urbanization while states with large negative values have low levels of urbanization.</p>
</div>
<div id="example-mnist-data-set" class="section level4 hasAnchor" number="4.8.3.2">
<h4><span class="header-section-number">4.8.3.2</span> Example: MNIST data set<a href="linear-algebra.html#example-mnist-data-set" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mnist"></span>
<img src="https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png" alt="MNIST digits" width="90%" />
<p class="caption">
Figure 4.5: MNIST digits
</p>
</div>
<p><strong>MNIST digits</strong> is a classic practice dataset for image classification. Each image is a standardized picture of a handwritten digit. We want to use the image to classify the digit as the actual number 0-9. We use the individual pixels and their intensity of black/white to generate these predictions. Rather than use all <span class="math inline">\(28 \times 28 = 784\)</span> individual pixels, we can use SVD/PCA to compress the data set to a smaller number of principal components that capture most of the variation in the rows/columns. To verify if this technique would work, we can visualize the observations along their first and second principal components. If those two components alone can distinguish between each of the ten possible digits, we should see unique clusters of observations in the scatterplot.</p>
<p><img src="04-linear-algebra_files/figure-html/pixels-pca-pc12-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p><img src="04-linear-algebra_files/figure-html/pixels-pca-pc12-facet-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="acknowledgements" class="section level2 unnumbered hasAnchor">
<h2>Acknowledgements<a href="linear-algebra.html#acknowledgements" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><a href="https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/">A Gentle Introduction to Singular-Value Decomposition for Machine Learning</a></li>
<li>Chapter 14.5, <span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-friedman2001" role="doc-biblioref">2001</a>)</span></li>
<li><a href="https://stats.idre.ucla.edu/r/codefragments/svd_demos/">Examples of SVD</a></li>
<li><a href="https://www.displayr.com/singular-value-decomposition-in-r/">Singular Value Decomposition (SVD): Tutorial Using Examples in R</a></li>
<li><a href="https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca">Relationship between SVD and PCA. How to use SVD to perform PCA?</a></li>
<li><a href="http://bigdata-madesimple.com/decoding-dimensionality-reduction-pca-and-svd/">Decoding Dimensionality Reduction, PCA and SVD</a></li>
</ul>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-francois2017" class="csl-entry">
Francois, Chollet. 2017. <em>Deep Learning with Python</em>. Manning Publications Company. <a href="https://www.manning.com/books/deep-learning-with-python">https://www.manning.com/books/deep-learning-with-python</a>.
</div>
<div id="ref-friedman2001" class="csl-entry">
Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. 10. Springer series in statistics New York.
</div>
<div id="ref-pemberton2015" class="csl-entry">
Pemberton, Malcolm, and Nicholas Rau. 2011. <em>Mathematics for Economists: An Introductory Textbook</em>. 4th edition. University of Toronto Press.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="10">
<li id="fn10"><p>Source: <a href="https://skymind.ai/wiki/neural-network">A Beginnerâs Guide to Neural Networks and Deep Learning</a><a href="linear-algebra.html#fnref10" class="footnote-back">â©ï¸</a></p></li>
<li id="fn11"><p>From Simon and Blume 6.2.1.<a href="linear-algebra.html#fnref11" class="footnote-back">â©ï¸</a></p></li>
<li id="fn12"><p><a href="http://www.aaronschlegel.com/image-compression-with-singular-value-decomposition/">See here for original example</a>.<a href="linear-algebra.html#fnref12" class="footnote-back">â©ï¸</a></p></li>
<li id="fn13"><p>First five rows and columns shown only.<a href="linear-algebra.html#fnref13" class="footnote-back">â©ï¸</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="critical-points.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multivariable-differentiation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/ysheng-uc/notes/main/04-linear-algebra.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
},
"theme": "readable",
"highlight": "pygment"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
