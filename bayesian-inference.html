<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Lecture 15 Bayesian inference | Computational Math Camp</title>
<meta name="description" content="Learning objectives Define the Bayesian philosophy and distinguish from frequentist inference Define core concepts for Bayesian methods Discuss the importance of simulation to estimate density...">
<meta name="generator" content="bookdown 0.35 with bs4_book()">
<meta property="og:title" content="Lecture 15 Bayesian inference | Computational Math Camp">
<meta property="og:type" content="book">
<meta property="og:description" content="Learning objectives Define the Bayesian philosophy and distinguish from frequentist inference Define core concepts for Bayesian methods Discuss the importance of simulation to estimate density...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Lecture 15 Bayesian inference | Computational Math Camp">
<meta name="twitter:description" content="Learning objectives Define the Bayesian philosophy and distinguish from frequentist inference Define core concepts for Bayesian methods Discuss the importance of simulation to estimate density...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.5.0/transition.js"></script><script src="libs/bs3compat-0.5.0/tabs.js"></script><script src="libs/bs3compat-0.5.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script><script src="libs/plotly-binding-4.10.0/plotly.js"></script><script src="libs/typedarray-0.1/typedarray.min.js"></script><link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet">
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script><link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script>
</head>
<body>
<p>
    \[
    \newcommand{\E}{\mathrm{E}}
    \newcommand{\Var}{\mathrm{Var}}
    \newcommand{\Cov}{\mathrm{Cov}}
    \newcommand{\se}{\text{se}}
    \newcommand{\sd}{\text{sd}}
    \newcommand{\Cor}{\mathrm{Cor}}
    \newcommand{\Lagr}{\mathcal{L}}
    \newcommand{\lagr}{\mathcal{l}}
    \]
    <script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style><style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style><link rel="stylesheet" href="bs4.css"></p>

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="Lecture Notes">Computational Math Camp</a>:
        <small class="text-muted">Lecture Notes</small>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled"><li><a class="" href="index.html"><span class="header-section-number">Lecture 15</span> Bayesian inference</a></li></ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="bayesian-inference" class="section level1" number="15">
<h1>
<span class="header-section-number">Lecture 15</span> Bayesian inference<a class="anchor" aria-label="anchor" href="#bayesian-inference"><i class="fas fa-link"></i></a>
</h1>
<div id="learning-objectives-13" class="section level2 unnumbered">
<h2>Learning objectives<a class="anchor" aria-label="anchor" href="#learning-objectives-13"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>Define the Bayesian philosophy and distinguish from frequentist inference</li>
<li>Define core concepts for Bayesian methods</li>
<li>Discuss the importance of simulation to estimate density functions</li>
<li>Assess methods for defining priors</li>
<li>Identify the strengths and weaknesses of Bayesian inference</li>
</ul>
</div>
<div id="supplemental-readings-13" class="section level2 unnumbered">
<h2>Supplemental readings<a class="anchor" aria-label="anchor" href="#supplemental-readings-13"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>Chapter 8.1 <span class="citation">Bertsekas and Tsitsiklis (<a href="references.html#ref-bertsekas2008">2008</a>)</span>
</li>
<li>
<span class="citation">Wasserman (<a href="references.html#ref-wasserman2013">2013</a>)</span>
<ul>
<li><a href="https://link-springer-com.proxy.uchicago.edu/content/pdf/10.1007%2F978-0-387-21736-9_11.pdf">Ch 11 - Bayesian Inference</a></li>
</ul>
</li>
</ul>
</div>
<div id="bayesian-philosophy" class="section level2" number="15.1">
<h2>
<span class="header-section-number">15.1</span> Bayesian philosophy<a class="anchor" aria-label="anchor" href="#bayesian-philosophy"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Frequentist methods</strong> are the major methods we’ve employed thus far. The frequentist point of view is based on the following postulates:</p>
<ol style="list-style-type: decimal">
<li>Probability refers to limiting relative frequencies. Probabilities are objective properties of the real world.</li>
<li>Parameters are fixed, unknown constants. Because they are not fluctuating, no useful probability statements can be made about parameters.</li>
<li>Statistical procedures should be designed to have well-defined long run frequency properties. For example, a 95% confidence interval should trap the true value of the parameter with limiting frequency at least 95 percent.</li>
</ol>
<p>An alternative approach to inference is called <strong>Bayesian inference</strong>. The Bayesian approach is based on the following postulates:</p>
<ol style="list-style-type: decimal">
<li>Probability describes degree of belief, not limiting frequency. As such, we can make probability statements about lots of things, not just data which are subject to random variables. For example, I might say “the probability that Donald Trump offended someone on November 25, 2018” is <span class="math inline">\(0.99\)</span>. This does not refer to any limiting frequency. It reflects my strength of belief that the proposition is true.</li>
<li>We can make probability statements about parameters, even though they are fixed constants.</li>
<li>We make inferences about a parameter <span class="math inline">\(\theta\)</span> by producing a probability distribution for <span class="math inline">\(\theta\)</span>. Inferences, such as point estimates and interval estimates, may then be extracted from this distribution.</li>
</ol>
</div>
<div id="bayes-theorem" class="section level2" number="15.2">
<h2>
<span class="header-section-number">15.2</span> Bayes’ theorem<a class="anchor" aria-label="anchor" href="#bayes-theorem"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Bayes’ theorem</strong> is a fundamental component of both probability and statistics and is central to understanding the differences between frequentist and Bayesian inference. For two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, Bayes’ theorem states that:</p>
<p><span class="math display">\[\Pr(B|A) = \frac{\Pr(A|B) \times \Pr(B)}{\Pr(A)}\]</span></p>
<p>Bayes’ rule tells us how to <strong>invert</strong> conditional probabilities. That is, to find <span class="math inline">\(\Pr(B|A)\)</span> from <span class="math inline">\(\Pr(A|B)\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-1" class="example"><strong>Example 15.1  (Coin tossing) </strong></span>Toss a coin 5 times. Let <span class="math inline">\(H_1 =\)</span> “first toss is heads” and let <span class="math inline">\(H_A =\)</span> “all 5 tosses are heads”. Therefore <span class="math inline">\(\Pr(H_1 | H_A) = 1\)</span> (if all five tosses are heads, then the first one must by definition also be heads) and <span class="math inline">\(\Pr(H_A | H_1) = \frac{1}{16}\)</span> (<span class="math inline">\(\frac{1}{2^4} = \frac{1}{16}\)</span>).</p>
<p>However we can also use Bayes’ theorem to calculate <span class="math inline">\(\Pr(H_1 | H_A)\)</span> using <span class="math inline">\(\Pr(H_A | H_1)\)</span>. The terms we need are:</p>
<ul>
<li><span class="math inline">\(\Pr(H_A | H_1) = \frac{1}{16}\)</span></li>
<li><span class="math inline">\(\Pr(H_1) = \frac{1}{2}\)</span></li>
<li><span class="math inline">\(\Pr(H_A) = \frac{1}{32}\)</span></li>
</ul>
<p>So,</p>
<p><span class="math display">\[\Pr(H_A | H_1) = \frac{\Pr(H_A | H_1) \times \Pr(H_1)}{\Pr(H_A)} = \frac{\frac{1}{16} \times \frac{1}{2}}{\frac{1}{32}} = 1\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-2" class="example"><strong>Example 15.2  (False positive fallacy) </strong></span>A test for a certain rare disease is assumed to be correct 95% of the time:</p>
<ul>
<li>If a person has the disease, then the test results are positive with probability <span class="math inline">\(0.95\)</span>
</li>
<li>If the person does not have the disease, then the test results are negative with probability <span class="math inline">\(0.95\)</span>
</li>
</ul>
<p>A random person drawn from a certain population has probability <span class="math inline">\(0.001\)</span> of having the disease. Given that the person just tested positive, what is the probability of having the disease?</p>
<ul>
<li><span class="math inline">\(A = {\text{person has the disease}}\)</span></li>
<li><span class="math inline">\(B = {\text{test result is positive for the disease}}\)</span></li>
<li><span class="math inline">\(\Pr(A) = 0.001\)</span></li>
<li><span class="math inline">\(\Pr(B | A) = 0.95\)</span></li>
<li><span class="math inline">\(\Pr(B | A = 0) = 0.05\)</span></li>
</ul>
<p><span class="math display">\[
\begin{align}
\Pr(\text{person has the disease} | \text{test is positive}) &amp;= \Pr(A|B) \\
&amp; = \frac{\Pr(A) \times \Pr(B|A)}{\Pr(B)} \\
&amp; = \frac{\Pr(A) \times \Pr(B|A)}{\Pr(A) \times \Pr(B|A) + \Pr(A = 0) \times(B | A = 0)} \\
&amp; = \frac{0.001 \times 0.95}{0.001 \times 0.95 + 0.999 \times 0.05} \\
&amp; = 0.0187
\end{align}
\]</span></p>
<p>Even though the test is fairly accurate, a person who has tested positive is still very unlikely (less than 2%) to have the disease. Because the base rate of the disease in the population is so low, the vast majority of people taking the test are healthy and even with an accurate test most of the positives will be healthy people.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Even worse, many physicians substantially miss the correct answer to this question.&lt;/p&gt;"><sup>1</sup></a></p>
</div>
</div>
<div id="bayesian-method" class="section level2" number="15.3">
<h2>
<span class="header-section-number">15.3</span> Bayesian method<a class="anchor" aria-label="anchor" href="#bayesian-method"><i class="fas fa-link"></i></a>
</h2>
<p>Bayesian inference is usually carried out in the following way:</p>
<ol style="list-style-type: decimal">
<li>Choose a probability density <span class="math inline">\(f(\theta)\)</span> – called the <strong>prior distribution</strong> – that expresses our beliefs about a parameter <span class="math inline">\(\theta\)</span> before we see any data.</li>
<li>Choose a statistical model <span class="math inline">\(f(x|\theta)\)</span> that reflects our beliefs about <span class="math inline">\(x\)</span> given <span class="math inline">\(\theta\)</span>. Note that we now write this as <span class="math inline">\(f(x|\theta)\)</span>, not <span class="math inline">\(f(x; \theta)\)</span>.</li>
<li>After observing data <span class="math inline">\(X_1, \ldots, X_n\)</span>, we update our beliefs and calculate the <strong>posterior distribution</strong> <span class="math inline">\(f(\theta | X_1, \ldots, X_n)\)</span>.</li>
</ol>
<p>To calculate the posterior, suppose that <span class="math inline">\(\theta\)</span> is discrete and that there is a single, discrete observation <span class="math inline">\(X\)</span>. We should use a capital letter to denote the parameter since we now treat it like a random variable, so let <span class="math inline">\(\Theta\)</span> denote the parameter. In this discrete setting,</p>
<p><span class="math display">\[
\begin{align}
\Pr(\Theta = \theta | X = x) &amp;= \frac{\Pr(X = x, \Theta = \theta)}{\Pr(X = x)} \\
&amp;= \frac{\Pr(X = x | \Theta = \theta) \Pr(\Theta = \theta)}{\sum_\theta \Pr (X = x| \Theta = \theta) \Pr (\Theta = \theta)}
\end{align}
\]</span></p>
<p>which is a basic application of Bayes’ theorem. The version for continuous variables is obtained using density functions</p>
<p><span class="math display">\[f(\theta | x) = \frac{f(x | \theta) f(\theta)}{\int f(x | \theta) f(\theta) d\theta}\]</span></p>
<p>If we have <span class="math inline">\(n\)</span> IID observations <span class="math inline">\(X_1, \ldots, X_n\)</span>, we replace <span class="math inline">\(f(x | \theta)\)</span> with</p>
<p><span class="math display">\[f(x_1, \ldots, x_n | \theta) = \prod_{i = 1}^n f(x_i | \theta) = \Lagr_n(\theta)\]</span></p>
<p>We will now write <span class="math inline">\(X^n\)</span> to mean <span class="math inline">\((X_1, \ldots, X_n)\)</span> and <span class="math inline">\(x^n\)</span> to mean <span class="math inline">\((x_1, \ldots, x_n)\)</span>. Now,</p>
<p><span class="math display">\[
\begin{align}
f(\theta | x^n) &amp;= \frac{f(x^n | \theta) f(\theta)}{\int f(x^n | \theta) f(\theta) d\theta} \\
&amp;= \frac{\Lagr_n(\theta) f(\theta)}{c_n} \\
&amp;\propto \Lagr_n(\theta) f(\theta)
\end{align}
\]</span></p>
<p>where</p>
<p><span class="math display">\[c_n = \int f(x^n | \theta) f(\theta) d\theta\]</span></p>
<p>is called the <strong>normalizing constant</strong>. We can summarize this by stating the <strong>posterior is proportional to Likelihood times Prior</strong>:</p>
<p><span class="math display">\[f(\theta | x^n) \propto \Lagr_n(\theta) f(\theta)\]</span></p>
<p>Since <span class="math inline">\(c_n\)</span> does not depend on <span class="math inline">\(\theta\)</span>, we can safely ignore it at this point, and in fact can recover the constant later on if we need it.</p>
<p>With the posterior distribution, we can get a point estimate by summarizing the center of the posterior. Typically this is the mean or mode of the posterior. The posterior mean is</p>
<p><span class="math display">\[\bar{\theta}_n = \int \theta f(\theta | x^n) d\theta = \frac{\int \theta \Lagr_n(\theta) f(\theta)}{\int \Lagr_n(\theta) f(\theta) d\theta}\]</span></p>
<p>We can also obtain a Bayesian interval estimate. We find <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> such that</p>
<p><span class="math display">\[\int_{-\infty}^a f(\theta | x^n) d\theta = \int_b^\infty f(\theta | x^n) d\theta = \frac{\alpha}{2}\]</span></p>
<p>Let <span class="math inline">\(C = (a,b)\)</span>. Then</p>
<p><span class="math display">\[\Pr (\theta \in C | x^n) = \int_a^b f(\theta | x^n) d\theta = 1 - \alpha\]</span></p>
<p>So <span class="math inline">\(C\)</span> is a <span class="math inline">\(1 - \alpha\)</span> <strong>posterior</strong> (or <strong>credible</strong>) <strong>interval</strong>.</p>
<div id="example-coin-tossing" class="section level3" number="15.3.1">
<h3>
<span class="header-section-number">15.3.1</span> Example: coin tossing<a class="anchor" aria-label="anchor" href="#example-coin-tossing"><i class="fas fa-link"></i></a>
</h3>
<p>There are three types of coins with different probabilities of landing heads when tossed.</p>
<ul>
<li>Type <span class="math inline">\(A\)</span> coins are fair, with <span class="math inline">\(p = 0.5\)</span> of heads</li>
<li>Type <span class="math inline">\(B\)</span> coins are bent, with <span class="math inline">\(p = 0.6\)</span> of heads</li>
<li>Type <span class="math inline">\(C\)</span> coins are bent, with <span class="math inline">\(p = 0.9\)</span> of heads</li>
</ul>
<p>Suppose I have a drawer containing 5 coins: 2 of type <span class="math inline">\(A\)</span>, 2 of type <span class="math inline">\(B\)</span>, and 1 of type <span class="math inline">\(C\)</span>. I reach into the drawer and pick a coin at random. Without showing you the coin I flip it once and get heads. What is the probability it is type <span class="math inline">\(A\)</span>? Type <span class="math inline">\(B\)</span>? Type <span class="math inline">\(C\)</span>?</p>
<div id="terminology" class="section level4" number="15.3.1.1">
<h4>
<span class="header-section-number">15.3.1.1</span> Terminology<a class="anchor" aria-label="anchor" href="#terminology"><i class="fas fa-link"></i></a>
</h4>
<p>Let <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(C\)</span> be the event the chosen coin was of the respective type. Let <span class="math inline">\(D\)</span> be the event that the toss is heads. The problem then asks us to find:</p>
<p><span class="math display">\[\Pr(A|D), \Pr(B|D), \Pr(C|D)\]</span></p>
<p>Before applying Bayes’ theorem, we need to define a few things:</p>
<ul>
<li><p><strong>Experiment</strong> - pick a coin from the drawer at random, flip it, and record the result</p></li>
<li><p><strong>Data</strong> - the result of the experiment. Here, <span class="math inline">\(D = \text{heads}\)</span>. <span class="math inline">\(D\)</span> is data that provides evidence for or against each hypothesis</p></li>
<li><p><strong>Hypotheses</strong> - we are testing three hypotheses: the coin is type <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, or <span class="math inline">\(C\)</span></p></li>
<li>
<p><strong>Prior probability</strong> - the probability of each hypothesis prior to tossing the coin (collecting data). Since the drawer has 2 coins of type <span class="math inline">\(A\)</span>, 2 of type <span class="math inline">\(B\)</span>, and 1 of type <span class="math inline">\(C\)</span>, we have:</p>
<p><span class="math display">\[\Pr(A) = 0.4, \Pr(B) = 0.4, \Pr(C) = 0.2\]</span></p>
</li>
<li>
<p><strong>Likelihood</strong> - the likelihood function is <span class="math inline">\(\Pr(D|H)\)</span>, the probability of the data assuming that the hypothesis is true. Most often we will consider the data as fixed and let the hypothesis vary. For example, <span class="math inline">\(\Pr(D|A) =\)</span> probability of heads if the coin is type <span class="math inline">\(A\)</span>. In our case, the likelihoods are:</p>
<p><span class="math display">\[\Pr(D|A) = 0.5, \Pr(D|B) = 0.6, \Pr(D|C) = 0.9\]</span></p>
<p>We can think of these as parameters for a series of Bernoulli distributions.</p>
</li>
<li>
<p><strong>Posterior probability</strong> - the probability (posterior to) of each hypothesis given the data from tossing the coin:</p>
<p><span class="math display">\[\Pr(A|D), \Pr(B|D), \Pr(C|D)\]</span></p>
<p>These posterior probabilities are what we want to find.</p>
</li>
</ul>
<p>We can now use Bayes’ theorem to compute each of the posterior probabilities. The theorem says:</p>
<p><span class="math display">\[\Pr(A|D) = \frac{\Pr(D|A) \times \Pr(A)}{\Pr(D)}\]</span>
<span class="math display">\[\Pr(B|D) = \frac{\Pr(D|B) \times \Pr(B)}{\Pr(D)}\]</span>
<span class="math display">\[\Pr(C|D) = \frac{\Pr(D|C) \times \Pr(C)}{\Pr(D)}\]</span></p>
<p><span class="math inline">\(\Pr(D)\)</span> can be computed using the law of total probability:</p>
<p><span class="math display">\[
\begin{align}
\Pr(D) &amp; = \Pr(D|A) \times \Pr(A) + \Pr(D|B) \times \Pr(B) + \Pr(D|C) \times \Pr(C) \\
&amp; = 0.5 \times 0.4 + 0.6 \times 0.4 + 0.9 \times 0.2 = 0.62
\end{align}
\]</span></p>
<p>So each of the posterior probabilities are:</p>
<p><span class="math display">\[\Pr(A|D) = \frac{\Pr(D|A) \times \Pr(A)}{\Pr(D)} = \frac{0.5 \times 0.4}{0.62} = \frac{0.2}{0.62}\]</span></p>
<p><span class="math display">\[\Pr(B|D) = \frac{\Pr(D|B) \times \Pr(B)}{\Pr(D)} = \frac{0.6 \times 0.4}{0.62} = \frac{0.24}{0.62}\]</span></p>
<p><span class="math display">\[\Pr(C|D) = \frac{\Pr(D|C) \times \Pr(C)}{\Pr(D)} = \frac{0.9 \times 0.2}{0.62} = \frac{0.18}{0.62}\]</span></p>
<p>Notice that the total probability <span class="math inline">\(\Pr(D)\)</span> is the same in each of the denominators and is the sum of the three numerators.</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="6%">
<col width="13%">
<col width="17%">
<col width="36%">
<col width="25%">
</colgroup>
<thead><tr class="header">
<th>hypothesis</th>
<th>prior</th>
<th>likelihood</th>
<th>Bayes numerator</th>
<th>posterior</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(H\)</span></td>
<td><span class="math inline">\(\Pr(H)\)</span></td>
<td><span class="math inline">\(\Pr(D\mid H)\)</span></td>
<td><span class="math inline">\(\Pr(D \mid H) \times \Pr(H)\)</span></td>
<td><span class="math inline">\(\Pr(H \mid D)\)</span></td>
</tr>
<tr class="even">
<td>A</td>
<td>0.4</td>
<td>0.5</td>
<td>0.2</td>
<td>0.3226</td>
</tr>
<tr class="odd">
<td>B</td>
<td>0.4</td>
<td>0.6</td>
<td>0.24</td>
<td>0.3871</td>
</tr>
<tr class="even">
<td>C</td>
<td>0.2</td>
<td>0.9</td>
<td>0.18</td>
<td>0.2903</td>
</tr>
<tr class="odd">
<td>total</td>
<td>1</td>
<td></td>
<td>0.62</td>
<td>1</td>
</tr>
</tbody>
</table></div>
<p>The <strong>Bayes numerator</strong> is the product of the prior and the likelihood. The posterior probability is obtained by dividing the Bayes numerator by <span class="math inline">\(\Pr(D) = 0.625\)</span>.</p>
<p>The process of going from the prior probability <span class="math inline">\(\Pr(H)\)</span> to the posterior <span class="math inline">\(\Pr(H|D)\)</span> is called <strong>Bayesian updating</strong>. Bayesian updating uses the data to alter our understanding of the probability of each hypothesis.</p>
</div>
<div id="things-to-notice" class="section level4" number="15.3.1.2">
<h4>
<span class="header-section-number">15.3.1.2</span> Things to notice<a class="anchor" aria-label="anchor" href="#things-to-notice"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>The posterior probabilities for each hypothesis are in the last column. Coin <span class="math inline">\(B\)</span> is the most probable, even with the decrease from the prior to the posterior. <span class="math inline">\(C\)</span> has increased from 0.2 to 0.29.</li>
<li>The Bayes numerator determines the posterior probability. To compute the posterior probability, simply rescale the Bayes numerator so that it sums to 1.</li>
<li>If all we care about is finding the most likely hypothesis, the Bayes numerator works as well as the normalized posterior.</li>
<li>The posterior probability represents the outcome of a tug-of-war between the likelihood and the prior. When calculating the posterior, a large prior may be deflated by a small likelihood, and a small prior may be inflated by a large likelihood.</li>
</ol>
<p>Therefore we can express Bayes’ theorem as:</p>
<p><span class="math display">\[\Pr(\text{hypothesis}| \text{data}) = \frac{\Pr(\text{data} | \text{hypothesis}) \times \Pr(\text{hypothesis})}{\Pr(\text{data})}\]</span></p>
<p><span class="math display">\[\Pr(H|D) = \frac{\Pr(D | H) \times \Pr(H)}{\Pr(D)}\]</span></p>
<p>With the data fixed, the denominator <span class="math inline">\(\Pr(D)\)</span> just serves to normalize the total posterior probability to 1. So we could express Bayes’ theorem as a statement about the proportionality of two functions of <span class="math inline">\(H\)</span>:</p>
<p><span class="math display">\[\Pr(\text{hypothesis}| \text{data}) \propto \Pr(\text{data} | \text{hypothesis}) \times \Pr(\text{hypothesis})\]</span>
<span class="math display">\[\text{Posterior} \propto \text{Likelihood} \times \text{Prior}\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-3" class="example"><strong>Example 15.3  (Bernoulli random variable) </strong></span>Let <span class="math inline">\(X_1, \ldots, X_n \sim \text{Bernoulli} (p)\)</span>. Suppose we take the uniform distribution <span class="math inline">\(f(p) = 1\)</span> as a prior. By Bayes’ theorem, the posterior has the form</p>
<p><span class="math display">\[
\begin{align}
f(p | x^n) &amp;\propto f(p) \Lagr_n(p) \\
&amp;= p^s (1 - p)^{n - s} \\
&amp;= p^{s + 1 - 1} (1 - p)^{n - s + 1 - 1}
\end{align}
\]</span></p>
<p>where <span class="math inline">\(s = \sum_{i=1}^n x_i\)</span> is the number of successes. Importantly, a random variable has a Beta distribution with parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> if its density is</p>
<p><span class="math display">\[f(p; \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)}p^{\alpha - 1} (1 - p)^{\beta - 1}\]</span></p>
<div class="inline-figure"><img src="14-bayesian-inference_files/figure-html/beta-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>We can see that the posterior for <span class="math inline">\(p\)</span> is a Beta distribution with parameters <span class="math inline">\(s + 1\)</span> and <span class="math inline">\(n - s + 1\)</span>. That is,</p>
<p><span class="math display">\[f(p | x^n) = \frac{\Gamma(n + 2)}{\Gamma(s + 1) \Gamma(n - s + 1)}p^{(s + 1) - 1} (1 - p)^{(n - s + 1) - 1}\]</span></p>
<p>We write this as</p>
<p><span class="math display">\[p | x^n \sim \text{Beta} (s + 1, n - s + 1)\]</span></p>
<p>Notice that we have figured out the normalizing constant <span class="math inline">\(c_n = \frac{\Gamma(n + 2)}{\Gamma(s + 1) \Gamma(n - s + 1)}\)</span> without actually doing the integral <span class="math inline">\(\int \Lagr_n(p) f(p) dp\)</span>. The mean of a <span class="math inline">\(\text{Beta}(\alpha, \beta)\)</span> distribution is <span class="math inline">\(\frac{\alpha}{\alpha + \beta}\)</span>, so the Bayes estimator is</p>
<p><span class="math display">\[\bar{p} = \frac{s + 1}{n + 2}\]</span></p>
<p>We can rewrite the estimator as</p>
<p><span class="math display">\[\bar{p} = \lambda_n \hat{p} + (1 - \lambda_n) \tilde{p}\]</span></p>
<p>where <span class="math inline">\(\hat{p} = \frac{s}{n}\)</span> is the MLE, <span class="math inline">\(\tilde{p} = \frac{1}{2}\)</span> is the prior mean, and <span class="math inline">\(\lambda_n = \frac{n}{n + 2} \approx 1\)</span>. So we can think of the MLE as the estimate for <span class="math inline">\(p\)</span> with a flat prior. If we have a non-flat prior, then our estimate <span class="math inline">\(\bar{p}\)</span> is a weighted average between the prior and the MLE.</p>
<p>A 95% credible interval can be obtained by numerically finding <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> such that <span class="math inline">\(\int_a^b f(p | x^n) dp = 0.95\)</span>.</p>
<p>Suppose that instead of a uniform prior, we use the prior <span class="math inline">\(p \sim \text{Beta} (\alpha, \beta)\)</span>. If we repeat the calculations from before, we see that <span class="math inline">\(p | x^n \sim \text{Beta} (\alpha + s, \beta + n - s)\)</span>. The flat prior is the special case with <span class="math inline">\(\alpha = \beta = 1\)</span>. The posterior mean is</p>
<p><span class="math display">\[\bar{p} = \frac{\alpha + s}{\alpha + \beta + n} = \left( \frac{n}{\alpha + \beta + n} \right) \hat{p} + \left( \frac{\alpha + \beta}{\alpha + \beta + n} \right) p_0\]</span></p>
<p>where <span class="math inline">\(p_0 = \frac{\alpha}{\alpha + \beta}\)</span> is the prior mean.</p>
</div>
</div>
</div>
</div>
<div id="updating-your-prior-beliefs" class="section level2" number="15.4">
<h2>
<span class="header-section-number">15.4</span> Updating your prior beliefs<a class="anchor" aria-label="anchor" href="#updating-your-prior-beliefs"><i class="fas fa-link"></i></a>
</h2>
<p>In life we continually update our beliefs with each new experience of the world. In Bayesian inference, today’s posterior is tomorrow’s prior.</p>
<div class="example">
<p><span id="exm:unlabeled-div-4" class="example"><strong>Example 15.4  (September 11, 2001) </strong></span>Consider the September 11th attacks in New York City.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;I’m currently having trouble remembering the inspiration for this example. I think it came from &lt;strong&gt;The Signal and the Noise&lt;/strong&gt; by Nate Silver.&lt;/p&gt;"><sup>2</sup></a> Say that before the first plane hit, our estimate of the probability of a terror attack on tall buildings in Manhattan was just 1 in 20,000, or <span class="math inline">\(0.00005\)</span>. But we also assign a low probability to a plane hitting the World Trade Center by accident: 1 in 12,500 on any given day.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Based on historical records of just two accidents involving planes hitting buildings in New York City from the 1940s-9/10/2011.&lt;/p&gt;"><sup>3</sup></a> Consider the use of Bayes’ theorem in this instance. What is the probability of terrorists crashing planes into Manhattan skyscrapers given the first plane hitting the World Trade Center?</p>
<ul>
<li>Our initial estimate of how likely it is that terrorists would crash planes into Manhattan skyscrapers is <span class="math inline">\(\Pr(\text{Terror attack}) = 0.00005\)</span>
</li>
<li>Probability of plane hitting if terrorists are attacking Manhattan is <span class="math inline">\(\Pr(\text{Plane hits the WTC} | \text{Terror attack}) = 1\)</span>
</li>
<li>Probability of plane hitting if terrorists are not attacking Manhattan skyscrapers (i.e. an accident) is <span class="math inline">\(\Pr(\text{Plane hits the WTC} | \text{No terror attack}) = 0.00008\)</span>
</li>
</ul>
<p>Our posterior probability of a terror attack, given the first plane hitting the world trade center, is:</p>
<ul>
<li>
<span class="math inline">\(A =\)</span> terror attack</li>
<li>
<span class="math inline">\(B =\)</span> plane hitting the World Trade Center</li>
<li>
<span class="math inline">\(\Pr(A) = 0.00005 =\)</span> probability that terrorists would crash a plane into the World Trade Center</li>
<li>
<span class="math inline">\(\Pr(A^C) = 0.99995 =\)</span> probability that terrorists would not crash a plane into the World Trade Center</li>
<li>
<span class="math inline">\(\Pr(B|A) = 1 =\)</span> probability of a plane crashing into the World Trade Center if terrorists are attacking the World Trade Center</li>
<li>
<span class="math inline">\(\Pr(B|A^C) = 0.00008 =\)</span> probability of a plane hitting if terrorists are not attacking the World Trade Center (i.e. an accident)</li>
</ul>
<p><span class="math display">\[
\begin{align}
\Pr(A|B) &amp;= \frac{\Pr(A) \times \Pr(B|A)}{\Pr(B)} \\
&amp;= \frac{\Pr(A) \times \Pr(B|A)}{ \Pr(A) \times \Pr(B|A) + \Pr(A^C) \times \Pr(B| A^C)} \\
&amp; = \frac{0.00005 \times 1}{0.00005 \times 1 + 0.99995 \times 0.00008} \\
&amp; = 0.385
\end{align}
\]</span></p>
<p>We would now estimate a posterior probability of a 38% chance of a terrorist attack on the World Trade Center. But we can continuously update this posterior probability as new data presents itself.</p>
<p><span class="math display">\[
\begin{align}
\Pr(A|B) &amp;= \frac{\Pr(A) \times \Pr(B|A)}{\Pr(B)} \\
&amp;= \frac{\Pr(A) \times \Pr(B|A)}{ \Pr(A) \times \Pr(B|A) + \Pr(A^C) \times \Pr(B| A^C)} \\
&amp; = \frac{0.385 \times 1}{0.385 \times 1 + 0.615 \times 0.00008} \\
&amp; \approx .9998
\end{align}
\]</span></p>
</div>
</div>
<div id="simulation" class="section level2" number="15.5">
<h2>
<span class="header-section-number">15.5</span> Simulation<a class="anchor" aria-label="anchor" href="#simulation"><i class="fas fa-link"></i></a>
</h2>
<p>The posterior can often be approximated by simulation. Suppose we draw <span class="math inline">\(\theta_1, \ldots, \theta_B \sim p(\theta | x^n)\)</span>. Then a histogram of <span class="math inline">\(\theta_1, \ldots, \theta_B\)</span> approximates the posterior density <span class="math inline">\(p(\theta | x^n)\)</span>. An approximation to the posterior mean <span class="math inline">\(\bar{\theta}_n = \E (\theta | x^n)\)</span> is <span class="math inline">\(\frac{\sum_{j=1}^B \theta_j}{B}\)</span>. The posterior <span class="math inline">\(1 - \alpha\)</span> interval can be approximated by <span class="math inline">\((\theta_{\alpha / 2}, \theta_{1 - \alpha /2})\)</span> where <span class="math inline">\(\theta_{\alpha / 2}\)</span> is the <span class="math inline">\(\alpha / 2\)</span> sample quantile of <span class="math inline">\(\theta_1, \ldots, \theta_B\)</span>.</p>
<p>Once we have a sample <span class="math inline">\(\theta_1, \ldots, \theta_B\)</span> from <span class="math inline">\(f(\theta | x^n)\)</span>, let <span class="math inline">\(\tau_i = g(\theta_i)\)</span>. Then <span class="math inline">\(\tau_1, \ldots, \tau_B\)</span> is a sample from <span class="math inline">\(f(\tau | x^n)\)</span>. This avoids the need to do any analytic calculations, especially when <span class="math inline">\(f(\theta | x^n)\)</span> is an especially complex function.</p>
<div class="example">
<p><span id="exm:unlabeled-div-5" class="example"><strong>Example 15.5  (Bernoulli random variable) </strong></span>Let <span class="math inline">\(X_1, \ldots, X_n \sim \text{Bernoulli} (p)\)</span> and <span class="math inline">\(f(p) = 1\)</span> so that <span class="math inline">\(p | X^n \sim \text{Beta} (s + 1, n - s + 1)\)</span> with <span class="math inline">\(s = \sum_{i=1}^n x_i\)</span>. Let <span class="math inline">\(\psi = \log \left( \frac{p}{1 - p} \right)\)</span> (i.e. the log-odds). If we wanted to calculate the PMF and CDF of <span class="math inline">\(\psi | x^n\)</span>, we could do a lot of calculus and analytic math to solve for these equations.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;See example 11.3 in Wasserman.&lt;/p&gt;"><sup>4</sup></a> Alternatively, we can approximate the posterior for <span class="math inline">\(\psi\)</span> without doing any calculus.</p>
<ol style="list-style-type: decimal">
<li>Draw <span class="math inline">\(P_1, \ldots, P_B \sim \text{Beta} (s + 1, n - s + 1)\)</span>.</li>
<li>Let <span class="math inline">\(\psi_i = \log \left( \frac{P_i}{1 - P_i} \right)\)</span>, for <span class="math inline">\(i = 1, \ldots, B\)</span>
</li>
</ol>
<p>Now <span class="math inline">\(\psi_1, \ldots, \psi_B\)</span> are IID draws from <span class="math inline">\(h(\psi | x^n)\)</span>. A histogram of these values provides an estimate of <span class="math inline">\(h(\psi | x^n)\)</span>.</p>
</div>
</div>
<div id="priors" class="section level2" number="15.6">
<h2>
<span class="header-section-number">15.6</span> Priors<a class="anchor" aria-label="anchor" href="#priors"><i class="fas fa-link"></i></a>
</h2>
<p>To employ Bayesian inference, one requires a prior. Where do you get the prior <span class="math inline">\(f(\theta)\)</span>? One approach is to use a <strong>subjective</strong> prior based on your subjective opinion about <span class="math inline">\(\theta\)</span> before you collect any data. This may be possible, but is impractical for many complicated problems (especially when there are many parameters). Some would argue this approach is also not “scientific” because our inferences should be as objective as possible.</p>
<p>An alternative approach is to define some sort of <strong>noninformative prior</strong>. One obvious choice is to use a flat prior <span class="math inline">\(f(\theta) \propto\)</span> constant. In the example earlier, taking <span class="math inline">\(f(p) = 1\)</span> leads to <span class="math inline">\(p | X^n \sim \text{Beta} (s + 1, n - s + 1)\)</span> which seems reasonable. But unfettered use of flat priors raises some questions.</p>
<div id="improper-priors" class="section level3" number="15.6.1">
<h3>
<span class="header-section-number">15.6.1</span> Improper priors<a class="anchor" aria-label="anchor" href="#improper-priors"><i class="fas fa-link"></i></a>
</h3>
<p>Let <span class="math inline">\(X \sim N(\theta, \sigma^2)\)</span> with <span class="math inline">\(\sigma\)</span> known. Suppose we adopt a flat prior <span class="math inline">\(f(\theta) \propto c\)</span> where <span class="math inline">\(c &gt; 0\)</span> is a constant. Note that <span class="math inline">\(\int f(\theta) d\theta = \infty\)</span>, so this is not a probability density in the usual sense (otherwise it would integrate to 1). Such a prior is called an <strong>improper prior</strong>. However, we can still carry out Bayes’ theorem and compute the posterior density by multiplying the prior and the likelihood:</p>
<p><span class="math display">\[f(\theta) \propto \Lagr_n(\theta) f(\theta) = \Lagr_n(\theta)\]</span></p>
<p>This gives <span class="math inline">\(\theta | X^n \sim N(\bar{X}, \sigma^2 / n)\)</span> and the resulting point and interval estimators agree exactly with their frequentist counterparts. In general, improper priors are not a problem as long as the resulting posterior is a well-defined probability distribution.</p>
</div>
<div id="flat-priors-are-not-invariant" class="section level3" number="15.6.2">
<h3>
<span class="header-section-number">15.6.2</span> Flat priors are not invariant<a class="anchor" aria-label="anchor" href="#flat-priors-are-not-invariant"><i class="fas fa-link"></i></a>
</h3>
<p>Let <span class="math inline">\(X \sim \text{Bernoulli} (p)\)</span> and suppose we use the flat prior <span class="math inline">\(f(p) = 1\)</span>. This flat prior represents our lack of knowledge about <span class="math inline">\(p\)</span> before the experiment. Now let <span class="math inline">\(\psi = \log(p / (1 - p))\)</span>. This is a transformation of <span class="math inline">\(p\)</span> and we can compute the resulting distribution for <span class="math inline">\(\psi\)</span></p>
<p><span class="math display">\[f_\Psi (\psi) = \frac{e^\psi}{(1 + e^\psi)^2}\]</span></p>
<p>which is not flat. But if we are ignorant of <span class="math inline">\(p\)</span>, then we are also ignorant about <span class="math inline">\(\psi\)</span> so we should use a flat prior for <span class="math inline">\(\psi\)</span>. This is a contradiction. In short, the notion of a flat prior is not well defined because a flat prior on a parameter does not imply a flat prior on a transformed version of the parameter. Flat priors are not <strong>transformation invariant</strong>.</p>
</div>
</div>
<div id="multiparameter-problems" class="section level2" number="15.7">
<h2>
<span class="header-section-number">15.7</span> Multiparameter problems<a class="anchor" aria-label="anchor" href="#multiparameter-problems"><i class="fas fa-link"></i></a>
</h2>
<p>Suppose that <span class="math inline">\(\theta = (\theta_1, \ldots, \theta_p)\)</span>. The posterior density is given by</p>
<p><span class="math display">\[f(\theta | x^n) \propto \Lagr_n(\theta) f(\theta)\]</span></p>
<p>However, now we need to consider how to extract inferences about one parameter. The key is to find the <strong>marginal posterior density</strong> for the parameter of interest. Suppose we want to make inferences about <span class="math inline">\(\theta_1\)</span>. The marginal posterior for <span class="math inline">\(\theta_1\)</span> is</p>
<p><span class="math display">\[f(\theta_1 | x^n) = \int \cdots \int f(\theta_1, \ldots, \theta_p | x^n) d\theta_2 \cdots d\theta_p\]</span></p>
<p>Essentially, we calculate the integral of the function over all parameters except <span class="math inline">\(\theta_1\)</span>. If there are two parameters <span class="math inline">\((\theta_1, \theta_2)\)</span>, we integrate with respect to <span class="math inline">\(\theta_2\)</span>. As the number of parameters increase, this operation gets extremely tricky (if not impossible) to solve analytically. Instead, simulation can be used to approximate by drawing randomly from the posterior</p>
<p><span class="math display">\[\theta^1, \ldots, \theta^B \sim f(\theta | x^n)\]</span></p>
<p>where the superscripts index the different draws. Each <span class="math inline">\(\theta^j\)</span> is a vector <span class="math inline">\(\theta^j = (\theta_1^j, \ldots, \theta_p^j)\)</span>. Now collect together the first component of each draw</p>
<p><span class="math display">\[\theta_1^1, \ldots, \theta_1^B\]</span></p>
<p>These are a sample from <span class="math inline">\(f(\theta_1 | x^n)\)</span> and we have avoided doing any integrals.</p>
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>Example 15.6  (Comparing two binomials) </strong></span>Suppose we have <span class="math inline">\(n_1\)</span> control patients and <span class="math inline">\(n_2\)</span> treatment patients and that <span class="math inline">\(X_1\)</span> control patients survive while <span class="math inline">\(X_2\)</span> treatment patients survive. We want to estimate <span class="math inline">\(\tau = g(p_1, p_2) = p_2 - p_1\)</span>. Then,</p>
<p><span class="math display">\[X_1 \sim \text{Binomial} (n_1, p_1) \, \text{and} \, X_2 \sim \text{Binomial} (n_2, p_2)\]</span></p>
<p>If <span class="math inline">\(f(p_1, p_2) = 1\)</span>, the posterior is</p>
<p><span class="math display">\[f(p_1, p_2 | x_1, x_2) \propto p_1^{x_1} (1 - p_1)^{n_1 - x_1} p_2^{x_2} (1 - p_2)^{n_2 - x_2}\]</span></p>
<p>Notice that</p>
<p><span class="math display">\[f(p_1, p_2 | x_1, x_2) = f(p_1 | x_1) f(p_2 | x_2)\]</span></p>
<p>where</p>
<p><span class="math display">\[f(p_1 | x_1) \propto p_1^{x_1} (1 - p_1)^{n_1 - x_1} \, \text{and} \, f(p_2 | x_2) \propto p_2^{x_2} (1 - p_2)^{n_2 - x_2}\]</span></p>
<p>which implies that <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> are independent under the posterior. Also</p>
<p><span class="math display">\[
\begin{align}
p_1 | x_1 &amp;\sim \text{Beta} (x_1 + 1, n_1 - x_1 + 1) \\
p_2 | x_2 &amp;\sim \text{Beta} (x_2 + 1, n_2 - x_2 + 1)
\end{align}
\]</span></p>
<p>If we simulate</p>
<p><span class="math display">\[
\begin{align}
P_{1,1}, \ldots, P_{1,B} &amp;\sim \text{Beta} (x_1 + 1, n_1 - x_1 + 1) \\
P_{2,1}, \ldots, P_{2,B} &amp;\sim \text{Beta} (x_2 + 1, n_2 - x_2 + 1)
\end{align}
\]</span></p>
<p>Then <span class="math inline">\(\tau_b = P_{2,b} - P_{1,b}, \, b = 1, \ldots, B\)</span> is a sample from <span class="math inline">\(f(\tau | x_1, x_2)\)</span>.</p>
</div>
</div>
<div id="critiques-and-defenses-of-bayesian-inference" class="section level2" number="15.8">
<h2>
<span class="header-section-number">15.8</span> Critiques and defenses of Bayesian inference<a class="anchor" aria-label="anchor" href="#critiques-and-defenses-of-bayesian-inference"><i class="fas fa-link"></i></a>
</h2>
<div id="critique-of-bayesian-inference" class="section level3" number="15.8.1">
<h3>
<span class="header-section-number">15.8.1</span> Critique of Bayesian inference<a class="anchor" aria-label="anchor" href="#critique-of-bayesian-inference"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>The subjective prior is subjective. There is no single method for choosing a prior, so different (well-intentioned) people will produce different priors and therefore arrive at different posteriors and conclusions. If you accept the premise of subjective priors, you still need good information to create a well-defined prior distribution.</li>
<li>Philosophically, some object to assigning probabilities to hypotheses as hypotheses do not constitute outcomes of repeatable experiments in which one can measure long-term frequency. Rather, a hypothesis is either true or false, regardless of whether one knows which is the case.
<ul>
<li>A coin is either fair or unfair</li>
<li>Treatment 1 is either better or worse than treatment 2</li>
<li>The sun will or will not come up tomorrow</li>
<li>I will either win or not win the lottery</li>
</ul>
</li>
<li>For many parametric models with large samples, Bayesian and frequentist methods give approximately the same inferences. Since frequentist methods are historically more common and easier to estimate, there is no reason to go through the steps of Bayesian inference.</li>
<li>Bayesian inference depends entirely on the likelihood function. In high dimensional and nonparametric methods, the likelihood function may not yield accurate inferences.</li>
</ol>
</div>
<div id="defense-of-bayesian-inference" class="section level3" number="15.8.2">
<h3>
<span class="header-section-number">15.8.2</span> Defense of Bayesian inference<a class="anchor" aria-label="anchor" href="#defense-of-bayesian-inference"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>The probability of hypotheses is exactly what we need to make decisions. When the doctor tells me a screening test came back positive for a disease, what I really want to know is the probability of the hypothesis “I’m sick”.</li>
<li>Bayes’ theorem is logically rigorous (once we obtain a prior).</li>
<li>By testing different priors we can see how sensitive our results are to the choice of prior.</li>
<li>It is easy to communicate a result framed in terms of probabilities of hypotheses (try explaining the result of a null hypothesis test to a layperson).</li>
<li>Priors can be defended based on the assumptions made to arrive at it.</li>
<li>Evidence derived from the data is independent of notions about “data more extreme” that depend on the exact experimental setup.</li>
<li>Data can be used as it comes in. We don’t have to wait for every contingency to be planned for ahead of time.</li>
</ol>
</div>
</div>
<div id="acknowledgements-1" class="section level2 unnumbered">
<h2>Acknowledgements<a class="anchor" aria-label="anchor" href="#acknowledgements-1"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>Material drawn from <a href="https://link-springer-com.proxy.uchicago.edu/book/10.1007%2F978-0-387-21736-9"><strong>All of Statistics</strong></a> by Larry Wasserman</li>
</ul>
</div>
</div>

  <div class="chapter-nav">
<div class="empty"></div>
<div class="empty"></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <div id="book-on-this-page"></div>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Computational Math Camp</strong>: Lecture Notes" was written by . It was last built on 2022-09-05.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
