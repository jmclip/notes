<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Lecture 14 Bayesian inference | Computational Math Camp</title>
  <meta name="description" content="Contains lecture notes for the 2022 Computational Math Camp." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="Lecture 14 Bayesian inference | Computational Math Camp" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Contains lecture notes for the 2022 Computational Math Camp." />
  <meta name="github-repo" content="math-camp/notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Lecture 14 Bayesian inference | Computational Math Camp" />
  
  <meta name="twitter:description" content="Contains lecture notes for the 2022 Computational Math Camp." />
  

<meta name="author" content="Yanyan Sheng" />


<meta name="date" content="2022-08-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mle-ols.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script>
\[
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\se}{\text{se}}
\newcommand{\sd}{\text{sd}}
\newcommand{\Cor}{\mathrm{Cor}}
\newcommand{\Lagr}{\mathcal{L}}
\newcommand{\lagr}{\mathcal{l}}
\]


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Overview</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#meeting-information"><i class="fa fa-check"></i>Meeting information</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#instructional-staff"><i class="fa fa-check"></i>Instructional staff</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#teaching-assistants"><i class="fa fa-check"></i>Teaching assistants</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-description"><i class="fa fa-check"></i>Course description</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-should-take-this-course"><i class="fa fa-check"></i>Who should take this course</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#grades"><i class="fa fa-check"></i>Grades</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#disability-services"><i class="fa fa-check"></i>Disability services</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#core-texts"><i class="fa fa-check"></i>Core texts</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-format"><i class="fa fa-check"></i>Course format</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#problem-sets"><i class="fa fa-check"></i>Problem sets</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-schedule"><i class="fa fa-check"></i>Course schedule</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="sets-functions.html"><a href="sets-functions.html"><i class="fa fa-check"></i><b>1</b> Linear equations, inequalities, sets and functions, quadratics, and logarithms</a>
<ul>
<li class="chapter" data-level="" data-path="sets-functions.html"><a href="sets-functions.html#learning-objectives"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="sets-functions.html"><a href="sets-functions.html#supplemental-readings"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="1.1" data-path="sets-functions.html"><a href="sets-functions.html#what-is-computational-social-science"><i class="fa fa-check"></i><b>1.1</b> What is computational social science?</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="sets-functions.html"><a href="sets-functions.html#disciplines-within-social-science"><i class="fa fa-check"></i><b>1.1.1</b> Disciplines within social science</a></li>
<li class="chapter" data-level="1.1.2" data-path="sets-functions.html"><a href="sets-functions.html#computational-social-science"><i class="fa fa-check"></i><b>1.1.2</b> Computational social science</a></li>
<li class="chapter" data-level="1.1.3" data-path="sets-functions.html"><a href="sets-functions.html#acquiring-css-skills"><i class="fa fa-check"></i><b>1.1.3</b> Acquiring CSS skills</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="sets-functions.html"><a href="sets-functions.html#difference-between-math-probability-and-statistics"><i class="fa fa-check"></i><b>1.2</b> Difference between math, probability, and statistics</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="sets-functions.html"><a href="sets-functions.html#mathematics"><i class="fa fa-check"></i><b>1.2.1</b> Mathematics</a></li>
<li class="chapter" data-level="1.2.2" data-path="sets-functions.html"><a href="sets-functions.html#probability"><i class="fa fa-check"></i><b>1.2.2</b> Probability</a></li>
<li class="chapter" data-level="1.2.3" data-path="sets-functions.html"><a href="sets-functions.html#statistics"><i class="fa fa-check"></i><b>1.2.3</b> Statistics</a></li>
<li class="chapter" data-level="1.2.4" data-path="sets-functions.html"><a href="sets-functions.html#their-uses"><i class="fa fa-check"></i><b>1.2.4</b> Their uses</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="sets-functions.html"><a href="sets-functions.html#goals-for-this-camp"><i class="fa fa-check"></i><b>1.3</b> Goals for this camp</a></li>
<li class="chapter" data-level="1.4" data-path="sets-functions.html"><a href="sets-functions.html#course-logistics"><i class="fa fa-check"></i><b>1.4</b> Course logistics</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="sets-functions.html"><a href="sets-functions.html#course-staff"><i class="fa fa-check"></i><b>1.4.1</b> Course staff</a></li>
<li class="chapter" data-level="1.4.2" data-path="sets-functions.html"><a href="sets-functions.html#teaching-assistants-1"><i class="fa fa-check"></i><b>1.4.2</b> Teaching assistants</a></li>
<li class="chapter" data-level="1.4.3" data-path="sets-functions.html"><a href="sets-functions.html#prerequisites-for-the-math-camp"><i class="fa fa-check"></i><b>1.4.3</b> Prerequisites for the math camp</a></li>
<li class="chapter" data-level="1.4.4" data-path="sets-functions.html"><a href="sets-functions.html#alternatives-to-this-camp"><i class="fa fa-check"></i><b>1.4.4</b> Alternatives to this camp</a></li>
<li class="chapter" data-level="1.4.5" data-path="sets-functions.html"><a href="sets-functions.html#evaluation"><i class="fa fa-check"></i><b>1.4.5</b> Evaluation</a></li>
<li class="chapter" data-level="1.4.6" data-path="sets-functions.html"><a href="sets-functions.html#why-are-we-doing-this"><i class="fa fa-check"></i><b>1.4.6</b> Why are we doing this</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="sets-functions.html"><a href="sets-functions.html#mathematical-notation"><i class="fa fa-check"></i><b>1.5</b> Mathematical notation</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="sets-functions.html"><a href="sets-functions.html#why-math-is-important-to-social-science"><i class="fa fa-check"></i><b>1.5.1</b> Why math is important to social science</a></li>
<li class="chapter" data-level="1.5.2" data-path="sets-functions.html"><a href="sets-functions.html#example-paradox-of-voting"><i class="fa fa-check"></i><b>1.5.2</b> Example: Paradox of voting</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="sets-functions.html"><a href="sets-functions.html#sets"><i class="fa fa-check"></i><b>1.6</b> Sets</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="sets-functions.html"><a href="sets-functions.html#set-builder-notation"><i class="fa fa-check"></i><b>1.6.1</b> Set builder notation</a></li>
<li class="chapter" data-level="1.6.2" data-path="sets-functions.html"><a href="sets-functions.html#set-operations"><i class="fa fa-check"></i><b>1.6.2</b> Set operations</a></li>
<li class="chapter" data-level="1.6.3" data-path="sets-functions.html"><a href="sets-functions.html#some-facts-about-sets"><i class="fa fa-check"></i><b>1.6.3</b> Some facts about sets</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="sets-functions.html"><a href="sets-functions.html#functions"><i class="fa fa-check"></i><b>1.7</b> Functions</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="sets-functions.html"><a href="sets-functions.html#ordered-pairs"><i class="fa fa-check"></i><b>1.7.1</b> Ordered pairs</a></li>
<li class="chapter" data-level="1.7.2" data-path="sets-functions.html"><a href="sets-functions.html#relation"><i class="fa fa-check"></i><b>1.7.2</b> Relation</a></li>
<li class="chapter" data-level="1.7.3" data-path="sets-functions.html"><a href="sets-functions.html#relation-vs.-function"><i class="fa fa-check"></i><b>1.7.3</b> Relation vs.Â function</a></li>
<li class="chapter" data-level="1.7.4" data-path="sets-functions.html"><a href="sets-functions.html#two-major-properties-of-functions"><i class="fa fa-check"></i><b>1.7.4</b> Two major properties of functions</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="sets-functions.html"><a href="sets-functions.html#quadratic-functions"><i class="fa fa-check"></i><b>1.8</b> Quadratic functions</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="sets-functions.html"><a href="sets-functions.html#quadratic-equation"><i class="fa fa-check"></i><b>1.8.1</b> Quadratic equation</a></li>
<li class="chapter" data-level="1.8.2" data-path="sets-functions.html"><a href="sets-functions.html#quadratic-formula"><i class="fa fa-check"></i><b>1.8.2</b> Quadratic formula</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="sets-functions.html"><a href="sets-functions.html#systems-of-linear-equations"><i class="fa fa-check"></i><b>1.9</b> Systems of linear equations</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="sets-functions.html"><a href="sets-functions.html#one-solution"><i class="fa fa-check"></i><b>1.9.1</b> One solution</a></li>
<li class="chapter" data-level="1.9.2" data-path="sets-functions.html"><a href="sets-functions.html#no-solution"><i class="fa fa-check"></i><b>1.9.2</b> No solution</a></li>
<li class="chapter" data-level="1.9.3" data-path="sets-functions.html"><a href="sets-functions.html#infinite-solutions"><i class="fa fa-check"></i><b>1.9.3</b> Infinite solutions</a></li>
<li class="chapter" data-level="1.9.4" data-path="sets-functions.html"><a href="sets-functions.html#three-equations-in-three-unknowns"><i class="fa fa-check"></i><b>1.9.4</b> Three equations in three unknowns</a></li>
<li class="chapter" data-level="1.9.5" data-path="sets-functions.html"><a href="sets-functions.html#gaussian-elimination"><i class="fa fa-check"></i><b>1.9.5</b> Gaussian elimination</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="sets-functions.html"><a href="sets-functions.html#logarithms-and-exponential-functions"><i class="fa fa-check"></i><b>1.10</b> Logarithms and exponential functions</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="sets-functions.html"><a href="sets-functions.html#functions-with-exponents"><i class="fa fa-check"></i><b>1.10.1</b> Functions with exponents</a></li>
<li class="chapter" data-level="1.10.2" data-path="sets-functions.html"><a href="sets-functions.html#common-rules-of-exponents"><i class="fa fa-check"></i><b>1.10.2</b> Common rules of exponents</a></li>
<li class="chapter" data-level="1.10.3" data-path="sets-functions.html"><a href="sets-functions.html#logarithms"><i class="fa fa-check"></i><b>1.10.3</b> Logarithms</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="sets-functions.html"><a href="sets-functions.html#bonus-content-computational-tools-for-the-future"><i class="fa fa-check"></i><b>1.11</b> Bonus content: Computational tools for the future</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="sets-functions.html"><a href="sets-functions.html#programming-languages-for-statistical-learning"><i class="fa fa-check"></i><b>1.11.1</b> Programming languages for statistical learning</a></li>
<li class="chapter" data-level="1.11.2" data-path="sets-functions.html"><a href="sets-functions.html#version-control-git"><i class="fa fa-check"></i><b>1.11.2</b> Version control (Git)</a></li>
<li class="chapter" data-level="1.11.3" data-path="sets-functions.html"><a href="sets-functions.html#publishing"><i class="fa fa-check"></i><b>1.11.3</b> Publishing</a></li>
<li class="chapter" data-level="1.11.4" data-path="sets-functions.html"><a href="sets-functions.html#how-will-you-acquire-these-skills"><i class="fa fa-check"></i><b>1.11.4</b> How will you acquire these skills?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html"><i class="fa fa-check"></i><b>2</b> Sequences, limits, continuity, and derivatives</a>
<ul>
<li class="chapter" data-level="" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#learning-objectives-1"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#supplemental-readings-1"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="2.1" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#sequence"><i class="fa fa-check"></i><b>2.1</b> Sequence</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#definition"><i class="fa fa-check"></i><b>2.1.1</b> Definition</a></li>
<li class="chapter" data-level="2.1.2" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#examples"><i class="fa fa-check"></i><b>2.1.2</b> Examples</a></li>
<li class="chapter" data-level="2.1.3" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#arithmetic-and-geometric-progressions"><i class="fa fa-check"></i><b>2.1.3</b> Arithmetic and geometric progressions</a></li>
<li class="chapter" data-level="2.1.4" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#convergence"><i class="fa fa-check"></i><b>2.1.4</b> Convergence</a></li>
<li class="chapter" data-level="2.1.5" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#algebra-of-sequences"><i class="fa fa-check"></i><b>2.1.5</b> Algebra of sequences</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#limits"><i class="fa fa-check"></i><b>2.2</b> Limits</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#sequences-leadsto-limits-of-functions"><i class="fa fa-check"></i><b>2.2.1</b> Sequences <span class="math inline">\(\leadsto\)</span> limits of functions</a></li>
<li class="chapter" data-level="2.2.2" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#limits-of-functions"><i class="fa fa-check"></i><b>2.2.2</b> Limits of functions</a></li>
<li class="chapter" data-level="2.2.3" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#examples-of-limits"><i class="fa fa-check"></i><b>2.2.3</b> Examples of limits</a></li>
<li class="chapter" data-level="2.2.4" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#not-all-functions-have-limits"><i class="fa fa-check"></i><b>2.2.4</b> Not all functions have limits</a></li>
<li class="chapter" data-level="2.2.5" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#intuitive-definition-of-a-limit"><i class="fa fa-check"></i><b>2.2.5</b> Intuitive definition of a limit</a></li>
<li class="chapter" data-level="2.2.6" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#algebra-of-limits"><i class="fa fa-check"></i><b>2.2.6</b> Algebra of limits</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#continuity"><i class="fa fa-check"></i><b>2.3</b> Continuity</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#defining-continuity"><i class="fa fa-check"></i><b>2.3.1</b> Defining continuity</a></li>
<li class="chapter" data-level="2.3.2" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#a-real-world-example-of-limits-measuring-incumbency-advantage"><i class="fa fa-check"></i><b>2.3.2</b> A real-world example of limits: Measuring incumbency advantage</a></li>
<li class="chapter" data-level="2.3.3" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#continuity-and-limits"><i class="fa fa-check"></i><b>2.3.3</b> Continuity and limits</a></li>
<li class="chapter" data-level="2.3.4" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#algebra-of-continuous-functions"><i class="fa fa-check"></i><b>2.3.4</b> Algebra of continuous functions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#what-is-calculus"><i class="fa fa-check"></i><b>2.4</b> What is calculus?</a></li>
<li class="chapter" data-level="2.5" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#derivatives"><i class="fa fa-check"></i><b>2.5</b> Derivatives</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#how-functions-change"><i class="fa fa-check"></i><b>2.5.1</b> How functions change</a></li>
<li class="chapter" data-level="2.5.2" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#the-tangent-as-a-limit"><i class="fa fa-check"></i><b>2.5.2</b> The tangent as a limit</a></li>
<li class="chapter" data-level="2.5.3" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#derivative"><i class="fa fa-check"></i><b>2.5.3</b> Derivative</a></li>
<li class="chapter" data-level="2.5.4" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#rates-of-change-in-a-function"><i class="fa fa-check"></i><b>2.5.4</b> Rates of change in a function</a></li>
<li class="chapter" data-level="2.5.5" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#examples-of-derivatives"><i class="fa fa-check"></i><b>2.5.5</b> Examples of derivatives</a></li>
<li class="chapter" data-level="2.5.6" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#continuity-and-derivatives"><i class="fa fa-check"></i><b>2.5.6</b> Continuity and derivatives</a></li>
<li class="chapter" data-level="2.5.7" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#what-goes-wrong"><i class="fa fa-check"></i><b>2.5.7</b> What goes wrong?</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#calculating-derivatives"><i class="fa fa-check"></i><b>2.6</b> Calculating derivatives</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#derivative-rules"><i class="fa fa-check"></i><b>2.6.1</b> Derivative rules</a></li>
<li class="chapter" data-level="2.6.2" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#challenge-problems"><i class="fa fa-check"></i><b>2.6.2</b> Challenge problems</a></li>
<li class="chapter" data-level="2.6.3" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#composite-functions"><i class="fa fa-check"></i><b>2.6.3</b> Composite functions</a></li>
<li class="chapter" data-level="2.6.4" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#chain-rule"><i class="fa fa-check"></i><b>2.6.4</b> Chain rule</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#derivatives-for-the-exponential-function-and-natural-logarithms"><i class="fa fa-check"></i><b>2.7</b> Derivatives for the exponential function and natural logarithms</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#derivative-of-exponential-function"><i class="fa fa-check"></i><b>2.7.1</b> Derivative of exponential function</a></li>
<li class="chapter" data-level="2.7.2" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#derivative-of-the-natural-logarithm"><i class="fa fa-check"></i><b>2.7.2</b> Derivative of the natural logarithm</a></li>
<li class="chapter" data-level="2.7.3" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#relevance-of-exponential-functions-and-natural-logarithm"><i class="fa fa-check"></i><b>2.7.3</b> Relevance of exponential functions and natural logarithm</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#derivatives-and-properties-of-functions"><i class="fa fa-check"></i><b>2.8</b> Derivatives and properties of functions</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#relative-maxima-minima-and-derivatives"><i class="fa fa-check"></i><b>2.8.1</b> Relative maxima, minima and derivatives</a></li>
<li class="chapter" data-level="2.8.2" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#mean-value-theorem"><i class="fa fa-check"></i><b>2.8.2</b> Mean value theorem</a></li>
<li class="chapter" data-level="2.8.3" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#applications-of-the-mean-value-theorem"><i class="fa fa-check"></i><b>2.8.3</b> Applications of the mean value theorem</a></li>
<li class="chapter" data-level="2.8.4" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#extension-to-indeterminate-form-limits"><i class="fa fa-check"></i><b>2.8.4</b> Extension to indeterminate form limits</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="critical-points.html"><a href="critical-points.html"><i class="fa fa-check"></i><b>3</b> Critical points and approximation</a>
<ul>
<li class="chapter" data-level="" data-path="critical-points.html"><a href="critical-points.html#learning-objectives-2"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="critical-points.html"><a href="critical-points.html#supplemental-readings-2"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="3.1" data-path="critical-points.html"><a href="critical-points.html#intuition"><i class="fa fa-check"></i><b>3.1</b> Intuition</a></li>
<li class="chapter" data-level="3.2" data-path="critical-points.html"><a href="critical-points.html#higher-order-derivatives"><i class="fa fa-check"></i><b>3.2</b> Higher order derivatives</a></li>
<li class="chapter" data-level="3.3" data-path="critical-points.html"><a href="critical-points.html#critical-points-1"><i class="fa fa-check"></i><b>3.3</b> Critical points</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="critical-points.html"><a href="critical-points.html#inflection-point"><i class="fa fa-check"></i><b>3.3.1</b> Inflection point</a></li>
<li class="chapter" data-level="3.3.2" data-path="critical-points.html"><a href="critical-points.html#concavity"><i class="fa fa-check"></i><b>3.3.2</b> Concavity</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="critical-points.html"><a href="critical-points.html#extrema"><i class="fa fa-check"></i><b>3.4</b> Extrema</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="critical-points.html"><a href="critical-points.html#minimum-and-maximum-on-the-interval-05-are-located-at-the-endpoints"><i class="fa fa-check"></i><b>3.4.1</b> Minimum and maximum on the interval <span class="math inline">\([0,5]\)</span> are located at the endpoints</a></li>
<li class="chapter" data-level="3.4.2" data-path="critical-points.html"><a href="critical-points.html#global-maximum-is-located-at-x0"><i class="fa fa-check"></i><b>3.4.2</b> Global maximum is located at <span class="math inline">\(x=0\)</span></a></li>
<li class="chapter" data-level="3.4.3" data-path="critical-points.html"><a href="critical-points.html#global-minimum-is-located-at-x---frac92"><i class="fa fa-check"></i><b>3.4.3</b> Global minimum is located at <span class="math inline">\(x= - \frac{9}{2}\)</span></a></li>
<li class="chapter" data-level="3.4.4" data-path="critical-points.html"><a href="critical-points.html#a-bunch-of-local-minima-and-maxima"><i class="fa fa-check"></i><b>3.4.4</b> A bunch of local minima and maxima</a></li>
<li class="chapter" data-level="3.4.5" data-path="critical-points.html"><a href="critical-points.html#x0-is-an-inflection-point-that-is-neither-a-minimum-nor-a-maximum-fx-0"><i class="fa fa-check"></i><b>3.4.5</b> <span class="math inline">\(x=0\)</span> is an inflection point that is neither a minimum nor a maximum (<span class="math inline">\(f&#39;&#39;(x) = 0\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="critical-points.html"><a href="critical-points.html#framework-for-analytical-optimization"><i class="fa fa-check"></i><b>3.5</b> Framework for analytical optimization</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="critical-points.html"><a href="critical-points.html#example-fx--x2-x-in--3-3"><i class="fa fa-check"></i><b>3.5.1</b> Example: <span class="math inline">\(f(x) = -x^2\)</span>, <span class="math inline">\(x \in [-3, 3]\)</span></a></li>
<li class="chapter" data-level="3.5.2" data-path="critical-points.html"><a href="critical-points.html#example-fx-x3-x-in--3-3"><i class="fa fa-check"></i><b>3.5.2</b> Example: <span class="math inline">\(f(x) = x^3\)</span>, <span class="math inline">\(x \in [-3, 3]\)</span></a></li>
<li class="chapter" data-level="3.5.3" data-path="critical-points.html"><a href="critical-points.html#example-spatial-model"><i class="fa fa-check"></i><b>3.5.3</b> Example: spatial model</a></li>
<li class="chapter" data-level="3.5.4" data-path="critical-points.html"><a href="critical-points.html#example-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>3.5.4</b> Example: Maximum likelihood estimation</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="critical-points.html"><a href="critical-points.html#computational-optimization-procedures"><i class="fa fa-check"></i><b>3.6</b> Computational optimization procedures</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="critical-points.html"><a href="critical-points.html#newton-raphson-root-finding"><i class="fa fa-check"></i><b>3.6.1</b> Newton-Raphson root finding</a></li>
<li class="chapter" data-level="3.6.2" data-path="critical-points.html"><a href="critical-points.html#grid-search"><i class="fa fa-check"></i><b>3.6.2</b> Grid search</a></li>
<li class="chapter" data-level="3.6.3" data-path="critical-points.html"><a href="critical-points.html#gradient-descent"><i class="fa fa-check"></i><b>3.6.3</b> Gradient descent</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-algebra.html"><a href="linear-algebra.html"><i class="fa fa-check"></i><b>4</b> Linear algebra</a>
<ul>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#learning-objectives-3"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#supplemental-readings-3"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="4.1" data-path="linear-algebra.html"><a href="linear-algebra.html#linear-algebra-1"><i class="fa fa-check"></i><b>4.1</b> Linear algebra</a></li>
<li class="chapter" data-level="4.2" data-path="linear-algebra.html"><a href="linear-algebra.html#points-and-vectors"><i class="fa fa-check"></i><b>4.2</b> Points and vectors</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="linear-algebra.html"><a href="linear-algebra.html#points"><i class="fa fa-check"></i><b>4.2.1</b> Points</a></li>
<li class="chapter" data-level="4.2.2" data-path="linear-algebra.html"><a href="linear-algebra.html#vectors"><i class="fa fa-check"></i><b>4.2.2</b> Vectors</a></li>
<li class="chapter" data-level="4.2.3" data-path="linear-algebra.html"><a href="linear-algebra.html#one-dimensional-example"><i class="fa fa-check"></i><b>4.2.3</b> One dimensional example</a></li>
<li class="chapter" data-level="4.2.4" data-path="linear-algebra.html"><a href="linear-algebra.html#two-dimensional-example"><i class="fa fa-check"></i><b>4.2.4</b> Two dimensional example</a></li>
<li class="chapter" data-level="4.2.5" data-path="linear-algebra.html"><a href="linear-algebra.html#three-dimensional-example"><i class="fa fa-check"></i><b>4.2.5</b> Three dimensional example</a></li>
<li class="chapter" data-level="4.2.6" data-path="linear-algebra.html"><a href="linear-algebra.html#n-dimensional-example"><i class="fa fa-check"></i><b>4.2.6</b> <span class="math inline">\(N\)</span>-dimensional example</a></li>
<li class="chapter" data-level="4.2.7" data-path="linear-algebra.html"><a href="linear-algebra.html#examples-of-some-basic-arithmetic"><i class="fa fa-check"></i><b>4.2.7</b> Examples of some basic arithmetic</a></li>
<li class="chapter" data-level="4.2.8" data-path="linear-algebra.html"><a href="linear-algebra.html#linear-dependence"><i class="fa fa-check"></i><b>4.2.8</b> Linear dependence</a></li>
<li class="chapter" data-level="4.2.9" data-path="linear-algebra.html"><a href="linear-algebra.html#inner-product"><i class="fa fa-check"></i><b>4.2.9</b> Inner product</a></li>
<li class="chapter" data-level="4.2.10" data-path="linear-algebra.html"><a href="linear-algebra.html#calculating-vector-length"><i class="fa fa-check"></i><b>4.2.10</b> Calculating vector length</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="linear-algebra.html"><a href="linear-algebra.html#example-text-analysis"><i class="fa fa-check"></i><b>4.3</b> Example: text analysis</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="linear-algebra.html"><a href="linear-algebra.html#measure-1-inner-product"><i class="fa fa-check"></i><b>4.3.1</b> Measure 1: inner product</a></li>
<li class="chapter" data-level="4.3.2" data-path="linear-algebra.html"><a href="linear-algebra.html#measure-2-cosine-similarity"><i class="fa fa-check"></i><b>4.3.2</b> Measure 2: cosine similarity</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="linear-algebra.html"><a href="linear-algebra.html#matricies"><i class="fa fa-check"></i><b>4.4</b> Matricies</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="linear-algebra.html"><a href="linear-algebra.html#basic-arithmetic"><i class="fa fa-check"></i><b>4.4.1</b> Basic arithmetic</a></li>
<li class="chapter" data-level="4.4.2" data-path="linear-algebra.html"><a href="linear-algebra.html#transposition"><i class="fa fa-check"></i><b>4.4.2</b> Transposition</a></li>
<li class="chapter" data-level="4.4.3" data-path="linear-algebra.html"><a href="linear-algebra.html#multiplication"><i class="fa fa-check"></i><b>4.4.3</b> Multiplication</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="linear-algebra.html"><a href="linear-algebra.html#example-neural-networks"><i class="fa fa-check"></i><b>4.5</b> Example: neural networks</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="linear-algebra.html"><a href="linear-algebra.html#how-are-neural-networks-used"><i class="fa fa-check"></i><b>4.5.1</b> How are neural networks used</a></li>
<li class="chapter" data-level="4.5.2" data-path="linear-algebra.html"><a href="linear-algebra.html#how-are-neural-networks-related-to-linear-algebra"><i class="fa fa-check"></i><b>4.5.2</b> How are neural networks related to linear algebra?</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="linear-algebra.html"><a href="linear-algebra.html#matrix-inversion"><i class="fa fa-check"></i><b>4.6</b> Matrix inversion</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="linear-algebra.html"><a href="linear-algebra.html#calculating-matrix-inversions"><i class="fa fa-check"></i><b>4.6.1</b> Calculating matrix inversions</a></li>
<li class="chapter" data-level="4.6.2" data-path="linear-algebra.html"><a href="linear-algebra.html#when-do-inverses-exist"><i class="fa fa-check"></i><b>4.6.2</b> When do inverses exist</a></li>
<li class="chapter" data-level="4.6.3" data-path="linear-algebra.html"><a href="linear-algebra.html#inverting-a-2-times-2-matrix"><i class="fa fa-check"></i><b>4.6.3</b> Inverting a <span class="math inline">\(2 \times 2\)</span> matrix</a></li>
<li class="chapter" data-level="4.6.4" data-path="linear-algebra.html"><a href="linear-algebra.html#inverting-an-n-times-n-matrix"><i class="fa fa-check"></i><b>4.6.4</b> Inverting an <span class="math inline">\(n \times n\)</span> matrix</a></li>
<li class="chapter" data-level="4.6.5" data-path="linear-algebra.html"><a href="linear-algebra.html#application-to-regression-analysis"><i class="fa fa-check"></i><b>4.6.5</b> Application to regression analysis</a></li>
<li class="chapter" data-level="4.6.6" data-path="linear-algebra.html"><a href="linear-algebra.html#application-to-solving-systems-of-equations-tax-benefits-of-charitable-contributions"><i class="fa fa-check"></i><b>4.6.6</b> Application to solving systems of equations: tax benefits of charitable contributions</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="linear-algebra.html"><a href="linear-algebra.html#determinant"><i class="fa fa-check"></i><b>4.7</b> Determinant</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="linear-algebra.html"><a href="linear-algebra.html#relevance-of-the-determinant"><i class="fa fa-check"></i><b>4.7.1</b> Relevance of the determinant</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="linear-algebra.html"><a href="linear-algebra.html#matrix-decomposition"><i class="fa fa-check"></i><b>4.8</b> Matrix decomposition</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="linear-algebra.html"><a href="linear-algebra.html#dimension-reduction"><i class="fa fa-check"></i><b>4.8.1</b> Dimension reduction</a></li>
<li class="chapter" data-level="4.8.2" data-path="linear-algebra.html"><a href="linear-algebra.html#singular-value-decomposition"><i class="fa fa-check"></i><b>4.8.2</b> Singular value decomposition</a></li>
<li class="chapter" data-level="4.8.3" data-path="linear-algebra.html"><a href="linear-algebra.html#principal-components-analysis"><i class="fa fa-check"></i><b>4.8.3</b> Principal components analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="linear-algebra.html"><a href="linear-algebra.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html"><i class="fa fa-check"></i><b>5</b> Functions of several variables and optimization with several variables</a>
<ul>
<li class="chapter" data-level="" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#learning-objectives-4"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#supplemental-readings-4"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="5.1" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#higher-order-derivatives-1"><i class="fa fa-check"></i><b>5.1</b> Higher order derivatives</a></li>
<li class="chapter" data-level="5.2" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#multivariate-function"><i class="fa fa-check"></i><b>5.2</b> Multivariate function</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#definition-2"><i class="fa fa-check"></i><b>5.2.1</b> Definition</a></li>
<li class="chapter" data-level="5.2.2" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#evaluating-multivariate-functions"><i class="fa fa-check"></i><b>5.2.2</b> Evaluating multivariate functions</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#multivariate-derivatives"><i class="fa fa-check"></i><b>5.3</b> Multivariate derivatives</a></li>
<li class="chapter" data-level="5.4" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#multivariate-optimization"><i class="fa fa-check"></i><b>5.4</b> Multivariate optimization</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#differences-from-single-variable-optimization-procedure"><i class="fa fa-check"></i><b>5.4.1</b> Differences from single variable optimization procedure</a></li>
<li class="chapter" data-level="5.4.2" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#first-derivative-test-gradient"><i class="fa fa-check"></i><b>5.4.2</b> First derivative test: Gradient</a></li>
<li class="chapter" data-level="5.4.3" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#second-derivative-test-hessian"><i class="fa fa-check"></i><b>5.4.3</b> Second derivative test: Hessian</a></li>
<li class="chapter" data-level="5.4.4" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#basic-procedure-summarized"><i class="fa fa-check"></i><b>5.4.4</b> Basic procedure summarized</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#a-simple-optimization-example"><i class="fa fa-check"></i><b>5.5</b> A simple optimization example</a></li>
<li class="chapter" data-level="5.6" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#maximum-likelihood-estimation-for-a-normal-distribution"><i class="fa fa-check"></i><b>5.6</b> Maximum likelihood estimation for a normal distribution</a></li>
<li class="chapter" data-level="5.7" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#computational-optimization-procedures-1"><i class="fa fa-check"></i><b>5.7</b> Computational optimization procedures</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#multivariate-newton-raphson"><i class="fa fa-check"></i><b>5.7.1</b> Multivariate Newton-Raphson</a></li>
<li class="chapter" data-level="5.7.2" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#grid-search-1"><i class="fa fa-check"></i><b>5.7.2</b> Grid search</a></li>
<li class="chapter" data-level="5.7.3" data-path="multivariable-differentiation.html"><a href="multivariable-differentiation.html#gradient-descent-1"><i class="fa fa-check"></i><b>5.7.3</b> Gradient descent</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="integral-calculus.html"><a href="integral-calculus.html"><i class="fa fa-check"></i><b>6</b> Integration and integral calculus</a>
<ul>
<li class="chapter" data-level="" data-path="integral-calculus.html"><a href="integral-calculus.html#learning-objectives-5"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="integral-calculus.html"><a href="integral-calculus.html#supplemental-readings-5"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="6.1" data-path="integral-calculus.html"><a href="integral-calculus.html#prepare-for-the-journey"><i class="fa fa-check"></i><b>6.1</b> Prepare for the journey</a></li>
<li class="chapter" data-level="6.2" data-path="integral-calculus.html"><a href="integral-calculus.html#indefinite-integration"><i class="fa fa-check"></i><b>6.2</b> Indefinite integration</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="integral-calculus.html"><a href="integral-calculus.html#many-possible-antiderivatives"><i class="fa fa-check"></i><b>6.2.1</b> Many possible antiderivatives</a></li>
<li class="chapter" data-level="6.2.2" data-path="integral-calculus.html"><a href="integral-calculus.html#common-rules-of-integration"><i class="fa fa-check"></i><b>6.2.2</b> Common rules of integration</a></li>
<li class="chapter" data-level="6.2.3" data-path="integral-calculus.html"><a href="integral-calculus.html#practice-integrating-functions"><i class="fa fa-check"></i><b>6.2.3</b> Practice integrating functions</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="integral-calculus.html"><a href="integral-calculus.html#the-definite-integral-area-under-the-curve"><i class="fa fa-check"></i><b>6.3</b> The definite integral: area under the curve</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="integral-calculus.html"><a href="integral-calculus.html#the-definite-integral-riemann"><i class="fa fa-check"></i><b>6.3.1</b> The definite integral (Riemann)</a></li>
<li class="chapter" data-level="6.3.2" data-path="integral-calculus.html"><a href="integral-calculus.html#counterexamples"><i class="fa fa-check"></i><b>6.3.2</b> Counterexamples</a></li>
<li class="chapter" data-level="6.3.3" data-path="integral-calculus.html"><a href="integral-calculus.html#fundamental-theorem-of-calculus"><i class="fa fa-check"></i><b>6.3.3</b> Fundamental theorem of calculus</a></li>
<li class="chapter" data-level="6.3.4" data-path="integral-calculus.html"><a href="integral-calculus.html#common-rules-for-definite-integrals"><i class="fa fa-check"></i><b>6.3.4</b> Common rules for definite integrals</a></li>
<li class="chapter" data-level="6.3.5" data-path="integral-calculus.html"><a href="integral-calculus.html#practice-solving-definite-integrals"><i class="fa fa-check"></i><b>6.3.5</b> Practice solving definite integrals</a></li>
<li class="chapter" data-level="6.3.6" data-path="integral-calculus.html"><a href="integral-calculus.html#integration-by-substitution"><i class="fa fa-check"></i><b>6.3.6</b> Integration by substitution</a></li>
<li class="chapter" data-level="6.3.7" data-path="integral-calculus.html"><a href="integral-calculus.html#integration-by-parts"><i class="fa fa-check"></i><b>6.3.7</b> Integration by parts</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="integral-calculus.html"><a href="integral-calculus.html#infinite-integrals"><i class="fa fa-check"></i><b>6.4</b> Infinite integrals</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="integral-calculus.html"><a href="integral-calculus.html#two-sided-infinite-integrals"><i class="fa fa-check"></i><b>6.4.1</b> Two-sided infinite integrals</a></li>
<li class="chapter" data-level="6.4.2" data-path="integral-calculus.html"><a href="integral-calculus.html#improper-integrals"><i class="fa fa-check"></i><b>6.4.2</b> Improper integrals</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="integral-calculus.html"><a href="integral-calculus.html#monte-carlo-and-integration"><i class="fa fa-check"></i><b>6.5</b> Monte Carlo and integration</a></li>
<li class="chapter" data-level="6.6" data-path="integral-calculus.html"><a href="integral-calculus.html#multivariate-integration"><i class="fa fa-check"></i><b>6.6</b> Multivariate integration</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="integral-calculus.html"><a href="integral-calculus.html#more-complicated-bounds-of-integration"><i class="fa fa-check"></i><b>6.6.1</b> More complicated bounds of integration</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="integral-calculus.html"><a href="integral-calculus.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="sample-space-probability.html"><a href="sample-space-probability.html"><i class="fa fa-check"></i><b>7</b> Sample space and probability</a>
<ul>
<li class="chapter" data-level="" data-path="sample-space-probability.html"><a href="sample-space-probability.html#learning-objectives-6"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="sample-space-probability.html"><a href="sample-space-probability.html#supplemental-readings-6"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="7.1" data-path="sample-space-probability.html"><a href="sample-space-probability.html#model-of-probability"><i class="fa fa-check"></i><b>7.1</b> Model of probability</a></li>
<li class="chapter" data-level="7.2" data-path="sample-space-probability.html"><a href="sample-space-probability.html#sample-space"><i class="fa fa-check"></i><b>7.2</b> Sample space</a></li>
<li class="chapter" data-level="7.3" data-path="sample-space-probability.html"><a href="sample-space-probability.html#events"><i class="fa fa-check"></i><b>7.3</b> Events</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="sample-space-probability.html"><a href="sample-space-probability.html#event-operations"><i class="fa fa-check"></i><b>7.3.1</b> Event operations</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="sample-space-probability.html"><a href="sample-space-probability.html#probability-1"><i class="fa fa-check"></i><b>7.4</b> Probability</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="sample-space-probability.html"><a href="sample-space-probability.html#three-axioms"><i class="fa fa-check"></i><b>7.4.1</b> Three axioms</a></li>
<li class="chapter" data-level="7.4.2" data-path="sample-space-probability.html"><a href="sample-space-probability.html#basic-examples"><i class="fa fa-check"></i><b>7.4.2</b> Basic examples</a></li>
<li class="chapter" data-level="7.4.3" data-path="sample-space-probability.html"><a href="sample-space-probability.html#surprising-probability-facts"><i class="fa fa-check"></i><b>7.4.3</b> Surprising probability facts</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="sample-space-probability.html"><a href="sample-space-probability.html#conditional-probability"><i class="fa fa-check"></i><b>7.5</b> Conditional probability</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="sample-space-probability.html"><a href="sample-space-probability.html#examples-1"><i class="fa fa-check"></i><b>7.5.1</b> Examples</a></li>
<li class="chapter" data-level="7.5.2" data-path="sample-space-probability.html"><a href="sample-space-probability.html#difference-between-prab-and-prba"><i class="fa fa-check"></i><b>7.5.2</b> Difference between <span class="math inline">\(\Pr(A|B)\)</span> and <span class="math inline">\(\Pr(B|A)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="sample-space-probability.html"><a href="sample-space-probability.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.6</b> Law of total probability</a></li>
<li class="chapter" data-level="7.7" data-path="sample-space-probability.html"><a href="sample-space-probability.html#bayes-rule"><i class="fa fa-check"></i><b>7.7</b> Bayesâ Rule</a></li>
<li class="chapter" data-level="7.8" data-path="sample-space-probability.html"><a href="sample-space-probability.html#independence-of-probabilities"><i class="fa fa-check"></i><b>7.8</b> Independence of probabilities</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="sample-space-probability.html"><a href="sample-space-probability.html#rolling-a-4-sided-die"><i class="fa fa-check"></i><b>7.8.1</b> Rolling a 4-sided die</a></li>
<li class="chapter" data-level="7.8.2" data-path="sample-space-probability.html"><a href="sample-space-probability.html#independence-and-causal-inference"><i class="fa fa-check"></i><b>7.8.2</b> Independence and causal inference</a></li>
<li class="chapter" data-level="7.8.3" data-path="sample-space-probability.html"><a href="sample-space-probability.html#independence-of-a-collection-of-events"><i class="fa fa-check"></i><b>7.8.3</b> Independence of a collection of events</a></li>
<li class="chapter" data-level="7.8.4" data-path="sample-space-probability.html"><a href="sample-space-probability.html#independent-trials-and-the-binomial-probabilities"><i class="fa fa-check"></i><b>7.8.4</b> Independent trials and the binomial probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="sample-space-probability.html"><a href="sample-space-probability.html#counting"><i class="fa fa-check"></i><b>7.9</b> Counting</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="sample-space-probability.html"><a href="sample-space-probability.html#counting-principle"><i class="fa fa-check"></i><b>7.9.1</b> Counting principle</a></li>
<li class="chapter" data-level="7.9.2" data-path="sample-space-probability.html"><a href="sample-space-probability.html#permutations"><i class="fa fa-check"></i><b>7.9.2</b> Permutations</a></li>
<li class="chapter" data-level="7.9.3" data-path="sample-space-probability.html"><a href="sample-space-probability.html#combinations"><i class="fa fa-check"></i><b>7.9.3</b> Combinations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html"><i class="fa fa-check"></i><b>8</b> Discrete random variables</a>
<ul>
<li class="chapter" data-level="" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#learning-objectives-7"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#supplemental-readings-7"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="8.1" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#random-variable"><i class="fa fa-check"></i><b>8.1</b> Random variable</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#discrete-random-variables-1"><i class="fa fa-check"></i><b>8.1.1</b> Discrete random variables</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#probability-mass-functions"><i class="fa fa-check"></i><b>8.2</b> Probability mass functions</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#intuition-1"><i class="fa fa-check"></i><b>8.2.1</b> Intuition</a></li>
<li class="chapter" data-level="8.2.2" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#definition-3"><i class="fa fa-check"></i><b>8.2.2</b> Definition</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#cumulative-mass-function"><i class="fa fa-check"></i><b>8.3</b> Cumulative mass function</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#three-person-experiment"><i class="fa fa-check"></i><b>8.3.1</b> Three person experiment</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#famous-discrete-random-variables"><i class="fa fa-check"></i><b>8.4</b> Famous discrete random variables</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#bernoulli"><i class="fa fa-check"></i><b>8.4.1</b> Bernoulli</a></li>
<li class="chapter" data-level="8.4.2" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#binomial"><i class="fa fa-check"></i><b>8.4.2</b> Binomial</a></li>
<li class="chapter" data-level="8.4.3" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#geometric"><i class="fa fa-check"></i><b>8.4.3</b> Geometric</a></li>
<li class="chapter" data-level="8.4.4" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#poisson"><i class="fa fa-check"></i><b>8.4.4</b> Poisson</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#functions-of-random-variables"><i class="fa fa-check"></i><b>8.5</b> Functions of random variables</a></li>
<li class="chapter" data-level="8.6" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#expectation-mean-and-variance"><i class="fa fa-check"></i><b>8.6</b> Expectation, mean, and variance</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#motivation"><i class="fa fa-check"></i><b>8.6.1</b> Motivation</a></li>
<li class="chapter" data-level="8.6.2" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#expectation"><i class="fa fa-check"></i><b>8.6.2</b> Expectation</a></li>
<li class="chapter" data-level="8.6.3" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#variance-moments-and-the-expected-value-rule"><i class="fa fa-check"></i><b>8.6.3</b> Variance, moments, and the expected value rule</a></li>
<li class="chapter" data-level="8.6.4" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#practice-calculating-expectation-and-variance"><i class="fa fa-check"></i><b>8.6.4</b> Practice calculating expectation and variance</a></li>
<li class="chapter" data-level="8.6.5" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#decision-making-using-expected-values"><i class="fa fa-check"></i><b>8.6.5</b> Decision making using expected values</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#cumulative-mass-function-redux"><i class="fa fa-check"></i><b>8.7</b> Cumulative mass function, redux</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="discrete-random-variables.html"><a href="discrete-random-variables.html#common-cmfs"><i class="fa fa-check"></i><b>8.7.1</b> Common CMFs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="general-random-variables.html"><a href="general-random-variables.html"><i class="fa fa-check"></i><b>9</b> General random variables</a>
<ul>
<li class="chapter" data-level="" data-path="general-random-variables.html"><a href="general-random-variables.html#learning-objectives-8"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="general-random-variables.html"><a href="general-random-variables.html#supplemental-readings-8"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="9.1" data-path="general-random-variables.html"><a href="general-random-variables.html#continuous-random-variables"><i class="fa fa-check"></i><b>9.1</b> Continuous random variables</a></li>
<li class="chapter" data-level="9.2" data-path="general-random-variables.html"><a href="general-random-variables.html#probability-density-function"><i class="fa fa-check"></i><b>9.2</b> Probability density function</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="general-random-variables.html"><a href="general-random-variables.html#definition-4"><i class="fa fa-check"></i><b>9.2.1</b> Definition</a></li>
<li class="chapter" data-level="9.2.2" data-path="general-random-variables.html"><a href="general-random-variables.html#example-uniform-random-variable"><i class="fa fa-check"></i><b>9.2.2</b> Example: Uniform Random Variable</a></li>
<li class="chapter" data-level="9.2.3" data-path="general-random-variables.html"><a href="general-random-variables.html#expectation-continuous"><i class="fa fa-check"></i><b>9.2.3</b> Expectation</a></li>
<li class="chapter" data-level="9.2.4" data-path="general-random-variables.html"><a href="general-random-variables.html#exponential-random-variable"><i class="fa fa-check"></i><b>9.2.4</b> Exponential random variable</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="general-random-variables.html"><a href="general-random-variables.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>9.3</b> Cumulative distribution function</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="general-random-variables.html"><a href="general-random-variables.html#properties-of-cdfs"><i class="fa fa-check"></i><b>9.3.1</b> Properties of CDFs</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="general-random-variables.html"><a href="general-random-variables.html#normal-distribution"><i class="fa fa-check"></i><b>9.4</b> Normal distribution</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="general-random-variables.html"><a href="general-random-variables.html#expected-valuevariance-of-normal-distribution"><i class="fa fa-check"></i><b>9.4.1</b> Expected value/variance of normal distribution</a></li>
<li class="chapter" data-level="9.4.2" data-path="general-random-variables.html"><a href="general-random-variables.html#why-rely-on-the-standard-normal-distribution"><i class="fa fa-check"></i><b>9.4.2</b> Why rely on the standard normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="general-random-variables.html"><a href="general-random-variables.html#gamma-distribution"><i class="fa fa-check"></i><b>9.5</b> Gamma distribution</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="general-random-variables.html"><a href="general-random-variables.html#properties-of-gamma-distributions"><i class="fa fa-check"></i><b>9.5.1</b> Properties of Gamma distributions</a></li>
<li class="chapter" data-level="9.5.2" data-path="general-random-variables.html"><a href="general-random-variables.html#importance-of-the-gamma-distribution"><i class="fa fa-check"></i><b>9.5.2</b> Importance of the Gamma distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="general-random-variables.html"><a href="general-random-variables.html#chi2-distribution"><i class="fa fa-check"></i><b>9.6</b> <span class="math inline">\(\chi^2\)</span> distribution</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="general-random-variables.html"><a href="general-random-variables.html#chi2-properties"><i class="fa fa-check"></i><b>9.6.1</b> <span class="math inline">\(\chi^2\)</span> properties</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="general-random-variables.html"><a href="general-random-variables.html#students-t-distribution"><i class="fa fa-check"></i><b>9.7</b> Studentâs <span class="math inline">\(t\)</span> distribution</a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="general-random-variables.html"><a href="general-random-variables.html#history-of-students-t"><i class="fa fa-check"></i><b>9.7.1</b> History of Studentâs <span class="math inline">\(t\)</span></a></li>
<li class="chapter" data-level="9.7.2" data-path="general-random-variables.html"><a href="general-random-variables.html#differences-from-the-normal-distribution"><i class="fa fa-check"></i><b>9.7.2</b> Differences from the Normal Distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="multivar-distribs.html"><a href="multivar-distribs.html"><i class="fa fa-check"></i><b>10</b> Multivariate distributions</a>
<ul>
<li class="chapter" data-level="" data-path="multivar-distribs.html"><a href="multivar-distribs.html#learning-objectives-9"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="multivar-distribs.html"><a href="multivar-distribs.html#supplemental-readings-9"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="10.1" data-path="multivar-distribs.html"><a href="multivar-distribs.html#multivariate-distribution"><i class="fa fa-check"></i><b>10.1</b> Multivariate distribution</a></li>
<li class="chapter" data-level="10.2" data-path="multivar-distribs.html"><a href="multivar-distribs.html#examples-of-joint-pdfs"><i class="fa fa-check"></i><b>10.2</b> Examples of joint PDFs</a></li>
<li class="chapter" data-level="10.3" data-path="multivar-distribs.html"><a href="multivar-distribs.html#multivariate-cumulative-density-function"><i class="fa fa-check"></i><b>10.3</b> Multivariate cumulative density function</a></li>
<li class="chapter" data-level="10.4" data-path="multivar-distribs.html"><a href="multivar-distribs.html#marginalization"><i class="fa fa-check"></i><b>10.4</b> Marginalization</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="multivar-distribs.html"><a href="multivar-distribs.html#joint-vs.-conditional-pdf"><i class="fa fa-check"></i><b>10.4.1</b> Joint vs.Â conditional PDF</a></li>
<li class="chapter" data-level="10.4.2" data-path="multivar-distribs.html"><a href="multivar-distribs.html#why-does-marginalization-work"><i class="fa fa-check"></i><b>10.4.2</b> Why does marginalization work?</a></li>
<li class="chapter" data-level="10.4.3" data-path="multivar-distribs.html"><a href="multivar-distribs.html#move-to-the-continuous-case"><i class="fa fa-check"></i><b>10.4.3</b> Move to the continuous case</a></li>
<li class="chapter" data-level="10.4.4" data-path="multivar-distribs.html"><a href="multivar-distribs.html#a-simple-example"><i class="fa fa-check"></i><b>10.4.4</b> A (simple) example</a></li>
<li class="chapter" data-level="10.4.5" data-path="multivar-distribs.html"><a href="multivar-distribs.html#more-complex-example"><i class="fa fa-check"></i><b>10.4.5</b> More complex example</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="multivar-distribs.html"><a href="multivar-distribs.html#conditional-distribution"><i class="fa fa-check"></i><b>10.5</b> Conditional distribution</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="multivar-distribs.html"><a href="multivar-distribs.html#a-simple-example-of-dependence"><i class="fa fa-check"></i><b>10.5.1</b> A (simple) example of dependence</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="multivar-distribs.html"><a href="multivar-distribs.html#expectation-1"><i class="fa fa-check"></i><b>10.6</b> Expectation</a></li>
<li class="chapter" data-level="10.7" data-path="multivar-distribs.html"><a href="multivar-distribs.html#covariance-and-correlation"><i class="fa fa-check"></i><b>10.7</b> Covariance and correlation</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="multivar-distribs.html"><a href="multivar-distribs.html#some-observations"><i class="fa fa-check"></i><b>10.7.1</b> Some observations</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="multivar-distribs.html"><a href="multivar-distribs.html#sums-of-random-variables"><i class="fa fa-check"></i><b>10.8</b> Sums of random variables</a></li>
<li class="chapter" data-level="10.9" data-path="multivar-distribs.html"><a href="multivar-distribs.html#multivariate-normal-distribution-1"><i class="fa fa-check"></i><b>10.9</b> Multivariate normal distribution</a>
<ul>
<li class="chapter" data-level="10.9.1" data-path="multivar-distribs.html"><a href="multivar-distribs.html#bivariate-example"><i class="fa fa-check"></i><b>10.9.1</b> Bivariate example</a></li>
<li class="chapter" data-level="10.9.2" data-path="multivar-distribs.html"><a href="multivar-distribs.html#properties-of-the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>10.9.2</b> Properties of the multivariate normal distribution</a></li>
<li class="chapter" data-level="10.9.3" data-path="multivar-distribs.html"><a href="multivar-distribs.html#independence-and-multivariate-normal"><i class="fa fa-check"></i><b>10.9.3</b> Independence and multivariate normal</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="sequences-derivatives.html"><a href="sequences-derivatives.html#limits"><i class="fa fa-check"></i><b>11</b> Properties of random variables and limit theorems</a>
<ul>
<li class="chapter" data-level="" data-path="limits.html"><a href="limits.html"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="limits.html"><a href="limits.html#supplemental-readings-10"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="11.1" data-path="limits.html"><a href="limits.html#iterated-expectations"><i class="fa fa-check"></i><b>11.1</b> Iterated Expectations</a></li>
<li class="chapter" data-level="11.2" data-path="limits.html"><a href="limits.html#change-of-coordinates"><i class="fa fa-check"></i><b>11.2</b> Change of coordinates</a></li>
<li class="chapter" data-level="11.3" data-path="limits.html"><a href="limits.html#moment-generating-functions"><i class="fa fa-check"></i><b>11.3</b> Moment generating functions</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="limits.html"><a href="limits.html#the-moments-of-the-normal-distribution"><i class="fa fa-check"></i><b>11.3.1</b> The moments of the normal distribution</a></li>
<li class="chapter" data-level="11.3.2" data-path="limits.html"><a href="limits.html#extracting-moments-of-the-normal-distribution"><i class="fa fa-check"></i><b>11.3.2</b> Extracting moments of the normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="limits.html"><a href="limits.html#sequences-of-independent-random-variables"><i class="fa fa-check"></i><b>11.4</b> Sequences of independent random variables</a></li>
<li class="chapter" data-level="11.5" data-path="limits.html"><a href="limits.html#inequalities-and-limit-theorems"><i class="fa fa-check"></i><b>11.5</b> Inequalities and limit theorems</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="limits.html"><a href="limits.html#limit-theorems"><i class="fa fa-check"></i><b>11.5.1</b> Limit theorems</a></li>
<li class="chapter" data-level="11.5.2" data-path="limits.html"><a href="limits.html#weak-law-of-large-numbers"><i class="fa fa-check"></i><b>11.5.2</b> Weak law of large numbers</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="limits.html"><a href="limits.html#sequence-of-random-variables"><i class="fa fa-check"></i><b>11.6</b> Sequence of random variables</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="limits.html"><a href="limits.html#meanvariance-of-sample-mean"><i class="fa fa-check"></i><b>11.6.1</b> Mean/variance of sample mean</a></li>
<li class="chapter" data-level="11.6.2" data-path="limits.html"><a href="limits.html#weak-law-of-large-numbers-1"><i class="fa fa-check"></i><b>11.6.2</b> Weak law of large numbers</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="limits.html"><a href="limits.html#sequences-and-convergence"><i class="fa fa-check"></i><b>11.7</b> Sequences and convergence</a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="limits.html"><a href="limits.html#convergence-definitions"><i class="fa fa-check"></i><b>11.7.1</b> Convergence definitions</a></li>
<li class="chapter" data-level="11.7.2" data-path="limits.html"><a href="limits.html#convergence-in-probability"><i class="fa fa-check"></i><b>11.7.2</b> Convergence in probability</a></li>
<li class="chapter" data-level="11.7.3" data-path="limits.html"><a href="limits.html#almost-sure-convergence"><i class="fa fa-check"></i><b>11.7.3</b> Almost sure convergence</a></li>
<li class="chapter" data-level="11.7.4" data-path="limits.html"><a href="limits.html#convergence-in-distribution"><i class="fa fa-check"></i><b>11.7.4</b> Convergence in distribution</a></li>
<li class="chapter" data-level="11.7.5" data-path="limits.html"><a href="limits.html#convergence-in-distribution-not-rightarrow-convergence-in-probability"><i class="fa fa-check"></i><b>11.7.5</b> Convergence in distribution <span class="math inline">\(\not \Rightarrow\)</span> convergence in probability</a></li>
<li class="chapter" data-level="11.7.6" data-path="limits.html"><a href="limits.html#central-limit-theorem"><i class="fa fa-check"></i><b>11.7.6</b> Central limit theorem</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="classic-inference.html"><a href="classic-inference.html"><i class="fa fa-check"></i><b>12</b> Classical statistical inference</a>
<ul>
<li class="chapter" data-level="" data-path="classic-inference.html"><a href="classic-inference.html#learning-objectives-11"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="classic-inference.html"><a href="classic-inference.html#supplemental-readings-11"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="12.1" data-path="classic-inference.html"><a href="classic-inference.html#statistical-inference"><i class="fa fa-check"></i><b>12.1</b> Statistical inference</a></li>
<li class="chapter" data-level="12.2" data-path="classic-inference.html"><a href="classic-inference.html#parametric-models"><i class="fa fa-check"></i><b>12.2</b> Parametric models</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="classic-inference.html"><a href="classic-inference.html#examples-of-parametric-models"><i class="fa fa-check"></i><b>12.2.1</b> Examples of parametric models</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="classic-inference.html"><a href="classic-inference.html#point-estimates"><i class="fa fa-check"></i><b>12.3</b> Point estimates</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="classic-inference.html"><a href="classic-inference.html#properties-of-point-estimates"><i class="fa fa-check"></i><b>12.3.1</b> Properties of point estimates</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="classic-inference.html"><a href="classic-inference.html#confidence-sets"><i class="fa fa-check"></i><b>12.4</b> Confidence sets</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="classic-inference.html"><a href="classic-inference.html#caution-interpreting-confidence-intervals"><i class="fa fa-check"></i><b>12.4.1</b> Caution interpreting confidence intervals</a></li>
<li class="chapter" data-level="12.4.2" data-path="classic-inference.html"><a href="classic-inference.html#constructing-confidence-intervals"><i class="fa fa-check"></i><b>12.4.2</b> Constructing confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="classic-inference.html"><a href="classic-inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>12.5</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="classic-inference.html"><a href="classic-inference.html#types-of-errors"><i class="fa fa-check"></i><b>12.5.1</b> Types of errors</a></li>
<li class="chapter" data-level="12.5.2" data-path="classic-inference.html"><a href="classic-inference.html#power-function"><i class="fa fa-check"></i><b>12.5.2</b> Power function</a></li>
<li class="chapter" data-level="12.5.3" data-path="classic-inference.html"><a href="classic-inference.html#sided-tests"><i class="fa fa-check"></i><b>12.5.3</b> Sided tests</a></li>
<li class="chapter" data-level="12.5.4" data-path="classic-inference.html"><a href="classic-inference.html#example-hypothesis-test"><i class="fa fa-check"></i><b>12.5.4</b> Example hypothesis test</a></li>
<li class="chapter" data-level="12.5.5" data-path="classic-inference.html"><a href="classic-inference.html#wald-test"><i class="fa fa-check"></i><b>12.5.5</b> Wald test</a></li>
<li class="chapter" data-level="12.5.6" data-path="classic-inference.html"><a href="classic-inference.html#wald-or-t-test"><i class="fa fa-check"></i><b>12.5.6</b> Wald or <span class="math inline">\(t\)</span>-test?</a></li>
<li class="chapter" data-level="12.5.7" data-path="classic-inference.html"><a href="classic-inference.html#relationship-to-confidence-intervals"><i class="fa fa-check"></i><b>12.5.7</b> Relationship to confidence intervals</a></li>
<li class="chapter" data-level="12.5.8" data-path="classic-inference.html"><a href="classic-inference.html#statistical-vs.-scientific-significance"><i class="fa fa-check"></i><b>12.5.8</b> Statistical vs.Â scientific significance</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="classic-inference.html"><a href="classic-inference.html#p-values"><i class="fa fa-check"></i><b>12.6</b> <span class="math inline">\(p\)</span>-values</a>
<ul>
<li class="chapter" data-level="12.6.1" data-path="classic-inference.html"><a href="classic-inference.html#interpreting-p-values"><i class="fa fa-check"></i><b>12.6.1</b> Interpreting <span class="math inline">\(p\)</span>-values</a></li>
<li class="chapter" data-level="12.6.2" data-path="classic-inference.html"><a href="classic-inference.html#calculating-p-values"><i class="fa fa-check"></i><b>12.6.2</b> Calculating <span class="math inline">\(p\)</span>-values</a></li>
<li class="chapter" data-level="12.6.3" data-path="classic-inference.html"><a href="classic-inference.html#pearsons-chi2-test-for-multinomial-data"><i class="fa fa-check"></i><b>12.6.3</b> Pearsonâs <span class="math inline">\(\chi^2\)</span> test for multinomial data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="mle-ols.html"><a href="mle-ols.html"><i class="fa fa-check"></i><b>13</b> Maximum likelihood estimation and linear regression</a>
<ul>
<li class="chapter" data-level="" data-path="mle-ols.html"><a href="mle-ols.html#learning-objectives-12"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="mle-ols.html"><a href="mle-ols.html#supplemental-readings-12"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="13.1" data-path="mle-ols.html"><a href="mle-ols.html#maximum-likelihood"><i class="fa fa-check"></i><b>13.1</b> Maximum likelihood</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="mle-ols.html"><a href="mle-ols.html#properties-of-maximum-likelihood-estimators"><i class="fa fa-check"></i><b>13.1.1</b> Properties of maximum likelihood estimators</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="mle-ols.html"><a href="mle-ols.html#least-squares-regression"><i class="fa fa-check"></i><b>13.2</b> Least squares regression</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="mle-ols.html"><a href="mle-ols.html#simple-linear-regression"><i class="fa fa-check"></i><b>13.2.1</b> Simple linear regression</a></li>
<li class="chapter" data-level="13.2.2" data-path="mle-ols.html"><a href="mle-ols.html#estimation-strategy"><i class="fa fa-check"></i><b>13.2.2</b> Estimation strategy</a></li>
<li class="chapter" data-level="13.2.3" data-path="mle-ols.html"><a href="mle-ols.html#least-squares-estimator"><i class="fa fa-check"></i><b>13.2.3</b> Least squares estimator</a></li>
<li class="chapter" data-level="13.2.4" data-path="mle-ols.html"><a href="mle-ols.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>13.2.4</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="13.2.5" data-path="mle-ols.html"><a href="mle-ols.html#properties-of-the-least-squares-estimator"><i class="fa fa-check"></i><b>13.2.5</b> Properties of the least squares estimator</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="mle-ols.html"><a href="mle-ols.html#assumptions-of-linear-regression-models"><i class="fa fa-check"></i><b>13.3</b> Assumptions of linear regression models</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="mle-ols.html"><a href="mle-ols.html#linearity"><i class="fa fa-check"></i><b>13.3.1</b> Linearity</a></li>
<li class="chapter" data-level="13.3.2" data-path="mle-ols.html"><a href="mle-ols.html#constant-variance"><i class="fa fa-check"></i><b>13.3.2</b> Constant variance</a></li>
<li class="chapter" data-level="13.3.3" data-path="mle-ols.html"><a href="mle-ols.html#normality"><i class="fa fa-check"></i><b>13.3.3</b> Normality</a></li>
<li class="chapter" data-level="13.3.4" data-path="mle-ols.html"><a href="mle-ols.html#independence"><i class="fa fa-check"></i><b>13.3.4</b> Independence</a></li>
<li class="chapter" data-level="13.3.5" data-path="mle-ols.html"><a href="mle-ols.html#fixed-x-or-x-measured-without-error-and-independent-of-the-error"><i class="fa fa-check"></i><b>13.3.5</b> Fixed <span class="math inline">\(X\)</span>, or <span class="math inline">\(X\)</span> measured without error and independent of the error</a></li>
<li class="chapter" data-level="13.3.6" data-path="mle-ols.html"><a href="mle-ols.html#x-is-not-invariant"><i class="fa fa-check"></i><b>13.3.6</b> <span class="math inline">\(X\)</span> is not invariant</a></li>
<li class="chapter" data-level="13.3.7" data-path="mle-ols.html"><a href="mle-ols.html#handling-violations-of-assumptions"><i class="fa fa-check"></i><b>13.3.7</b> Handling violations of assumptions</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="mle-ols.html"><a href="mle-ols.html#unusual-and-influential-data"><i class="fa fa-check"></i><b>13.4</b> Unusual and influential data</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="mle-ols.html"><a href="mle-ols.html#terms"><i class="fa fa-check"></i><b>13.4.1</b> Terms</a></li>
<li class="chapter" data-level="13.4.2" data-path="mle-ols.html"><a href="mle-ols.html#measuring-leverage"><i class="fa fa-check"></i><b>13.4.2</b> Measuring leverage</a></li>
<li class="chapter" data-level="13.4.3" data-path="mle-ols.html"><a href="mle-ols.html#measuring-discrepancy"><i class="fa fa-check"></i><b>13.4.3</b> Measuring discrepancy</a></li>
<li class="chapter" data-level="13.4.4" data-path="mle-ols.html"><a href="mle-ols.html#measuring-influence"><i class="fa fa-check"></i><b>13.4.4</b> Measuring influence</a></li>
<li class="chapter" data-level="13.4.5" data-path="mle-ols.html"><a href="mle-ols.html#visualizing-leverage-discrepancy-and-influence"><i class="fa fa-check"></i><b>13.4.5</b> Visualizing leverage, discrepancy, and influence</a></li>
<li class="chapter" data-level="13.4.6" data-path="mle-ols.html"><a href="mle-ols.html#numerical-rules-of-thumb"><i class="fa fa-check"></i><b>13.4.6</b> Numerical rules of thumb</a></li>
<li class="chapter" data-level="13.4.7" data-path="mle-ols.html"><a href="mle-ols.html#how-to-treat-unusual-observations"><i class="fa fa-check"></i><b>13.4.7</b> How to treat unusual observations</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="mle-ols.html"><a href="mle-ols.html#non-normally-distributed-errors"><i class="fa fa-check"></i><b>13.5</b> Non-normally distributed errors</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="mle-ols.html"><a href="mle-ols.html#detecting-non-normally-distributed-errors"><i class="fa fa-check"></i><b>13.5.1</b> Detecting non-normally distributed errors</a></li>
<li class="chapter" data-level="13.5.2" data-path="mle-ols.html"><a href="mle-ols.html#fixing-non-normally-distributed-errors"><i class="fa fa-check"></i><b>13.5.2</b> Fixing non-normally distributed errors</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="mle-ols.html"><a href="mle-ols.html#non-constant-error-variance"><i class="fa fa-check"></i><b>13.6</b> Non-constant error variance</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="mle-ols.html"><a href="mle-ols.html#detecting-heteroscedasticity"><i class="fa fa-check"></i><b>13.6.1</b> Detecting heteroscedasticity</a></li>
<li class="chapter" data-level="13.6.2" data-path="mle-ols.html"><a href="mle-ols.html#accounting-for-heteroscedasticity"><i class="fa fa-check"></i><b>13.6.2</b> Accounting for heteroscedasticity</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="mle-ols.html"><a href="mle-ols.html#non-linearity-in-the-data"><i class="fa fa-check"></i><b>13.7</b> Non-linearity in the data</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="mle-ols.html"><a href="mle-ols.html#partial-residual-plots"><i class="fa fa-check"></i><b>13.7.1</b> Partial residual plots</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="mle-ols.html"><a href="mle-ols.html#collinearity"><i class="fa fa-check"></i><b>13.8</b> Collinearity</a>
<ul>
<li class="chapter" data-level="13.8.1" data-path="mle-ols.html"><a href="mle-ols.html#perfect-collinearity"><i class="fa fa-check"></i><b>13.8.1</b> Perfect collinearity</a></li>
<li class="chapter" data-level="13.8.2" data-path="mle-ols.html"><a href="mle-ols.html#less-than-perfect-collinearity"><i class="fa fa-check"></i><b>13.8.2</b> Less-than-perfect collinearity</a></li>
<li class="chapter" data-level="13.8.3" data-path="mle-ols.html"><a href="mle-ols.html#fixing-multicollinearity"><i class="fa fa-check"></i><b>13.8.3</b> Fixing multicollinearity</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>14</b> Bayesian inference</a>
<ul>
<li class="chapter" data-level="" data-path="bayesian-inference.html"><a href="bayesian-inference.html#learning-objectives-13"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="bayesian-inference.html"><a href="bayesian-inference.html#supplemental-readings-13"><i class="fa fa-check"></i>Supplemental readings</a></li>
<li class="chapter" data-level="14.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bayesian-philosophy"><i class="fa fa-check"></i><b>14.1</b> Bayesian philosophy</a></li>
<li class="chapter" data-level="14.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bayes-theorem"><i class="fa fa-check"></i><b>14.2</b> Bayesâ theorem</a></li>
<li class="chapter" data-level="14.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bayesian-method"><i class="fa fa-check"></i><b>14.3</b> Bayesian method</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#example-coin-tossing"><i class="fa fa-check"></i><b>14.3.1</b> Example: coin tossing</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#updating-your-prior-beliefs"><i class="fa fa-check"></i><b>14.4</b> Updating your prior beliefs</a></li>
<li class="chapter" data-level="14.5" data-path="bayesian-inference.html"><a href="bayesian-inference.html#simulation"><i class="fa fa-check"></i><b>14.5</b> Simulation</a></li>
<li class="chapter" data-level="14.6" data-path="bayesian-inference.html"><a href="bayesian-inference.html#priors"><i class="fa fa-check"></i><b>14.6</b> Priors</a>
<ul>
<li class="chapter" data-level="14.6.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#improper-priors"><i class="fa fa-check"></i><b>14.6.1</b> Improper priors</a></li>
<li class="chapter" data-level="14.6.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#flat-priors-are-not-invariant"><i class="fa fa-check"></i><b>14.6.2</b> Flat priors are not invariant</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="bayesian-inference.html"><a href="bayesian-inference.html#multiparameter-problems"><i class="fa fa-check"></i><b>14.7</b> Multiparameter problems</a></li>
<li class="chapter" data-level="14.8" data-path="bayesian-inference.html"><a href="bayesian-inference.html#critiques-and-defenses-of-bayesian-inference"><i class="fa fa-check"></i><b>14.8</b> Critiques and defenses of Bayesian inference</a>
<ul>
<li class="chapter" data-level="14.8.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#critique-of-bayesian-inference"><i class="fa fa-check"></i><b>14.8.1</b> Critique of Bayesian inference</a></li>
<li class="chapter" data-level="14.8.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#defense-of-bayesian-inference"><i class="fa fa-check"></i><b>14.8.2</b> Defense of Bayesian inference</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bayesian-inference.html"><a href="bayesian-inference.html#acknowledgements-1"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Math Camp</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian-inference" class="section level1 hasAnchor" number="14">
<h1><span class="header-section-number">Lecture 14</span> Bayesian inference<a href="bayesian-inference.html#bayesian-inference" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="learning-objectives-13" class="section level2 unnumbered hasAnchor">
<h2>Learning objectives<a href="bayesian-inference.html#learning-objectives-13" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Define the Bayesian philosophy and distinguish from frequentist inference</li>
<li>Define core concepts for Bayesian methods</li>
<li>Discuss the importance of simulation to estimate density functions</li>
<li>Assess methods for defining priors</li>
<li>Identify the strengths and weaknesses of Bayesian inference</li>
</ul>
</div>
<div id="supplemental-readings-13" class="section level2 unnumbered hasAnchor">
<h2>Supplemental readings<a href="bayesian-inference.html#supplemental-readings-13" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Chapter 8.1 <span class="citation">Bertsekas and Tsitsiklis (<a href="#ref-bertsekas2008" role="doc-biblioref">2008</a>)</span></li>
<li><span class="citation">Wasserman (<a href="#ref-wasserman2013" role="doc-biblioref">2013</a>)</span>
<ul>
<li><a href="https://link-springer-com.proxy.uchicago.edu/content/pdf/10.1007%2F978-0-387-21736-9_11.pdf">Ch 11 - Bayesian Inference</a></li>
</ul></li>
</ul>
</div>
<div id="bayesian-philosophy" class="section level2 hasAnchor" number="14.1">
<h2><span class="header-section-number">14.1</span> Bayesian philosophy<a href="bayesian-inference.html#bayesian-philosophy" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Frequentist methods</strong> are the major methods weâve employed thus far. The frequentist point of view is based on the following postulates:</p>
<ol style="list-style-type: decimal">
<li>Probability refers to limiting relative frequencies. Probabilities are objective properties of the real world.</li>
<li>Parameters are fixed, unknown constants. Because they are not fluctuating, no useful probability statements can be made about parameters.</li>
<li>Statistical procedures should be designed to have well-defined long run frequency properties. For example, a 95% confidence interval should trap the true value of the parameter with limiting frequency at least 95 percent.</li>
</ol>
<p>An alternative approach to inference is called <strong>Bayesian inference</strong>. The Bayesian approach is based on the following postulates:</p>
<ol style="list-style-type: decimal">
<li>Probability describes degree of belief, not limiting frequency. As such, we can make probability statements about lots of things, not just data which are subject to random variables. For example, I might say âthe probability that Donald Trump offended someone on November 25, 2018â is <span class="math inline">\(0.99\)</span>. This does not refer to any limiting frequency. It reflects my strength of belief that the proposition is true.</li>
<li>We can make probability statements about parameters, even though they are fixed constants.</li>
<li>We make inferences about a parameter <span class="math inline">\(\theta\)</span> by producing a probability distribution for <span class="math inline">\(\theta\)</span>. Inferences, such as point estimates and interval estimates, may then be extracted from this distribution.</li>
</ol>
</div>
<div id="bayes-theorem" class="section level2 hasAnchor" number="14.2">
<h2><span class="header-section-number">14.2</span> Bayesâ theorem<a href="bayesian-inference.html#bayes-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Bayesâ theorem</strong> is a fundamental component of both probability and statistics and is central to understanding the differences between frequentist and Bayesian inference. For two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, Bayesâ theorem states that:</p>
<p><span class="math display">\[\Pr(B|A) = \frac{\Pr(A|B) \times \Pr(B)}{\Pr(A)}\]</span></p>
<p>Bayesâ rule tells us how to <strong>invert</strong> conditional probabilities. That is, to find <span class="math inline">\(\Pr(B|A)\)</span> from <span class="math inline">\(\Pr(A|B)\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-199" class="example"><strong>Example 14.1  (Coin tossing) </strong></span>Toss a coin 5 times. Let <span class="math inline">\(H_1 =\)</span> âfirst toss is headsâ and let <span class="math inline">\(H_A =\)</span> âall 5 tosses are headsâ. Therefore <span class="math inline">\(\Pr(H_1 | H_A) = 1\)</span> (if all five tosses are heads, then the first one must by definition also be heads) and <span class="math inline">\(\Pr(H_A | H_1) = \frac{1}{16}\)</span> (<span class="math inline">\(\frac{1}{2^4} = \frac{1}{16}\)</span>).</p>
<p>However we can also use Bayesâ theorem to calculate <span class="math inline">\(\Pr(H_1 | H_A)\)</span> using <span class="math inline">\(\Pr(H_A | H_1)\)</span>. The terms we need are:</p>
<ul>
<li><span class="math inline">\(\Pr(H_A | H_1) = \frac{1}{16}\)</span></li>
<li><span class="math inline">\(\Pr(H_1) = \frac{1}{2}\)</span></li>
<li><span class="math inline">\(\Pr(H_A) = \frac{1}{32}\)</span></li>
</ul>
<p>So,</p>
<p><span class="math display">\[\Pr(H_A | H_1) = \frac{\Pr(H_A | H_1) \times \Pr(H_1)}{\Pr(H_A)} = \frac{\frac{1}{16} \times \frac{1}{2}}{\frac{1}{32}} = 1\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-200" class="example"><strong>Example 14.2  (False positive fallacy) </strong></span>A test for a certain rare disease is assumed to be correct 95% of the time:</p>
<ul>
<li>If a person has the disease, then the test results are positive with probability <span class="math inline">\(0.95\)</span></li>
<li>If the person does not have the disease, then the test results are negative with probability <span class="math inline">\(0.95\)</span></li>
</ul>
<p>A random person drawn from a certain population has probability <span class="math inline">\(0.001\)</span> of having the disease. Given that the person just tested positive, what is the probability of having the disease?</p>
<ul>
<li><span class="math inline">\(A = {\text{person has the disease}}\)</span></li>
<li><span class="math inline">\(B = {\text{test result is positive for the disease}}\)</span></li>
<li><span class="math inline">\(\Pr(A) = 0.001\)</span></li>
<li><span class="math inline">\(\Pr(B | A) = 0.95\)</span></li>
<li><span class="math inline">\(\Pr(B | A = 0) = 0.05\)</span></li>
</ul>
<p><span class="math display">\[
\begin{align}
\Pr(\text{person has the disease} | \text{test is positive}) &amp;= \Pr(A|B) \\
&amp; = \frac{\Pr(A) \times \Pr(B|A)}{\Pr(B)} \\
&amp; = \frac{\Pr(A) \times \Pr(B|A)}{\Pr(A) \times \Pr(B|A) + \Pr(A = 0) \times(B | A = 0)} \\
&amp; = \frac{0.001 \times 0.95}{0.001 \times 0.95 + 0.999 \times 0.05} \\
&amp; = 0.0187
\end{align}
\]</span></p>
<p>Even though the test is fairly accurate, a person who has tested positive is still very unlikely (less than 2%) to have the disease. Because the base rate of the disease in the population is so low, the vast majority of people taking the test are healthy and even with an accurate test most of the positives will be healthy people.<a href="#fn28" class="footnote-ref" id="fnref28"><sup>28</sup></a></p>
</div>
</div>
<div id="bayesian-method" class="section level2 hasAnchor" number="14.3">
<h2><span class="header-section-number">14.3</span> Bayesian method<a href="bayesian-inference.html#bayesian-method" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bayesian inference is usually carried out in the following way:</p>
<ol style="list-style-type: decimal">
<li>Choose a probability density <span class="math inline">\(f(\theta)\)</span> â called the <strong>prior distribution</strong> â that expresses our beliefs about a parameter <span class="math inline">\(\theta\)</span> before we see any data.</li>
<li>Choose a statistical model <span class="math inline">\(f(x|\theta)\)</span> that reflects our beliefs about <span class="math inline">\(x\)</span> given <span class="math inline">\(\theta\)</span>. Note that we now write this as <span class="math inline">\(f(x|\theta)\)</span>, not <span class="math inline">\(f(x; \theta)\)</span>.</li>
<li>After observing data <span class="math inline">\(X_1, \ldots, X_n\)</span>, we update our beliefs and calculate the <strong>posterior distribution</strong> <span class="math inline">\(f(\theta | X_1, \ldots, X_n)\)</span>.</li>
</ol>
<p>To calculate the posterior, suppose that <span class="math inline">\(\theta\)</span> is discrete and that there is a single, discrete observation <span class="math inline">\(X\)</span>. We should use a capital letter to denote the parameter since we now treat it like a random variable, so let <span class="math inline">\(\Theta\)</span> denote the parameter. In this discrete setting,</p>
<p><span class="math display">\[
\begin{align}
\Pr(\Theta = \theta | X = x) &amp;= \frac{\Pr(X = x, \Theta = \theta)}{\Pr(X = x)} \\
&amp;= \frac{\Pr(X = x | \Theta = \theta) \Pr(\Theta = \theta)}{\sum_\theta \Pr (X = x| \Theta = \theta) \Pr (\Theta = \theta)}
\end{align}
\]</span></p>
<p>which is a basic application of Bayesâ theorem. The version for continuous variables is obtained using density functions</p>
<p><span class="math display">\[f(\theta | x) = \frac{f(x | \theta) f(\theta)}{\int f(x | \theta) f(\theta) d\theta}\]</span></p>
<p>If we have <span class="math inline">\(n\)</span> IID observations <span class="math inline">\(X_1, \ldots, X_n\)</span>, we replace <span class="math inline">\(f(x | \theta)\)</span> with</p>
<p><span class="math display">\[f(x_1, \ldots, x_n | \theta) = \prod_{i = 1}^n f(x_i | \theta) = \Lagr_n(\theta)\]</span></p>
<p>We will now write <span class="math inline">\(X^n\)</span> to mean <span class="math inline">\((X_1, \ldots, X_n)\)</span> and <span class="math inline">\(x^n\)</span> to mean <span class="math inline">\((x_1, \ldots, x_n)\)</span>. Now,</p>
<p><span class="math display">\[
\begin{align}
f(\theta | x^n) &amp;= \frac{f(x^n | \theta) f(\theta)}{\int f(x^n | \theta) f(\theta) d\theta} \\
&amp;= \frac{\Lagr_n(\theta) f(\theta)}{c_n} \\
&amp;\propto \Lagr_n(\theta) f(\theta)
\end{align}
\]</span></p>
<p>where</p>
<p><span class="math display">\[c_n = \int f(x^n | \theta) f(\theta) d\theta\]</span></p>
<p>is called the <strong>normalizing constant</strong>. We can summarize this by stating the <strong>posterior is proportional to Likelihood times Prior</strong>:</p>
<p><span class="math display">\[f(\theta | x^n) \propto \Lagr_n(\theta) f(\theta)\]</span></p>
<p>Since <span class="math inline">\(c_n\)</span> does not depend on <span class="math inline">\(\theta\)</span>, we can safely ignore it at this point, and in fact can recover the constant later on if we need it.</p>
<p>With the posterior distribution, we can get a point estimate by summarizing the center of the posterior. Typically this is the mean or mode of the posterior. The posterior mean is</p>
<p><span class="math display">\[\bar{\theta}_n = \int \theta f(\theta | x^n) d\theta = \frac{\int \theta \Lagr_n(\theta) f(\theta)}{\int \Lagr_n(\theta) f(\theta) d\theta}\]</span></p>
<p>We can also obtain a Bayesian interval estimate. We find <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> such that</p>
<p><span class="math display">\[\int_{-\infty}^a f(\theta | x^n) d\theta = \int_b^\infty f(\theta | x^n) d\theta = \frac{\alpha}{2}\]</span></p>
<p>Let <span class="math inline">\(C = (a,b)\)</span>. Then</p>
<p><span class="math display">\[\Pr (\theta \in C | x^n) = \int_a^b f(\theta | x^n) d\theta = 1 - \alpha\]</span></p>
<p>So <span class="math inline">\(C\)</span> is a <span class="math inline">\(1 - \alpha\)</span> <strong>posterior</strong> (or <strong>credible</strong>) <strong>interval</strong>.</p>
<div id="example-coin-tossing" class="section level3 hasAnchor" number="14.3.1">
<h3><span class="header-section-number">14.3.1</span> Example: coin tossing<a href="bayesian-inference.html#example-coin-tossing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are three types of coins with different probabilities of landing heads when tossed.</p>
<ul>
<li>Type <span class="math inline">\(A\)</span> coins are fair, with <span class="math inline">\(p = 0.5\)</span> of heads</li>
<li>Type <span class="math inline">\(B\)</span> coins are bent, with <span class="math inline">\(p = 0.6\)</span> of heads</li>
<li>Type <span class="math inline">\(C\)</span> coins are bent, with <span class="math inline">\(p = 0.9\)</span> of heads</li>
</ul>
<p>Suppose I have a drawer containing 5 coins: 2 of type <span class="math inline">\(A\)</span>, 2 of type <span class="math inline">\(B\)</span>, and 1 of type <span class="math inline">\(C\)</span>. I reach into the drawer and pick a coin at random. Without showing you the coin I flip it once and get heads. What is the probability it is type <span class="math inline">\(A\)</span>? Type <span class="math inline">\(B\)</span>? Type <span class="math inline">\(C\)</span>?</p>
<div id="terminology" class="section level4 hasAnchor" number="14.3.1.1">
<h4><span class="header-section-number">14.3.1.1</span> Terminology<a href="bayesian-inference.html#terminology" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(C\)</span> be the event the chosen coin was of the respective type. Let <span class="math inline">\(D\)</span> be the event that the toss is heads. The problem then asks us to find:</p>
<p><span class="math display">\[\Pr(A|D), \Pr(B|D), \Pr(C|D)\]</span></p>
<p>Before applying Bayesâ theorem, we need to define a few things:</p>
<ul>
<li><p><strong>Experiment</strong> - pick a coin from the drawer at random, flip it, and record the result</p></li>
<li><p><strong>Data</strong> - the result of the experiment. Here, <span class="math inline">\(D = \text{heads}\)</span>. <span class="math inline">\(D\)</span> is data that provides evidence for or against each hypothesis</p></li>
<li><p><strong>Hypotheses</strong> - we are testing three hypotheses: the coin is type <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, or <span class="math inline">\(C\)</span></p></li>
<li><p><strong>Prior probability</strong> - the probability of each hypothesis prior to tossing the coin (collecting data). Since the drawer has 2 coins of type <span class="math inline">\(A\)</span>, 2 of type <span class="math inline">\(B\)</span>, and 1 of type <span class="math inline">\(C\)</span>, we have:</p>
<p><span class="math display">\[\Pr(A) = 0.4, \Pr(B) = 0.4, \Pr(C) = 0.2\]</span></p></li>
<li><p><strong>Likelihood</strong> - the likelihood function is <span class="math inline">\(\Pr(D|H)\)</span>, the probability of the data assuming that the hypothesis is true. Most often we will consider the data as fixed and let the hypothesis vary. For example, <span class="math inline">\(\Pr(D|A) =\)</span> probability of heads if the coin is type <span class="math inline">\(A\)</span>. In our case, the likelihoods are:</p>
<p><span class="math display">\[\Pr(D|A) = 0.5, \Pr(D|B) = 0.6, \Pr(D|C) = 0.9\]</span></p>
<p>We can think of these as parameters for a series of Bernoulli distributions.</p></li>
<li><p><strong>Posterior probability</strong> - the probability (posterior to) of each hypothesis given the data from tossing the coin:</p>
<p><span class="math display">\[\Pr(A|D), \Pr(B|D), \Pr(C|D)\]</span></p>
<p>These posterior probabilities are what we want to find.</p></li>
</ul>
<p>We can now use Bayesâ theorem to compute each of the posterior probabilities. The theorem says:</p>
<p><span class="math display">\[\Pr(A|D) = \frac{\Pr(D|A) \times \Pr(A)}{\Pr(D)}\]</span>
<span class="math display">\[\Pr(B|D) = \frac{\Pr(D|B) \times \Pr(B)}{\Pr(D)}\]</span>
<span class="math display">\[\Pr(C|D) = \frac{\Pr(D|C) \times \Pr(C)}{\Pr(D)}\]</span></p>
<p><span class="math inline">\(\Pr(D)\)</span> can be computed using the law of total probability:</p>
<p><span class="math display">\[
\begin{align}
\Pr(D) &amp; = \Pr(D|A) \times \Pr(A) + \Pr(D|B) \times \Pr(B) + \Pr(D|C) \times \Pr(C) \\
&amp; = 0.5 \times 0.4 + 0.6 \times 0.4 + 0.9 \times 0.2 = 0.62
\end{align}
\]</span></p>
<p>So each of the posterior probabilities are:</p>
<p><span class="math display">\[\Pr(A|D) = \frac{\Pr(D|A) \times \Pr(A)}{\Pr(D)} = \frac{0.5 \times 0.4}{0.62} = \frac{0.2}{0.62}\]</span></p>
<p><span class="math display">\[\Pr(B|D) = \frac{\Pr(D|B) \times \Pr(B)}{\Pr(D)} = \frac{0.6 \times 0.4}{0.62} = \frac{0.24}{0.62}\]</span></p>
<p><span class="math display">\[\Pr(C|D) = \frac{\Pr(D|C) \times \Pr(C)}{\Pr(D)} = \frac{0.9 \times 0.2}{0.62} = \frac{0.18}{0.62}\]</span></p>
<p>Notice that the total probability <span class="math inline">\(\Pr(D)\)</span> is the same in each of the denominators and is the sum of the three numerators.</p>
<table>
<colgroup>
<col width="6%" />
<col width="13%" />
<col width="17%" />
<col width="36%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th>hypothesis</th>
<th>prior</th>
<th>likelihood</th>
<th>Bayes numerator</th>
<th>posterior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(H\)</span></td>
<td><span class="math inline">\(\Pr(H)\)</span></td>
<td><span class="math inline">\(\Pr(D\mid H)\)</span></td>
<td><span class="math inline">\(\Pr(D \mid H) \times \Pr(H)\)</span></td>
<td><span class="math inline">\(\Pr(H \mid D)\)</span></td>
</tr>
<tr class="even">
<td>A</td>
<td>0.4</td>
<td>0.5</td>
<td>0.2</td>
<td>0.3226</td>
</tr>
<tr class="odd">
<td>B</td>
<td>0.4</td>
<td>0.6</td>
<td>0.24</td>
<td>0.3871</td>
</tr>
<tr class="even">
<td>C</td>
<td>0.2</td>
<td>0.9</td>
<td>0.18</td>
<td>0.2903</td>
</tr>
<tr class="odd">
<td>total</td>
<td>1</td>
<td></td>
<td>0.62</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>The <strong>Bayes numerator</strong> is the product of the prior and the likelihood. The posterior probability is obtained by dividing the Bayes numerator by <span class="math inline">\(\Pr(D) = 0.625\)</span>.</p>
<p>The process of going from the prior probability <span class="math inline">\(\Pr(H)\)</span> to the posterior <span class="math inline">\(\Pr(H|D)\)</span> is called <strong>Bayesian updating</strong>. Bayesian updating uses the data to alter our understanding of the probability of each hypothesis.</p>
</div>
<div id="things-to-notice" class="section level4 hasAnchor" number="14.3.1.2">
<h4><span class="header-section-number">14.3.1.2</span> Things to notice<a href="bayesian-inference.html#things-to-notice" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li>The posterior probabilities for each hypothesis are in the last column. Coin <span class="math inline">\(B\)</span> is the most probable, even with the decrease from the prior to the posterior. <span class="math inline">\(C\)</span> has increased from 0.2 to 0.29.</li>
<li>The Bayes numerator determines the posterior probability. To compute the posterior probability, simply rescale the Bayes numerator so that it sums to 1.</li>
<li>If all we care about is finding the most likely hypothesis, the Bayes numerator works as well as the normalized posterior.</li>
<li>The posterior probability represents the outcome of a tug-of-war between the likelihood and the prior. When calculating the posterior, a large prior may be deflated by a small likelihood, and a small prior may be inflated by a large likelihood.</li>
</ol>
<p>Therefore we can express Bayesâ theorem as:</p>
<p><span class="math display">\[\Pr(\text{hypothesis}| \text{data}) = \frac{\Pr(\text{data} | \text{hypothesis}) \times \Pr(\text{hypothesis})}{\Pr(\text{data})}\]</span></p>
<p><span class="math display">\[\Pr(H|D) = \frac{\Pr(D | H) \times \Pr(H)}{\Pr(D)}\]</span></p>
<p>With the data fixed, the denominator <span class="math inline">\(\Pr(D)\)</span> just serves to normalize the total posterior probability to 1. So we could express Bayesâ theorem as a statement about the proportionality of two functions of <span class="math inline">\(H\)</span>:</p>
<p><span class="math display">\[\Pr(\text{hypothesis}| \text{data}) \propto \Pr(\text{data} | \text{hypothesis}) \times \Pr(\text{hypothesis})\]</span>
<span class="math display">\[\text{Posterior} \propto \text{Likelihood} \times \text{Prior}\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-201" class="example"><strong>Example 14.3  (Bernoulli random variable) </strong></span>Let <span class="math inline">\(X_1, \ldots, X_n \sim \text{Bernoulli} (p)\)</span>. Suppose we take the uniform distribution <span class="math inline">\(f(p) = 1\)</span> as a prior. By Bayesâ theorem, the posterior has the form</p>
<p><span class="math display">\[
\begin{align}
f(p | x^n) &amp;\propto f(p) \Lagr_n(p) \\
&amp;= p^s (1 - p)^{n - s} \\
&amp;= p^{s + 1 - 1} (1 - p)^{n - s + 1 - 1}
\end{align}
\]</span></p>
<p>where <span class="math inline">\(s = \sum_{i=1}^n x_i\)</span> is the number of successes. Importantly, a random variable has a Beta distribution with parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> if its density is</p>
<p><span class="math display">\[f(p; \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)}p^{\alpha - 1} (1 - p)^{\beta - 1}\]</span></p>
<p><img src="14-bayesian-inference_files/figure-html/beta-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>We can see that the posterior for <span class="math inline">\(p\)</span> is a Beta distribution with parameters <span class="math inline">\(s + 1\)</span> and <span class="math inline">\(n - s + 1\)</span>. That is,</p>
<p><span class="math display">\[f(p | x^n) = \frac{\Gamma(n + 2)}{\Gamma(s + 1) \Gamma(n - s + 1)}p^{(s + 1) - 1} (1 - p)^{(n - s + 1) - 1}\]</span></p>
<p>We write this as</p>
<p><span class="math display">\[p | x^n \sim \text{Beta} (s + 1, n - s + 1)\]</span></p>
<p>Notice that we have figured out the normalizing constant <span class="math inline">\(c_n = \frac{\Gamma(n + 2)}{\Gamma(s + 1) \Gamma(n - s + 1)}\)</span> without actually doing the integral <span class="math inline">\(\int \Lagr_n(p) f(p) dp\)</span>. The mean of a <span class="math inline">\(\text{Beta}(\alpha, \beta)\)</span> distribution is <span class="math inline">\(\frac{\alpha}{\alpha + \beta}\)</span>, so the Bayes estimator is</p>
<p><span class="math display">\[\bar{p} = \frac{s + 1}{n + 2}\]</span></p>
<p>We can rewrite the estimator as</p>
<p><span class="math display">\[\bar{p} = \lambda_n \hat{p} + (1 - \lambda_n) \tilde{p}\]</span></p>
<p>where <span class="math inline">\(\hat{p} = \frac{s}{n}\)</span> is the MLE, <span class="math inline">\(\tilde{p} = \frac{1}{2}\)</span> is the prior mean, and <span class="math inline">\(\lambda_n = \frac{n}{n + 2} \approx 1\)</span>. So we can think of the MLE as the estimate for <span class="math inline">\(p\)</span> with a flat prior. If we have a non-flat prior, then our estimate <span class="math inline">\(\bar{p}\)</span> is a weighted average between the prior and the MLE.</p>
<p>A 95% credible interval can be obtained by numerically finding <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> such that <span class="math inline">\(\int_a^b f(p | x^n) dp = 0.95\)</span>.</p>
<p>Suppose that instead of a uniform prior, we use the prior <span class="math inline">\(p \sim \text{Beta} (\alpha, \beta)\)</span>. If we repeat the calculations from before, we see that <span class="math inline">\(p | x^n \sim \text{Beta} (\alpha + s, \beta + n - s)\)</span>. The flat prior is the special case with <span class="math inline">\(\alpha = \beta = 1\)</span>. The posterior mean is</p>
<p><span class="math display">\[\bar{p} = \frac{\alpha + s}{\alpha + \beta + n} = \left( \frac{n}{\alpha + \beta + n} \right) \hat{p} + \left( \frac{\alpha + \beta}{\alpha + \beta + n} \right) p_0\]</span></p>
<p>where <span class="math inline">\(p_0 = \frac{\alpha}{\alpha + \beta}\)</span> is the prior mean.</p>
</div>
</div>
</div>
</div>
<div id="updating-your-prior-beliefs" class="section level2 hasAnchor" number="14.4">
<h2><span class="header-section-number">14.4</span> Updating your prior beliefs<a href="bayesian-inference.html#updating-your-prior-beliefs" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In life we continually update our beliefs with each new experience of the world. In Bayesian inference, todayâs posterior is tomorrowâs prior.</p>
<div class="example">
<p><span id="exm:unlabeled-div-202" class="example"><strong>Example 14.4  (September 11, 2001) </strong></span>Consider the September 11th attacks in New York City.<a href="#fn29" class="footnote-ref" id="fnref29"><sup>29</sup></a> Say that before the first plane hit, our estimate of the probability of a terror attack on tall buildings in Manhattan was just 1 in 20,000, or <span class="math inline">\(0.00005\)</span>. But we also assign a low probability to a plane hitting the World Trade Center by accident: 1 in 12,500 on any given day.<a href="#fn30" class="footnote-ref" id="fnref30"><sup>30</sup></a> Consider the use of Bayesâ theorem in this instance. What is the probability of terrorists crashing planes into Manhattan skyscrapers given the first plane hitting the World Trade Center?</p>
<ul>
<li>Our initial estimate of how likely it is that terrorists would crash planes into Manhattan skyscrapers is <span class="math inline">\(\Pr(\text{Terror attack}) = 0.00005\)</span></li>
<li>Probability of plane hitting if terrorists are attacking Manhattan is <span class="math inline">\(\Pr(\text{Plane hits the WTC} | \text{Terror attack}) = 1\)</span></li>
<li>Probability of plane hitting if terrorists are not attacking Manhattan skyscrapers (i.e.Â an accident) is <span class="math inline">\(\Pr(\text{Plane hits the WTC} | \text{No terror attack}) = 0.00008\)</span></li>
</ul>
<p>Our posterior probability of a terror attack, given the first plane hitting the world trade center, is:</p>
<ul>
<li><span class="math inline">\(A =\)</span> terror attack</li>
<li><span class="math inline">\(B =\)</span> plane hitting the World Trade Center</li>
<li><span class="math inline">\(\Pr(A) = 0.00005 =\)</span> probability that terrorists would crash a plane into the World Trade Center</li>
<li><span class="math inline">\(\Pr(A^C) = 0.99995 =\)</span> probability that terrorists would not crash a plane into the World Trade Center</li>
<li><span class="math inline">\(\Pr(B|A) = 1 =\)</span> probability of a plane crashing into the World Trade Center if terrorists are attacking the World Trade Center</li>
<li><span class="math inline">\(\Pr(B|A^C) = 0.00008 =\)</span> probability of a plane hitting if terrorists are not attacking the World Trade Center (i.e.Â an accident)</li>
</ul>
<p><span class="math display">\[
\begin{align}
\Pr(A|B) &amp;= \frac{\Pr(A) \times \Pr(B|A)}{\Pr(B)} \\
&amp;= \frac{\Pr(A) \times \Pr(B|A)}{ \Pr(A) \times \Pr(B|A) + \Pr(A^C) \times \Pr(B| A^C)} \\
&amp; = \frac{0.00005 \times 1}{0.00005 \times 1 + 0.99995 \times 0.00008} \\
&amp; = 0.385
\end{align}
\]</span></p>
<p>We would now estimate a posterior probability of a 38% chance of a terrorist attack on the World Trade Center. But we can continuously update this posterior probability as new data presents itself.</p>
<p><span class="math display">\[
\begin{align}
\Pr(A|B) &amp;= \frac{\Pr(A) \times \Pr(B|A)}{\Pr(B)} \\
&amp;= \frac{\Pr(A) \times \Pr(B|A)}{ \Pr(A) \times \Pr(B|A) + \Pr(A^C) \times \Pr(B| A^C)} \\
&amp; = \frac{0.385 \times 1}{0.385 \times 1 + 0.615 \times 0.00008} \\
&amp; \approx .9998
\end{align}
\]</span></p>
</div>
</div>
<div id="simulation" class="section level2 hasAnchor" number="14.5">
<h2><span class="header-section-number">14.5</span> Simulation<a href="bayesian-inference.html#simulation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The posterior can often be approximated by simulation. Suppose we draw <span class="math inline">\(\theta_1, \ldots, \theta_B \sim p(\theta | x^n)\)</span>. Then a histogram of <span class="math inline">\(\theta_1, \ldots, \theta_B\)</span> approximates the posterior density <span class="math inline">\(p(\theta | x^n)\)</span>. An approximation to the posterior mean <span class="math inline">\(\bar{\theta}_n = \E (\theta | x^n)\)</span> is <span class="math inline">\(\frac{\sum_{j=1}^B \theta_j}{B}\)</span>. The posterior <span class="math inline">\(1 - \alpha\)</span> interval can be approximated by <span class="math inline">\((\theta_{\alpha / 2}, \theta_{1 - \alpha /2})\)</span> where <span class="math inline">\(\theta_{\alpha / 2}\)</span> is the <span class="math inline">\(\alpha / 2\)</span> sample quantile of <span class="math inline">\(\theta_1, \ldots, \theta_B\)</span>.</p>
<p>Once we have a sample <span class="math inline">\(\theta_1, \ldots, \theta_B\)</span> from <span class="math inline">\(f(\theta | x^n)\)</span>, let <span class="math inline">\(\tau_i = g(\theta_i)\)</span>. Then <span class="math inline">\(\tau_1, \ldots, \tau_B\)</span> is a sample from <span class="math inline">\(f(\tau | x^n)\)</span>. This avoids the need to do any analytic calculations, especially when <span class="math inline">\(f(\theta | x^n)\)</span> is an especially complex function.</p>
<div class="example">
<p><span id="exm:unlabeled-div-203" class="example"><strong>Example 14.5  (Bernoulli random variable) </strong></span>Let <span class="math inline">\(X_1, \ldots, X_n \sim \text{Bernoulli} (p)\)</span> and <span class="math inline">\(f(p) = 1\)</span> so that <span class="math inline">\(p | X^n \sim \text{Beta} (s + 1, n - s + 1)\)</span> with <span class="math inline">\(s = \sum_{i=1}^n x_i\)</span>. Let <span class="math inline">\(\psi = \log \left( \frac{p}{1 - p} \right)\)</span> (i.e.Â the log-odds). If we wanted to calculate the PMF and CDF of <span class="math inline">\(\psi | x^n\)</span>, we could do a lot of calculus and analytic math to solve for these equations.<a href="#fn31" class="footnote-ref" id="fnref31"><sup>31</sup></a> Alternatively, we can approximate the posterior for <span class="math inline">\(\psi\)</span> without doing any calculus.</p>
<ol style="list-style-type: decimal">
<li>Draw <span class="math inline">\(P_1, \ldots, P_B \sim \text{Beta} (s + 1, n - s + 1)\)</span>.</li>
<li>Let <span class="math inline">\(\psi_i = \log \left( \frac{P_i}{1 - P_i} \right)\)</span>, for <span class="math inline">\(i = 1, \ldots, B\)</span></li>
</ol>
<p>Now <span class="math inline">\(\psi_1, \ldots, \psi_B\)</span> are IID draws from <span class="math inline">\(h(\psi | x^n)\)</span>. A histogram of these values provides an estimate of <span class="math inline">\(h(\psi | x^n)\)</span>.</p>
</div>
</div>
<div id="priors" class="section level2 hasAnchor" number="14.6">
<h2><span class="header-section-number">14.6</span> Priors<a href="bayesian-inference.html#priors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To employ Bayesian inference, one requires a prior. Where do you get the prior <span class="math inline">\(f(\theta)\)</span>? One approach is to use a <strong>subjective</strong> prior based on your subjective opinion about <span class="math inline">\(\theta\)</span> before you collect any data. This may be possible, but is impractical for many complicated problems (especially when there are many parameters). Some would argue this approach is also not âscientificâ because our inferences should be as objective as possible.</p>
<p>An alternative approach is to define some sort of <strong>noninformative prior</strong>. One obvious choice is to use a flat prior <span class="math inline">\(f(\theta) \propto\)</span> constant. In the example earlier, taking <span class="math inline">\(f(p) = 1\)</span> leads to <span class="math inline">\(p | X^n \sim \text{Beta} (s + 1, n - s + 1)\)</span> which seems reasonable. But unfettered use of flat priors raises some questions.</p>
<div id="improper-priors" class="section level3 hasAnchor" number="14.6.1">
<h3><span class="header-section-number">14.6.1</span> Improper priors<a href="bayesian-inference.html#improper-priors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(X \sim N(\theta, \sigma^2)\)</span> with <span class="math inline">\(\sigma\)</span> known. Suppose we adopt a flat prior <span class="math inline">\(f(\theta) \propto c\)</span> where <span class="math inline">\(c &gt; 0\)</span> is a constant. Note that <span class="math inline">\(\int f(\theta) d\theta = \infty\)</span>, so this is not a probability density in the usual sense (otherwise it would integrate to 1). Such a prior is called an <strong>improper prior</strong>. However, we can still carry out Bayesâ theorem and compute the posterior density by multiplying the prior and the likelihood:</p>
<p><span class="math display">\[f(\theta) \propto \Lagr_n(\theta) f(\theta) = \Lagr_n(\theta)\]</span></p>
<p>This gives <span class="math inline">\(\theta | X^n \sim N(\bar{X}, \sigma^2 / n)\)</span> and the resulting point and interval estimators agree exactly with their frequentist counterparts. In general, improper priors are not a problem as long as the resulting posterior is a well-defined probability distribution.</p>
</div>
<div id="flat-priors-are-not-invariant" class="section level3 hasAnchor" number="14.6.2">
<h3><span class="header-section-number">14.6.2</span> Flat priors are not invariant<a href="bayesian-inference.html#flat-priors-are-not-invariant" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(X \sim \text{Bernoulli} (p)\)</span> and suppose we use the flat prior <span class="math inline">\(f(p) = 1\)</span>. This flat prior represents our lack of knowledge about <span class="math inline">\(p\)</span> before the experiment. Now let <span class="math inline">\(\psi = \log(p / (1 - p))\)</span>. This is a transformation of <span class="math inline">\(p\)</span> and we can compute the resulting distribution for <span class="math inline">\(\psi\)</span></p>
<p><span class="math display">\[f_\Psi (\psi) = \frac{e^\psi}{(1 + e^\psi)^2}\]</span></p>
<p>which is not flat. But if we are ignorant of <span class="math inline">\(p\)</span>, then we are also ignorant about <span class="math inline">\(\psi\)</span> so we should use a flat prior for <span class="math inline">\(\psi\)</span>. This is a contradiction. In short, the notion of a flat prior is not well defined because a flat prior on a parameter does not imply a flat prior on a transformed version of the parameter. Flat priors are not <strong>transformation invariant</strong>.</p>
</div>
</div>
<div id="multiparameter-problems" class="section level2 hasAnchor" number="14.7">
<h2><span class="header-section-number">14.7</span> Multiparameter problems<a href="bayesian-inference.html#multiparameter-problems" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose that <span class="math inline">\(\theta = (\theta_1, \ldots, \theta_p)\)</span>. The posterior density is given by</p>
<p><span class="math display">\[f(\theta | x^n) \propto \Lagr_n(\theta) f(\theta)\]</span></p>
<p>However, now we need to consider how to extract inferences about one parameter. The key is to find the <strong>marginal posterior density</strong> for the parameter of interest. Suppose we want to make inferences about <span class="math inline">\(\theta_1\)</span>. The marginal posterior for <span class="math inline">\(\theta_1\)</span> is</p>
<p><span class="math display">\[f(\theta_1 | x^n) = \int \cdots \int f(\theta_1, \ldots, \theta_p | x^n) d\theta_2 \cdots d\theta_p\]</span></p>
<p>Essentially, we calculate the integral of the function over all parameters except <span class="math inline">\(\theta_1\)</span>. If there are two parameters <span class="math inline">\((\theta_1, \theta_2)\)</span>, we integrate with respect to <span class="math inline">\(\theta_2\)</span>. As the number of parameters increase, this operation gets extremely tricky (if not impossible) to solve analytically. Instead, simulation can be used to approximate by drawing randomly from the posterior</p>
<p><span class="math display">\[\theta^1, \ldots, \theta^B \sim f(\theta | x^n)\]</span></p>
<p>where the superscripts index the different draws. Each <span class="math inline">\(\theta^j\)</span> is a vector <span class="math inline">\(\theta^j = (\theta_1^j, \ldots, \theta_p^j)\)</span>. Now collect together the first component of each draw</p>
<p><span class="math display">\[\theta_1^1, \ldots, \theta_1^B\]</span></p>
<p>These are a sample from <span class="math inline">\(f(\theta_1 | x^n)\)</span> and we have avoided doing any integrals.</p>
<div class="example">
<p><span id="exm:unlabeled-div-204" class="example"><strong>Example 14.6  (Comparing two binomials) </strong></span>Suppose we have <span class="math inline">\(n_1\)</span> control patients and <span class="math inline">\(n_2\)</span> treatment patients and that <span class="math inline">\(X_1\)</span> control patients survive while <span class="math inline">\(X_2\)</span> treatment patients survive. We want to estimate <span class="math inline">\(\tau = g(p_1, p_2) = p_2 - p_1\)</span>. Then,</p>
<p><span class="math display">\[X_1 \sim \text{Binomial} (n_1, p_1) \, \text{and} \, X_2 \sim \text{Binomial} (n_2, p_2)\]</span></p>
<p>If <span class="math inline">\(f(p_1, p_2) = 1\)</span>, the posterior is</p>
<p><span class="math display">\[f(p_1, p_2 | x_1, x_2) \propto p_1^{x_1} (1 - p_1)^{n_1 - x_1} p_2^{x_2} (1 - p_2)^{n_2 - x_2}\]</span></p>
<p>Notice that</p>
<p><span class="math display">\[f(p_1, p_2 | x_1, x_2) = f(p_1 | x_1) f(p_2 | x_2)\]</span></p>
<p>where</p>
<p><span class="math display">\[f(p_1 | x_1) \propto p_1^{x_1} (1 - p_1)^{n_1 - x_1} \, \text{and} \, f(p_2 | x_2) \propto p_2^{x_2} (1 - p_2)^{n_2 - x_2}\]</span></p>
<p>which implies that <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> are independent under the posterior. Also</p>
<p><span class="math display">\[
\begin{align}
p_1 | x_1 &amp;\sim \text{Beta} (x_1 + 1, n_1 - x_1 + 1) \\
p_2 | x_2 &amp;\sim \text{Beta} (x_2 + 1, n_2 - x_2 + 1)
\end{align}
\]</span></p>
<p>If we simulate</p>
<p><span class="math display">\[
\begin{align}
P_{1,1}, \ldots, P_{1,B} &amp;\sim \text{Beta} (x_1 + 1, n_1 - x_1 + 1) \\
P_{2,1}, \ldots, P_{2,B} &amp;\sim \text{Beta} (x_2 + 1, n_2 - x_2 + 1)
\end{align}
\]</span></p>
<p>Then <span class="math inline">\(\tau_b = P_{2,b} - P_{1,b}, \, b = 1, \ldots, B\)</span> is a sample from <span class="math inline">\(f(\tau | x_1, x_2)\)</span>.</p>
</div>
</div>
<div id="critiques-and-defenses-of-bayesian-inference" class="section level2 hasAnchor" number="14.8">
<h2><span class="header-section-number">14.8</span> Critiques and defenses of Bayesian inference<a href="bayesian-inference.html#critiques-and-defenses-of-bayesian-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="critique-of-bayesian-inference" class="section level3 hasAnchor" number="14.8.1">
<h3><span class="header-section-number">14.8.1</span> Critique of Bayesian inference<a href="bayesian-inference.html#critique-of-bayesian-inference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li>The subjective prior is subjective. There is no single method for choosing a prior, so different (well-intentioned) people will produce different priors and therefore arrive at different posteriors and conclusions. If you accept the premise of subjective priors, you still need good information to create a well-defined prior distribution.</li>
<li>Philosophically, some object to assigning probabilities to hypotheses as hypotheses do not constitute outcomes of repeatable experiments in which one can measure long-term frequency. Rather, a hypothesis is either true or false, regardless of whether one knows which is the case.
<ul>
<li>A coin is either fair or unfair</li>
<li>Treatment 1 is either better or worse than treatment 2</li>
<li>The sun will or will not come up tomorrow</li>
<li>I will either win or not win the lottery</li>
</ul></li>
<li>For many parametric models with large samples, Bayesian and frequentist methods give approximately the same inferences. Since frequentist methods are historically more common and easier to estimate, there is no reason to go through the steps of Bayesian inference.</li>
<li>Bayesian inference depends entirely on the likelihood function. In high dimensional and nonparametric methods, the likelihood function may not yield accurate inferences.</li>
</ol>
</div>
<div id="defense-of-bayesian-inference" class="section level3 hasAnchor" number="14.8.2">
<h3><span class="header-section-number">14.8.2</span> Defense of Bayesian inference<a href="bayesian-inference.html#defense-of-bayesian-inference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li>The probability of hypotheses is exactly what we need to make decisions. When the doctor tells me a screening test came back positive for a disease, what I really want to know is the probability of the hypothesis âIâm sickâ.</li>
<li>Bayesâ theorem is logically rigorous (once we obtain a prior).</li>
<li>By testing different priors we can see how sensitive our results are to the choice of prior.</li>
<li>It is easy to communicate a result framed in terms of probabilities of hypotheses (try explaining the result of a null hypothesis test to a layperson).</li>
<li>Priors can be defended based on the assumptions made to arrive at it.</li>
<li>Evidence derived from the data is independent of notions about âdata more extremeâ that depend on the exact experimental setup.</li>
<li>Data can be used as it comes in. We donât have to wait for every contingency to be planned for ahead of time.</li>
</ol>
</div>
</div>
<div id="acknowledgements-1" class="section level2 unnumbered hasAnchor">
<h2>Acknowledgements<a href="bayesian-inference.html#acknowledgements-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Material drawn from <a href="https://link-springer-com.proxy.uchicago.edu/book/10.1007%2F978-0-387-21736-9"><strong>All of Statistics</strong></a> by Larry Wasserman</li>
</ul>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-bertsekas2008" class="csl-entry">
Bertsekas, Dimitri P, and John N Tsitsiklis. 2008. <span>âIntroduction to Probability.â</span>
</div>
<div id="ref-wasserman2013" class="csl-entry">
Wasserman, Larry. 2013. <em>All of Statistics: A Concise Course in Statistical Inference</em>. Springer Science &amp; Business Media. <a href="https://link-springer-com.proxy.uchicago.edu/book/10.1007/978-0-387-21736-9">https://link-springer-com.proxy.uchicago.edu/book/10.1007/978-0-387-21736-9</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="28">
<li id="fn28"><p>Even worse, many physicians substantially miss the correct answer to this question.<a href="bayesian-inference.html#fnref28" class="footnote-back">â©ï¸</a></p></li>
<li id="fn29"><p>Iâm currently having trouble remembering the inspiration for this example. I think it came from <strong>The Signal and the Noise</strong> by Nate Silver.<a href="bayesian-inference.html#fnref29" class="footnote-back">â©ï¸</a></p></li>
<li id="fn30"><p>Based on historical records of just two accidents involving planes hitting buildings in New York City from the 1940s-9/10/2011.<a href="bayesian-inference.html#fnref30" class="footnote-back">â©ï¸</a></p></li>
<li id="fn31"><p>See example 11.3 in Wasserman.<a href="bayesian-inference.html#fnref31" class="footnote-back">â©ï¸</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mle-ols.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/math-camp/notes/edit/master/14-bayesian-inference.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
},
"theme": "readable",
"highlight": "pygment"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
