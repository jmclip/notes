[{"path":"index.html","id":"overview","chapter":"Overview","heading":"Overview","text":"","code":""},{"path":"index.html","id":"meeting-information","chapter":"Overview","heading":"Meeting information","text":"Meeting day: August 30-September 17, MTWThFTime: 9am-1pm CTLocation:\n1155 E 60th St - rooms 140A, 140B, 140C, 295\nRemotely Zoom\n1155 E 60th St - rooms 140A, 140B, 140C, 295Remotely Zoom","code":""},{"path":"index.html","id":"instructional-staff","chapter":"Overview","heading":"Instructional staff","text":"Dr. Benjamin SoltoffEmail: soltoffbc@uchicago.eduOffice: Room 219, 1155 E. 60th St.Office hours: appointment","code":""},{"path":"index.html","id":"teaching-assistants","chapter":"Overview","heading":"Teaching assistants","text":"Jade BensonAngelica BoskoFrancisco MendesJinfei Zhu","code":""},{"path":"index.html","id":"course-description","chapter":"Overview","heading":"Course description","text":"course surveys mathematical statistical tools foundational computational social science. Topics reviewed include mathematical notation linear equations, calculus, linear algebra, probability theory, statistical inference. Students assumed encountered topics previously, camp serves refresher rather teaching entirely new topics. Class sessions emphasize problem solving -class exercises applying techniques. Students successfully complete camp situated pass MACSS math statistics placement exam enroll computationally-enhanced course offerings University Chicago without prior introductory coursework.","code":""},{"path":"index.html","id":"who-should-take-this-course","chapter":"Overview","heading":"Who should take this course","text":"Students Masters Computational Social ScienceMA PhD students social sciences significant prior training experience mathematics statistics seek complete Certificate Computational Social ScienceStudents looking slower-paced camp focused specifically algebra, calculus, probability enroll SOSC 30100 - Mathematics Social Sciences. three-week course makes assumption prior math/stats training.","code":""},{"path":"index.html","id":"grades","chapter":"Overview","heading":"Grades","text":"course may taken pass/fail (non-credit). Assignments comprised daily problem sets. encouraged work groups, instructional staff available consultation class hours. expect students able finish problem sets class hours. Students must satisfactorily complete minimum ten problem sets pass camp.","code":""},{"path":"index.html","id":"disability-services","chapter":"Overview","heading":"Disability services","text":"University Chicago committed diversity rigorous inquiry multiple perspectives. MAPSS, CIR, Computation programs share commitment seek foster productive learning environments based upon inclusion, open communication, mutual respect diverse range identities, experiences, positions.course open students meet academic requirements participation. student documented need accommodation contact Student Disability Services (773-702-6000 disabilities@uchicago.edu) provide (Dr. Soltoff) copy Accommodation Determination Letter soon possible.","code":""},{"path":"index.html","id":"core-texts","chapter":"Overview","heading":"Core texts","text":"required textbooks camp. Readings drawn course notes. topics less confidence, supplementary readings drawn :Bertsekas, D. P., & Tsitsiklis, J. N. (2008). Introduction probability, 2nd edition. Belmont, MA: Athena Scientific.\nbook available hardcopy\nauthors provided digital copies lecture notes published prior publishing full textbook. made available Canvas. Note lecture notes include chapters frequentist Bayesian statistical inference, include full set practice exercises textbook.\nbook available hardcopyThe authors provided digital copies lecture notes published prior publishing full textbook. made available Canvas. Note lecture notes include chapters frequentist Bayesian statistical inference, include full set practice exercises textbook.Pemberton, M., & Rau, N. (2015). Mathematics economists: introductory textbook, 4th edition. Oxford University Press.OpenStax\nCalculus, Volume 1\nCalculus, Volume 2\nCalculus, Volume 3\nCollege Algebra\nCalculus, Volume 1Calculus, Volume 2Calculus, Volume 3College AlgebraWasserman, Larry. 2013. Statistics: Concise Course Statistical Inference. Springer Science & Business Media.","code":""},{"path":"index.html","id":"course-format","chapter":"Overview","heading":"Course format","text":"","code":""},{"path":"index.html","id":"those-participating-in-person-in-chicago","chapter":"Overview","heading":"Those participating in-person in Chicago","text":"day’s material, including course notes, supplementary readings, problem sets, released 5pm Chicago time night date course schedule . includes required readings assigned problem set. strongly encourage complete reading evening.day invited come campus 9am-1pm work problem sets small groups. TA available answer questions readings problem sets. may leave soon finished problem set, submitted later 11:59pm.","code":""},{"path":"index.html","id":"those-participating-remotely","chapter":"Overview","heading":"Those participating remotely","text":"strongly encourage students participate -person possible. provide best experience acclimate University Chicago. students attend -person required adhere university’s current health safety protocols.day’s material, including course notes, supplementary readings, problem sets, released 5pm Chicago time night date course schedule . allow students Eastern Hemisphere access materials local morning time. regularly scheduled synchronous meetings camp. Instead, expected complete readings day submit problem set 11:59pm Chicago time.TAs available throughout day drop-office hours. appointment necessary. can find Zoom login information sessions Canvas site Zoom tab.questions confusions material problem sets, come speak TA. recognize students able utilize hours since TAs based United States. can also post questions discussion thread day’s material TA respond soon possible.also office hours available appointment. can schedule . Feel free meet discuss concepts questions regarding material math camp, well questions MACSS program upcoming fall quarter.","code":""},{"path":"index.html","id":"problem-sets","chapter":"Overview","heading":"Problem sets","text":"problem set designed take three hours. spending three hours problem set, make sure asking questions confused. problem sets submitted via Canvas. problem set, write answers exercise paper show work. need scan answers PDF upload Canvas. scanner home (?), can use phone scan work. couple recommended scanning apps :iOS, Evernote good choiceFor Android, Genius Scan works wellIt crucial show work responses, otherwise receive incomplete assignment. care process follow get answer, rather answer . please make sure writing legible!","code":""},{"path":"index.html","id":"suggested-time-schedules-for-remote-students","chapter":"Overview","heading":"Suggested time schedules for remote students","text":"","code":""},{"path":"index.html","id":"for-students-in-the-western-hemisphere","chapter":"Overview","heading":"For students in the Western hemisphere","text":"night date course schedule, check assigned readings read/skim . Focus closely areas unfamiliar .morning, finish readings take look lecture notes. Start working problem set.early afternoon, submit problem set.","code":""},{"path":"index.html","id":"for-students-in-the-eastern-hemisphere","chapter":"Overview","heading":"For students in the Eastern Hemisphere","text":"morning, check assigned readings read/skim . Focus closely areas unfamiliar .early afternoon, finish readings take look lecture notes. Start working problem set.evening, submit problem set.","code":""},{"path":"index.html","id":"course-schedule","chapter":"Overview","heading":"Course schedule","text":"","code":""},{"path":"sets-functions.html","id":"sets-functions","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","text":"","code":""},{"path":"sets-functions.html","id":"learning-objectives","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"Learning objectives","text":"Define computational social scienceConnect mathematics statistics social scientific inquiryReview course logisticsExplain pedagogical approach campDefine basic mathematical notationAssess use mathematical notation rational voter theoryDefine functions propertiesDefine setsPractice root findingDefine linear systems equations solve linear systems equations via backsubstitution Gaussian eliminationDefine logarithmic exponential functionsPractice simplifying power, logarithmic, exponential functions","code":""},{"path":"sets-functions.html","id":"supplemental-readings","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"Supplemental readings","text":"Chapters 1.1-.3, 2.1, 3, 4, Pemberton Rau (2011)OpenStax Calculus: Volume 1, ch 1OpenStax College Algebra, ch 7.1-.2","code":""},{"path":"sets-functions.html","id":"what-is-computational-social-science","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.1 What is computational social science?","text":"Social science defined scientific study human society social relationships.1","code":""},{"path":"sets-functions.html","id":"disciplines-within-social-science","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.1.1 Disciplines within social science","text":"University Chicago, social sciences broken several distinct disciplines:2Anthropology - science human societies cultures developmentsEconomics - science production, consumption, transfer wealthHistory - science past events, particularly human affairsPolitical science - science gets , , how3Psychology - science behavior mindSociology - science development, structure, functioning human societyThe Division Social Sciences also includes interdisciplinary committees bridge across multiple disciplines:Comparative Human Development - “research examines issues central concern socio-cultural anthropology, medical anthropology, comparative education, behavioral biology, language thought, cultural developmental psychology”Conceptual Historical Studies Science - “areas concerned history, philosophy, social relations science”Social Thought - “serious study many academic topics, many philosophical, historical, theological literary works, best prepared wide deep acquaintance fundamental issues presupposed studies, students learn issues acquainting select number major ancient modern texts inter-disciplinary atmosphere”","code":""},{"path":"sets-functions.html","id":"computational-social-science","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.1.2 Computational social science","text":"Computational social science (CSS) modern interdisciplinary approach study social scientific phenomena. Researchers field use computers model, simulated, analyze social phenomena. historically CSS limited application numerical methods simulation (e.g. agent-based modeling) complex issues social science research, past decade evolved describe intersection computer science, math/statistics, social science Lazer et al. (2009).\nFigure 1.1: Source: http://giventhedata.blogspot.com/2013/03/data-science--businesscomputational.html\nComputational social scientists leverage “big data” computational research designs analyze study social phenomena. requires sophistication training across three major domains.","code":""},{"path":"sets-functions.html","id":"acquiring-css-skills","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.1.3 Acquiring CSS skills","text":"competently trained computational social scientist, one needs develop training expertise computer science, social science, math/statistics. University Chicago within MACSS, receive training range courses.Computer science\nMACS 30121/122/123 sequence\nMACS 30500\nComputer science electives\nMACS 30121/122/123 sequenceMACS 30500Computer science electivesSocial science\nPerspectives sequence (MACS 30000/100/200)\nDepartmental electives\nSeminars\nNon-computational courses\nPerspectives sequence (MACS 30000/100/200)Departmental electivesSeminarsNon-computational coursesMath/statistics\nProbability statistics used across sciences\nStart Computational Math Camp\nMath/stats electives\nMachine learning\nCausal inference\nBayesian inference\nNetwork analysis\nDeep learning\nSpatial data science\nNatural language processing\n\n\nProbability statistics used across sciencesStart Computational Math CampMath/stats electives\nMachine learning\nCausal inference\nBayesian inference\nNetwork analysis\nDeep learning\nSpatial data science\nNatural language processing\n\nMachine learningCausal inferenceBayesian inferenceNetwork analysisDeep learningSpatial data scienceNatural language processingAnd ","code":""},{"path":"sets-functions.html","id":"difference-between-math-probability-and-statistics","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.2 Difference between math, probability, and statistics","text":"camp, review fundamental methods across fields mathematics, probability, statistics. related, fields distinct one another. Key attributes listed .","code":""},{"path":"sets-functions.html","id":"mathematics","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.2.1 Mathematics","text":"Mathematics purely abstractBased axioms independent real worldIf axioms accepted, mathematical inferences certainLanguage expressing structure relationshipsGenerally proof-based","code":""},{"path":"sets-functions.html","id":"probability","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.2.2 Probability","text":"Systematic rigorous method treating uncertainty“Mathematical models uncertain reality”Derivation “applied mathematics”","code":""},{"path":"sets-functions.html","id":"statistics","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.2.3 Statistics","text":"Practice science collecting analyzing numerical data large quantities, especially purpose inferring proportions whole sampleMaking inferences data entirely certainBased mathematical models, deviations model (deterministic)","code":""},{"path":"sets-functions.html","id":"their-uses","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.2.4 Their uses","text":"used study social scientific phenomena:Mathematical models\nGame theory\nFormal theory\nMuch bigger economics\nDefining statistical models relationships\nGame theoryFormal theoryMuch bigger economicsDefining statistical models relationshipsProbability/statistics\nEstablishing structure relationships variables using data\nInferring relationships assessing validity\nEstablishing structure relationships variables using dataInferring relationships assessing validity","code":""},{"path":"sets-functions.html","id":"goals-for-this-camp","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.3 Goals for this camp","text":"Survey math statistical tools foundational CSSReview common mathematical notationApply math/statistics methodsPrepare students pass MACSS math/stats placement exam enroll computationally-enhanced course offerings UChicago","code":""},{"path":"sets-functions.html","id":"course-logistics","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.4 Course logistics","text":"course materials can found course Canvas site","code":""},{"path":"sets-functions.html","id":"course-staff","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.4.1 Course staff","text":"","code":""},{"path":"sets-functions.html","id":"me-dr.-benjamin-soltoff","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.4.1.1 Me (Dr. Benjamin Soltoff)","text":"Assistant Senior Instructional Professor Computational Social ScienceAssociate Director Masters Computational Social Science programPhD political science, fields including American politics, political methodology, law & courtsCurrent teaching rotation\nComputational Math Camp\nComputing Social Sciences\nData Storytelling\nPerspectives Computational Analysis\nComputational Math CampComputing Social SciencesData StorytellingPerspectives Computational Analysis","code":""},{"path":"sets-functions.html","id":"teaching-assistants-1","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.4.2 Teaching assistants","text":"Jade BensonAngelica BoskoFrancisco MendesJinfei Zhu","code":""},{"path":"sets-functions.html","id":"prerequisites-for-the-math-camp","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.4.3 Prerequisites for the math camp","text":"formal prerequisitesWho likely succeed class? ’ll probably prior training :\nLinear algebra\nCalculus\nProbability theory\nStatistical inference (data description, assessing bivariate relationships continuous categorical variables, linear/logistic regression, etc.)\nHigh school/AP/IB training may sufficient depending depth course recently completed \nLinear algebraCalculusProbability theoryStatistical inference (data description, assessing bivariate relationships continuous categorical variables, linear/logistic regression, etc.)High school/AP/IB training may sufficient depending depth course recently completed itWe assume prior exposure content covered camp - goal refresh material previously learned, teach fresh\n* Impossible teach material scratch just three weeks","code":""},{"path":"sets-functions.html","id":"alternatives-to-this-camp","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.4.4 Alternatives to this camp","text":"SOSC 30100 - Mathematics Social Sciences (aka Hansen math camp)Starts next week - can stay camp week decide want switch (recommend switch)","code":""},{"path":"sets-functions.html","id":"evaluation","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.4.5 Evaluation","text":"","code":""},{"path":"sets-functions.html","id":"grades-1","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.4.5.1 Grades","text":"don’t take course grade\nPass/fail (credit)\nShow (virtually), complete problem sets satisfactorily (perfectly), ’ll pass\nPass/fail (credit)Show (virtually), complete problem sets satisfactorily (perfectly), ’ll passThat said, matterThe irony grad school\nGrades longer matter ()\nLearn much material possible\ntruly care learning material, ’ll get amazing grades\nGrades longer matter ()Learn much material possibleIf truly care learning material, ’ll get amazing grades","code":""},{"path":"sets-functions.html","id":"pedagogical-approach-for-the-camp","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.4.5.2 Pedagogical approach for the camp","text":"follow flipped-classroom design(-ish).day course notes provided lieu actual lecture. Read closely problem sets drawn directly material.Supplemental readings published textbooks listed day. may choose read find necessary. expected read cover--cover. Instead, suggest concentrating certain techniques methods find especially challenging confusing.Problem sets distributed electronically day, submitted via Canvas evaluation.“Homework” reading next dayThe instructional staff (TAs ) “sherpas” guide along journey. assist - please make liberal use us peers complete problem sets. goal provide quick feedback don’t waste time home struggling problems understand resolve.Problem sets evaluated returned within 24 hours, solution key posted review.","code":""},{"path":"sets-functions.html","id":"minute-rule","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.4.5.3 15 minute rule","text":"\n15 min rule: stuck, try 15 min; 15 min, ask help.- Brain AMA pic.twitter.com/MS7FnjXoGH\nfollow 15 minute rule class. encounter issue problem sets, spend 15 minutes troubleshooting problem . However, 15 minutes still solve problem, ask help. instructional staff office hours spaced throughout day. Feel free log Zoom ask question.","code":""},{"path":"sets-functions.html","id":"why-are-we-doing-this","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.4.6 Why are we doing this","text":"point solving math/stats hand computers can us?truth math skills translate domains. perhaps overly simplified, computer science essentially bunch math. Applying mathematical techniques completing exercises problem sets requires focus, determination, grit. ’ll need survive grad school.\nFigure 1.2: Calvin Hobbes Bill Watterson February 15, 2010\n","code":""},{"path":"sets-functions.html","id":"mathematical-notation","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.5 Mathematical notation","text":"","code":""},{"path":"sets-functions.html","id":"why-math-is-important-to-social-science","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.5.1 Why math is important to social science","text":"Math provides consistent language communicate ideas orderly systematic way. Science uses highly precise language easily interpretable outsiders. Consider definition term “theory”:Definition 1.1  (Lay definition theory) unproved assumption.Definition 1.2  (Scientific definition theory) well-substantiated explanation aspect natural world, based body facts repeatedly confirmed observation experiment (e.g. empirical support, falsifiable, repeatedly tested).leads potential confusion interactions scientific community general public.Example 1.1  (Climate change) Predominantly United States ideological divide whether people believe true. real just “theory?”Example 1.2  (COVID-19) Consider discussion scientific evidence related COVID-19 pandemic associated conspiracy theories.Mathematics effective way describe world. Many physical processes follow precise mathematical models. Social phenomena less , can still learn mathematical modeling. Mathematical notation lets us convey precision minimizes risk misinterpretation scholars","code":""},{"path":"sets-functions.html","id":"example-paradox-of-voting","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.5.2 Example: Paradox of voting","text":"Consider classic paradox voting first introduced Downs others (1957). Downs identifies rational voters weigh rewards vs. costs voting. core principle costs vote rewards gained action, individuals vote elections. difference cost reward defined utility person receives act (based unknown preference scale).can codify model using mathematical notation?\\[R = PB - C\\]\\(R =\\) utility satisfaction voting\\(P =\\) actual probability voter affect outcome particular vote\\(B =\\) perceived difference benefits two candidates measured utiles (units utility)\\(C =\\) actual cost voting utiles (e.g. time, effort, money)","code":""},{"path":"sets-functions.html","id":"think-pair-share","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.5.2.1 Think-pair-share","text":"implications model provide?express mathematically?\npotential implications :leads paradox voting - rationally one vote, yet many people still . ? clearly question worth answering political science. Downs others (1957) cited 32000 times, know people care question. pure mathematical model may fully accurate, allows us delve deeper paradox. measure \\(B\\)? \\(C\\)?","code":""},{"path":"sets-functions.html","id":"sets","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.6 Sets","text":"set collection objects.Example 1.3  \\[\n\\begin{aligned}\n& = \\{1, 2, 3\\} \\nonumber  \\\\\nB  & = \\{4, 5, 6\\}\\nonumber  \\\\\nC  & = \\{ \\text{First year cohort} \\} \\\\\nD & = \\{ \\text{U Chicago Lecturers} \\}\n\\end{aligned}\n\\]Definition 1.3  \\(\\) set, say \\(x\\) element \\(\\) writing, \\(x \\\\). \\(x\\) element \\(\\) , write \\(x \\notin \\).Example 1.4  \\[\n\\begin{aligned}\n1 &\\\\{ 1, 2, 3\\} \\\\\n4 &\\\\{4, 5, 6\\} \\\\\n\\text{} &\\notin \\{ \\text{First year cohort} \\} \\\\\n\\text{Benjamin} &\\\\{ \\text{U Chicago Lecturers} \\}\n\\end{aligned}\n\\]care sets?Sets necessary probability theoryDefining set equivalent choosing population interest (usually)Definition 1.4  \\(\\) \\(B\\) sets, say \\(= B\\) , \\(x \\\\) \\(x \\B\\) \\(y \\B\\) \\(y \\\\).test determine equality:Take elements \\(\\), see \\(B\\)Take elements \\(B\\), see \\(\\)Definition 1.5  \\(\\) \\(B\\) sets, say \\(\\subset B\\) , \\(x \\\\), \\(x \\B\\).difference definitions?first definition, \\(\\) \\(B\\) identicalIn second definition, \\(x \\\\) included \\(B\\), \\(y \\B\\) necessarily \\(\\)","code":""},{"path":"sets-functions.html","id":"set-builder-notation","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.6.1 Set builder notation","text":"famous sets:\\(\\mathbb{N} = \\{1, 2, 3, \\ldots \\}\\)\\(\\mathbb{Z} = \\{\\ldots, -2, -1, 0, 1, 2, \\ldots, \\}\\)\\(\\Re = \\mbox{Real numbers}\\)Use set builder notation identify subsets:\\([, b] = \\{x: x \\\\Re \\text{ } \\leq x \\leq b \\}\\)\\((, b] = \\{x: x \\\\Re \\text{ } < x \\leq b \\}\\)\\([, b) = \\{x: x \\\\Re \\text{ } \\leq x < b \\}\\)\\((, b) = \\{x: x \\\\Re \\text{ } < x < b \\}\\)\\(\\emptyset\\) - empty null set","code":""},{"path":"sets-functions.html","id":"set-operations","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.6.2 Set operations","text":"can build new sets set operations.Definition 1.6  (Union) Suppose \\(\\) \\(B\\) sets. Define Union sets \\(\\) \\(B\\) new set contains elements set \\(\\) set \\(B\\). notation,\\[\n\\begin{aligned}\nC & = \\cup B  \\\\\n   & = \\{x: x \\\\text{ } x \\B \\}\n\\end{aligned}\n\\]Example 1.5  (Example unions) \\[\n\\begin{aligned}\n&= \\{1, 2, 3\\} \\\\\nB &= \\{3, 4, 5\\} \\\\\nC &= \\cup B  = \\{ 1, 2, 3, 4, 5\\}\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nD &= \\{\\text{First Year Cohort} \\} \\\\\nE &= \\{\\text{} \\} \\\\\nF &= D \\cup E = \\{ \\text{First Year Cohort, } \\}\n\\end{aligned}\n\\]Definition 1.7  (Intersection) Suppose \\(\\) \\(B\\) sets. Define Intersection sets \\(\\) \\(B\\) new set contains elements set \\(\\) set \\(B\\). notation,\\[\n\\begin{aligned}\nC  & = \\cap B \\\\\n  & = \\{x: x \\\\text{ } x \\B \\}\n\\end{aligned}\n\\]Example 1.6  (Example intersections) \\[\n\\begin{aligned}\n&= \\{1, 2, 3\\} \\\\\nB &= \\{3, 4, 5\\} \\\\\nC &= \\cap B = \\{3\\}\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nD &= \\{\\text{First Year Cohort} \\} \\\\\nE &= \\{\\text{} \\} \\\\\nF &= D \\cap E = \\emptyset\n\\end{aligned}\n\\]","code":""},{"path":"sets-functions.html","id":"some-facts-about-sets","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.6.3 Some facts about sets","text":"\\(\\cap B = B \\cap \\)\nintersection \\(\\) \\(B\\) intersection \\(B\\) \\(\\)\nintersection \\(\\) \\(B\\) intersection \\(B\\) \\(\\)\\(\\cup B = B \\cup \\)\nunion \\(\\) \\(B\\) intersection \\(B\\) \\(\\)\nunion \\(\\) \\(B\\) intersection \\(B\\) \\(\\)\\((\\cap B) \\cap C = \\cap (B \\cap C)\\)\nTwo different ways describing set objects belongs three \\(, B, C\\)\nTwo different ways describing set objects belongs three \\(, B, C\\)\\((\\cup B) \\cup C = \\cup (B \\cup C)\\)\nTwo different ways describing set objects belongs least one \\(, B, C\\)\nTwo different ways describing set objects belongs least one \\(, B, C\\)\\(\\cap (B \\cup C) = (\\cap B) \\cup (\\cap C)\\)\nTwo different ways describing set members \\(\\) also either \\(B\\) \\(C\\)\nTwo different ways describing set members \\(\\) also either \\(B\\) \\(C\\)\\(\\cup (B \\cap C) = (\\cup B) \\cap (\\cup C)\\)\nTwo different ways describing set contains either members \\(\\) intersection sets \\(B\\) \\(C\\)\nTwo different ways describing set contains either members \\(\\) intersection sets \\(B\\) \\(C\\)","code":""},{"path":"sets-functions.html","id":"functions","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.7 Functions","text":"","code":""},{"path":"sets-functions.html","id":"ordered-pairs","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.7.1 Ordered pairs","text":"’ve seen ordered pair ,\\[(, b)\\]Definition 1.8  (Cartesian product) Suppose two sets, \\(\\) \\(B\\). Define Cartesian product \\(\\) \\(B\\), \\(\\times B\\) set ordered pairs \\((, b)\\), \\(\\\\) \\(b \\B\\). words,\\[\n\\times B = \\{(, b): \\\\text{ } b \\B \\}\n\\]Example 1.7  (Cartesian product) \\(= \\{1, 2\\}\\) \\(B = \\{3, 4\\}\\), ,\\[\\times B = \\{ (1, 3); (1, 4); (2, 3); (2, 4) \\}\\]","code":""},{"path":"sets-functions.html","id":"relation","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.7.2 Relation","text":"Definition 1.9  (Relation) relation set ordered pairs. function \\(F\\) relation ,\\[\n(x, y) \\F ; (x, z) \\F \\Rightarrow y = z\n\\]commonly write function \\(F(x)\\), \\(x \\\\mbox{Domain} \\, F\\) \\(F(x) \\\\mbox{Codomain} \\, F\\). common see people write,\\[\nF:\\rightarrow B\n\\]\\(\\) domain \\(B\\) codomain.Examples include:","code":""},{"path":"sets-functions.html","id":"relation-vs.-function","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.7.3 Relation vs. function","text":"mathematical function mapping gives correspondence one measure onto exactly one value. means mapping one defined space another, \\(F \\colon \\Re \\rightarrow \\Re\\). Think machine takes input value, applies transformation , spits new valueExample 1.8  \\[F(x) = x^2 - 1\\]Maps \\(x\\) \\(F(x)\\) squaring \\(x\\) subtracting 1.","code":""},{"path":"sets-functions.html","id":"not-a-function","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.7.3.1 Not a function","text":"functions relations, relations functions. Functions exactly one value returned \\(F(x)\\) value \\(x\\), whereas relations may one value returned.","code":""},{"path":"sets-functions.html","id":"two-major-properties-of-functions","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.7.4 Two major properties of functions","text":"\\[F(x) = y\\]function continuous gaps mapping \\(x\\) \\(y\\)function continuous gaps mapping \\(x\\) \\(y\\)function invertible reverse operation exists:\n\\[G^{-1}(y) = x, \\text{} G^{-1}(G(x)) = x\\]function invertible reverse operation exists:\\[G^{-1}(y) = x, \\text{} G^{-1}(G(x)) = x\\]functions continuous invertible:\\[\nF(x) = \\left\\{\n        \\begin{array}{ll}\n            \\frac{1}{x} & \\quad x \\neq 0 \\text{ } x \\text{ rational}\\\\\n            0 & \\quad \\text{otherwise}\n        \\end{array}\n    \\right.\n\\]want functions continuous invertible. example, functions must continuous invertible calculate derivatives calculus. important optimization solving parameter values modeling strategies.non-continuous functions, much fix . However non-invertible functions can made invertible restricting domain.","code":""},{"path":"sets-functions.html","id":"quadratic-functions","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.8 Quadratic functions","text":"Consider function\\[y = ax^2\\]\\(\\) real number \\(\\neq 0\\). graph function one two possible shapes, depending whether \\(\\) positive negative:Definition 1.10  (Quadratic function) function form\\[y = ax^2 + bx + c\\]\\(,b,c\\) real numbers \\(\\neq 0\\).graph function still takes form parabola, vertex necessarily origin.","code":""},{"path":"sets-functions.html","id":"quadratic-equation","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.8.1 Quadratic equation","text":"Definition 1.11  (Quadratic equation) quadratic equation takes form \\(f(x) = 0\\) \\(f\\) quadratic function.solve equation, need determine value \\(x\\) satisfies equation.Example 1.9  \\[\n\\begin{aligned}\nx^2 - 7 &= 0 \\\\\nx^2 &= 7 \\\\\nx &= \\pm \\sqrt{7}\n\\end{aligned}\n\\]Example 1.10  \\[\n\\begin{aligned}\n-3x^2 + 30x - 27 &= 0 \\\\\n-3 (x^2 - 10x + 9) &= 0 \\\\\n-3(x - 9)(x - 1) &= 0 \\\\\n(x - 9)(x - 1) &= 0 \\\\\nx &= 1, 9\n\\end{aligned}\n\\]","code":""},{"path":"sets-functions.html","id":"quadratic-formula","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.8.2 Quadratic formula","text":"Definition 1.12  (Quadratic formula) quadratic equation general form\\[ax^2 + bx + c = 0\\]\\(,b,c\\) real numbers \\(\\neq 0\\)can solved using formula\\[\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n\\]Example 1.11  \\[x^2 + x - 12 = 0\\]\\[\n\\begin{aligned}\nx &= \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\\\\n&= \\frac{-1 \\pm \\sqrt{1^2 - 4 \\times 1 \\times 12}}{2 \\times 1} \\\\\n&= \\frac{-1 \\pm \\sqrt{1 - (-48)}}{2} \\\\\n&= \\frac{-1 \\pm \\sqrt{49}}{2} \\\\\n&= \\frac{-1 \\pm 7}{2} \\\\\n&= 3, -4 \\\\\n\\end{aligned}\n\\]","code":""},{"path":"sets-functions.html","id":"systems-of-linear-equations","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.9 Systems of linear equations","text":"Extending previous examples, often need find solution systems linear equations, multiple equations overlapping variables :\\[\n\\begin{matrix}\n            x  & - & 3y & = & -3\\\\\n            2x & + &  y & = &  8\n            \\end{matrix}\n\\]generally, might system \\(m\\) equations \\(n\\) unknowns\\[\n\\begin{matrix}\n            a_{11}x_1  & + & a_{12}x_2 & + & \\cdots & + & a_{1n}x_n & = & b_1\\\\\n            a_{21}x_1  & + & a_{22}x_2 & + & \\cdots & + & a_{2n}x_n & = & b_2\\\\\n            \\vdots     &   &     &   & \\vdots &   &     & \\vdots & \\\\\n            a_{m1}x_1  & + & a_{m2}x_2 & + & \\cdots & + & a_{mn}x_n & = & b_m\n\\end{matrix}\n\\]solution linear system \\(m\\) equations \\(n\\) unknowns set \\(n\\) numbers \\(x_1, x_2, \\cdots, x_n\\) satisfy \\(m\\) equations.Example 1.12  \\(x=3\\) \\(y=2\\) solution \\(2\\times 2\\) linear system. graph two lines, find intersect \\((3,2)\\).linear system one, , multiple solutions? system 2 equations 2 unknowns (.e., two lines):One solution: lines intersect exactly one point.solution: lines parallel.Infinite solutions: lines coincide.","code":""},{"path":"sets-functions.html","id":"one-solution","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.9.1 One solution","text":"consider solving linear system substitution elimination variables. Consider example\\[\n\\begin{aligned}\n3x + 2y &= 8 \\\\\n2x + 5y &= 9\n\\end{aligned}\n\\]start need eliminate one unknown one equation, allowing us solve unknown. can eliminating \\(x\\) second equation subtracting suitable multiple first equation. Specifically, can multiply first equation \\(2/3\\) (\\(2x + \\frac{4}{3}y = \\frac{16}{3}\\)) give us resulting system equations:\\[\n\\begin{aligned}\n3x + 2y &= 8 \\\\\n(5 - \\frac{4}{3})y &= 9 - \\frac{16}{3}\n\\end{aligned}\n\\]second equation resolves \\[\n\\begin{aligned}\n\\frac{11}{3}y &= \\frac{11}{3} \\\\\ny &= 1\n\\end{aligned}\n\\]substitute \\(y = 1\\) back first equation find \\[\n\\begin{aligned}\n3x + 2(1) &= 8 \\\\\n3x &= 6 \\\\\nx &= 2\n\\end{aligned}\n\\]solution system \\[x = 2, y = 1\\]","code":""},{"path":"sets-functions.html","id":"no-solution","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.9.2 No solution","text":"Consider instead linear system\\[\n\\begin{aligned}\n3x + 2y &= 8 \\\\\n6x + 4y &= 9\n\\end{aligned}\n\\]try solve system equations subtracting twice first equation second equation, end new equation \\[0 = -7\\]Clearly nonsensical. graphically can see two parallel lines never intersect.","code":""},{"path":"sets-functions.html","id":"infinite-solutions","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.9.3 Infinite solutions","text":"final type outcome illustrated system equations\\[\n\\begin{aligned}\n3x + 2y &= 8 \\\\\n6x + 4y &= 16\n\\end{aligned}\n\\]subtract twice first equation second equation, get\\[0 = 0\\]true, somewhat tautological. , really one equation. second equation just 2 times first equation.","code":""},{"path":"sets-functions.html","id":"three-equations-in-three-unknowns","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.9.4 Three equations in three unknowns","text":"can extend general method systems three linear equations three unknowns.4 Consider system equations\\[\n\\begin{aligned}\n2x &+ 7y &+ z &= 2 \\\\\n&\\quad\\; 3y &- 2z &= 7 \\\\\n&& 4z &= 4\n\\end{aligned}\n\\]triangular system unique trianglular pattern equations. relatively easy solve via back-substitution since can immediately solve \\(z\\) third equation (\\(z = 1\\)), substitute value second equation calculate\\[\n\\begin{aligned}\n3y - 2 (1) &= 7 \\\\\n3y - 2 &= 7 \\\\\n3y &= 9 \\\\\ny &= 3\n\\end{aligned}\n\\]substitute values first equation\\[\n\\begin{aligned}\n2x + 7(3) + 1 &= 2 \\\\\n2x + 22 &= 2 \\\\\n2x &= -20 \\\\\nx &= -10\n\\end{aligned}\n\\]solution therefore\\[x = -10, y = 3, z = 1\\]systems already follow form, can eliminate unknowns equations arrive triangular system. Consider\\[\n\\begin{aligned}\n2x &+ 4y &+ z &= 5 \\\\\nx &+ y &+ z &= 6 \\\\\n2x &+ 3y &+ 2z &= 6\n\\end{aligned}\n\\]can leave first equation eliminate \\(x\\) second third equations subtracting suitable multiples first equation\\[\n\\begin{aligned}\n2x &+ 4y &+ z &= 5 & \\\\\n&- y &+ \\frac{1}{2}z &= \\frac{7}{2} &\\quad \\text{- 1/2 times first equation}\\\\\n&- y &+ z &= 1 &\\quad \\text{- first equation}\\\\\n\\end{aligned}\n\\]Finally, eliminate \\(y\\) third equation subtracting second equation.\\[\n\\begin{aligned}\n2x &+ 4y &+ z &= 5 \\\\\n&- y &+ \\frac{1}{2}z &= \\frac{7}{2} \\\\\n& &+ \\frac{1}{2}z &= -\\frac{5}{2}\n\\end{aligned}\n\\]Using backsubstitution, arrive solution\\[z = -5, y = \\frac{1}{2}(z - 7) = \\frac{1}{2} \\times (-12) = -6, x = \\frac{1}{2}(5 - z) - 2y = \\frac{1}{2} \\times 10 + 12 = 17\\]\\[x = 17, y = -6, z = -5\\]","code":""},{"path":"sets-functions.html","id":"gaussian-elimination","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.9.5 Gaussian elimination","text":"addition adding subtracting multiples one equation another, can also introduce elementary operations interchange order equations easily arrive triangular system. technique known Gaussian elimination. Elementary operatons includeEO1 - writing equations different orderEO2 - subtracting multiple one equation another equationWe can alternate use operations necessary simplify system linear equations triangular form, apply backsubstitution solve unknowns.Example 1.13  Consider example\\[\n\\begin{aligned}\n& \\quad\\; y &+ 2z &= 2 \\\\\n2 x &&+ z &= -1 \\\\\nx &+ 2 y &\\quad &= -1 \n\\end{aligned}\n\\]make formatting bit easier, fill missing unknowns 0 coefficients.\\[\n\\begin{aligned}\n0x & +y &+ 2z &= 2 \\\\\n2 x &+ 0y &+ z &= -1 \\\\\nx &+ 2 y &+ 0z &= -1 \n\\end{aligned}\n\\]Swap equation 1 equation 2:\n\\[\n \\begin{aligned}\n 2 x &+ 0y &+ z &= -1 \\\\\n 0x & +y &+ 2z &= 2 \\\\\n x &+ 2 y &+ 0z &= -1 \n \\end{aligned}\n \\]Swap equation 1 equation 2:\\[\n \\begin{aligned}\n 2 x &+ 0y &+ z &= -1 \\\\\n 0x & +y &+ 2z &= 2 \\\\\n x &+ 2 y &+ 0z &= -1 \n \\end{aligned}\n \\]Subtract 1/2 × (equation 1) equation 3:\n\\[\n \\begin{aligned}\n 2 x &+ 0y &+ z &= -1 \\\\\n 0x & +y &+ 2z &= 2 \\\\\n 0x &+ 2 y &- \\frac{z}{2} &= -\\frac{1}{2} \n \\end{aligned}\n \\]Subtract 1/2 × (equation 1) equation 3:\\[\n \\begin{aligned}\n 2 x &+ 0y &+ z &= -1 \\\\\n 0x & +y &+ 2z &= 2 \\\\\n 0x &+ 2 y &- \\frac{z}{2} &= -\\frac{1}{2} \n \\end{aligned}\n \\]Multiply equation 3 2:\n\\[\n \\begin{aligned}\n 2 x &+ 0y &+ z &= -1 \\\\\n 0x & +y &+ 2z &= 2 \\\\\n 0x &+ 4 y &- z &= -1\n \\end{aligned}\n \\]Multiply equation 3 2:\\[\n \\begin{aligned}\n 2 x &+ 0y &+ z &= -1 \\\\\n 0x & +y &+ 2z &= 2 \\\\\n 0x &+ 4 y &- z &= -1\n \\end{aligned}\n \\]Swap equation 2 equation 3:\n\\[\n \\begin{aligned}\n 2 x &+ 0y &+ z &= -1 \\\\\n 0x &+ 4 y &- z &= -1 \\\\\n 0x & +y &+ 2z &= 2 \\\\\n \\end{aligned}\n \\]Swap equation 2 equation 3:\\[\n \\begin{aligned}\n 2 x &+ 0y &+ z &= -1 \\\\\n 0x &+ 4 y &- z &= -1 \\\\\n 0x & +y &+ 2z &= 2 \\\\\n \\end{aligned}\n \\]Subtract 1/4 × (equation 2) equation 3:\n\\[\n \\begin{aligned}\n 2 x &+ 0y &+ z &= -1 \\\\\n 0x &+ 4 y &- z &= -1 \\\\\n 0x & + 0y &+ \\frac{9}{4}z &= \\frac{9}{4} \\\\\n \\end{aligned}\n \\]Subtract 1/4 × (equation 2) equation 3:\\[\n \\begin{aligned}\n 2 x &+ 0y &+ z &= -1 \\\\\n 0x &+ 4 y &- z &= -1 \\\\\n 0x & + 0y &+ \\frac{9}{4}z &= \\frac{9}{4} \\\\\n \\end{aligned}\n \\]point solve via backsubstitution.\\[\n\\begin{aligned}\n\\frac{9}{4}z &= \\frac{9}{4} \\\\\nz &= 1\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\n4 y -z &= -1 \\\\\n4 y -1 &= -1 \\\\\n4y &= 0 \\\\\ny &= 0\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\n2 x + z &= -1 \\\\\n2x + 1 &= -1 \\\\\n2x &= -2 \\\\\nx &= -1\n\\end{aligned}\n\\]\\[x = -1, y = 0, z = 1\\]","code":""},{"path":"sets-functions.html","id":"logarithms-and-exponential-functions","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.10 Logarithms and exponential functions","text":"Important component many mathematical statistical methods social scienceDefinition 1.13  (Exponent) Repeatedly multiply number itselfDefinition 1.14  (Logarithm) Reverse exponent","code":""},{"path":"sets-functions.html","id":"functions-with-exponents","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.10.1 Functions with exponents","text":"\\[f(x) = x \\times x = x^2\\]\\[f(x) = x \\times x \\times x = x^3\\]","code":""},{"path":"sets-functions.html","id":"common-rules-of-exponents","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.10.2 Common rules of exponents","text":"\\(x^0 = 1\\)\\(x^1 = x\\)\\(\\left ( \\frac{x}{y} \\right )^= \\left ( \\frac{x^}{y^}\\right ) = x^y^{-}\\)\\((x^)^b = x^{ab}\\)\\((xy)^= x^y^\\)\\(x^\\times x^b = x^{+b}\\)","code":""},{"path":"sets-functions.html","id":"logarithms","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.10.3 Logarithms","text":"Class functions\\(\\log_{b}(x) = \\Rightarrow b^= x\\)number \\(\\) solves \\(b^= x\\)","code":""},{"path":"sets-functions.html","id":"commonly-used-bases","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.10.3.1 Commonly used bases","text":"","code":""},{"path":"sets-functions.html","id":"base-10","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.10.3.1.1 Base 10","text":"\\[\\log_{10}(100) = 2 \\Rightarrow 10^2 = 100\\]\\[\\log_{10}(0.1) = -1 \\Rightarrow 10^{-1} = 0.1\\]","code":""},{"path":"sets-functions.html","id":"base-2","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.10.3.1.2 Base 2","text":"\\[\\log_{2}(8) = 3 \\Rightarrow 2^3 = 8\\]\\[\\log_{2}(1) = 0 \\Rightarrow 2^0 = 1\\]","code":""},{"path":"sets-functions.html","id":"base-e","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.10.3.1.3 Base \\(e\\)","text":"Euler’s number (aka natural logarithm)\\[\\log_{e}(e) = 1 \\Rightarrow e^1 = e\\]Natural logarithms incredibly useful math. Often \\(\\log()\\) assumed natural log. may also see written \\(\\ln()\\).","code":""},{"path":"sets-functions.html","id":"rules-of-logarithms","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.10.3.2 Rules of logarithms","text":"\\(\\log_b(1) = 0\\)\\(\\log(x \\times y) = \\log(x) + \\log(y)\\)\\(\\log(\\frac{x}{y}) = \\log(x) - \\log(y)\\)\\(\\log(x^y) = y \\log(x)\\)","code":""},{"path":"sets-functions.html","id":"bonus-content-computational-tools-for-the-future","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.11 Bonus content: Computational tools for the future","text":"learning programming camp. simply enough time. However want learn appropriate computational tools CSS, primer teach MACSS /things learn .Vision open-source - software free whose source code licensed public usageShift away proprietary formats (e.g. SPSS, Stata, SAS)Emphasis reproducibility\nCode\nResults\nAnalysis\nPublication\nCodeResultsAnalysisPublication","code":""},{"path":"sets-functions.html","id":"programming-languages-for-statistical-learning","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.11.1 Programming languages for statistical learning","text":"PythonR","code":""},{"path":"sets-functions.html","id":"version-control-git","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.11.2 Version control (Git)","text":"ReproducibilityBackupTrack changesAssign blame/responsibility","code":""},{"path":"sets-functions.html","id":"publishing","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.11.3 Publishing","text":"","code":""},{"path":"sets-functions.html","id":"move-away-from-wysiwyg","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.11.3.1 Move away from WYSIWYG","text":"probably lots experience Microsoft Word Google Docs. programs see get (WYSIWYG) – formatting performed via point click see final version screen write. reproducible format.Hard incorporate changes data analysisHard format scientific notation/equationsDifficult customize appearance maintain theme across documentsNot scripted","code":""},{"path":"sets-functions.html","id":"reproducible-formats","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.11.3.2 Reproducible formats","text":"","code":""},{"path":"sets-functions.html","id":"notebooks","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.11.3.2.1 Notebooks","text":"Integrate code, output, written textReproducible\nRerun notebook regenerate output\nRerun notebook regenerate outputGood prototyping exploratory data analysisFor Python - Jupyter NotebooksFor R - R Markdown","code":""},{"path":"sets-functions.html","id":"section","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.11.3.2.2 ","text":"High-quality typesetting systemHigh-quality typesetting systemDe facto standard production technical scientific documentation\nBooks\nJournal articles\nDe facto standard production technical scientific documentationBooksJournal articlesFree softwareFree softwareRenders documents PDFsRenders documents PDFsMakes typesetting easy\n$$f(x) = \\frac{\\exp(-\\frac{(x - \\mu)^2}{2\\sigma^2} )}{ \\sqrt{2\\pi \\sigma^2}}$$\n\\[f(x) = \\frac{\\exp(-\\frac{(x - \\mu)^2}{2\\sigma^2} )}{ \\sqrt{2\\pi \\sigma^2}}\\]Makes typesetting easy\\[f(x) = \\frac{\\exp(-\\frac{(x - \\mu)^2}{2\\sigma^2} )}{ \\sqrt{2\\pi \\sigma^2}}\\]Tables/figures/general typesetting/nice presentations - easier \nPapers\nBooks\nDissertations/theses\nSlides (beamer)\nTables/figures/general typesetting/nice presentations - easier PapersBooksDissertations/thesesSlides (beamer)Steep learning curve front, leads big dividends laterSteep learning curve front, leads big dividends later","code":"$$f(x) = \\frac{\\exp(-\\frac{(x - \\mu)^2}{2\\sigma^2} )}{ \\sqrt{2\\pi \\sigma^2}}$$"},{"path":"sets-functions.html","id":"markdown","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.11.3.2.3 Markdown","text":"Lightweight markup language plain text formatting syntaxEasy convert HTML, PDF, moreUsed commonly GitHub documentation, Jupyter Notebooks, R Markdown, moreSimplified syntax compared - also less flexibilityPublishing formats\nHTML\nPDF\nWebsites\nSlides\nDashboards\nWord/ODT/RTF\nHTMLPDFWebsitesSlidesDashboardsWord/ODT/RTF","code":""},{"path":"sets-functions.html","id":"how-will-you-acquire-these-skills","chapter":"Day 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms","heading":"1.11.4 How will you acquire these skills?","text":"CAPP 30121/122Perspectives sequenceMACS 30500On ","code":""},{"path":"sequences-derivatives.html","id":"sequences-derivatives","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"Day 2 Sequences, limits, continuity, and derivatives","text":"","code":""},{"path":"sequences-derivatives.html","id":"learning-objectives-1","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"Learning objectives","text":"Define sequencesDistinguish convergence divergenceDefine limitsDefine continuityCalculate limits sequences functionsDefine slope lineSummarize tangent lines, rates change, derivativesDefine derivative rules common functionsApply product, quotient, chain rules differentiationSummarize exponential function natural logarithmsIdentify properties derivatives helpful statistical methods","code":""},{"path":"sequences-derivatives.html","id":"supplemental-readings-1","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"Supplemental readings","text":"Chapter 5-7, 9, Pemberton Rau (2011)OpenStax Calculus: Volume 1, ch 2-4OpenStax Calculus: Volume 2, ch 5","code":""},{"path":"sequences-derivatives.html","id":"sequence","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.1 Sequence","text":"","code":""},{"path":"sequences-derivatives.html","id":"definition","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.1.1 Definition","text":"Definition 2.1  (Sequence) sequence function whose domain set positive integersWe’ll write sequence ,\\[\\left\\{u_{n} \\right\\}_{n=1}^{\\infty} = (u_{1} , u_{2}, \\ldots, u_{N}, \\ldots )\\]","code":""},{"path":"sequences-derivatives.html","id":"examples","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.1.2 Examples","text":"\\[\\left\\{\\frac{1}{n} \\right\\} = (1, 1/2, 1/3, 1/4, \\ldots, 1/N, \\ldots, )\\]\\[\\left\\{\\frac{1}{n^2} \\right\\} = (1, 1/4, 1/9, 1/16, \\ldots, 1/N^2, \\ldots, )   \\\\\\]\\[\\left\\{\\frac{1 + (-1)^n}{2} \\right\\} = (0, 1, 0, 1, \\ldots, 0,1,0,1 \\ldots, ) \\\\\\]","code":""},{"path":"sequences-derivatives.html","id":"arithmetic-and-geometric-progressions","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.1.3 Arithmetic and geometric progressions","text":"Definition 2.2  (Arithmetic progression) arithmetic progression sequence \\(\\{ u_n \\}\\) property difference pair successive terms : \\(u_{n+1} - u_n\\) \\(n\\). arithmetic progression first term \\(\\) common difference \\(d\\) \\[, + d, + 2d, +3d, \\ldots\\]\\(n\\)th term given \\[u_n = + (n-1)d\\]Definition 2.3  (Geometric progression) geometric progression sequence \\(\\{ u_n \\}\\) term obtained preceding one multiplication number: ratio \\(\\frac{u_{n+1}}{u_n}\\) \\(n\\). geometric progression first term \\(\\) common ratio \\(x\\) \\[, ax, ax^2, ax^3, \\ldots\\]\\(n\\)th term given \\[u_n = ax^{n-1}\\]Illustrative principal convergence. applications geometric progressions occur economics (e.g. compounding interest).","code":""},{"path":"sequences-derivatives.html","id":"convergence","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.1.4 Convergence","text":"Consider sequence:\\[\\left\\{\\frac{(-1)^{n} }{n} \\right \\} = (-1, \\frac{1}{2}, \\frac{-1}{3}, \\frac{1}{4}, \\frac{-1}{5}, \\frac{1}{6}, \\frac{-1}{7}, \\frac{1}{8}, \\ldots )\\]Definition 2.4  (Convergence) sequence \\(\\left\\{u_{n} \\right\\}_{n=1}^{\\infty}\\) converges real number \\(\\) \\(\\epsilon >0\\) positive integer \\(N\\) \\(n \\geq N\\) \\(|u_{n} - | < \\epsilon\\).sequence converges, converges one number. call \\(\\).\\(\\epsilon>0\\) arbitrary real-valued number. Think error tolerance. Notice \\(\\epsilon > 0\\).see \\(N\\) depend upon \\(\\epsilon\\).Implies sequence never gets \\(\\epsilon\\) away \\(\\).Definition 2.5  (Divergence Bounded) sequence, \\(\\left\\{u_{n} \\right\\}\\) converges ’ll call convergent. doesn’t ’ll call divergent. number \\(M\\) , \\(n\\) \\(|u_{n}|<M\\), ’ll call bounded.unbounded sequence\n\\[\\left\\{ n \\right \\} = (1, 2, 3, 4, \\ldots, N, \\ldots )\\]\nunbounded sequence\\[\\left\\{ n \\right \\} = (1, 2, 3, 4, \\ldots, N, \\ldots )\\]bounded sequence doesn’t converge\n\\[\\left\\{\\frac{1 + (-1)^n}{2} \\right\\} = (0, 1, 0, 1, \\ldots, 0,1,0,1 \\ldots, )\\]\nbounded sequence doesn’t converge\\[\\left\\{\\frac{1 + (-1)^n}{2} \\right\\} = (0, 1, 0, 1, \\ldots, 0,1,0,1 \\ldots, )\\]convergent sequences bounded. sequence constant, \\(\\left\\{C \\right \\}\\) converges \\(C\\).","code":""},{"path":"sequences-derivatives.html","id":"algebra-of-sequences","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.1.5 Algebra of sequences","text":"add, multiply, divide sequences?Theorem 2.1  Suppose \\(\\left\\{a_{n} \\right \\}\\) converges \\(\\) \\(\\left\\{b_{n} \\right\\}\\) converges \\(B\\). ,\\(\\left\\{a_{n} + b_{n} \\right\\}\\) converges \\(+ B\\).\\(\\left\\{a_{n} b_{n} \\right\\}\\) converges \\(\\times B\\).Suppose \\(b_{n} \\neq 0 \\forall n\\) \\(B \\neq 0\\). \\(\\left\\{\\frac{a_{n}}{b_{n}} \\right\\}\\) converges \\(\\frac{}{B}\\).","code":""},{"path":"sequences-derivatives.html","id":"think-pair-share-1","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.1.5.1 Think, pair, share","text":"Consider sequence \\(\\left\\{\\frac{1}{n} \\right\\}\\) - converge ?Consider sequence \\(\\left\\{\\frac{1}{2n} \\right \\}\\) - converge ?\n0 - \\(2 \\times\\) really big number still leads really big number denominator0 - \\(n\\) gets bigger, fraction continually decreases towards 0","code":""},{"path":"sequences-derivatives.html","id":"challenge-questions","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.1.5.2 Challenge questions","text":"\\(\\left\\{3 + \\frac{1}{n}\\right\\}\\) converge ?\nClick solution\n\n\\[\\lim_{x \\rightarrow \\infty} \\left\\{3 + \\frac{1}{n}\\right\\} = 3\\]\n<\/p>\n\\(\\left\\{3 + \\frac{1}{n}\\right\\}\\) converge ?Click solution\n\\[\\lim_{x \\rightarrow \\infty} \\left\\{3 + \\frac{1}{n}\\right\\} = 3\\]\\(\\left\\{ (3 + \\frac{1}{n} ) (100 + \\frac{1}{n^4} ) \\right\\}\\)?\nClick solution\n\n\\[\n\\begin{aligned}\n\\lim_{x \\rightarrow \\infty} \\left\\{ (3 + \\frac{1}{n} ) (100  + \\frac{1}{n^4} ) \\right\\} &= \\lim_{x \\rightarrow \\infty} \\left\\{ (3 + \\frac{1}{n} ) \\right\\}  \\times \\lim_{x \\rightarrow \\infty} \\left\\{ (100  + \\frac{1}{n^4} ) \\right\\} \\\\\n&= 3 \\times 100 \\\\\n&= 300\n\\end{aligned}\n\\]\n<\/p>\n\\(\\left\\{ (3 + \\frac{1}{n} ) (100 + \\frac{1}{n^4} ) \\right\\}\\)?Click solution\n\\[\n\\begin{aligned}\n\\lim_{x \\rightarrow \\infty} \\left\\{ (3 + \\frac{1}{n} ) (100  + \\frac{1}{n^4} ) \\right\\} &= \\lim_{x \\rightarrow \\infty} \\left\\{ (3 + \\frac{1}{n} ) \\right\\}  \\times \\lim_{x \\rightarrow \\infty} \\left\\{ (100  + \\frac{1}{n^4} ) \\right\\} \\\\\n&= 3 \\times 100 \\\\\n&= 300\n\\end{aligned}\n\\]Finally, \\(\\left\\{ \\frac{ 300 + \\frac{1}{n} }{100 + \\frac{1}{n^4}} \\right\\}\\)?\nClick solution\n\n\\[\n\\begin{aligned}\n\\lim_{x \\rightarrow \\infty} \\left\\{ \\frac{ 300 + \\frac{1}{n} }{100  + \\frac{1}{n^4}} \\right\\} &= \\frac{300}{100} \\\\\n&= 3\n\\end{aligned}\n\\]\n<\/p>\nFinally, \\(\\left\\{ \\frac{ 300 + \\frac{1}{n} }{100 + \\frac{1}{n^4}} \\right\\}\\)?Click solution\n\\[\n\\begin{aligned}\n\\lim_{x \\rightarrow \\infty} \\left\\{ \\frac{ 300 + \\frac{1}{n} }{100  + \\frac{1}{n^4}} \\right\\} &= \\frac{300}{100} \\\\\n&= 3\n\\end{aligned}\n\\]","code":"<\/p><\/p><\/p>"},{"path":"sequences-derivatives.html","id":"limits","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.2 Limits","text":"","code":""},{"path":"sequences-derivatives.html","id":"sequences-leadsto-limits-of-functions","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.2.1 Sequences \\(\\leadsto\\) limits of functions","text":"Calculus/Real Analysis: study functions real lineLimit function: function behave gets close particular point?Relevant understanding application :DerivativesAsymptoticsGame Theory","code":""},{"path":"sequences-derivatives.html","id":"limits-of-functions","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.2.2 Limits of functions","text":"Definition 2.6  (Limit function) Suppose \\(f: \\Re \\rightarrow \\Re\\). say \\(f\\) limit \\(L\\) \\(x_{0}\\) , \\(\\epsilon>0\\), \\(\\delta>0\\) \\[|f(x) - L| < \\epsilon \\, \\forall \\, x \\backepsilon 0 < |x - x_0 | < \\delta\\]\\[|f(x) - L| < \\epsilon \\, \\text{} \\, x \\, \\text{} \\, 0 < |x - x_0 | < \\delta\\]Limits behavior functions points. \\(x_{0}\\). sequences, let \\(\\epsilon\\) define error rate. \\(\\delta\\) defines area around \\(x_{0}\\) \\(f(x)\\) going within error rate.","code":""},{"path":"sequences-derivatives.html","id":"examples-of-limits","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.2.3 Examples of limits","text":"Theorem 2.2  function \\(f(x) = x + 1\\) limit \\(1\\) \\(x_{0} = 0\\).Proof. Without loss generalization (WLOG) choose \\(\\epsilon >0\\). want show \\(\\delta_{\\epsilon}\\) \\(|f(x) - 1| < \\epsilon \\, \\text{} \\, x \\, \\text{} \\, 0 < |x - x_0 | < \\delta\\). words,\\[\n\\begin{aligned}\n|(x + 1) - 1| < \\epsilon \\, \\text{} \\, x \\, &\\text{} \\, 0 < |x - 0 | < \\delta \\\\\n|x| < \\epsilon \\, \\text{} \\, x \\, &\\text{} \\, 0 < |x | < \\delta \\\\\n\\end{aligned}\n\\]\\(\\delta_{\\epsilon} = \\epsilon\\) holds, done.function can limit \\(L\\) \\(x_{0}\\) even \\(f(x_{0} ) \\neq L\\)(!)Theorem 2.3  function \\(f(x) = \\frac{x^2 - 1}{x - 1}\\) limit \\(2\\) \\(x_{0} = 1\\).Proof. \\(x \\neq 1\\),\\[\n\\begin{aligned}\n\\frac{x^2 - 1}{x - 1} & = \\frac{(x + 1)(x - 1) }{x - 1} \\\\                  \n                                & = x + 1 \n\\end{aligned}\n\\]Choose \\(\\epsilon >0\\) set \\(x_{0}=1\\). , ’re looking \\(\\delta_{\\epsilon}\\) \\[\n\\begin{aligned}\n|(x + 1) -2 | < \\epsilon \\, \\text{} \\, x \\, &\\text{} \\, 0 < |x - 1 | < \\delta \\\\\n|x - 1 | < \\epsilon \\, \\text{} \\, x \\, &\\text{} \\, 0 < |x - 1 | < \\delta \\\\\n\\end{aligned}\n\\], \\(\\delta_{\\epsilon} = \\epsilon\\), satisfied.","code":""},{"path":"sequences-derivatives.html","id":"not-all-functions-have-limits","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.2.4 Not all functions have limits","text":"Theorem 2.4  Consider \\(f:(0,1) \\rightarrow \\Re\\), \\(f(x) = \\frac{1}{x}\\). \\(f(x)\\) limit \\(x_{0}=0\\)Proof. Choose \\(\\epsilon>0\\). need show exist\\[\n\\begin{aligned}\n|\\frac{1}{x} - L| < \\epsilon \\, \\text{} \\, x \\, &\\text{} \\, 0 < |x - 0 | < \\delta \\\\\n|\\frac{1}{x} - L| < \\epsilon \\, \\text{} \\, x \\, &\\text{} \\, 0 < |x| < \\delta \\\\\n\\end{aligned}\n\\], problem. \\[\n\\begin{aligned}\n\\frac{1}{x} - L & < \\epsilon \\\\\n\\frac{1}{x} & < \\epsilon + L \\\\\nx & > \\frac{1}{L + \\epsilon}  \n\\end{aligned}\n\\]implies can’t \\(\\delta\\), \\(x\\) bigger \\(\\frac{1}{L + \\epsilon}\\).","code":""},{"path":"sequences-derivatives.html","id":"intuitive-definition-of-a-limit","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.2.5 Intuitive definition of a limit","text":"Definition 2.7  (Limit) function \\(f\\) tends \\(L\\) point \\(x_{0}\\) say limit \\(L\\) \\(x_{0}\\) commonly write,\\[\\lim_{x \\rightarrow x_{0}} f(x) = L\\]Definition 2.8  (Right left-hand limits) function \\(f\\) tends \\(L\\) point \\(x_{0}\\) approach right, write\\[\\lim_{x \\rightarrow x_{0}^{+} } f(x) = L\\]call right hand limit.function \\(f\\) tends \\(L\\) point \\(x_{0}\\) approach left, write\\[\\lim_{x \\rightarrow x_{0}^{-} } f(x) = L\\]call left-hand limit.","code":""},{"path":"sequences-derivatives.html","id":"algebra-of-limits","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.2.6 Algebra of limits","text":"Theorem 2.5  Suppose \\(f:\\Re \\rightarrow \\Re\\) \\(g: \\Re \\rightarrow \\Re\\) limits \\(\\) \\(B\\) \\(x_{0}\\). ,\\[\n\\begin{aligned}\n\\text{.) } \\lim_{x \\rightarrow x_{0} } (f(x) + g(x) ) & = \\lim_{x \\rightarrow x_{0}} f(x) + \\lim_{x \\rightarrow x_{0}} g(x)  = + B \\\\\n\\text{ii.) }\\lim_{x \\rightarrow x_{0} } f(x) g(x) & = \\lim_{x \\rightarrow x_{0}} f(x) \\lim_{x\\rightarrow x_{0}} g(x)  = B \n\\end{aligned}\n\\]Suppose \\(g(x) \\neq 0\\) \\(x \\\\Re\\) \\(B \\neq 0\\) \\(\\frac{f(x)}{g(x)}\\) limit \\(x_{0}\\) \\[\\lim_{x \\rightarrow x_{0}} \\frac{f(x)}{g(x)} =  \\frac{\\lim_{x\\rightarrow x_{0} } f(x) }{\\lim_{x \\rightarrow x_{0} } g(x) } = \\frac{}{B}\\]","code":""},{"path":"sequences-derivatives.html","id":"continuity","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.3 Continuity","text":"example , limit exists 1. hole function. function fails pencil test, discontinuous 1.Definition 2.9  (Pencil test) Imagine drawing whole function pencil. can without lifting pencil paper, function continuous. lift pencil , even one single point, function discontinuous.5","code":""},{"path":"sequences-derivatives.html","id":"defining-continuity","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.3.1 Defining continuity","text":"Definition 2.10  Suppose \\(f:\\Re \\rightarrow \\Re\\) consider \\(x_{0} \\\\Re\\). say \\(f\\) continuous \\(x_{0}\\) \\(\\epsilon>0\\) \\(\\delta>0\\) ,\\[\n\\begin{aligned}\n|x - x_{0} | & < \\delta \\text{  } x \\\\Re \\text{ } \\nonumber \\\\\n|f(x) - f(x_{0})| & < \\epsilon \\nonumber \n\\end{aligned}\n\\]Previously \\(f(x_{0})\\) replaced \\(L\\). Now \\(f(x)\\) converge \\(x_{0}\\). Continuity restrictive limit.","code":""},{"path":"sequences-derivatives.html","id":"examples-of-continuity","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.3.1.1 Examples of continuity","text":"","code":""},{"path":"sequences-derivatives.html","id":"a-real-world-example-of-limits-measuring-incumbency-advantage","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.3.2 A real-world example of limits: Measuring incumbency advantage","text":"Incumbency advantage overall causal impact current incumbent party district votes obtained district’s election. Lee (2008), unit analysis congressional district U.S. House Representatives. United States, incumbent parties win consistently high rate elections U.S. House (\\(>90\\%\\) win rate).Incumbent candidates also high win rate, though bit smaller due retirement (\\(\\approx 88\\%\\) probability running reelection, \\(\\approx 90\\%\\) probability winning conditional running election). Compare runner-– \\(3\\%\\) chance winning next election, \\(20\\%\\) chance running next election.electoral advantage incumbency? , expect incumbents use privileges resources office gain “unfair” advantage potential challengers. Therefore electoral advantage incumbency – winning causal influence probability candidate run office eventually win next election.Can proven observational study? – compare incumbent non-incumbent electoral outcomes. difference win probabilities selection effect – incumbents , definition, politicians successful previous election – therefore incumbency cause advantage?","code":""},{"path":"sequences-derivatives.html","id":"ideal-experiment","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.3.2.1 Ideal experiment","text":"Randomly assign incumbent parties district Democrats RepublicansKeep factors constantCorresponding increase Democratic/Republican electoral success next election represent overall electoral benefit due incumbent party districtObviously realistic","code":""},{"path":"sequences-derivatives.html","id":"regression-discontinuity-design","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.3.2.2 Regression discontinuity design","text":"RDDs - dichotomous treatment deterministic function single, continuous covariateTreatment assigned individuals whose score crosses known threshold.know score, can reverse-engineer treatment assignment assume -random assignment local neighborhood around probability \\(50\\%\\).context incumbency advantage, consider whether Democrats incumbent party Congressional district deterministic function vote share prior election. Democrats incumbent party whenever two-party margin victor greater \\(0\\). plausible within local range value, district assignment Democrats Republicans -random. differences estimated probability winning election can attributed effect incumbent parties.\nFigure 2.1: Source: Randomized experiments non-random selection U.S. House elections. Lee (2008).\napparent figure, large discontinuous jump 0 point. Democrats barely win election much likely run office succeed next election, compared Democrats barely lose. causal effect enormous. Nowhere else large jump function well-behaved smooth except threshold determining victory defeat. discontinuity key evidence causal effect incumbency advantage electoral success.","code":""},{"path":"sequences-derivatives.html","id":"continuity-and-limits","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.3.3 Continuity and limits","text":"Theorem 2.6  Let \\(f: \\Re \\rightarrow \\Re\\) \\(x_{0} \\\\Re\\). \\(f\\) continuous \\(x_{0}\\) \\(f\\) limit \\(x_{0}\\) \\(\\lim_{x \\rightarrow x_{0} } f(x) = f(x_{0})\\).Proof. \\((\\Rightarrow)\\). Suppose \\(f\\) continuous \\(x_{0}\\). implies \\(|f(x) - f(x_0)| < \\epsilon \\, \\text{} \\, x \\, \\text{} \\, |x - x_0 | < \\delta\\). definition limit, \\(L = f(x_{0})\\).\\((\\Leftarrow)\\). Suppose \\(f\\) limit \\(x_{0}\\) limit \\(f(x_{0})\\). implies \\(|f(x) - f(x_0)| < \\epsilon \\, \\text{} \\, x \\, \\text{} \\, |x - x_0 | < \\delta\\). definition continuity.","code":""},{"path":"sequences-derivatives.html","id":"algebra-of-continuous-functions","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.3.4 Algebra of continuous functions","text":"Theorem 2.7  Suppose \\(f:\\Re \\rightarrow \\Re\\) \\(g:\\Re \\rightarrow \\Re\\) continuous \\(x_{0}\\). ,\\(f(x) + g(x)\\) continuous \\(x_{0}\\)\\(f(x) g(x)\\) continuous \\(x_{0}\\)\\(g(x_0) \\neq 0\\), \\(\\frac{f(x) } {g(x) }\\) continuous \\(x_{0}\\)","code":""},{"path":"sequences-derivatives.html","id":"what-is-calculus","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.4 What is calculus?","text":"Calculus study continuous change within functions. Within calculus falls differential calculus (concerning instantaneous rates change slopes curves) integral calculus (concerning accumulation quantities areas curves), forever intertwined one another. Calculus broad applications mathematical statistical methods social sciences. Calculus fundamental part type statistics exercise. Although may taking derivatives integral daily work analyst, calculus undergirds many concepts use: maximization, expectation, cumulative probability.Within computational social science, calculus crucial finding identifying extreme values: maxima minima. process known optimization, uses empirical studies well formal theory:Given data, likely value parameter(s)?Game theory: given another player’s strategy, action maximizes utility?","code":""},{"path":"sequences-derivatives.html","id":"derivatives","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.5 Derivatives","text":"","code":""},{"path":"sequences-derivatives.html","id":"how-functions-change","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.5.1 How functions change","text":"Derivatives rates change functions. can think special type limit.","code":""},{"path":"sequences-derivatives.html","id":"the-tangent-as-a-limit","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.5.2 The tangent as a limit","text":"Say \\(y = f(x)\\) point \\(P\\) curve. Let \\(Q\\) another point curve, let \\(L\\) straight line \\(P\\) \\(Q\\). think \\(L\\) entire straight line \\(P\\) \\(Q\\), extending forever directions.Now suppose move point \\(Q\\) along graph direction \\(Q\\). curve reasonably smooth, slope \\(L\\) tends limit \\(Q\\) approaches \\(P\\), limit whether \\(P\\) approached right left. slope curve \\(P\\) limit slope \\(L\\) \\(Q\\) approaches \\(P\\).tangent curve \\(P\\), labeled \\(T\\), straight line \\(P\\) whose slope slope curve \\(P\\).denote coordinates \\(P\\) \\(Q\\) ordered pairs \\((x_0, y_0)\\) \\((x_1, y_1)\\) respectively. ThenWe can rewrite equation emphasize fact \\(P\\) \\(Q\\) lie graph \\(y = f(x)\\). Let \\(h = x_1 - x_0\\). \\[x_1 = x_0 + h, \\quad y_0 = f(x_0), \\quad y_1 = f(x_0 + h)\\]\\[\\text{slope } L = \\frac{f(x_0 + h) - f(x_0)}{h}\\]say \\(Q\\) approaches \\(P\\) along curve saying \\(h\\) approaches \\(0\\). Thus slope point \\(P\\) graph \\(y = f(x)\\) limit right-hand side \\(h\\) approaches \\(0\\).","code":""},{"path":"sequences-derivatives.html","id":"derivative","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.5.3 Derivative","text":"Suppose \\(f:\\Re \\rightarrow \\Re\\). Measure rate change point \\(x_{0}\\) function \\(R(x)\\),\\[\nR(x) = \\frac{f(x) - f(x_{0}) }{ x- x_{0} }\n\\]\\(R(x)\\) defines rate change. derivative examine happens small perturbation \\(x_{0}\\).Definition 2.11  (Derivative) Let \\(f:\\Re \\rightarrow \\Re\\). limit\\[\n\\begin{aligned}\n\\lim_{x\\rightarrow x_{0}} R(x) & = \\frac{f(x) - f(x_{0}) }{x - x_{0}} \\\\\n& = f^{'}(x_{0})\n\\end{aligned}\n\\]exists say \\(f\\) differentiable \\(x_{0}\\). \\(f^{'}(x_{0})\\) exists \\(x \\\\text{Domain}\\), say \\(f\\) differentiable.Let \\(f\\) function whose domain includes open interval containing point \\(x\\). derivative \\(f\\) \\(x\\) given \\[\n\\frac{d}{dx}f(x) =\\lim\\limits_{h\\0} \\frac{f(x+h)-f(x)}{(x+h)-x} = \\lim\\limits_{h\\0} \\frac{f(x+h)-f(x)}{h}\n\\]two main ways denote derivate:Leibniz Notation: \\(\\frac{d}{dx}(f(x))\\)Prime Lagrange Notation: \\(f'(x)\\)\\(f(x)\\) straight line, derivative slope. curve, slope changes values \\(x\\), derivative slope line tangent curve \\(x\\).\nFigure 2.2: Derivative Slope\n\\(f'(x)\\) exists point \\(x_0\\), \\(f\\) said differentiable \\(x_0\\). also implies \\(f(x)\\) continuous \\(x_0\\).","code":""},{"path":"sequences-derivatives.html","id":"rates-of-change-in-a-function","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.5.4 Rates of change in a function","text":"Another framework consider function \\(y = f(x)\\). \\(x\\) changes \\(x_0\\) \\(x_0 + h\\), value function changes \\(f(x_0)\\) \\(f(x_0 + h)\\). Thus change \\(x\\) \\(h\\), change \\(f(x)\\) \\(f(x_0 + h) - f(x_0)\\), rate change \\(f(x)\\) defined \\[\\frac{f(x_0 + h) - f(x_0)}{h}\\]define rate change \\(f(x)\\) \\(x=x_0\\) limit, \\(h \\rightarrow 0\\), rate change \\(f(x)\\) \\(x\\) changes \\(x_0\\) \\(x_0 + h\\). equivalent derivative \\(f'(x_0)\\).Consider example looking relationship campaign spending candidate’s vote share congressional election:Rate change \\(\\leadsto\\) return vote share dollars investedInstantaneous rate change \\(\\leadsto\\) increase vote share response infinitesimally small increase spendingA type limit","code":""},{"path":"sequences-derivatives.html","id":"examples-of-derivatives","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.5.5 Examples of derivatives","text":"Example 2.1  Suppose \\(f(x) = x^2\\) consider \\(x_{0} = 1\\). ,\\[\n\\begin{aligned}\n\\lim_{x\\rightarrow 1}R(x) & = \\lim_{x\\rightarrow 1} \\frac{x^2 - 1^2}{x - 1}  \\\\\n    & = \\lim_{x\\rightarrow 1} \\frac{(x- 1)(x + 1) }{ x- 1}   \\\\\n    & =  \\lim_{x\\rightarrow 1} x + 1  \\\\\n    & = 2\n\\end{aligned}\n\\]Example 2.2  Suppose \\(f(x) = |x|\\) consider \\(x_{0} = 0\\). ,\\[\n\\lim_{x\\rightarrow 0} R(x) = \\lim_{x\\rightarrow 0} \\frac{ |x| } {x} \n\\]\\(\\lim_{x \\rightarrow 0^{-}} R(x) = -1\\), \\(\\lim_{x \\rightarrow 0^{+}} R(x) = 1\\). , differentiable \\(0\\).","code":""},{"path":"sequences-derivatives.html","id":"continuity-and-derivatives","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.5.6 Continuity and derivatives","text":"\\(f(x) = |x|\\) continuous differentiable. change abrupt. suggests differentiability stronger condition.Theorem 2.8  Let \\(f:\\Re \\rightarrow \\Re\\) differentiable \\(x_{0}\\). \\(f\\) continuous \\(x_{0}\\).","code":""},{"path":"sequences-derivatives.html","id":"what-goes-wrong","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.5.7 What goes wrong?","text":"Consider following piecewise function:\\[\n\\begin{aligned}\nf(x)  & = x^{2} \\text{  } x \\\\Re \\setminus 0 \\\\\nf(x) & = 1000  \\text{  } x = 0\n\\end{aligned}\n\\]Consider derivative 0. ,\\[\n\\begin{aligned}\n\\lim_{x \\rightarrow 0 } R(x) & = \\lim_{x \\rightarrow 0 } \\frac{f(x) - 1000}{ x - 0   } \\\\\n                                        &= \\lim_{x \\rightarrow 0 } \\frac{x^2}{x} - \\lim_{x \\rightarrow 0 } \\frac{1000}{x}\n\\end{aligned} \n\\]\\(\\lim_{x \\rightarrow 0 } \\frac{1000}{x}\\) diverges, limit doesn’t exist.","code":""},{"path":"sequences-derivatives.html","id":"calculating-derivatives","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.6 Calculating derivatives","text":"Rarely take limit calculate derivative. Rather, rely rules properties derivatives. strategy:Algebra theoremsSome specific derivativesWork problems","code":""},{"path":"sequences-derivatives.html","id":"derivative-rules","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.6.1 Derivative rules","text":"Suppose \\(\\) constant, \\(f(x)\\) \\(g(x)\\) functions:\\[\n\\begin{aligned}\nf(x) &= x & \\quad f^{'}(x) &= 1 \\\\\nf(x) &= x^{k} & \\quad f^{'}(x) &= () (k) x ^{k-1} \\\\\nf(x) &= e^{x } & \\quad f^{'} (x) &= e^{x} \\\\\nf(x) &= \\sin(x) & \\quad f^{'} (x) &= \\cos (x) \\\\\nf(x) &= \\cos(x) & \\quad f^{'} (x) &= - \\sin(x) \\\\\n\\end{aligned}\n\\]Suppose \\(f\\) \\(g\\) functions differentiable \\(x\\) \\(k\\) scalar value. following rules apply:Definition 2.12  (Constant rule) \\[\\left[k f(x)\\right]' = k f'(x)\\]Definition 2.13  (Sum rule) \\[\\left[f(x)\\pm g(x)\\right]' = f'(x)\\pm g'(x)\\]Definition 2.14  (Product rule) \\[\\left[f(x)g(x)\\right]' = f'(x)g(x)+f(x)g'(x)\\]Definition 2.15  (Quotient rule) \\[\\frac{f(x)}{g(x)}' = \\frac{f'(x)g(x)-f(x)g'(x)}{[g(x)]^2}, g(x)\\neq 0\\]Definition 2.16  (Power rule) \\[\\left[x^k\\right]' = k x^{k-1}\\]“rules” become apparent applying definition derivative things “derived,” come frequently best repeat muscle memory.","code":""},{"path":"sequences-derivatives.html","id":"challenge-problems","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.6.2 Challenge problems","text":"Differentiate following functions evaluate specified value:\\(f(x)= x^3 + 5 x^2 + 4 x\\), \\(x_{0} = 2\\)\nClick solution\n\nPower rule.\n\\[\n   \\begin{aligned}\n   f'(x) &= 3x^2 + 10x + 4 \\\\\n   f'(2) &= 3 \\times 2^2 + 10 \\times 2 + 4 \\\\\n   &= 3 \\times 4 + 10 \\times 2 + 4 \\\\\n   &= 12 + 20 + 4 \\\\\n   &= 36\n   \\end{aligned}\n   \\]\n\n\\(f(x)= x^3 + 5 x^2 + 4 x\\), \\(x_{0} = 2\\)Click solution\nPower rule.\\[\n   \\begin{aligned}\n   f'(x) &= 3x^2 + 10x + 4 \\\\\n   f'(2) &= 3 \\times 2^2 + 10 \\times 2 + 4 \\\\\n   &= 3 \\times 4 + 10 \\times 2 + 4 \\\\\n   &= 12 + 20 + 4 \\\\\n   &= 36\n   \\end{aligned}\n   \\]\\(f(x) = \\sin(x) x^3\\) \\(x_{0} = 2\\)\nClick solution\n\nApplication product rule definition derivative \\(\\sin(x)\\).\n\\[\n   \\begin{aligned}\n   g(x) &= \\sin(x) &\\quad h(x) &= x^3 \\\\\n   g'(x) &= \\cos(x) &\\quad h'(x) &= 3x^2\n   \\end{aligned}\n   \\]\n\\[\n   \\begin{aligned}\n   f'(x) &= g'(x) h(x) + g(x) h'(x) \\\\\n   &= \\cos(x) x^3 + \\sin(x) 3x^2 \\\\\n   &= x^2 (x \\cos(x) + 3 \\sin(x)) \\\\\n   f'(2) &= 2^2 (2 \\cos(2) + 3 \\sin(2)) \\\\\n   &= 4 (2 \\cos(2) + 3 \\sin(2)) \\\\\n   &= 8 \\cos(2) + 12 \\sin(2) \\\\\n   &\\approx 7.582\n   \\end{aligned}\n   \\]\n\n\\(f(x) = \\sin(x) x^3\\) \\(x_{0} = 2\\)Click solution\nApplication product rule definition derivative \\(\\sin(x)\\).\\[\n   \\begin{aligned}\n   g(x) &= \\sin(x) &\\quad h(x) &= x^3 \\\\\n   g'(x) &= \\cos(x) &\\quad h'(x) &= 3x^2\n   \\end{aligned}\n   \\]\\[\n   \\begin{aligned}\n   f'(x) &= g'(x) h(x) + g(x) h'(x) \\\\\n   &= \\cos(x) x^3 + \\sin(x) 3x^2 \\\\\n   &= x^2 (x \\cos(x) + 3 \\sin(x)) \\\\\n   f'(2) &= 2^2 (2 \\cos(2) + 3 \\sin(2)) \\\\\n   &= 4 (2 \\cos(2) + 3 \\sin(2)) \\\\\n   &= 8 \\cos(2) + 12 \\sin(2) \\\\\n   &\\approx 7.582\n   \\end{aligned}\n   \\]\\(h(x) = \\dfrac{e^{x}}{x^3}\\) \\(x_0 = 2\\)\nClick solution\n\nApplication quotient rule definition derivative \\(e^x\\).\n\\[\n   \\begin{aligned}\n   f(x) &= e^x &\\quad g(x) &= x^3 \\\\\n   f'(x) &= e^x &\\quad g'(x) &= 3x^2\n   \\end{aligned}\n   \\]\n\\[\n   \\begin{aligned}\n   h'(x) &= \\frac{f'(x)g(x)-f(x)g'(x)}{[g(x)]^2}, g(x)\\neq 0 \\\\\n   &= \\frac{e^x x^3 - e^x 3x^2}{(x^3)^2} \\\\\n   &= \\frac{e^x x^2 (x - 3)}{x^6} \\\\\n   &= \\frac{e^x (x - 3)}{x^4}, g(x)\\neq 0 \\\\\n   h'(2) &= \\frac{e^2 (2 - 3)}{2^4} \\\\\n   &= \\frac{-(e^2)}{16} \\\\\n   &\\approx \\frac{-7.389}{16} \\\\\n   &\\approx -0.462\n   \\end{aligned}\n   \\]\n\n\\(h(x) = \\dfrac{e^{x}}{x^3}\\) \\(x_0 = 2\\)Click solution\nApplication quotient rule definition derivative \\(e^x\\).\\[\n   \\begin{aligned}\n   f(x) &= e^x &\\quad g(x) &= x^3 \\\\\n   f'(x) &= e^x &\\quad g'(x) &= 3x^2\n   \\end{aligned}\n   \\]\\[\n   \\begin{aligned}\n   h'(x) &= \\frac{f'(x)g(x)-f(x)g'(x)}{[g(x)]^2}, g(x)\\neq 0 \\\\\n   &= \\frac{e^x x^3 - e^x 3x^2}{(x^3)^2} \\\\\n   &= \\frac{e^x x^2 (x - 3)}{x^6} \\\\\n   &= \\frac{e^x (x - 3)}{x^4}, g(x)\\neq 0 \\\\\n   h'(2) &= \\frac{e^2 (2 - 3)}{2^4} \\\\\n   &= \\frac{-(e^2)}{16} \\\\\n   &\\approx \\frac{-7.389}{16} \\\\\n   &\\approx -0.462\n   \\end{aligned}\n   \\]\\(h(x) = \\log (x) x^3\\) \\(x_0 = e\\)\nClick solution\n\nRequires product rule combined power rule knowledge derivative \\(\\log(x)\\).\n\\[\n   \\begin{aligned}\n   f(x) &= \\log(x) &\\quad g(x) &= x^3 \\\\\n   f'(x) &= \\frac{1}{x} &\\quad g'(x) &= 3x^2\n   \\end{aligned}\n   \\]\n\\[\n   \\begin{aligned}\n   h'(x) &= f'(x)g(x) + f(x)g'(x) \\\\\n   &= \\frac{1}{x} \\times x^3 + \\log(x) \\times 3x^2 \\\\\n   &= x^2 + 3x^2 \\log(x) \\\\\n   &= x^2(1 + 3 \\log(x)) \\\\\n   h'(e) &= e^2(1 + 3 \\log(e)) \\\\\n   &= e^2 (1 + 3 * 1) \\\\\n   &= 4e^2\n   \\end{aligned}\n   \\]\n\n\\(h(x) = \\log (x) x^3\\) \\(x_0 = e\\)Click solution\nRequires product rule combined power rule knowledge derivative \\(\\log(x)\\).\\[\n   \\begin{aligned}\n   f(x) &= \\log(x) &\\quad g(x) &= x^3 \\\\\n   f'(x) &= \\frac{1}{x} &\\quad g'(x) &= 3x^2\n   \\end{aligned}\n   \\]\\[\n   \\begin{aligned}\n   h'(x) &= f'(x)g(x) + f(x)g'(x) \\\\\n   &= \\frac{1}{x} \\times x^3 + \\log(x) \\times 3x^2 \\\\\n   &= x^2 + 3x^2 \\log(x) \\\\\n   &= x^2(1 + 3 \\log(x)) \\\\\n   h'(e) &= e^2(1 + 3 \\log(e)) \\\\\n   &= e^2 (1 + 3 * 1) \\\\\n   &= 4e^2\n   \\end{aligned}\n   \\]","code":""},{"path":"sequences-derivatives.html","id":"composite-functions","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.6.3 Composite functions","text":"useful rules , many functions ’ll see won’t fit neatly case immediately. Instead, functions functions. example, difference \\(x^2 + 1^2\\) \\((x^2 + 1)^2\\) may look trivial, sum rule can easily applied former, ’s actually obvious latter.Composite functions formed substituting one function another denoted \\[f \\circ g=f[g(x)]\\]form \\(f[g(x)]\\), range \\(g\\) must contained (least part) within domain \\(f\\). domain \\(f\\circ g\\) consists points domain \\(g\\) \\(g(x)\\) domain \\(f\\).example, let \\(f(x)=\\log x\\) \\(0<x<\\infty\\) \\(g(x)=x^2\\) \\(-\\infty<x<\\infty\\).\\[f\\circ g=\\log x^2, -\\infty<x<\\infty - \\{0\\}\\]Also\\[g\\circ f = [\\log x]^2, 0<x<\\infty\\]Notice \\(f\\circ g\\) \\(g\\circ f\\) functions.notation composite functions place, now can introduce helpful additional rule deal derivative composite functions chain concentric derivatives.","code":""},{"path":"sequences-derivatives.html","id":"chain-rule","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.6.4 Chain rule","text":"Let \\(y=f\\circ g= f[g(x)]\\). derivative \\(y\\) respect \\(x\\) \\[\\frac{d}{dx} \\{ f[g(x)] \\} = f'[g(x)] g'(x)\\]can read : “derivative composite function \\(y\\) derivative \\(f\\) evaluated \\(g(x)\\), times derivative \\(g\\).”chain rule can thought derivative “outside” times derivative “inside,” remembering derivative outside function evaluated value inside function.","code":""},{"path":"sequences-derivatives.html","id":"examples-of-the-chain-rule","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.6.4.1 Examples of the chain rule","text":"Example 2.3  \\[\n\\begin{aligned}\nh(x) &= e^{2x} \\\\\ng(x) &= e^{x} \\\\\nf(x) &= 2x\n\\end{aligned}\n\\]\\[h(x) = g(f(x)) = g(2x) = e^{2x}\\]Taking derivatives, \\[h^{'}(x) = g^{'}(f(x))f^{'}(x) = e^{2x}2\\]Example 2.4  \\[\n\\begin{aligned}\nh(x) &= \\log(\\cos(x) ) \\\\\ng(x) &= \\log(x) \\\\\nf(x) &= \\cos(x)\n\\end{aligned}\n\\]\\[h(x) = g(f(x)) = g( \\cos(x)) = \\log(\\cos(x))\\]\\[h^{'}(x) = g^{'}(f(x))f^{'}(x) = \\frac{-1}{\\cos(x)} \\sin(x) = -\\tan (x)\\]","code":""},{"path":"sequences-derivatives.html","id":"generalized-power-rule","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.6.4.1.1 Generalized Power Rule","text":"direct use chain rule exponent function, power rule applied generally:\\(f(x)=[g(x)]^p\\) rational number \\(p\\),\\[f^\\prime(x) =p[g(x)]^{p-1}g^\\prime(x)\\]","code":""},{"path":"sequences-derivatives.html","id":"derivatives-for-the-exponential-function-and-natural-logarithms","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.7 Derivatives for the exponential function and natural logarithms","text":"exponential function one important functions mathematics.previously discussed common rules exponents logarithms. , focus properties derivatives.","code":""},{"path":"sequences-derivatives.html","id":"derivative-of-exponential-function","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.7.1 Derivative of exponential function","text":"function \\(e^x\\) continuous differentiable domains, first derivative \\[\\frac{d}{dx}(e^x) = e^x\\]? According limit definition derivative:6\\[\n\\begin{aligned}\n\\frac{d}{dx}f(x) & = \\lim\\limits_{h\\0} \\frac{f(x+h)-f(x)}{h} \\\\\n& = \\lim\\limits_{h\\0} \\frac{e^{x + h} - e^x}{h}\n\\end{aligned}\n\\]law exponents, can split addition exponents multiplication base:\\[\\frac{d}{dx}f(x) = \\lim\\limits_{h\\0} \\frac{e^x e^h - e^x}{h}\\]Factor \\(e^x\\):\\[\\frac{d}{dx}f(x) = \\lim\\limits_{h\\0} \\frac{e^x(e^h - 1)}{h}\\]can put \\(e^x\\) front limit multiplicative constant (variable \\(x\\) term, limit \\(h \\rightarrow 0\\), \\(x \\rightarrow 0\\)):\\[\\frac{d}{dx}f(x) = e^x \\lim\\limits_{h\\0} \\frac{e^h - 1}{h}\\]\\(h\\) approaches \\(0\\), limit gets closer \\(\\frac{0}{0}\\) indeterminant form. visually inspect happening point:can clearly see \\(x\\) approaches \\(0\\), function converging towards 1, even never actually gets .7 can therefore substitute equation:\\[\n\\begin{aligned}\n\\frac{d}{dx}f(x) & = e^x \\lim\\limits_{h\\0} \\frac{e^h - 1}{h} \\\\\n& = e^x (1) \\\\\n& = e^x\n\\end{aligned}\n\\]Therefore, \\(e^x\\) derivative \\(e^x\\).\nFigure 2.3: Derivative Exponential Function\n","code":""},{"path":"sequences-derivatives.html","id":"derivative-of-the-natural-logarithm","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.7.2 Derivative of the natural logarithm","text":"natural logarithm \\(x\\) logarithm base \\(e\\) \\(x\\), \\(e\\) defined Euler’s number (\\(e^1 \\approx 2.7182818\\)):\\[y = \\log_e (x) \\iff x = e^y\\]direct relationship \\(e^x\\) \\(\\log_e(x)\\) (aka \\(\\log\\) \\(\\ln\\)):\\[\n\\begin{aligned}\ne^{\\log(x)} &= x \\, \\mbox{every positive number} \\, x \\\\\n\\log(e^y) &= y \\, \\mbox{every real number} \\, y \\\\\n\\end{aligned}\n\\]short, natural logarithm inverse function exponential function.\nFigure 2.4: Exponential function natural logarithm\nderivative natural logarithm \\[\\frac{d}{dx} \\log(x) = \\frac{1}{x}\\]follows inverse function rule states monotonic function \\(f\\) inverse \\(g\\), derivatives related :\\[\n\\begin{aligned}\ng'(y) &= \\frac{1}{f'(x)} \\\\\n\\frac{dx}{dy} &= \\frac{1}{\\frac{dy}{dx}}\n\\end{aligned}\n\\]well fact exponential function derivative. Let \\(y = \\log(x)\\). \\(x = e^y\\), \\(\\frac{dx}{dy} = e^y = x\\), \\[\\frac{dy}{dx} = \\frac{1}{\\frac{dx}{dy}} = \\frac{1}{x}\\]\nFigure 2.5: Derivative Natural Log\n","code":""},{"path":"sequences-derivatives.html","id":"relevance-of-exponential-functions-and-natural-logarithm","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.7.3 Relevance of exponential functions and natural logarithm","text":"exponential function popular economics growth time (e.g. compounding interest). Natural logarithms can used elasticity models, well transforming variables regression models appear normally distributed.","code":""},{"path":"sequences-derivatives.html","id":"derivatives-and-properties-of-functions","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.8 Derivatives and properties of functions","text":"Derivatives often used optimize function (tomorrow). also reveal average rates change crucial properties functions. want introduce ideas, hopefully make less shocking see work.","code":""},{"path":"sequences-derivatives.html","id":"relative-maxima-minima-and-derivatives","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.8.1 Relative maxima, minima and derivatives","text":"Theorem 2.9  (Rolle's theorem) Suppose \\(f:[, b] \\rightarrow \\Re\\). Suppose \\(f\\) relative maxima minima \\((,b)\\) call \\(c \\(, b)\\). \\(f'(c) = 0\\).Intuition:Proof. Consider (without loss generalization) relative maximum \\(c\\). Consider left-hand right-hand limits\\[\n\\begin{aligned}\n\\lim_{x \\rightarrow c^{-}} \\frac{f(x) - f(c) }{x - c } & \\geq 0  \\\\\n\\lim_{x \\rightarrow c^{+}} \\frac{f(x) - f(c) } {x - c }  & \\leq 0  \n\\end{aligned}\n\\]also know \\[\n\\begin{aligned}\n\\lim_{x \\rightarrow c^{-}} \\frac{f(x) - f(c ) }{x - c } & = f^{'}(c)  \\\\\n\\lim_{x \\rightarrow c^{+}} \\frac{f(x) - f(c) } {x - c }  &  =  f^{'}(c)   \n\\end{aligned}\n\\]way, , \\(\\lim_{x \\rightarrow c^{-}} \\frac{f(x) - f(c) }{x -c} = \\lim_{x \\rightarrow c^{+}} \\frac{f(x) - f(c) } {x - c}\\) \\(f^{'}(c) = 0\\).","code":""},{"path":"sequences-derivatives.html","id":"mean-value-theorem","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.8.2 Mean value theorem","text":"Rolle’s theorem special case mean value theorem, \\(f'(c) = 0\\).Theorem 2.10  (Mean value theorem) \\(f:[,b] \\rightarrow \\Re\\) continuous \\([,b]\\) differentiable \\((,b)\\), \\(c \\(,b)\\) \\[\nf^{'}(c) = \\frac{f(b) - f() } { b - } \n\\]","code":""},{"path":"sequences-derivatives.html","id":"applications-of-the-mean-value-theorem","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.8.3 Applications of the mean value theorem","text":"come formal theory article. ’ll least know look. allows us say lots powerful stuff functions, especially useful approximating derivatives.Corollary 2.1  Suppose \\(f:[,b] \\rightarrow \\Re\\) continuous \\([,b]\\) differentiable \\((,b)\\). ,\\(f^{'}(x) \\neq 0\\) \\(x \\(,b)\\) \\(f\\) 1-1If \\(f^{'}(x) = 0\\) \\(f(x)\\) constantIf \\(f^{'}(x)> 0\\) \\(x \\(,b)\\) \\(f\\) strictly increasingIf \\(f^{'}(x)<0\\) \\(x \\(,b)\\) \\(f\\) strictly decreasingLet’s prove turn. ? just applying ideas.","code":""},{"path":"sequences-derivatives.html","id":"if-fx-neq-0-for-all-x-in-ab-then-f-is-1-1","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.8.3.1 If \\(f^{'}(x) \\neq 0\\) for all \\(x \\in (a,b)\\) then \\(f\\) is 1-1","text":"one--one function function every element range function corresponds exactly one element domain.Proof. way contradiction, suppose \\(f\\) 1-1. \\(x, y \\(,b)\\) \\(f(x) = f(y)\\). ,\\[f'(c) = \\frac{f(x) - f(y)}{x- y} = \\frac{0}{x -y}  = 0\\]means \\(f' \\neq 0\\) \\(x\\)!","code":""},{"path":"sequences-derivatives.html","id":"if-fx-0-then-fx-is-constant","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.8.3.2 If \\(f^{'}(x) = 0\\) then \\(f(x)\\) is constant","text":"Proof. way contradiction, suppose \\(x, y \\(,b)\\) \\(f(x) \\neq f(y)\\). ,\\[f'(c) = \\frac{f(x) - f(y) } {x - y} \\neq 0\\]","code":""},{"path":"sequences-derivatives.html","id":"if-fx-0-for-all-x-in-ab-then-then-f-is-strictly-increasing","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.8.3.3 If \\(f^{'}(x)> 0\\) for all \\(x \\in (a,b)\\) then then \\(f\\) is strictly increasing","text":"Proof. way contradiction, suppose \\(x, y \\(,b)\\) \\(y<x\\) \\(f(y)>f(x)\\). ,\\[f'(c) = \\frac{f(x) - f(y) }{x - y } < 0\\]Bonus: proof strictly decreasing reverse ","code":""},{"path":"sequences-derivatives.html","id":"extension-to-indeterminate-form-limits","chapter":"Day 2 Sequences, limits, continuity, and derivatives","heading":"2.8.4 Extension to indeterminate form limits","text":"mean value theorem generalizes form known Cauchy mean value theorem.Theorem 2.11  (Cauchy mean value theorem) Suppose \\(f\\) \\(g\\) differentiable functions \\(\\) \\(b\\) real numbers \\(< b\\). Suppose also \\(g'(x) \\neq 0\\) \\(x\\) \\(< x < b\\). exists real number \\(c\\) \\(< c < b\\) \\[\\frac{f'(c)}{g'(c)} = \\frac{f(b) - f()}{g(b) - g()}\\]ordinary mean value theorem special case \\(g(x) = x\\) \\(x\\).extraordinarily helpful want calculate limit ratio\\[\\lim_{x \\rightarrow } \\frac{f(x)}{g(x)}\\]\\(f\\) \\(g\\) continuous functions. \\(g() \\neq 0\\) \\[\\lim_{x \\rightarrow } \\frac{f(x)}{g(x)} = \\frac{f()}{g()}\\]\\(g() = 0\\) \\(f() \\neq 0\\), limit exists. case \\(f() = g() = 0\\), known indeterminate form - limit may may exist case. Examples indeterminate forms include \\(\\frac{0}{0}\\) \\(\\frac{\\infty}{\\infty}\\).can use L’Hôpital’s Rule (derived Cauchy mean value theorem) simplify expression solve limit.Theorem 2.12  (L'Hôpital's Rule) Suppose \\(f() = g() = 0\\) \\(g'(x) \\neq 0\\) \\(x\\) close equal \\(\\). \\[\\lim_{x \\rightarrow } \\frac{f(x)}{g(x)} = \\lim_{x \\rightarrow } \\frac{f'(x)}{g'(x)}\\]provided limit right-hand side exists. follows Cauchy mean value thoerem. Using theorem assumption \\(f() = g() = 0\\), see given \\(x\\) close equal \\(\\), number \\(p\\) \\(\\) \\(b\\) \\[\n\\begin{aligned}\n\\frac{f(x) - f()}{g(x) - g()} &= \\frac{f'(p)}{g'(p)} \\\\\n\\frac{f(x) - 0}{g(x) - 0} &= \\frac{f'(p)}{g'(p)} \\\\\n\\frac{f(x)}{g(x)} &= \\frac{f'(p)}{g'(p)}\n\\end{aligned}\n\\]\\(x\\) approaches \\(\\), \\(p\\).Example 2.5  \\[\\lim_{x \\rightarrow 0} \\frac{(1 + x)^{1/3} - 1}{x - x^2}\\]Denote numerator fraction \\(f(x)\\) denominator \\(g(x)\\). \\(f(0) = g(0) = 0\\), form indeterminate. Now\\[\n\\begin{aligned}\nf'(x) &= \\frac{1}{3} (1 + x)^{-2/3} \\\\\nf'(0) &= \\frac{1}{3} (1)^{-2/3} = \\frac{1}{3} (1) = \\frac{1}{3} \\\\\ng'(x) &= 1 - 2x \\\\\ng'(0) &= 1 - 2(0) = 1\n\\end{aligned}\n\\]\\[\\lim_{x \\rightarrow } \\frac{f(x)}{g(x)} = \\lim_{x \\rightarrow } \\frac{f'(x)}{g'(x)} = \\frac{1/3}{1} = \\frac{1}{3}\\]Example 2.6  \\[l = \\lim_{x \\rightarrow 0} \\frac{x - \\log(1 + x)}{x^2}\\]\\[\n\\begin{aligned}\nf(x) &= x - \\log(1 + x) \\\\\nf'(x) &= 1 - \\frac{1}{1 + x} \\\\\ng(x) &= x^2 \\\\\ng'(x) &= 2x\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nL &= \\lim_{x \\rightarrow 0} \\frac{1 - \\frac{1}{1 + x}}{2x} \\\\\n&= \\lim_{x \\rightarrow 0} \\frac{1}{2x} - \\frac{\\frac{1}{1 + x}}{2x} \\\\\n&= \\lim_{x \\rightarrow 0} \\frac{1}{2x} - \\frac{1}{2x(1 + x)} \\\\\n&= \\lim_{x \\rightarrow 0} \\frac{1(1 + x)}{2x(1 + x)} - \\frac{1}{2x(1 + x)} \\\\\n&= \\lim_{x \\rightarrow 0} \\frac{1(1 + x) - 1}{2x(1 + x)} \\\\\n&= \\lim_{x \\rightarrow 0} \\frac{1 + x - 1}{2x(1 + x)} \\\\\n&= \\lim_{x \\rightarrow 0} \\frac{x}{2x(1 + x)} \\\\\n&= \\lim_{x \\rightarrow 0} \\frac{1}{2(1 + x)} \\\\\n&= \\lim_{x \\rightarrow 0} \\frac{1}{2(1 + 0)} = \\frac{1}{2}\n\\end{aligned}\n\\]Letting \\(x \\rightarrow 0\\), \\(L = \\frac{1}{2}\\).Note instead simplifying expression, use approach iteratively.\\[\\lim_{x \\rightarrow } \\frac{f(x)}{g(x)} = \\lim_{x \\rightarrow } \\frac{f'(x)}{g'(x)} = \\lim_{x \\rightarrow } \\frac{f''(x)}{g''(x)} = \\ldots\\]\\[\n\\begin{aligned}\nf''(x) &= \\frac{1}{(1 +x)^{2}} \\\\\ng''(x) &= 2 \\\\\n\\lim_{x \\rightarrow 0} \\frac{f''(x)}{g''(x)} &= \\frac{(1 + x)^{-2}}{2} = \\frac{1^{-2}}{2} = \\frac{1}{2}\n\\end{aligned}\n\\]","code":""},{"path":"critical-points.html","id":"critical-points","chapter":"Day 3 Critical points and approximation","heading":"Day 3 Critical points and approximation","text":"","code":""},{"path":"critical-points.html","id":"learning-objectives-2","chapter":"Day 3 Critical points and approximation","heading":"Learning objectives","text":"Define critical pointsCalculate critical points via analytical methodsDemonstrate optimization using maximum likelihood estimationIdentify need approximation methods calculating critical pointsExplain demonstrate root finding procedures using Newton-Raphson hill climberDemonstrate comptuational optimization using gradient descent","code":""},{"path":"critical-points.html","id":"supplemental-readings-2","chapter":"Day 3 Critical points and approximation","heading":"Supplemental readings","text":"Chapters 8, 9.1-.2, 10, Pemberton Rau (2011)OpenStax Calculus: Volume 1, ch 4","code":""},{"path":"critical-points.html","id":"intuition","chapter":"Day 3 Critical points and approximation","heading":"3.1 Intuition","text":"Recall Rolle’s Theorem:Theorem 3.1  (Rolle's theorem) Suppose \\(f:[, b] \\rightarrow \\Re\\). Suppose \\(f\\) relative maxima minima \\((,b)\\) call \\(c \\(, b)\\). \\(f'(c) = 0\\).Intuition:Rolle’s theorem guarantee’s , point, \\(f^{'}(x) = 0\\). Think intuition given theorem. happens approach left? happens approach right?","code":""},{"path":"critical-points.html","id":"higher-order-derivatives","chapter":"Day 3 Critical points and approximation","heading":"3.2 Higher order derivatives","text":"first derivative applying definition derivatives function, can expressed \\[f'(x),  ~~ y',  ~~ \\frac{d}{dx}f(x), ~~ \\frac{dy}{dx}\\]can keep applying differentiation process functions derivatives. derivative \\(f'(x)\\) respect \\(x\\), \\[f''(x)=\\lim\\limits_{h\\0}\\frac{f'(x+h)-f'(x)}{h}\\] can therefore call Second derivative:\\[f''(x), ~~ y'', ~~ \\frac{d^2}{dx^2}f(x), ~~ \\frac{d^2y}{dx^2}\\]Similarly, derivative \\(f''(x)\\) called third derivative denoted \\(f'''(x)\\). extension, nth derivative expressed \\(\\frac{d^n}{dx^n}f(x)\\), \\(\\frac{d^ny}{dx^n}\\).\\[\n\\begin{aligned}\nf(x) &=x^3\\\\\nf^{\\prime}(x) &=3x^2\\\\\nf^{\\prime\\prime}(x) &=6x \\\\\nf^{\\prime\\prime\\prime}(x) &=6\\\\\nf^{\\prime\\prime\\prime\\prime}(x) &=0\\\\\n\\end{aligned}\n\\]Earlier, said function differentiable given point, must continuous. , \\(f'(x)\\) continuous, \\(f(x)\\) called continuously differentiable. matters many findings optimization rely differentiation, want function differentiable many layers. function continuously differentiable infinitely called smooth. examples include:\\[\n\\begin{aligned}\nf(x) &= x^2 \\\\\nf(x) &= e^x\n\\end{aligned}\n\\]","code":""},{"path":"critical-points.html","id":"critical-points-1","chapter":"Day 3 Critical points and approximation","heading":"3.3 Critical points","text":"One important purpose derivatives find critical points along function. Critical points points domain function function either differentiable derivative equal zero.","code":""},{"path":"critical-points.html","id":"inflection-point","chapter":"Day 3 Critical points and approximation","heading":"3.3.1 Inflection point","text":"given function \\(y = f(x)\\), point \\((x^∗, y^∗)\\) called inflection point second derivative immediately one side point signed oppositely second derivative immediately side. Graphically, occurs tangent line switches sides function inflection point.","code":""},{"path":"critical-points.html","id":"concavity","chapter":"Day 3 Critical points and approximation","heading":"3.3.2 Concavity","text":"Concave (convex) - segment function, every possible chord (line segment connecting points along \\(f(x)\\)) functionConcave (concave) - segment function, every possible chord functionThis can verified graphically analytically using second derivative. function twice differentiable concave area, function concave \\(f''(x) < 0\\) concave \\(f''(x) > 0\\).functions strictly concave concave second derivative constantly signed entire domain \\(x\\).","code":""},{"path":"critical-points.html","id":"exponential-function-concave-up","chapter":"Day 3 Critical points and approximation","heading":"3.3.2.0.1 Exponential function (concave up)","text":"\\[\n\\begin{aligned}\nf(x) & =  e^{x} \\\\\nf^{'}(x) & =  e^{x} \\\\\nf^{''}(x) & =  e^{x} \n\\end{aligned}\n\\]","code":""},{"path":"critical-points.html","id":"natural-log-concave-down","chapter":"Day 3 Critical points and approximation","heading":"3.3.2.0.2 Natural log (concave down)","text":"\\[\n\\begin{aligned}\nf(x) & =  \\log(x) \\\\\nf^{'}(x) & =  \\frac{1}{x} \\\\\nf^{''}(x) & =  -\\frac{1}{x^2}\n\\end{aligned}\n\\]","code":""},{"path":"critical-points.html","id":"extrema","chapter":"Day 3 Critical points and approximation","heading":"3.4 Extrema","text":"Extreme values values function either maximum minimum value.Theorem 3.2  (Extreme value theorem) Suppose \\(f:[, b] \\rightarrow \\Re\\). exists numbers \\(c\\) \\(d\\) \\([, b]\\) \\(f(c) \\ge f(x) \\ge f(d)\\quad\\forall x\\[,b]\\). , \\(f\\) must attain maximum minimum, least .values can locally (potentially) globally across entire domain \\(f\\).","code":""},{"path":"critical-points.html","id":"minimum-and-maximum-on-the-interval-05-are-located-at-the-endpoints","chapter":"Day 3 Critical points and approximation","heading":"3.4.1 Minimum and maximum on the interval \\([0,5]\\) are located at the endpoints","text":"","code":""},{"path":"critical-points.html","id":"global-maximum-is-located-at-x0","chapter":"Day 3 Critical points and approximation","heading":"3.4.2 Global maximum is located at \\(x=0\\)","text":"","code":""},{"path":"critical-points.html","id":"global-minimum-is-located-at-x---frac92","chapter":"Day 3 Critical points and approximation","heading":"3.4.3 Global minimum is located at \\(x= - \\frac{9}{2}\\)","text":"","code":""},{"path":"critical-points.html","id":"a-bunch-of-local-minima-and-maxima","chapter":"Day 3 Critical points and approximation","heading":"3.4.4 A bunch of local minima and maxima","text":"","code":""},{"path":"critical-points.html","id":"x0-is-an-inflection-point-that-is-neither-a-minimum-nor-a-maximum-fx-0","chapter":"Day 3 Critical points and approximation","heading":"3.4.5 \\(x=0\\) is an inflection point that is neither a minimum nor a maximum (\\(f''(x) = 0\\))","text":"Also known saddle point.","code":""},{"path":"critical-points.html","id":"framework-for-analytical-optimization","chapter":"Day 3 Critical points and approximation","heading":"3.5 Framework for analytical optimization","text":"see critical points can used find extrema saddle points.Find \\(f'(x)\\)Set \\(f'(x)=0\\) solve \\(x\\). Call \\(x_0\\) \\(f'(x_0)=0\\) critical valuesFind \\(f''(x)\\). Evaluate \\(x_0\\)\n\\(f''(x) > 0\\), concave , therefore local minimum\n\\(f''(x) < 0\\), concave , therefore local maximum\n’s global maximum/minimum, produce largest/smallest value \\(f(x)\\)\nclosed range along domain, check endpoints well\n\\(f''(x) > 0\\), concave , therefore local minimumIf \\(f''(x) < 0\\), concave , therefore local maximumIf ’s global maximum/minimum, produce largest/smallest value \\(f(x)\\)closed range along domain, check endpoints well","code":""},{"path":"critical-points.html","id":"example-fx--x2-x-in--3-3","chapter":"Day 3 Critical points and approximation","heading":"3.5.1 Example: \\(f(x) = -x^2\\), \\(x \\in [-3, 3]\\)","text":"","code":""},{"path":"critical-points.html","id":"critical-value","chapter":"Day 3 Critical points and approximation","heading":"3.5.1.1 Critical Value","text":"\\[\n\\begin{eqnarray}\nf'(x) & = & - 2 x  \\\\\n0 & = & - 2 x^{*}  \\\\\nx^{*} & = & 0 \n\\end{eqnarray}\n\\]","code":""},{"path":"critical-points.html","id":"second-derivative","chapter":"Day 3 Critical points and approximation","heading":"3.5.1.2 Second Derivative","text":"\\[\n\\begin{eqnarray}\nf^{'}(x) & = & - 2x  \\\\\nf^{''}(x)  & = & - 2  \n\\end{eqnarray}\n\\]\\(f^{''}(x)< 0\\), local maximum","code":""},{"path":"critical-points.html","id":"example-fx-x3-x-in--3-3","chapter":"Day 3 Critical points and approximation","heading":"3.5.2 Example: \\(f(x) = x^3\\), \\(x \\in [-3, 3]\\)","text":"","code":""},{"path":"critical-points.html","id":"critical-value-1","chapter":"Day 3 Critical points and approximation","heading":"3.5.2.1 Critical Value","text":"\\[\n\\begin{eqnarray}\nf'(x) & = & 3 x^2  \\\\\n0 & = & 3 (x^{*})^2  \\\\\nx^{*} & = & 0 \n\\end{eqnarray}\n\\]","code":""},{"path":"critical-points.html","id":"second-derivative-1","chapter":"Day 3 Critical points and approximation","heading":"3.5.2.2 Second Derivative","text":"\\[\n\\begin{eqnarray}\nf^{''}(x) & = & 6x  \\\\\nf^{''}(0)  & = & 0\n\\end{eqnarray}\n\\]Neither minimum maximum, saddle point","code":""},{"path":"critical-points.html","id":"example-spatial-model","chapter":"Day 3 Critical points and approximation","heading":"3.5.3 Example: spatial model","text":"large literature Congress supposes legislators policies can situated policy space. Suppose legislator \\(\\) policies \\(x, \\\\Re\\). Define legislator \\(\\)’s utility , \\(U:\\Re \\rightarrow \\Re\\),\\[\n\\begin{aligned}\nU_{} (x) & = - (x - \\mu)^{2} \\\\ \nU_{}(x) & = - x^2 +  2 x \\mu  - \\mu^2\n\\end{aligned}\n\\]\\(\\)’s optimal policy range \\(x \\[\\mu- 2, \\mu + 2]\\)?","code":""},{"path":"critical-points.html","id":"first-derivative","chapter":"Day 3 Critical points and approximation","heading":"3.5.3.1 First derivative","text":"\\[\n\\begin{aligned}\nU_{}^{'} (x) & = -2 (x - \\mu) \\\\\n0 & = -2x^{*} + 2 \\mu \\\\ \nx^{*} & = \\mu\n\\end{aligned}\n\\]","code":""},{"path":"critical-points.html","id":"second-derivative-test","chapter":"Day 3 Critical points and approximation","heading":"3.5.3.2 Second derivative test","text":"\\[U^{''}_{}(x) = -2 <0 \\rightarrow \\text{Concave }\\]call \\(\\mu\\) legislator \\(\\)’s ideal point\\[\n\\begin{aligned}\nU_{}(\\mu) & = - (\\mu - \\mu)^2 = 0  \\\\\nU_{}(\\mu - 2) & = - (\\mu - 2 - \\mu)^2 = -4  \\\\\nU_{} (\\mu + 2) & = - (\\mu + 2 - \\mu)^2 = -4  \n\\end{aligned}\n\\]legislator maximizes utility \\(\\mu\\).","code":""},{"path":"critical-points.html","id":"example-maximum-likelihood-estimation","chapter":"Day 3 Critical points and approximation","heading":"3.5.4 Example: Maximum likelihood estimation","text":"likelihood function function calculating parameters statistical model, given specific observed data. ’ll see days, likelihood related , , probability. frequentist inference:Probability plausibility random outcome occurring, given model parameter valueLikelihood plausibility model parameter(s) value, given specific observed valueGenerally problems data known parameters unknown. order estimate best values parameters, can use optimization maximize function find values function located global maximum likelihood function.example likelihood function. want find maximum likelihood estimate function:8\\[\n\\begin{aligned}\nf(\\mu) & = \\prod_{=1}^{N} \\exp( \\frac{-(Y_{} - \\mu)^2}{ 2}) \\\\\n& = \\exp(- \\frac{(Y_{1} - \\mu)^2}{ 2}) \\times \\ldots \\times \\exp(- \\frac{(Y_{N} - \\mu)^2}{ 2}) \\\\\n& = \\exp( - \\frac{\\sum_{=1}^{N} (Y_{} - \\mu)^2} {2})\n\\end{aligned}\n\\]\\[\\exp(x) \\equiv e^{x}\\]\\(\\prod_{=1}^{N}\\) product observations.can rewrite function sum \\(\\sum_{=1}^{N} (Y_{} - \\mu)^2\\) multiplicative exponents rule.\nvalue small (exponentiation negative exponent). Trying find maximum \\(f(\\mu)\\) slight changes across different values \\(\\mu\\) extremely difficult. However, suppose \\(f:\\Re \\rightarrow (0, \\infty)\\). \\(x_{0}\\) maximizes \\(f\\), \\(x_{0}\\) maximizes \\(\\log(f(x))\\). , don’t need exact value likelihood function, just value \\(\\mu\\) maximized. value \\(\\mu^{*}\\) either function, instead maximizing likelihood ’ll maximize log-likelihood.\\[\n\\begin{aligned}\n\\log f(\\mu) & = \\log \\left( \\exp( - \\frac{\\sum_{=1}^{N} (Y_{} - \\mu)^2} {2}) \\right)  \\\\\n& = - \\frac{\\sum_{=1}^{N} (Y_{} - \\mu)^2} {2} \\\\\n    & = -\\frac{1}{2} \\left(\\sum_{=1}^{N} Y_{}^2 - 2\\mu \\sum_{=1}^{N} Y_{} + N\\times\\mu^2 \\right) \\\\\n\\frac{ \\partial \\log f(\\mu) }{ \\partial \\mu } & =       -\\frac{1}{2} \\left( - 2\\sum_{=1}^{N} Y_{} + 2 N \\mu \\right) \n\\end{aligned}\n\\]\\(\\log{e^{f(x)}} = f(x)\\)Expand terms summation, separate separate terms\n\\(\\sum_{= 1}^N \\mu^1 = N \\times \\mu^2\\)\n\\(\\sum_{= 1}^N \\mu^1 = N \\times \\mu^2\\)Calculate derivative\n\\(\\sum_{=1}^{N} Y_{}^2\\) constant respect \\(\\mu\\) drops \n\\(\\sum_{=1}^{N} Y_{}^2\\) constant respect \\(\\mu\\) drops \\[\n\\begin{aligned}\n0 & = -\\frac{1}{2} \\left( - 2 \\sum_{=1}^{N} Y_{} + 2 N \\mu^{*} \\right) \\\\\n0 & = \\sum_{=1}^{N} Y_{} - N \\mu^{*}  \\\\\nN \\mu^{*}  & =  \\sum_{=1}^{N}Y_{} \\\\\n\\mu^{*} & = \\frac{\\sum_{=1}^{N}Y_{}}{N} \\\\\n\\mu^{*} & = \\bar{Y}\n\\end{aligned}\n\\]Apply second derivative test:\\[\n\\begin{aligned}\nf^{'}(\\mu ) & = -\\frac{1}{2} \\left( - 2\\sum_{=1}^{N} Y_{} + 2 N \\mu \\right) \\\\\nf^{'}(\\mu ) & = \\sum_{=1}^{N} Y_{} - N \\mu \\\\\nf^{''}(\\mu ) & = -N \n\\end{aligned}\n\\]\\(-N<0\\), function concave point therefore maximum. arithmetic mean \\(Y\\) maximum likelihood estimator variable.","code":""},{"path":"critical-points.html","id":"computational-optimization-procedures","chapter":"Day 3 Critical points and approximation","heading":"3.6 Computational optimization procedures","text":"Analytic approaches can difficult impossible applications social sciences. Computational approaches simplify problem various forms approximations. Different algorithms available benefits/drawbacks. examples include:Newton-Raphson - expensiveGrid search - tediousGradient descent - parallelizable","code":""},{"path":"critical-points.html","id":"newton-raphson-root-finding","chapter":"Day 3 Critical points and approximation","heading":"3.6.1 Newton-Raphson root finding","text":"Roots values function \\(f(x)\\) \\(f(x) = 0\\), function crosses \\(x\\)-axis. Root solving necessary optimize function since first calculate first derivative \\(f'(x)\\), set equal 0, solve \\(x^{*}\\).However, always realistic method easy compute value. Instead, can use general iterative procedures approximate \\(x^{*}\\), decent reliability. Newton’s method (also called Newton-Raphson Newton-Raphson hill climber) one procedure relies Taylor series expansion iterate series possible \\(x^{*}\\) values converging final estimate.","code":""},{"path":"critical-points.html","id":"tangent-lines","chapter":"Day 3 Critical points and approximation","heading":"3.6.1.1 Tangent lines","text":"Solving \\(f(x) = 0\\) can challenging \\(f(x)\\) non-linear function. However, solving \\(x\\) \\(f(x)\\) linear relatively easy. can approximate function \\(f(x)\\) using linear functions, problem becomes less difficult.start, define tangent point line point whose slope slope curve.formula tangent line \\(x_0\\) :\\[g(x) = f^{'}(x_{0}) (x - x_{0} ) + f(x_{0})\\]’ll use formula tangent line generate updates.","code":""},{"path":"critical-points.html","id":"newton-raphson-method","chapter":"Day 3 Critical points and approximation","heading":"3.6.1.2 Newton-Raphson method","text":"Suppose initial guess \\(x_{0}\\). ’re going approximate \\(f(x)\\) tangent line generate new guess:\\[\n\\begin{aligned}\ng(x) & = f^{'}(x_{0})(x - x_{0} ) + f(x_{0} ) \\\\\n0 & = f^{'}(x_{0}) (x_{1} - x_{0}) + f(x_{0} ) \\\\\nx_{1} & =  x_{0} - \\frac{f(x_{0}) }{f^{'}(x_{0})}\n\\end{aligned}\n\\]procedure algorithmic iterative initial guess \\(x_1\\) optimal. However, can use procedure multiple times substituting new value \\(x_1\\) function \\(x_0\\) updating \\(x_1\\). Repeat step sufficiently \\(f(x_{j+1})\\) sufficiently close zero, stop.","code":""},{"path":"critical-points.html","id":"example-y--x2","chapter":"Day 3 Critical points and approximation","heading":"3.6.1.3 Example: \\(y = -x^2\\)","text":"Let’s learn Newton’s method using arbitrary function. requirement want impose function twice differentiable (soon notice ). Say want optimize following arbitrary function:\\[y = -x^2\\]first derivative function \\(\\frac{\\partial y}{\\partial x} = -2x\\).second derivative \\(\\frac{\\partial^2 y}{\\partial x^2} = -2\\)function looks like :want use Newton-Raphson find critical point:","code":""},{"path":"critical-points.html","id":"implementing-newton-raphson","chapter":"Day 3 Critical points and approximation","heading":"3.6.1.3.1 Implementing Newton-Raphson","text":"Recall given starting value \\(x_0\\), determine next guess optimum function \\(f(x)\\) using following equation:\\[x_1 = x_0 - \\frac{f'(x_0)}{f''(x_0)}\\], generally, \\((n+1)^{th}\\) guess, use\\[x_{n+1} = x_n - \\frac{f'(x_n)}{f''(x_n)}\\]Given function,\\[x_{n+1} = x_n - \\frac{-2x}{-2}\\]want continue looking new guesses till difference guess previous one sufficiently small. example, say want stick guess \\(x_n\\) \\(\\mid x_n - x_{n-1} \\mid < 0.0001\\).Let’s find maximum function. Let’s start relatively high guess \\(x_0 = 3\\):gives us maximum. plug second derivative, see value \\(-2\\) local maximum second derivative evaluated point negative. Moreover, find first derivative evaluated point (\\(-2\\)) (close ) zero.","code":""},{"path":"critical-points.html","id":"example-y-x3-2x2---3x-4","chapter":"Day 3 Critical points and approximation","heading":"3.6.1.4 Example: \\(y = x^3 + 2x^2 - 3x + 4\\)","text":"Now let’s consider function\\[y = x^3 + 2x^2 - 3x + 4\\]first derivative function \\(\\frac{\\partial y}{\\partial x} = 3x^2 + 4x - 3\\).second derivative \\(\\frac{\\partial^2 y}{\\partial x^2} = 6x + 4\\)function looks like :want use Newton-Raphson find following two points (optima):Given function,\\[x_{n+1} = x_n - \\frac{3x_n^2 + 4x_n - 3}{6x_n + 4}\\]Let’s start initial guess \\(x_0 = 10\\):gives us first point: \\(x \\approx 0.535\\). plug second derivative, see value local minimum second derivative evaluated point positive (\\(7.211\\)). Moreover, find first derivative evaluated point (\\(4.441\\times 10^{-16}\\)) (close ) zero.However function also second critical value: local maximum. Let’s try find second optimum plugging negative value (\\(x_0 = -10\\)) Newton-Raphson function:, can plug point second derivative confirm local maximum (\\(-15.706\\)).key point Newton-Raphson guaranteed find global minimum/maximum, even guaranteed find specific critical point. use procedure, initialize algorithm several different \\(x_0\\) verify converge differing answers.","code":""},{"path":"critical-points.html","id":"grid-search","chapter":"Day 3 Critical points and approximation","heading":"3.6.2 Grid search","text":"grid search exhaustive search algorithm maximizing/minimizing function. works defining specified set \\(x_i\\), calculating \\(f(x_i)\\), comparing resulting values determine largest/smallest.Consider function\\[y = -x^2\\]can evaluate function \\(x \\\\{ -2, -1.99, -1.98, \\ldots, 1.98, 1.99, 2 \\}\\) determine \\(x_i\\) generates largest value.simple example , method works reasonably well. However also computationally inefficient calculate \\(f(x)\\) reasonable values \\(x\\). reasonable starting point search, search extremely large range values. reason, grid searches – simple – also tend disfavored.","code":""},{"path":"critical-points.html","id":"gradient-descent","chapter":"Day 3 Critical points and approximation","heading":"3.6.3 Gradient descent","text":"Let’s start function9\\[f(x) = 1.2 x^2 - 4.8 x + 8\\]solve analytically:\\[\n\\begin{aligned}\nf'(x) &= 2.4x - 4.8 \\\\\n0 &= 2.4x - 4.8 \\\\\n4.8 &= 2.4x \\\\\nx &= 2\n\\end{aligned}\n\\]According second derivative test,\\[\n\\begin{aligned}\nf''(x) &= 2.4 \\\\\nf''(2) &= 2.4\n\\end{aligned}\n\\]\\(f''(2) > 0\\), \\(x=2\\) minimum. easy function simple. function complicated, can use technique called gradient descent.\\[\nx_1 = x_0 - \\alpha f'(x_0)\n\\]\\(f'(x)\\) - first derivative (gradient) function\\(\\alpha\\) - learning rate (set manually)Like Newton’s method, gradient descent iterative algorithm. repeat process algorithm converges stable solution. Unlike Newton’s method, rate learning controlled first derivative learning rate, second derivative \\(f''(x)\\).Imagine gradient descent ’re top mountain want get bottom, choose two things. First direction wish descend second size steps wish take. choosing things, keep taking step size direction reach bottom.\\(\\alpha\\) corresponds size steps wish take \\(f'(x)\\) gives direction take given formula. Note order formula start calculating assign initial value \\(x\\).Compare function using gradient descent learning rate \\(\\alpha = 0.1\\):algorithm converges slowly smaller learning rate. many contexts, use different learning rate. learning rate high can cause algorithm converge quickly suboptimal solution (local minima opposed global minima), whereas learning rate small can cause process get stuck. Consider example higher learning rate:","code":""},{"path":"linear-algebra.html","id":"linear-algebra","chapter":"Day 4 Linear algebra","heading":"Day 4 Linear algebra","text":"","code":""},{"path":"linear-algebra.html","id":"learning-objectives-3","chapter":"Day 4 Linear algebra","heading":"Learning objectives","text":"Define vector matrixVisualize vectors multiple dimensionsDemonstrate applicability linear algebra text analysis cosine similarityPerform basic algebraic operations vectors matriciesGeneralize linear algebra tensors neural networksDefine matrix inversionDemonstrate solve systems linear equations using matrix inversionDefine determinant matrixDefine matrix decompositionExplain singular value decomposition demonstrate applicability matrix algebra real-world problems","code":""},{"path":"linear-algebra.html","id":"supplemental-readings-3","chapter":"Day 4 Linear algebra","heading":"Supplemental readings","text":"Chapters 11-12, 13.1-.3, Pemberton Rau (2011)OpenStax Calculus: Volume 3, ch 2OpenStax College Algebra, ch 7.5-.8","code":""},{"path":"linear-algebra.html","id":"linear-algebra-1","chapter":"Day 4 Linear algebra","heading":"4.1 Linear algebra","text":"matrix rectangular array numbers arranged rows columns.\nFigure 4.1: matrix.\nMany common statistical methods social sciences rely data structured matricies (e.g. ordinary least squares regression). computational social science expands data sources explode complexity scope, big data needs stored processed many higher dimensional spaces.Linear algebra algebra matricies. allows us examine geometry high dimensional space, expand calculus functions multiple variables. important regression/machine learning/deep learning methods encounter coursework research.","code":""},{"path":"linear-algebra.html","id":"points-and-vectors","chapter":"Day 4 Linear algebra","heading":"4.2 Points and vectors","text":"","code":""},{"path":"linear-algebra.html","id":"points","chapter":"Day 4 Linear algebra","heading":"4.2.1 Points","text":"point exists single dimension (\\(\\Re^1\\))\n\\(1\\)\n\\(\\pi\\)\n\\(e\\)\n\\(1\\)\\(\\pi\\)\\(e\\)ordered pair exists two dimensions (\\(\\Re^2 = \\Re \\times \\Re\\))\n\\((1,2)\\)\n\\((0,0)\\)\n\\((\\pi, e)\\)\n\\((1,2)\\)\\((0,0)\\)\\((\\pi, e)\\)ordered triple three dimensions (\\(\\Re^3 = \\Re \\times \\Re \\times \\Re\\))\n\\((3.1, 4.5, 6.1132)\\)\n\\((3.1, 4.5, 6.1132)\\)ordered \\(n\\)-tuple \\(n\\)-dimensions \\(R^n = \\Re \\times \\Re \\times \\ldots \\times \\Re\\)\n\\((a_{1}, a_{2}, \\ldots, a_{n})\\)\n\\((a_{1}, a_{2}, \\ldots, a_{n})\\)","code":""},{"path":"linear-algebra.html","id":"vectors","chapter":"Day 4 Linear algebra","heading":"4.2.2 Vectors","text":"point \\(\\mathbf{x} \\\\Re^{n}\\) ordered n-tuple, \\((x_{1}, x_{2}, \\ldots, x_{n})\\). vector \\(\\mathbf{x} \\\\Re^{n}\\) arrow pointing origin \\((0, 0, \\ldots, 0)\\) \\(\\mathbf{x}\\).","code":""},{"path":"linear-algebra.html","id":"one-dimensional-example","chapter":"Day 4 Linear algebra","heading":"4.2.3 One dimensional example","text":"","code":""},{"path":"linear-algebra.html","id":"two-dimensional-example","chapter":"Day 4 Linear algebra","heading":"4.2.4 Two dimensional example","text":"","code":""},{"path":"linear-algebra.html","id":"three-dimensional-example","chapter":"Day 4 Linear algebra","heading":"4.2.5 Three dimensional example","text":"(Latitude, Longitude, Elevation)\\((1,2,3)\\)\\((0,1,2)\\)","code":""},{"path":"linear-algebra.html","id":"n-dimensional-example","chapter":"Day 4 Linear algebra","heading":"4.2.6 \\(N\\)-dimensional example","text":"Individual campaign donation records\n\\[\\mathbf{x} = (1000, 0, 10, 50, 15, 4, 0, 0, 0, \\ldots, 2400000000)\\]U.S. counties’ proportion vote Donald Trump\n\\[\\mathbf{y} = (0.8, 0.5, 0.6, \\ldots, 0.2)\\]Run experiment, assess feeling thermometer elected official\n\\[\\mathbf{t} = (0, 100, 50, 70, 80, \\ldots, 100)\\]","code":""},{"path":"linear-algebra.html","id":"examples-of-some-basic-arithmetic","chapter":"Day 4 Linear algebra","heading":"4.2.7 Examples of some basic arithmetic","text":"","code":""},{"path":"linear-algebra.html","id":"vectorscalar-additionmultiplication","chapter":"Day 4 Linear algebra","heading":"4.2.7.1 Vector/scalar addition/multiplication","text":"Suppose:\\[\n\\begin{aligned}\n\\mathbf{u} & =  (1, 2, 3, 4, 5)  \\\\\n\\mathbf{v} & =  (1, 1, 1, 1, 1)  \\\\\nk & =  2\n\\end{aligned}\n\\],\\[\n\\begin{aligned}\n\\mathbf{u}  + \\mathbf{v} & = (1 + 1, 2 + 1, 3+ 1, 4 + 1, 5+ 1)  = (2, 3, 4, 5, 6) \\\\\nk \\mathbf{u} & = (2 \\times 1, 2 \\times 2, 2 \\times 3, 2 \\times 4, 2 \\times 5) = (2, 4, 6, 8, 10)  \\\\\nk \\mathbf{v} & = (2 \\times 1,2 \\times 1,2 \\times 1,2 \\times 1,2 \\times 1) = (2, 2, 2, 2, 2)\n\\end{aligned}\n\\]","code":""},{"path":"linear-algebra.html","id":"linear-dependence","chapter":"Day 4 Linear algebra","heading":"4.2.8 Linear dependence","text":"Expressions \\(\\mathbf{} + \\mathbf{b}\\) \\(2\\mathbf{} - 3\\mathbf{b}\\) examples linear combinations vectors \\(\\mathbf{}\\) \\(\\mathbf{b}\\). Generally, linear combination two vectors \\(\\mathbf{}\\) \\(\\mathbf{b}\\) vector form \\(\\alpha \\mathbf{} + \\beta\\mathbf{b}\\), \\(\\alpha, \\beta\\) scalars. form extends linear combinations two vectors\\[\\alpha \\mathbf{} + \\beta\\mathbf{b} + \\gamma\\mathbf{c} + \\delta\\mathbf{d} + \\ldots\\]Suppose set \\(k\\) \\(n\\)-vectors \\(\\mathbf{b}^1, \\mathbf{b}^2, \\ldots \\mathbf{b}^k\\). say vectors \\(\\mathbf{b}^1, \\mathbf{b}^2, \\ldots \\mathbf{b}^k\\) linearly independent linearly dependent; thus, \\(\\mathbf{b}^1, \\mathbf{b}^2, \\ldots \\mathbf{b}^k\\) linearly independent none vectors can expressed linear combination others.","code":""},{"path":"linear-algebra.html","id":"example-of-linear-dependence","chapter":"Day 4 Linear algebra","heading":"4.2.8.1 Example of linear dependence","text":"\\[\\mathbf{} = \\begin{bmatrix}\n3 \\\\\n1\n\\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix}\n2 \\\\\n2\n\\end{bmatrix}, \\quad \\mathbf{c} = \\begin{bmatrix}\n1 \\\\\n3\n\\end{bmatrix}\\]obvious \\(\\mathbf{b} = \\frac{1}{2} (\\mathbf{} + \\mathbf{c})\\). Therefore vectors \\(\\mathbf{}, \\mathbf{b}, \\mathbf{c}\\) linearly dependent.","code":""},{"path":"linear-algebra.html","id":"detecting-linear-dependence","chapter":"Day 4 Linear algebra","heading":"4.2.8.2 Detecting linear dependence","text":"simple criterion linear dependence following: \\(\\mathbf{b}^1, \\mathbf{b}^2, \\ldots \\mathbf{b}^k\\) linearly dependent exist scalars \\(\\alpha_1, \\alpha_2, \\ldots, \\alpha_k\\) zero \\[\\alpha_1 \\mathbf{b}^1 + \\alpha_2 \\mathbf{b}^2 + \\ldots + \\alpha_k \\mathbf{b}^k = \\mathbf{0}\\]","code":""},{"path":"linear-algebra.html","id":"example-1","chapter":"Day 4 Linear algebra","heading":"4.2.8.2.1 Example 1","text":"Consider vectors\\[\\mathbf{} = \\begin{bmatrix}\n2 \\\\\n1 \\\\\n2\n\\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix}\n4 \\\\\n1 \\\\\n3\n\\end{bmatrix}, \\quad \\mathbf{c} = \\begin{bmatrix}\n1 \\\\\n1 \\\\\n2\n\\end{bmatrix}\\]Suppose scalars \\(\\alpha, \\beta, \\gamma\\) \\[\\alpha\\mathbf{} + \\beta\\mathbf{b} + \\gamma\\mathbf{c} = \\mathbf{0}\\]can express system equations:\\[\n\\begin{aligned}\n2\\alpha &+ 4\\beta &+ \\gamma &= 0 \\\\\n\\alpha &+ \\beta &+ \\gamma &= 0 \\\\\n2\\alpha &+ 3\\beta &+ 2\\gamma &= 0\n\\end{aligned}\n\\]can solve system equations reducing one equation just single term remaining. instance, eliminate \\(\\alpha\\) second third equations subtracting equations suitable multiples first equation:\\[\n\\begin{aligned}\n2\\alpha &+ 4\\beta &+ \\gamma &= 0 \\\\\n &- \\beta &+ \\frac{1}{2}\\gamma &= 0 \\\\\n &- \\beta &+ \\gamma &= 0\n\\end{aligned}\n\\]Next, eliminate \\(\\beta\\) third equation subtracting suitable multiple second:\\[\n\\begin{aligned}\n2\\alpha &+ 4\\beta &+ \\gamma &= 0 \\\\\n &- \\beta &+ \\frac{1}{2}\\gamma &= 0 \\\\\n & &+ \\frac{1}{2} \\gamma &= 0\n\\end{aligned}\n\\]can now use back-substitution solve \\(\\gamma\\), plug value solve \\(\\beta\\) \\(\\alpha\\). Since output equation \\(0\\), reduces \\(\\gamma = \\beta = \\alpha = 0\\), vectors \\(\\mathbf{}, \\mathbf{b}, \\mathbf{c}\\) linearly independent.","code":""},{"path":"linear-algebra.html","id":"example-2","chapter":"Day 4 Linear algebra","heading":"4.2.8.2.2 Example 2","text":"Consider vectors\\[\\mathbf{} = \\begin{bmatrix}\n2 \\\\\n1 \\\\\n2\n\\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix}\n4 \\\\\n1 \\\\\n3\n\\end{bmatrix}, \\quad \\mathbf{c} = \\begin{bmatrix}\n2 \\\\\n2 \\\\\n3\n\\end{bmatrix}\\]Suppose scalars \\(\\alpha, \\beta, \\gamma\\) \\[\\alpha\\mathbf{} + \\beta\\mathbf{b} + \\gamma\\mathbf{c} = \\mathbf{0}\\]Express system equations:\\[\n\\begin{aligned}\n2\\alpha &+ 4\\beta &+ 2\\gamma &= 0 \\\\\n\\alpha &+ \\beta &+ 2\\gamma &= 0 \\\\\n2\\alpha &+ 3\\beta &+ 3\\gamma &= 0\n\\end{aligned}\n\\]Let’s try eliminate \\(\\alpha\\) last two equations subtracting multiples first equation:\\[\n\\begin{aligned}\n2\\alpha &+ 4\\beta &+ 2\\gamma &= 0 \\\\\n &- \\beta &+ \\gamma &= 0 \\\\\n &- \\beta &+ \\gamma &= 0\n\\end{aligned}\n\\]can cancel third equation \\(0 = 0\\), leaving us \\[\n\\begin{aligned}\n2\\alpha &+ 4\\beta &+ 2\\gamma &= 0 \\\\\n &- \\beta &+ \\gamma &= 0\n\\end{aligned}\n\\]find complete solution assigning arbitrary value \\(\\gamma\\) finding \\(\\beta\\) \\(\\alpha\\). However, simply putting \\(\\gamma = 1\\) leads \\(\\beta = 1\\) \\(\\alpha = -3\\). thus found non-zero values \\(\\alpha, \\beta, \\gamma\\), zero, \\(\\alpha\\mathbf{} + \\beta\\mathbf{b} + \\gamma\\mathbf{c} = \\mathbf{0}\\). Therefore vectors \\(\\mathbf{}, \\mathbf{b}, \\mathbf{c}\\) linearly dependent.","code":""},{"path":"linear-algebra.html","id":"inner-product","chapter":"Day 4 Linear algebra","heading":"4.2.9 Inner product","text":"inner product, also known dot product, two vectors results scalar (single value). Suppose \\(\\mathbf{u} \\\\Re^{n}\\) \\(\\mathbf{v} \\\\Re^{n}\\). inner product \\(\\mathbf{u} \\cdot \\mathbf{v}\\) sum item--item products:\\[\n\\begin{aligned}\n\\mathbf{u} \\cdot \\mathbf{v} &=  u_{1} v_{1} + u_{2}v_{2} + \\ldots + u_{n} v_{n}  \\\\\n                                                        & =  \\sum_{=1}^{N} u_{} v_{} \n\\end{aligned}\n\\]Suppose \\(\\mathbf{u} = (1, 2, 3)\\) \\(\\mathbf{v} = (2, 3, 1)\\). :\\[\n\\begin{aligned}\n\\mathbf{u} \\cdot \\mathbf{v} & =  1 \\times 2 +  2 \\times 3 +  3 \\times 1 \\\\\n                & = 2+ 6 + 3 \\\\\n                & = 11              \n\\end{aligned}\n\\]","code":""},{"path":"linear-algebra.html","id":"calculating-vector-length","chapter":"Day 4 Linear algebra","heading":"4.2.10 Calculating vector length","text":"standard measurement length vector known vector norm. illustrate, consider vector \\(\\Re^2\\):Pythagorean Theorem states right triangle sides length \\(\\) \\(b\\), length hypotenuse \\(c = \\sqrt{^2 + b^2}\\). \\(c\\) vector \\(\\Re^2\\), can directly apply Pythagorean Theorem calculate length origin \\((0,0)\\). guaranteed right triangle \\(\\) \\(b\\) simply correspond first second scalar values contained \\(c\\) (.e. \\((3, 4)\\)).generalizes \\(\\Re^n\\) given :\\[\n\\begin{aligned}\n\\| \\mathbf{v}\\| & = (\\mathbf{v} \\cdot \\mathbf{v} )^{1/2} \\\\\n                           & = (v_{1}^2 + v_{2}^{2} + v_{3}^{2} + \\ldots + v_{n}^{2} )^{1/2}\n\\end{aligned}\n\\]calculate vector norm three-dimensional vector \\(\\mathbf{x} = (1,1,1)\\):\\[\n\\begin{aligned}\n\\| \\mathbf{x}\\| & = (\\mathbf{x} \\cdot \\mathbf{x} )^{1/2} \\\\\n                           & = (x_{1}^2 + x_{2}^{2} + x_{3}^{2})^{1/2} \\\\\n                           & = (1 + 1 + 1)^{1/2} \\\\\n                           &= \\sqrt{3}\n\\end{aligned}\n\\]","code":""},{"path":"linear-algebra.html","id":"example-text-analysis","chapter":"Day 4 Linear algebra","heading":"4.3 Example: text analysis","text":"Text analysis common research method social sciences. order computationally analyze documents, need store meaningful format. mean format human-readable, simply interpretable computer. simplistic form, documents represented vectors, value counts frequency word appears given document. throw away information word order, can represent information mathematical fashion using vector.Mathematically, represented vector \\((43,0,0,0,0,10,\\dots)\\)Consider two hypothetical documents:\\[\n\\begin{aligned}\n\\text{Doc1} & = (1, 1, 3, \\ldots, 5) \\\\\n\\text{Doc2} & = (2, 0, 0, \\ldots, 1) \\\\\n\\textbf{Doc1}, \\textbf{Doc2} & \\\\Re^{M}\n\\end{aligned}\n\\]vector length \\(m\\). Linear algebra provides many potentially useful operations. example, inner product documents :\\[\n\\begin{aligned}\n\\textbf{Doc1} \\cdot \\textbf{Doc2}  &  =  (1, 1, 3, \\ldots, 5) (2, 0, 0, \\ldots, 1)'  \\\\\n & =  1 \\times 2 + 1 \\times 0 + 3 \\times 0 + \\ldots + 5 \\times 1 \\\\\n& = 7 \n\\end{aligned}\n\\]length document :\\[\n\\begin{aligned}\n\\| \\textbf{Doc1} \\| & \\equiv  \\sqrt{ \\textbf{Doc1} \\cdot \\textbf{Doc1} } \\\\\n & =  \\sqrt{(1, 1, 3, \\ldots , 5) (1, 1, 3, \\ldots, 5)' } \\\\\n  & =  \\sqrt{1^{2} +1^{2} + 3^{2} + 5^{2} } \\\\\n   & =   6\n\\end{aligned}\n\\]cosine angle documents:\\[\n\\begin{aligned}\n\\cos (\\theta) & \\equiv  \\left(\\frac{\\textbf{Doc1} \\cdot \\textbf{Doc2}}{\\| \\textbf{Doc1}\\| \\|\\textbf{Doc2} \\|} \\right) \\\\\n & = \\frac{7} { 6 \\times  2.24} \\\\\n  & = 0.52\n\\end{aligned}\n\\]purpose property? value tell us? tells us similarity vector space documents.Measuring similarity documents useful. methods used :Assessing potential plagiarismComparing similarity legislative textsWhat properties similarity measure ?maximum document itselfThe minimum documents words common (.e. orthogonal)Increasing words usedNormalized document length","code":" a abandoned abc ability able about above abroad absorbed absorbing abstract\n43         0   0       0    0    10     0      0        0         0        1"},{"path":"linear-algebra.html","id":"measure-1-inner-product","chapter":"Day 4 Linear algebra","heading":"4.3.1 Measure 1: inner product","text":"Consider used inner product measure similarity.\\[(2,1) \\cdot (1,4) = 6\\]problem inner product length dependent. Consider method calculated vector purple:\\[(4,2) \\cdot (1,4) = 12\\]get different measures similarity, yet new document really different? want something consistent accounts varying overall document length.","code":""},{"path":"linear-algebra.html","id":"measure-2-cosine-similarity","chapter":"Day 4 Linear algebra","heading":"4.3.2 Measure 2: cosine similarity","text":"\\[\n\\begin{aligned}\n(4,2) \\cdot (1,4) &= 12 \\\\\n\\mathbf{} \\cdot \\mathbf{b} &= \\|\\mathbf{} \\| \\times \\|\\mathbf{b} \\| \\times \\cos(\\theta) \\\\\n\\frac{\\mathbf{} \\cdot \\mathbf{b}}{\\|\\mathbf{} \\| \\times \\|\\mathbf{b} \\|}  &= \\cos(\\theta)\n\\end{aligned}\n\\]\\(\\cos(\\theta)\\) removes document length similarity measure.\\[\n\\begin{aligned}\n\\cos (\\theta) & \\equiv  \\left(\\frac{\\textbf{Doc1} \\cdot \\textbf{Doc2}}{\\| \\textbf{Doc1}\\| \\|\\textbf{Doc2} \\|} \\right) \\\\\n & = \\frac{(2, 1) \\cdot (1, 4)} {\\| (2,1)\\| \\| (1,4) \\|} \\\\\n & = \\frac{6} {(\\sqrt{2^2 + 1^2}) (\\sqrt{1^2 + 4^2})} \\\\\n & = \\frac{6} {(\\sqrt{5}) (\\sqrt{17})} \\\\\n  & \\approx 0.65\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\n\\cos (\\theta) & \\equiv  \\left(\\frac{\\textbf{Doc3} \\cdot \\textbf{Doc2}}{\\| \\textbf{Doc3}\\| \\|\\textbf{Doc2} \\|} \\right) \\\\\n & = \\frac{(4,2) \\cdot (1, 4)} {\\| (24,2)\\| \\| (1,4) \\|} \\\\\n & = \\frac{12} {(\\sqrt{4^2 + 2^2}) (\\sqrt{1^2 + 4^2})} \\\\\n & = \\frac{12} {(\\sqrt{20}) (\\sqrt{17})} \\\\\n  & \\approx 0.65\n\\end{aligned}\n\\]measure works documents length unique number words.","code":""},{"path":"linear-algebra.html","id":"matricies","chapter":"Day 4 Linear algebra","heading":"4.4 Matricies","text":"matrix rectangular arrangement (array) numbers defined two axes (colloquially known dimensions):RowsColumns\\[\n\\mathbf{} =\n\\begin{array}{rrrr}\na_{11} & a_{12} & \\ldots & a_{1n} \\\\\na_{21} & a_{22} & \\ldots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\ldots & a_{mn} \\\\\n\\end{array} \n\\]\\(\\mathbf{}\\) \\(m\\) rows \\(n\\) columns say \\(\\mathbf{}\\) \\(m \\times n\\) matrix. Suppose \\(\\mathbf{X}\\) \\(\\mathbf{Y}\\) \\(m \\times n\\) matrices. \\(\\mathbf{X} = \\mathbf{Y}\\) \\(x_{ij} = y_{ij}\\) \\(\\) \\(j\\).","code":""},{"path":"linear-algebra.html","id":"simple-examples","chapter":"Day 4 Linear algebra","heading":"4.4.0.1 Simple examples","text":"\\[\n\\mathbf{X} = \\left[ \\begin{array}{rrr}\n1 & 2 & 3 \\\\\n2 & 1 & 4 \\\\\n\\end{array} \\right]\n\\]\\(\\mathbf{X}\\) \\(2 \\times 3\\) matrix.\\[\n\\mathbf{Y} = \\left[ \\begin{array}{rr}\n1 & 2 \\\\\n3 & 2 \\\\\n1 & 4 \\\\\n\\end{array} \\right]\n\\]\\(\\mathbf{Y}\\) \\(3 \\times 2\\) matrix. \\(\\mathbf{X} \\neq \\mathbf{Y}\\) dimensions different.","code":""},{"path":"linear-algebra.html","id":"basic-arithmetic","chapter":"Day 4 Linear algebra","heading":"4.4.1 Basic arithmetic","text":"","code":""},{"path":"linear-algebra.html","id":"addition","chapter":"Day 4 Linear algebra","heading":"4.4.1.1 Addition","text":"","code":""},{"path":"linear-algebra.html","id":"matrix","chapter":"Day 4 Linear algebra","heading":"4.4.1.1.1 Matrix","text":"Suppose \\(\\mathbf{X}\\) \\(\\mathbf{Y}\\) \\(m \\times n\\) matrices. :\\[\n\\begin{aligned} \n\\mathbf{X} + \\mathbf{Y} & =  \\begin{bmatrix} \nx_{11} & x_{12} & \\ldots & x_{1n} \\\\\nx_{21} & x_{22} & \\ldots & x_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{m1} & x_{m2} & \\ldots & x_{mn} \\\\\n\\end{bmatrix} + \n\\begin{bmatrix} \ny_{11} & y_{12} & \\ldots & y_{1n} \\\\\ny_{21} & y_{22} & \\ldots & y_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ny_{m1} & y_{m2} & \\ldots & y_{mn} \\\\\n\\end{bmatrix}\n \\\\\n& = \\begin{bmatrix} \nx_{11} + y_{11} & x_{12} + y_{12} & \\ldots & x_{1n} + y_{1n} \\\\\nx_{21} + y_{21} & x_{22} + y_{22} & \\ldots & x_{2n} + y_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{m1} + y_{m1} & x_{m2} + y_{m2} & \\ldots & x_{mn} + y_{mn} \\\\\n\\end{bmatrix} \n\\end{aligned}\n\\]","code":""},{"path":"linear-algebra.html","id":"scalar","chapter":"Day 4 Linear algebra","heading":"4.4.1.1.2 Scalar","text":"Suppose \\(\\mathbf{X}\\) \\(m \\times n\\) matrix \\(k \\\\Re\\). :\\[\n\\begin{aligned}\nk \\mathbf{X} & =  \\begin{bmatrix} \nk x_{11} & k x_{12} & \\ldots &  k x_{1n} \\\\\nk x_{21} & k x_{22} & \\ldots & k x_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nk x_{m1} & k x_{m2} & \\ldots & k x_{mn} \\\\\n\\end{bmatrix}\n\\end{aligned}\n\\]","code":""},{"path":"linear-algebra.html","id":"transposition","chapter":"Day 4 Linear algebra","heading":"4.4.2 Transposition","text":"Matricies must conformable order perform certain operations. example, matrix addition requires matricies possess number rows \\(m\\) columns \\(n\\) order add element together. second matrix instead \\(n\\) rows \\(m\\) columns, can transpose flip dimensionality matrix.\\[\n\\begin{aligned}\n\\mathbf{X} & = \\begin{bmatrix}\nx_{11} & x_{12} & \\ldots & x_{1n} \\\\\nx_{21} & x_{22} & \\ldots & x_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{m1} & x_{m2} & \\ldots & x_{mn} \\\\\n\\end{bmatrix} \\\\\n\\mathbf{X}' & =  \\begin{bmatrix} \nx_{11} & x_{21} & \\ldots & x_{m1} \\\\\nx_{12} & x_{22} & \\ldots & x_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{1n} & x_{2n} & \\ldots & x_{mn}\n\\end{bmatrix}\n\\end{aligned}\n\\]\\(\\mathbf{X}\\) \\(m \\times n\\) \\(\\mathbf{X}'\\) \\(n \\times m\\)\\(\\mathbf{X} = \\mathbf{X}'\\) say \\(\\mathbf{X}\\) symmetric\nsquare matricies can symmetric\nsquare matricies can symmetric","code":""},{"path":"linear-algebra.html","id":"multiplication","chapter":"Day 4 Linear algebra","heading":"4.4.3 Multiplication","text":"Matrix multiplication extraordinarily useful many computational problems, though somewhat tedious calculate hand. Suppose two matrices:\\[\\mathbf{X} = \\begin{bmatrix} 1 & 1 \\\\ 1& 1 \\\\ \\end{bmatrix} , \\quad \\mathbf{Y} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ \\end{bmatrix} \\]create new matrix \\(\\mathbf{}\\) matrix multiplication:\\[\n\\begin{aligned} \n\\mathbf{} & = \\mathbf{X} \\mathbf{Y} \\\\\n& = \\begin{bmatrix}\n1 & 1 \\\\ \n1 & 1 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 \\times 1 + 1 \\times 3 & 1 \\times 2 + 1 \\times 4 \\\\\n1 \\times 1 + 1 \\times 3 & 1 \\times 2 + 1 \\times 4\\\\\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n4 & 6 \\\\\n4 & 6\n\\end{bmatrix}\n\\end{aligned}\n\\]","code":""},{"path":"linear-algebra.html","id":"algebraic-properties","chapter":"Day 4 Linear algebra","heading":"4.4.3.1 Algebraic properties","text":"Matricies must conformable - , number columns \\(\\mathbf{}\\) must equal number rows \\(\\mathbf{B}\\). \\(\\mathbf{AB}\\) exists call matricies conformable.Associative property: \\((\\mathbf{XY})\\mathbf{Z} = \\mathbf{X}(\\mathbf{YZ})\\)Additive distributive property: \\((\\mathbf{X} + \\mathbf{Y})\\mathbf{Z} = \\mathbf{XZ} + \\mathbf{YZ}\\)Zero property: \\(\\mathbf{X0} = 0\\)Order matters: \\(\\mathbf{XY} \\neq \\mathbf{YX}\\)\nDifferent scalar multiplication: \\(xy = yx\\)\nDifferent scalar multiplication: \\(xy = yx\\)","code":""},{"path":"linear-algebra.html","id":"example-neural-networks","chapter":"Day 4 Linear algebra","heading":"4.5 Example: neural networks","text":"care vectors/matricies arithmetic operations? use computational social science? ’m glad asked! Vectors/matricies linear algebra form heart neural network.\nFigure 4.2: Source: Wikipedia\nDeep learning name use stacked neural networks; , networks composed several layers. layers made nodes. node just place computation happens, loosely patterned neuron human brain, fires encounters sufficient stimuli.10A node combines input data set coefficients, weights, either amplify dampen input, thereby assigning significance inputs regard task algorithm trying learn; e.g. input helpful classifying data without error? input-weight products summed sum passed node’s -called activation function, determine whether extent signal progress network affect ultimate outcome, say, act classification. signals passes , neuron “activated.”","code":""},{"path":"linear-algebra.html","id":"how-are-neural-networks-used","chapter":"Day 4 Linear algebra","heading":"4.5.1 How are neural networks used","text":"Self-driving carsVoice activated assistantsAutomatic machine translationImage recognitionDetection diseases","code":""},{"path":"linear-algebra.html","id":"how-are-neural-networks-related-to-linear-algebra","chapter":"Day 4 Linear algebra","heading":"4.5.2 How are neural networks related to linear algebra?","text":"tensor core unit data deep learning. generalization vectors matricies higher-dimensions.Scalars (0D tensors) - tensor containing single numberVectors (1D tensors) - tensor one-dimensional array numbersMatricies (2D tensors) - tensor two-dimensional array numbers3D tensors higher-dimensional tensors - array matricies","code":""},{"path":"linear-algebra.html","id":"tensor-operations","chapter":"Day 4 Linear algebra","heading":"4.5.2.1 Tensor operations","text":"transformations inside deep neural network reduced handful tensor operations, generalizations matrix operations (e.g. addition, multiplication). key point can matrix, can tensor.","code":""},{"path":"linear-algebra.html","id":"geometric-interpretation","chapter":"Day 4 Linear algebra","heading":"4.5.2.2 Geometric interpretation","text":"Deep learning chain small tensor operations geometric transformations input dataWe interpret complex geometric transformation high-dimensional space, implemented via long series simple stepsConsider two pieces paper - one red one blueStick together crumple small ball\ncrumpled paper ball input data\nsheet paper class data\nwant classify points paper red blue\ncrumpled paper ball input dataEach sheet paper class dataWe want classify points paper red blueDeep learning series steps necessary uncrumple ball two classes cleanly separable ","code":""},{"path":"linear-algebra.html","id":"here-is-the-linear-algebra","chapter":"Day 4 Linear algebra","heading":"4.5.2.3 Here is the linear algebra","text":"Mathematically, layer network example transforms input data follows:\\[\\mathbf{Y} = \\text{activation}(\\mathbf{W} \\cdot \\mathbf{X} + \\mathbf{B})\\]\\(\\mathbf{X}\\) - input tensor\\(\\mathbf{Y}\\) - output tensor\\(\\mathbf{W}, \\mathbf{B}\\) - weight/parameter tensors (attributes layer, determined optimization process)\\(\\text{activation}()\\)\nModifies input non-linear fashion allow non-linear relationships features/independent variables data\nKey traits\nNon-linear\nContinuously differentiable\nFixed range\n\nCommon activation functions\nRectified Linear Units (RELU)\n\\[R(z) = \\max(0, z)\\]\nSigmoid function (aka logistic regression)\n\\[S(z) = \\frac{1}{1 + e^{-z}}\\]\n\nModifies input non-linear fashion allow non-linear relationships features/independent variables dataKey traits\nNon-linear\nContinuously differentiable\nFixed range\nNon-linearContinuously differentiableFixed rangeCommon activation functions\nRectified Linear Units (RELU)\n\\[R(z) = \\max(0, z)\\]\nSigmoid function (aka logistic regression)\n\\[S(z) = \\frac{1}{1 + e^{-z}}\\]\nRectified Linear Units (RELU)\n\\[R(z) = \\max(0, z)\\]Rectified Linear Units (RELU)\\[R(z) = \\max(0, z)\\]Sigmoid function (aka logistic regression)\n\\[S(z) = \\frac{1}{1 + e^{-z}}\\]Sigmoid function (aka logistic regression)\\[S(z) = \\frac{1}{1 + e^{-z}}\\]determine values weights parameters? Come back next week discuss optimization.layer adjusts tensor create new, ideally clear, structures underlying data. basic neural network may contain thousands, millions, tensor operations. complete hand, important understand underlying principles seek implement neural networks.","code":""},{"path":"linear-algebra.html","id":"matrix-inversion","chapter":"Day 4 Linear algebra","heading":"4.6 Matrix inversion","text":"Suppose \\(\\mathbf{X}\\) \\(n \\times n\\) matrix. want find matrix \\(\\mathbf{X}^{-1}\\) \\[\n\\mathbf{X}^{-1} \\mathbf{X} = \\mathbf{X} \\mathbf{X}^{-1} = \\mathbf{}\n\\]\\(\\mathbf{}\\) \\(n \\times n\\) identity matrix.useful? Matrix inversion necessary :Solve systems equationsPerform linear regressionProvide intuition colinearity, fixed effects, relevant design matricies social scientists.","code":""},{"path":"linear-algebra.html","id":"calculating-matrix-inversions","chapter":"Day 4 Linear algebra","heading":"4.6.1 Calculating matrix inversions","text":"Consider following equations:\\[\n\\begin{aligned}\nx_{1} + x_{2} + x_{3} &= 0 \\\\\n0x_{1}  +   5x_{2} + 0x_{3}  & = 5 \\\\\n0 x_{1} + 0 x_{2} + 3 x_{3} & =  6 \\\\\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\n\\mathbf{}  &= \\begin{bmatrix} 1 & 1 & 1 \\\\ 0 & 5 & 0 \\\\ 0 & 0 & 3 \\end{bmatrix} \\\\\n\\mathbf{x} &= (x_{1} , x_{2}, x_{3} ) \\\\\n\\mathbf{b} &= (0, 5, 6)\n\\end{aligned}\n\\]system equations now,\\[\\mathbf{}\\mathbf{x} =\\mathbf{b}\\]\\(\\mathbf{}^{-1}\\) exists \\(\\mathbf{}\\mathbf{x} = \\mathbf{b}\\) one solution.","code":""},{"path":"linear-algebra.html","id":"definition-1","chapter":"Day 4 Linear algebra","heading":"4.6.1.1 Definition","text":"Suppose \\(\\mathbf{X}\\) \\(n \\times n\\) matrix. call \\(\\mathbf{X}^{-1}\\) inverse \\(\\mathbf{X}\\) \\[\n\\mathbf{X}^{-1} \\mathbf{X} = \\mathbf{X} \\mathbf{X}^{-1} = \\mathbf{}\n\\]\\(\\mathbf{X}^{-1}\\) exists \\(\\mathbf{X}\\) invertible. \\(\\mathbf{X}^{-1}\\) exist, say \\(\\mathbf{X}\\) singular.Solved via R:","code":"##      [,1] [,2]   [,3]\n## [1,]    1 -0.2 -0.333\n## [2,]    0  0.2  0.000\n## [3,]    0  0.0  0.333"},{"path":"linear-algebra.html","id":"when-do-inverses-exist","chapter":"Day 4 Linear algebra","heading":"4.6.2 When do inverses exist","text":"Inverses exist columns rows linearly independent - , repeated information matrix.","code":""},{"path":"linear-algebra.html","id":"example-1-1","chapter":"Day 4 Linear algebra","heading":"4.6.2.1 Example 1","text":"Consider \\(\\mathbf{v}_{1} = (1, 0, 0)\\), \\(\\mathbf{v}_{2} = (0,1,0)\\), \\(\\mathbf{v}_{3} = (0,0,1)\\)Can write vectors combination vectors? .","code":""},{"path":"linear-algebra.html","id":"example-2-1","chapter":"Day 4 Linear algebra","heading":"4.6.2.2 Example 2","text":"Consider \\(\\mathbf{v}_{1} = (1, 0, 0)\\), \\(\\mathbf{v}_{2} = (0,1,0)\\), \\(\\mathbf{v}_{3} = (0,0,1)\\), \\(\\mathbf{v}_{4} = (1, 2, 3)\\).Can write combination vectors?\\[\\mathbf{v}_{4} = \\mathbf{v}_{1} + 2 \\mathbf{v}_{2} + 3\\mathbf{v}_{3}\\]matrix \\(\\mathbf{V}\\) containing vectors columns linearly independent, therefore noninvertible (also known singular).","code":""},{"path":"linear-algebra.html","id":"inverting-a-2-times-2-matrix","chapter":"Day 4 Linear algebra","heading":"4.6.3 Inverting a \\(2 \\times 2\\) matrix","text":"\\(\\mathbf{} = \\begin{bmatrix} & b \\\\ c & d \\end{bmatrix}\\) \\(ad \\neq bc\\), \\(\\mathbf{}\\) invertible \\[\\mathbf{}^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix}\nd & -b \\\\\n-c & \n\\end{bmatrix}\\]example\\[\n\\begin{aligned}\n\\mathbf{} &= \\begin{bmatrix}\n9 & 7 \\\\\n2 & 1\n\\end{bmatrix} \\\\\n\\mathbf{}^{-1} &= \\frac{1}{(-5)} \\begin{bmatrix}\n1 & -7 \\\\\n-2 & 9\n\\end{bmatrix} = \\begin{bmatrix}\n-0.2 & 1.4 \\\\\n0.4 & -1.8\n\\end{bmatrix}\n\\end{aligned}\n\\]can verify \\[\\mathbf{}^{-1} \\mathbf{} = \\begin{bmatrix}\n9 & 7 \\\\\n2 & 1\n\\end{bmatrix} \\begin{bmatrix}\n-0.2 & 1.4 \\\\\n0.4 & -1.8\n\\end{bmatrix} = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = \\mathbf{}\\]","code":""},{"path":"linear-algebra.html","id":"inverting-an-n-times-n-matrix","chapter":"Day 4 Linear algebra","heading":"4.6.4 Inverting an \\(n \\times n\\) matrix","text":"can use process Gauss-Jordan elimination calculate inverse \\(n \\times n\\) matrix. Gauss-Jordan elimination procedure solve system equations. Using augmented matrix, apply series elementary row operations modify augmented matrix diagonal matrix left-hand side. Elementary row operations include:Exchanging two rows matrixSubtracting multiple one row another rowFirst setup augmented matrix \\([\\mathbf{} \\; \\mathbf{}]\\) reduce form \\([\\mathbf{D} \\; \\mathbf{B}]\\), \\(\\mathbf{D}\\) diagonal matrix diagonal entry equal \\(0\\). \\(\\mathbf{}^{-1}\\) found dividing row \\(\\mathbf{B}\\) corresponding diagonal entry \\(\\mathbf{D}\\).","code":""},{"path":"linear-algebra.html","id":"example-1-2","chapter":"Day 4 Linear algebra","heading":"4.6.4.1 Example 1","text":"example, let us invert\\[\\mathbf{} = \\begin{bmatrix}\n2 & 1 & 2 \\\\\n3 & 1 & 1 \\\\\n3 & 1 & 2\n\\end{bmatrix}\\]First setup augmented matrix:\\[\n\\left[\n\\begin{array}{rrr|rrr}\n2 & 1 & 2 & 1 & 0 & 0 \\\\\n3 & 1 & 1 & 0 & 1 & 0 \\\\\n3 & 1 & 2 & 0 & 0 & 1\n\\end{array}\n\\right]\n\\]Next substract \\(3/2\\) times first row rows get:\\[\n\\left[\n\\begin{array}{rrr|rrr}\n2 & 1 & 2 & 1 & 0 & 0 \\\\\n0 & -1/2 & -2 & -3/2 & 1 & 0 \\\\\n0 & -1/2 & -1 & -3/2 & 0 & 1\n\\end{array}\n\\right]\n\\]next step add twice second row first row, subtract second row third row. obtain:\\[\n\\left[\n\\begin{array}{rrr|rrr}\n2 & 0 & -2 & -2 & 2 & 0 \\\\\n0 & -1/2 & -2 & -3/2 & 1 & 0 \\\\\n0 & 0 & 1 & 0 & -1 & 1\n\\end{array}\n\\right]\n\\]Finally add twice third row first second rows:\\[\n\\left[\n\\begin{array}{rrr|rrr}\n2 & 0 & 0 & -2 & 0 & 2 \\\\\n0 & -1/2 & 0 & -3/2 & -1 & 2 \\\\\n0 & 0 & 1 & 0 & -1 & 1\n\\end{array}\n\\right]\n\\]point \\(\\mathbf{B}\\) diagonal matrix non-zero elements diagonal. obtain \\(\\mathbf{}^{-1}\\) dividing first row \\(\\mathbf{B}\\) \\(2\\), second row \\(-\\frac{1}{2}\\), third row \\(1\\):\\[\\mathbf{}^{-1} = \\begin{bmatrix}\n-1 & 0 & 1 \\\\\n3 & 2 & -4 \\\\\n0 & -1 & 1\n\\end{bmatrix}\\]","code":""},{"path":"linear-algebra.html","id":"example-2-2","chapter":"Day 4 Linear algebra","heading":"4.6.4.2 Example 2","text":"Invert matrix\\[\\mathbf{} = \\begin{bmatrix}\n1 & 3 & 5 \\\\\n1 & 7 & 5 \\\\\n5 & 10 & 15\n\\end{bmatrix}\\]First setup augmented matrix:\n\\[\n \\left[\n \\begin{array}{rrr|rrr}\n 1 & 3 & 5 & 1 & 0 & 0 \\\\\n 1 & 7 & 5 & 0 & 1 & 0 \\\\\n 5 & 10 & 15 & 0 & 0 & 1\n \\end{array}\n \\right]\n \\]First setup augmented matrix:\\[\n \\left[\n \\begin{array}{rrr|rrr}\n 1 & 3 & 5 & 1 & 0 & 0 \\\\\n 1 & 7 & 5 & 0 & 1 & 0 \\\\\n 5 & 10 & 15 & 0 & 0 & 1\n \\end{array}\n \\right]\n \\]Subtract row 1 row 2:\n\\[\n \\left[\n \\begin{array}{rrr|rrr}\n 1 & 3 & 5 & 1 & 0 & 0 \\\\\n 0 & 4 & 0 & -1 & 1 & 0 \\\\\n 5 & 10 & 15 & 0 & 0 & 1\n \\end{array}\n \\right]\n \\]Subtract row 1 row 2:\\[\n \\left[\n \\begin{array}{rrr|rrr}\n 1 & 3 & 5 & 1 & 0 & 0 \\\\\n 0 & 4 & 0 & -1 & 1 & 0 \\\\\n 5 & 10 & 15 & 0 & 0 & 1\n \\end{array}\n \\right]\n \\]Subtract 5 × (row 1) row 3:\n\\[\n \\left[\n \\begin{array}{rrr|rrr}\n 1 & 3 & 5 & 1 & 0 & 0 \\\\\n 0 & 4 & 0 & -1 & 1 & 0 \\\\\n 0 & -5 & -10 & -5 & 0 & 1\n \\end{array}\n \\right]\n \\]Subtract 5 × (row 1) row 3:\\[\n \\left[\n \\begin{array}{rrr|rrr}\n 1 & 3 & 5 & 1 & 0 & 0 \\\\\n 0 & 4 & 0 & -1 & 1 & 0 \\\\\n 0 & -5 & -10 & -5 & 0 & 1\n \\end{array}\n \\right]\n \\]Swap row 2 row 3:\n\\[\n \\left[\n \\begin{array}{rrr|rrr}\n 1 & 3 & 5 & 1 & 0 & 0 \\\\\n 0 & -5 & -10 & -5 & 0 & 1 \\\\\n 0 & 4 & 0 & -1 & 1 & 0 \\\\\n \\end{array}\n \\right]\n \\]Swap row 2 row 3:\\[\n \\left[\n \\begin{array}{rrr|rrr}\n 1 & 3 & 5 & 1 & 0 & 0 \\\\\n 0 & -5 & -10 & -5 & 0 & 1 \\\\\n 0 & 4 & 0 & -1 & 1 & 0 \\\\\n \\end{array}\n \\right]\n \\]Add 4/5 × (row 2) row 3:\n\\[\n \\left[\n \\begin{array}{rrr|rrr}\n 1 & 3 & 5 & 1 & 0 & 0 \\\\\n 0 & -5 & -10 & -5 & 0 & 1 \\\\\n 0 & 0 & -8 & -5 & 1 & \\frac{4}{5} \\\\\n \\end{array}\n \\right]\n \\]Add 4/5 × (row 2) row 3:\\[\n \\left[\n \\begin{array}{rrr|rrr}\n 1 & 3 & 5 & 1 & 0 & 0 \\\\\n 0 & -5 & -10 & -5 & 0 & 1 \\\\\n 0 & 0 & -8 & -5 & 1 & \\frac{4}{5} \\\\\n \\end{array}\n \\right]\n \\]Divide row 3 -8:\n\\[\n \\left[\n \\begin{array}{rrr|rrr}\n 1 & 3 & 5 & 1 & 0 & 0 \\\\\n 0 & -5 & -10 & -5 & 0 & 1 \\\\\n 0 & 0 & 1 & \\frac{5}{8} & -\\frac{1}{8} & -\\frac{1}{10} \\\\\n \\end{array}\n \\right]\n \\]Divide row 3 -8:\\[\n \\left[\n \\begin{array}{rrr|rrr}\n 1 & 3 & 5 & 1 & 0 & 0 \\\\\n 0 & -5 & -10 & -5 & 0 & 1 \\\\\n 0 & 0 & 1 & \\frac{5}{8} & -\\frac{1}{8} & -\\frac{1}{10} \\\\\n \\end{array}\n \\right]\n \\]Add 10 × (row 3) row 2:\n\\[\n \\left[\n \\begin{array}{rrr|rrr}\n 1 & 3 & 5 & 1 & 0 & 0 \\\\\n 0 & -5 & 0 & \\frac{5}{4} & -\\frac{5}{4} & 0 \\\\\n 0 & 0 & 1 & \\frac{5}{8} & -\\frac{1}{8} & -\\frac{1}{10} \\\\\n \\end{array}\n \\right]\n \\]Add 10 × (row 3) row 2:\\[\n \\left[\n \\begin{array}{rrr|rrr}\n 1 & 3 & 5 & 1 & 0 & 0 \\\\\n 0 & -5 & 0 & \\frac{5}{4} & -\\frac{5}{4} & 0 \\\\\n 0 & 0 & 1 & \\frac{5}{8} & -\\frac{1}{8} & -\\frac{1}{10} \\\\\n \\end{array}\n \\right]\n \\]Subtract 5 × (row 3) row 1:\n\\[\n \\left[\n \\begin{array}{rrr|rrr}\n 1 & 3 & 0 & -\\frac{17}{8} & \\frac{5}{8} & \\frac{1}{2} \\\\\n 0 & -5 & 0 & \\frac{5}{4} & -\\frac{5}{4} & 0 \\\\\n 0 & 0 & 1 & \\frac{5}{8} & -\\frac{1}{8} & -\\frac{1}{10} \\\\\n \\end{array}\n \\right]\n \\]Subtract 5 × (row 3) row 1:\\[\n \\left[\n \\begin{array}{rrr|rrr}\n 1 & 3 & 0 & -\\frac{17}{8} & \\frac{5}{8} & \\frac{1}{2} \\\\\n 0 & -5 & 0 & \\frac{5}{4} & -\\frac{5}{4} & 0 \\\\\n 0 & 0 & 1 & \\frac{5}{8} & -\\frac{1}{8} & -\\frac{1}{10} \\\\\n \\end{array}\n \\right]\n \\]Divide row 2 -5:\n\\[\n \\left[\n \\begin{array}{rrr|rrr}\n 1 & 3 & 0 & -\\frac{17}{8} & \\frac{5}{8} & \\frac{1}{2} \\\\\n 0 & 1 & 0 & -\\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n 0 & 0 & 1 & \\frac{5}{8} & -\\frac{1}{8} & -\\frac{1}{10} \\\\\n \\end{array}\n \\right]\n \\]Divide row 2 -5:\\[\n \\left[\n \\begin{array}{rrr|rrr}\n 1 & 3 & 0 & -\\frac{17}{8} & \\frac{5}{8} & \\frac{1}{2} \\\\\n 0 & 1 & 0 & -\\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n 0 & 0 & 1 & \\frac{5}{8} & -\\frac{1}{8} & -\\frac{1}{10} \\\\\n \\end{array}\n \\right]\n \\]Subtract 3 × (row 2) row 1:\n\\[\n \\left[\n \\begin{array}{rrr|rrr}\n 1 & 0 & 0 & -\\frac{11}{8} & -\\frac{1}{8} & \\frac{1}{2} \\\\\n 0 & 1 & 0 & -\\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n 0 & 0 & 1 & \\frac{5}{8} & -\\frac{1}{8} & -\\frac{1}{10} \\\\\n \\end{array}\n \\right]\n \\]Subtract 3 × (row 2) row 1:\\[\n \\left[\n \\begin{array}{rrr|rrr}\n 1 & 0 & 0 & -\\frac{11}{8} & -\\frac{1}{8} & \\frac{1}{2} \\\\\n 0 & 1 & 0 & -\\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n 0 & 0 & 1 & \\frac{5}{8} & -\\frac{1}{8} & -\\frac{1}{10} \\\\\n \\end{array}\n \\right]\n \\]\\[\\mathbf{}^{-1} = \\begin{bmatrix}\n-\\frac{11}{8} & -\\frac{1}{8} & \\frac{1}{2} \\\\\n-\\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n\\frac{5}{8} & -\\frac{1}{8} & -\\frac{1}{10}\n\\end{bmatrix}\\]can simplify factoring appropriate term:\\[\\mathbf{}^{-1} = \\frac{1}{40} \\begin{bmatrix}\n-55 & -5 & 20 \\\\\n-10 & 10 & 0 \\\\\n25 & -5 & -4\n\\end{bmatrix}\\]","code":""},{"path":"linear-algebra.html","id":"application-to-regression-analysis","chapter":"Day 4 Linear algebra","heading":"4.6.5 Application to regression analysis","text":"methods classes learn linear regression. \\(\\) (individual) observe covariates \\(x_{i1}, x_{i2}, \\ldots, x_{ik}\\) dependent variable \\(Y_{}\\). ,\\[\n\\begin{aligned}\nY_{1} & = \\beta_{0} + \\beta_{1} x_{11} + \\beta_{2} x_{12} + \\ldots + \\beta_{k} x_{1k} \\\\\nY_{2} & = \\beta_{0} + \\beta_{1} x_{21} + \\beta_{2} x_{22} + \\ldots + \\beta_{k} x_{2k} \\\\\n\\vdots & \\vdots & \\vdots \\\\\nY_{} & = \\beta_{0} + \\beta_{1} x_{i1} + \\beta_{2} x_{i2} + \\ldots + \\beta_{k} x_{ik} \\\\\n\\vdots & \\vdots & \\vdots \\\\\nY_{n} & = \\beta_{0} + \\beta_{1} x_{n1} + \\beta_{2} x_{n2} + \\ldots + \\beta_{k} x_{nk}\n\\end{aligned}\n\\]\\(\\mathbf{x}_{} = (1, x_{i1}, x_{i2}, \\ldots, x_{ik})\\)\\(\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1}\\\\\\mathbf{x}_{2}\\\\ \\vdots \\\\ \\mathbf{x}_{n} \\end{bmatrix}\\)\\(\\boldsymbol{\\beta} = (\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{k} )\\)\\(\\mathbf{Y} = (Y_{1}, Y_{2}, \\ldots, Y_{n})\\)can write\\[\n\\begin{aligned}\n\\mathbf{Y} &= \\mathbf{X}\\mathbf{\\beta} \\\\\n\\mathbf{X}^{'} \\mathbf{Y} &= \\mathbf{X}^{'} \\mathbf{X} \\mathbf{\\beta} \\\\\n(\\mathbf{X}^{'}\\mathbf{X})^{-1} \\mathbf{X}^{'} \\mathbf{Y} &= (\\mathbf{X}^{'}\\mathbf{X})^{-1}\\mathbf{X}^{'} \\mathbf{X} \\mathbf{\\beta} \\\\\n(\\mathbf{X}^{'}\\mathbf{X})^{-1} \\mathbf{X}^{'} \\mathbf{Y} &=\\mathbf{\\beta} \n\\end{aligned}\n\\]Pre-multiply sides \\(\\mathbf{X}'\\)Pre-multiply sides \\((\\mathbf{X}^{'}\\mathbf{X})^{-1}\\)\\((\\mathbf{X}^{'}\\mathbf{X})^{-1}\\mathbf{X}^{'} \\mathbf{X} = \\mathbf{}\\)depends \\((\\mathbf{X}^{'}\\mathbf{X})^{-1}\\) invertible. true, can calculate values \\(\\boldsymbol{\\beta}\\). , . might occur? ’ll see occurences today’s future problem sets.","code":""},{"path":"linear-algebra.html","id":"application-to-solving-systems-of-equations-tax-benefits-of-charitable-contributions","chapter":"Day 4 Linear algebra","heading":"4.6.6 Application to solving systems of equations: tax benefits of charitable contributions","text":"Suppose company earns -tax profits $100,000.11 agreed contribute 10% -tax profits Red Cross Relief Fund. must pay state tax 5% profits (Red Cross donation) federal tax 40% profits (donation state taxes paid). much company pay state taxes, federal taxes, Red Cross donation?Without model, difficult problem payment takes consideration payments. However, write linear equations describe deductions payments, can understand relationships payments solve straightforward manner.Let \\(C\\), \\(S\\), \\(F\\) represent amounts charitable contributin, state tax, federal tax, respectively. -profits \\(\\$100{,}000 - (S + F)\\), \\(C = 0.10 \\times (100{,}000 - (S + F))\\). can write \\[C + 0.1S + 0.1F = 10{,}000\\]putting variables one side. statement state tax 5% profits net donation becomes \\(S = 0.05 \\times (100{,}000 - C)\\), \\[0.05C + S = 5{,}000\\]Federal taxes 40% profit deducting \\(C\\) \\(S\\), relationship expressed \\(F = 0.40 \\times [100{,}000 - (C+S)]\\), \\[0.4C + 0.4S + F = 40{,}000\\]can summarize payments single system linear equations:\\[\n\\begin{aligned}\nC & + & 0.1S & + & 0.1F &= 10{,}000 \\\\\n0.05C & + & S & &&= 5{,}000 \\\\\n0.4C & + & 0.4S & + & F &= 40{,}000\n\\end{aligned}\n\\]substitute middle equation \\(S\\) terms \\(C\\) solve resulting system. , can use matrix inversion:","code":"## [1]  5956  4702 35737"},{"path":"linear-algebra.html","id":"determinant","chapter":"Day 4 Linear algebra","heading":"4.7 Determinant","text":"determinant square matrix single number summary. determinant uses values square matrix provide summary structure. Unfortunately rather complicated calculate larger matricies. First let’s consider calculate determinant \\(2 \\times 2\\) matrix, difference diagonal products.\\[\\det(\\mathbf{X}) = \\mid \\mathbf{X} \\mid = \\left| \\begin{matrix}\nx_{11} & x_{12} \\\\\nx_{21} & x_{22}\n\\end{matrix} \\right| = x_{11}x_{22} - x_{12}x_{21}\\]simple examples include\\[\\left| \\begin{matrix}\n1 & 2 \\\\\n3 & 4\n\\end{matrix} \\right| = (1)(4) - (2)(3) = 4 - 6 = -2\\]\\[\\left| \\begin{matrix}\n10 & \\frac{1}{2} \\\\\n4 & 1\n\\end{matrix} \\right| = (10)(1) - \\left( \\frac{1}{2} \\right)(4) = 10 - 2 = 8\\]\\[\\left| \\begin{matrix}\n2 & 3 \\\\\n6 & 9\n\\end{matrix} \\right| = (2)(9) - (3)(6) = 18 - 18 = 0\\]last case, determinant \\(0\\), important case shall see shortly.Unfortunately calculating determinants gets much involved square matricies larger \\(2 \\times 2\\). First need define submatrix. submatrix simply form achieved deleting rows /columns matrix, leaving remaining elements respective places. matrix \\(\\mathbf{X}\\), notice following submatricies:\\[\n\\mathbf{X} = \\begin{bmatrix}\nx_{11} & x_{12} & x_{13} & x_{14} \\\\\nx_{21} & x_{22} & x_{23} & x_{24} \\\\\nx_{31} & x_{32} & x_{33} & x_{34} \\\\\nx_{41} & x_{42} & x_{43} & x_{44} \\\\\n\\end{bmatrix}\n\\]\\[\n\\mathbf{X}_{[11]} = \\begin{bmatrix}\nx_{22} & x_{23} & x_{24} \\\\\nx_{32} & x_{33} & x_{34} \\\\\nx_{42} & x_{43} & x_{44} \\\\\n\\end{bmatrix},\n\\mathbf{X}_{[24]} = \\begin{bmatrix}\nx_{11} & x_{12} & x_{13}  \\\\\nx_{31} & x_{32} & x_{33}  \\\\\nx_{41} & x_{42} & x_{43}  \\\\\n\\end{bmatrix}\n\\]generalize \\(n \\times n\\) matricies, determinant can calculated \\[\\mid \\mathbf{X} \\mid = \\sum_{j=1}^n (-1)^{+j} x_{ij} \\mid\\mathbf{X}_{[ij]}\\mid\\]\\(ij\\)th minor \\(\\mathbf{X}\\) \\(x_{ij}\\), \\(\\mid\\mathbf{X}_{[ij]}\\mid\\), determinant \\((n - 1) \\times (n - 1)\\) submatrix results taking \\(\\)th row \\(j\\)th column . cofactor \\(\\mathbf{X}\\) minor signed \\((-1)^{+j} x_{ij} \\mid\\mathbf{X}_{[ij]}\\mid\\). calculate determinant cycle recursively columns take sums formula multiplies cofactor determining value.instance, method applied \\(3 \\times 3\\) matrix:\\[\n\\begin{aligned}\n\\mathbf{X} &= \\begin{bmatrix}\nx_{11} & x_{12} & x_{13} \\\\\nx_{21} & x_{22} & x_{23} \\\\\nx_{31} & x_{32} & x_{33} \\\\\n\\end{bmatrix} \\\\\n\\det(\\mathbf{X}) &= (+1)x_{11} \\left| \\begin{matrix}\nx_{22} & x_{23} \\\\\nx_{32} & x_{33} \\\\\n\\end{matrix} \\right| +(-1)x_{12} \\left| \\begin{matrix}\nx_{21} & x_{23} \\\\\nx_{31} & x_{33} \\\\\n\\end{matrix} \\right| + (+1)x_{13} \\left| \\begin{matrix}\nx_{21} & x_{22} \\\\\nx_{31} & x_{32} \\\\\n\\end{matrix} \\right|\n\\end{aligned}\n\\]Now problem simplified subsequent three determinant calculations \\(2 \\times 2\\) matricies.","code":""},{"path":"linear-algebra.html","id":"relevance-of-the-determinant","chapter":"Day 4 Linear algebra","heading":"4.7.1 Relevance of the determinant","text":"Remember wanted invert \\(2 \\times 2\\) matrix previously?\\[\\mathbf{}^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix}\nd & -b \\\\\n-c & \n\\end{bmatrix}\\]\\(\\frac{1}{ad - bc}\\) formula determinant \\(2 \\times 2\\) matrix! Recall non-invertible (singular) matricies square matricies columns rows linearly dependent. Well surprise know singular matricies also unique property whereby determinant \\(0\\). also important move eigenvectors diagonalization.","code":""},{"path":"linear-algebra.html","id":"matrix-decomposition","chapter":"Day 4 Linear algebra","heading":"4.8 Matrix decomposition","text":"Matrix decomposition factorization matrix product matricies. , matrix can decomposed efficient matricies depending calculations needing performed. LU decomposition applies square matricies:\\[\\mathbf{} = \\mathbf{L}\\mathbf{U}\\]\\(\\mathbf{L}\\) lower triangular matrix \\(\\mathbf{U}\\) upper triangular matrix. benefit decomposition solving system linear equations \\(\\mathbf{}\\mathbf{x} =\\mathbf{b}\\) reduce number steps necessary Gauss-Jordan elimination invert matrix. Hence, computationally efficient.LU decomposition works square matricies. many forms decomposition used solving systems linear equations, commonly social sciences dimension reduction.","code":""},{"path":"linear-algebra.html","id":"dimension-reduction","chapter":"Day 4 Linear algebra","heading":"4.8.1 Dimension reduction","text":"Dimension reduction refers decreasing number dimensions dataset. couple reasons might :want visualize data lot variables. generate something like scatterplot matrix, handful variables even become difficult interpret.want use variables supervised learning framework, reduce total number predictors make estimation efficient.either case, goal reduce dimensionality data identifying smaller number representative variables/vectors/columns collectively explain variability original dataset. several methods available performing task. First examine example applying dimension reduction techniques summarize roll-call voting United States.","code":""},{"path":"linear-algebra.html","id":"application-dw-nominate","chapter":"Day 4 Linear algebra","heading":"4.8.1.1 Application: DW-NOMINATE","text":"1990s, dimension reduction techniques revolutionized study U.S. legislative politics. Measuring ideology legislators prior point difficult method locating legislators along ideological spectrum (liberal-conservative) manner allowed comparisons time. , liberal Democrat 1870 compared Democrat 1995? Additionally, supposed wanted predict legislator vote given bill. Roll-call votes record individual legislator behavior, use past votes predict future ones. tens thousands recorded votes course U.S. Congress. Even given term Congress, Senate may cast hundreds recorded votes. 100 senators (present), estimate regression model number predictors \\(p\\) larger number observations \\(n\\). need method reducing dimensionality data handful variables explain much variation roll-call voting possible.Multidimensional scaling techniques can used perform feat. technical details specific application beyond scope class, Keith Poole Howard Rosenthal developed specific procedure called NOMINATE reduce dimensionality data. Rather using \\(p\\) predictors explain predict individual legislator’s roll-call votes, \\(p\\) total number roll-call votes recorded history U.S. Congress, Poole Rosenthal examined similarity legislators’ votes given session Congress time identify two major dimensions roll-call voting U.S. Congress. , roll-call votes Congress can generally explained two variables can estimated every past present member Congress. two dimensions inherent substantive interpretation, graphically examining two dimensions, becomes clear represent two specific factors legislative voting:First dimension - political ideology. dimension appears represent political ideology liberal-conservative spectrum. Positive values dimension refer increasingly conservative voting patterns, negative values refer increasingly liberal voting patterns.Second dimension - “issue day.” dimension appears pick attitudes salient different points nation’s history. regional differences (Southern vs. non-Southern states), attitudes towards specific policy issues (.e. slavery).data can used wide range research questions. example, use assess degree polarization U.S. Congress time:\nFigure 4.3: Source: Polarization Congress\n\nFigure 4.4: Source: Polarization Congress\n","code":""},{"path":"linear-algebra.html","id":"singular-value-decomposition","chapter":"Day 4 Linear algebra","heading":"4.8.2 Singular value decomposition","text":"Singular-Value Decomposition, SVD, matrix decomposition method reducing matrix constitutent parts order make subsequent matrix calculations simpler. Unlike LU decomposition, SVD works rectangular matrix (just square matricies). Suppose \\(\\mathbf{M}\\) \\(m \\times n\\) matrix. exists factorization form\\[\\mathbf{M} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^{*}\\]\\(\\mathbf{U}\\) \\(m \\times n\\) matrix\\(\\boldsymbol{\\Sigma}\\) \\(n \\times n\\) diagonal matrix\\(\\mathbf{V}^{*}\\) transpose \\(n \\times n\\) matrixThe diagonal entries \\(\\sigma_i\\) \\(\\boldsymbol{\\Sigma}\\) known singular values \\(\\mathbf{M}\\). columns \\(\\mathbf{U}\\) called left-singular vectors \\(\\mathbf{M}\\), columns V called right-singular vectors \\(\\mathbf{M}\\).","code":""},{"path":"linear-algebra.html","id":"image-compression","chapter":"Day 4 Linear algebra","heading":"4.8.2.1 Image compression","text":"Digital images can compressed using technique. image treated matrix pixels corresponding color values decomposed smaller ranks (.e. columns) retain essential information comprises image.12The picture lion can stored 600 337 matrix, value number 0 1 indicates white black pixel appear.SVD matrix results 3 new matricies:13\\(\\mathbf{U}\\)\n##         [,1]    [,2]     [,3]     [,4]     [,5]\n## [1,] -0.0398 -0.0291 -0.02032 0.019709 -0.01329\n## [2,] -0.0405 -0.0150 -0.00198 0.000273 -0.00208\n## [3,] -0.0396 -0.0186 -0.01972 0.020905  0.01126\n## [4,] -0.0390 -0.0264 -0.02890 0.039385  0.01012\n## [5,] -0.0398 -0.0300 -0.03199 0.037500  0.00553\nMatrix size: \\((600, 337)\\)\\(\\mathbf{U}\\)Matrix size: \\((600, 337)\\)\\(\\boldsymbol{\\Sigma}\\)\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]  193  0.0  0.0    0  0.0\n## [2,]    0 29.2  0.0    0  0.0\n## [3,]    0  0.0 16.2    0  0.0\n## [4,]    0  0.0  0.0   15  0.0\n## [5,]    0  0.0  0.0    0 12.2\nLength: \\(337\\)\\(\\boldsymbol{\\Sigma}\\)Length: \\(337\\)\\(\\mathbf{V}^{*}\\)\n##         [,1]    [,2]   [,3]   [,4]    [,5]\n## [1,] -0.0556 0.00838 0.0211 0.0377 -0.0119\n## [2,] -0.0558 0.00848 0.0179 0.0391 -0.0131\n## [3,] -0.0560 0.00874 0.0138 0.0405 -0.0146\n## [4,] -0.0561 0.00888 0.0114 0.0405 -0.0159\n## [5,] -0.0561 0.00874 0.0102 0.0394 -0.0159\nMatrix size: \\((337, 337)\\)\\(\\mathbf{V}^{*}\\)Matrix size: \\((337, 337)\\)","code":"##       [,1]  [,2]  [,3]  [,4]  [,5]\n## [1,] 0.361 0.369 0.381 0.393 0.403\n## [2,] 0.365 0.373 0.385 0.397 0.407\n## [3,] 0.369 0.377 0.389 0.399 0.411\n## [4,] 0.377 0.385 0.395 0.407 0.420\n## [5,] 0.388 0.391 0.403 0.416 0.424##         [,1]    [,2]     [,3]     [,4]     [,5]\n## [1,] -0.0398 -0.0291 -0.02032 0.019709 -0.01329\n## [2,] -0.0405 -0.0150 -0.00198 0.000273 -0.00208\n## [3,] -0.0396 -0.0186 -0.01972 0.020905  0.01126\n## [4,] -0.0390 -0.0264 -0.02890 0.039385  0.01012\n## [5,] -0.0398 -0.0300 -0.03199 0.037500  0.00553##      [,1] [,2] [,3] [,4] [,5]\n## [1,]  193  0.0  0.0    0  0.0\n## [2,]    0 29.2  0.0    0  0.0\n## [3,]    0  0.0 16.2    0  0.0\n## [4,]    0  0.0  0.0   15  0.0\n## [5,]    0  0.0  0.0    0 12.2##         [,1]    [,2]   [,3]   [,4]    [,5]\n## [1,] -0.0556 0.00838 0.0211 0.0377 -0.0119\n## [2,] -0.0558 0.00848 0.0179 0.0391 -0.0131\n## [3,] -0.0560 0.00874 0.0138 0.0405 -0.0146\n## [4,] -0.0561 0.00888 0.0114 0.0405 -0.0159\n## [5,] -0.0561 0.00874 0.0102 0.0394 -0.0159"},{"path":"linear-algebra.html","id":"interesting-properties-of-svd","chapter":"Day 4 Linear algebra","heading":"4.8.2.2 Interesting properties of SVD","text":"","code":""},{"path":"linear-algebra.html","id":"recovering-the-data","chapter":"Day 4 Linear algebra","heading":"4.8.2.2.1 Recovering the data","text":"can recover original matrix multiplying matricies back together:\\[\\mathbf{M} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^{*}\\]","code":"##       [,1]  [,2]  [,3]  [,4]  [,5]\n## [1,] 0.361 0.369 0.381 0.393 0.403\n## [2,] 0.365 0.373 0.385 0.397 0.407\n## [3,] 0.369 0.377 0.389 0.399 0.411\n## [4,] 0.377 0.385 0.395 0.407 0.420\n## [5,] 0.388 0.391 0.403 0.416 0.424"},{"path":"linear-algebra.html","id":"reducing-the-data","chapter":"Day 4 Linear algebra","heading":"4.8.2.2.2 Reducing the data","text":"next useful property SVD values diagonal matrix \\(\\Sigma\\). Notice sorted descending order.tell us relative importance column \\(\\mathbf{U}\\) \\(\\mathbf{V}^{*}\\). values multiplied really small numbers (even 0), contribute much information. original image 337 columns. want represent much original information possible, compact form?first column alone explains 92.967% variation original matrix. just used first two columns redraw picture?Okay, doesn’t appear enough. fewest get away ?Rank 173 doesn’t look bad, Rank 215 looks pretty indistinguishable original. original matrix contained 202,200 different values matrix. used SVD compress image use first 215 columns individual matrix, shrink size image 28%.","code":"##   [1] 193.4417  29.1733  16.1600  14.9806  12.1708  11.3756  10.5788   8.9693\n##   [9]   8.3404   7.6359   7.4752   6.8798   6.1244   5.9575   5.5327   5.3978\n##  [17]   5.1953   4.8511   4.6521   4.6020   4.2501   4.1820   4.0820   4.0382\n##  [25]   3.8938   3.8375   3.7173   3.5563   3.5273   3.4986   3.4396   3.4027\n##  [33]   3.3417   3.2681   3.2025   3.1409   3.0671   3.0221   3.0124   2.9543\n##  [41]   2.8912   2.8365   2.8076   2.7306   2.6768   2.6547   2.6008   2.5562\n##  [49]   2.5353   2.5186   2.4892   2.4669   2.3997   2.3361   2.3274   2.2823\n##  [57]   2.2424   2.2378   2.1923   2.1692   2.1122   2.0840   2.0704   2.0510\n##  [65]   2.0241   2.0196   1.9849   1.9568   1.9305   1.9237   1.9052   1.8737\n##  [73]   1.8433   1.8222   1.8107   1.7891   1.7699   1.7554   1.7195   1.7039\n##  [81]   1.6870   1.6695   1.6453   1.6310   1.6101   1.5815   1.5727   1.5373\n##  [89]   1.5198   1.5105   1.4861   1.4748   1.4609   1.4378   1.4321   1.4016\n##  [97]   1.4001   1.3788   1.3624   1.3386   1.3301   1.3169   1.3057   1.2704\n## [105]   1.2593   1.2419   1.2376   1.2065   1.1922   1.1825   1.1741   1.1584\n## [113]   1.1405   1.1314   1.1157   1.1003   1.0921   1.0705   1.0602   1.0480\n## [121]   1.0406   1.0314   1.0191   0.9983   0.9939   0.9919   0.9634   0.9500\n## [129]   0.9434   0.9337   0.9213   0.9153   0.9044   0.8910   0.8777   0.8528\n## [137]   0.8458   0.8419   0.8246   0.8196   0.8005   0.7967   0.7924   0.7866\n## [145]   0.7734   0.7591   0.7564   0.7469   0.7365   0.7283   0.7198   0.7159\n## [153]   0.7118   0.7009   0.6926   0.6874   0.6817   0.6634   0.6552   0.6517\n## [161]   0.6493   0.6352   0.6184   0.6127   0.6073   0.6039   0.6014   0.5949\n## [169]   0.5915   0.5810   0.5767   0.5627   0.5547   0.5456   0.5381   0.5351\n## [177]   0.5310   0.5247   0.5211   0.5139   0.5025   0.4998   0.4966   0.4808\n## [185]   0.4763   0.4725   0.4613   0.4552   0.4529   0.4471   0.4411   0.4374\n## [193]   0.4326   0.4309   0.4232   0.4178   0.4152   0.4047   0.4005   0.3970\n## [201]   0.3884   0.3795   0.3790   0.3770   0.3705   0.3690   0.3597   0.3535\n## [209]   0.3506   0.3465   0.3434   0.3387   0.3341   0.3243   0.3201   0.3183\n## [217]   0.3099   0.3073   0.3020   0.2980   0.2972   0.2953   0.2911   0.2826\n## [225]   0.2787   0.2738   0.2705   0.2644   0.2584   0.2542   0.2533   0.2472\n## [233]   0.2424   0.2397   0.2356   0.2320   0.2300   0.2268   0.2205   0.2187\n## [241]   0.2160   0.2096   0.2077   0.1980   0.1961   0.1930   0.1895   0.1891\n## [249]   0.1853   0.1814   0.1798   0.1772   0.1720   0.1704   0.1681   0.1658\n## [257]   0.1650   0.1617   0.1539   0.1523   0.1483   0.1457   0.1436   0.1424\n## [265]   0.1367   0.1360   0.1332   0.1304   0.1276   0.1265   0.1259   0.1232\n## [273]   0.1201   0.1158   0.1119   0.1112   0.1079   0.1069   0.1044   0.1010\n## [281]   0.0993   0.0980   0.0934   0.0905   0.0900   0.0878   0.0868   0.0847\n## [289]   0.0838   0.0796   0.0763   0.0744   0.0733   0.0710   0.0682   0.0674\n## [297]   0.0671   0.0637   0.0612   0.0595   0.0570   0.0556   0.0537   0.0501\n## [305]   0.0485   0.0446   0.0435   0.0426   0.0401   0.0361   0.0354   0.0336\n## [313]   0.0311   0.0295   0.0286   0.0257   0.0248   0.0238   0.0235   0.0233\n## [321]   0.0224   0.0221   0.0218   0.0208   0.0203   0.0200   0.0195   0.0191\n## [329]   0.0184   0.0181   0.0175   0.0174   0.0170   0.0162   0.0157   0.0155\n## [337]   0.0152##       [,1]  [,2]  [,3]  [,4]  [,5]\n## [1,] 0.421 0.422 0.424 0.424 0.424\n## [2,] 0.432 0.434 0.435 0.436 0.436\n## [3,] 0.421 0.423 0.424 0.425 0.425\n## [4,] 0.413 0.414 0.416 0.416 0.416\n## [5,] 0.421 0.423 0.424 0.425 0.425"},{"path":"linear-algebra.html","id":"principal-components-analysis","chapter":"Day 4 Linear algebra","heading":"4.8.3 Principal components analysis","text":"Principal components analysis (PCA) basic technique dimension reduction. goal find low-dimensional representation data contains much possible variation. example, say original dataset 30 columns (.e. variables, dimensions). want reduce number columns still maintaining overall structure matrix. can helpful many reasons, includingExploratory data analysis - visualize \\(p\\) dimensions simple \\(2\\) dimensional plotStatistical learning - reduce number features/independent variables statistical learning model improve efficiency remove multicollinearityThe PCA algorithm implemented :Rescale column mean 0 standard deviation 1. prevents variables larger values variances dominating projection.Compute covariance matrix \\(\\mathbf{S}\\). \\(\\mathbf{X}\\) data matrix:\n\\[\\mathbf{S} = \\dfrac{1}{N} \\mathbf{X}' \\mathbf{X}\\]Compute \\(K\\) largest eigenvectors \\(\\mathbf{S}\\). eigenvectors principal components dataset. Remember every eigenvector corresponding eigenvalue. eigenvector defines direction line, eigenvalue tells much variance data direction (essentially spread data line).Computing covariance matrix expensive \\(\\mathbf{X}\\) large \\(\\mathbf{X}\\) small. SVD can used make process efficient computing SVD original matrix. \\(\\mathbf{V}^{*}\\) contains principal directions (eigenvectors), columns \\(\\mathbf{U} \\boldsymbol{\\Sigma}\\) principal components (scores) observation, values diagonal elements \\(\\boldsymbol{\\Sigma}\\) equivalent eigenvalues computed \\(\\mathbf{S}\\) (amount variance explained principal components).total number principal components given \\(n \\times p\\) data set \\(\\min(n,p)\\), either number observations data number variables data (whichever smaller). estimate principal components, can plot order produce low-dimensional visualization data.","code":""},{"path":"linear-algebra.html","id":"example-usarrests","chapter":"Day 4 Linear algebra","heading":"4.8.3.1 Example: USArrests","text":"Let’s look use PCA USArrests dataset, reproduced Introduction Statistical Learning.principal component score vectors length \\(n=50\\) principal component loading vectors length \\(p=4\\) (data set, \\(p < n\\)). biplot visualizes relationship first two principal components dataset, including scores loading vectors. first principal component places approximately equal weight murder, assault, rape. can tell vectors’ length first principal component dimension roughly , whereas length urban population smaller. Conversely, second principal component (vertical axis) places emphasis urban population. Intuitively makes sense murder, assault, rape measures violent crime, makes sense correlated one another (.e. states high murder rates likely high rates rape well).can also interpret plot individual states based positions along two dimensions. States large positive values first principal component high crime rates states large negative values low crime rates; states large positive values second principal component high levels urbanization states large negative values low levels urbanization.","code":""},{"path":"linear-algebra.html","id":"example-mnist-data-set","chapter":"Day 4 Linear algebra","heading":"4.8.3.2 Example: MNIST data set","text":"\nFigure 4.5: MNIST digits\nMNIST digits classic practice dataset image classification. image standardized picture handwritten digit. want use image classify digit actual number 0-9. use individual pixels intensity black/white generate predictions. Rather use \\(28 \\times 28 = 784\\) individual pixels, can use SVD/PCA compress data set smaller number principal components capture variation rows/columns. verify technique work, can visualize observations along first second principal components. two components alone can distinguish ten possible digits, see unique clusters observations scatterplot.","code":""},{"path":"linear-algebra.html","id":"acknowledgements","chapter":"Day 4 Linear algebra","heading":"Acknowledgements","text":"Gentle Introduction Singular-Value Decomposition Machine LearningChapter 14.5, Friedman, Hastie, Tibshirani (2001)Examples SVDSingular Value Decomposition (SVD): Tutorial Using Examples RRelationship SVD PCA. use SVD perform PCA?Decoding Dimensionality Reduction, PCA SVD","code":""},{"path":"multivariable-differentiation.html","id":"multivariable-differentiation","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"Day 5 Functions of several variables and optimization with several variables","text":"","code":""},{"path":"multivariable-differentiation.html","id":"learning-objectives-4","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"Learning objectives","text":"Define partial derivativeIdentify higher order derivatives partial derivativesDefine notation calculus performed vector matrix formsDemonstrate multivariable calculus methods social scientific researchCalculate critical points, partial derivatives, double integrals","code":""},{"path":"multivariable-differentiation.html","id":"supplemental-readings-4","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"Supplemental readings","text":"Chapter 14, Pemberton Rau (2011)OpenStax Calculus: Volume 3, ch 4","code":""},{"path":"multivariable-differentiation.html","id":"higher-order-derivatives-1","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.1 Higher order derivatives","text":"first derivative applying definition derivatives function, can expressed \\[f'(x),  ~~ y',  ~~ \\frac{d}{dx}f(x), ~~ \\frac{dy}{dx}\\]can keep applying differentiation process functions derivatives. derivative \\(f'(x)\\) respect \\(x\\), \\[f''(x)=\\lim\\limits_{h\\0}\\frac{f'(x+h)-f'(x)}{h}\\] can therefore call Second derivative:\\[f''(x), ~~ y'', ~~ \\frac{d^2}{dx^2}f(x), ~~ \\frac{d^2y}{dx^2}\\]Similarly, derivative \\(f''(x)\\) called third derivative denoted \\(f'''(x)\\). extension, nth derivative expressed \\(\\frac{d^n}{dx^n}f(x)\\), \\(\\frac{d^ny}{dx^n}\\).\\[\n\\begin{aligned}\nf(x) &=x^3\\\\\nf^{\\prime}(x) &=3x^2\\\\\nf^{\\prime\\prime}(x) &=6x \\\\\nf^{\\prime\\prime\\prime}(x) &=6\\\\\nf^{\\prime\\prime\\prime\\prime}(x) &=0\\\\\n\\end{aligned}\n\\]Earlier, said function differentiable given point, must continuous. , \\(f'(x)\\) continuous, \\(f(x)\\) called continuously differentiable. matters many findings optimization rely differentiation, want function differentiable many layers. function continuously differentiable infinitely called smooth. examples include:\\[\n\\begin{aligned}\nf(x) &= x^2 \\\\\nf(x) &= e^x\n\\end{aligned}\n\\]","code":""},{"path":"multivariable-differentiation.html","id":"multivariate-function","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.2 Multivariate function","text":"multivariate function function one argument.","code":""},{"path":"multivariable-differentiation.html","id":"example-1-3","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.2.0.0.1 Example 1","text":"\\[f(x_{1}, x_{2}) = x_{1}  + x_{2}\\]","code":""},{"path":"multivariable-differentiation.html","id":"example-2-3","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.2.0.0.2 Example 2","text":"\\[f(x_{1}, x_{2}) = x_{1}^2 + x_{2}^2\\]","code":""},{"path":"multivariable-differentiation.html","id":"example-3","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.2.0.0.3 Example 3","text":"\\[f(x_{1}, x_{2}) = \\sin(x_1)\\cos(x_2)\\]","code":""},{"path":"multivariable-differentiation.html","id":"example-4","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.2.0.0.4 Example 4","text":"\\[f(x_{1}, x_{2}) =  -(x-5)^2 - (y-2)^2\\]","code":""},{"path":"multivariable-differentiation.html","id":"example-5","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.2.0.0.5 Example 5","text":"\\[f(x_{1}, x_{2}, x_{3} ) = x_1 + x_2 + x_3\\]","code":""},{"path":"multivariable-differentiation.html","id":"example-6","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.2.0.0.6 Example 6","text":"\\[\n\\begin{aligned}\nf(\\mathbf{x} )&= f(x_{1}, x_{2}, \\ldots, x_{N} ) \\\\\n                            &= x_{1} +x_{2} + \\ldots + x_{N} \\\\\n                            &= \\sum_{=1}^{N} x_{} \n\\end{aligned}\n\\]","code":""},{"path":"multivariable-differentiation.html","id":"definition-2","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.2.1 Definition","text":"Definition 5.1  (Multivariate function) Suppose \\(f:\\Re^{n} \\rightarrow \\Re^{1}\\). call \\(f\\) multivariate function. commonly write,\\[f(\\mathbf{x}) = f(x_{1}, x_{2}, x_{3}, \\ldots, x_{n} )\\]\\(\\Re^{n} = \\Re \\underbrace{\\times}_{\\text{cartesian}} \\Re \\times \\Re \\times \\ldots \\Re\\)function consider take \\(n\\) inputs output single number (lives \\(\\Re^{1}\\), real line)","code":""},{"path":"multivariable-differentiation.html","id":"evaluating-multivariate-functions","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.2.2 Evaluating multivariate functions","text":"Example 5.1  \\[f(x_{1}, x_{2}, x_{3}) = x_1  + x_2 + x_3\\]Evaluate \\(\\mathbf{x} = (x_{1}, x_{2}, x_{3}) = (2, 3, 2)\\)\\[\n\\begin{aligned}\nf(2, 3, 2) & = 2 + 3 + 2 \\\\\n            & = 7  \n\\end{aligned}\n\\]Example 5.2  \\[f(x_{1}, x_{2} ) = x_{1} + x_{2} + x_{1} x_{2}\\]Evaluate \\(\\mathbf{w} = (w_{1}, w_{2} ) = (1, 2)\\)\\[\n\\begin{aligned}\nf(w_{1}, w_{2}) & = w_{1} + w_{2} + w_{1} w_{2}  \\\\\n                                & = 1  + 2  + 1 \\times 2  \\\\\n                                & = 5  \n\\end{aligned}                               \n\\]Example 5.3  (Preferences multidimensional policy) Recall spatial model, suppose policy political actors located space. Suppose policy \\(N\\) dimensional - \\(\\mathbf{x} \\\\Re^{N}\\). Suppose legislator \\(\\)’s utility \\(U:\\Re^{N} \\rightarrow \\Re^{1}\\) given ,\\[\n\\begin{aligned}\nU(\\mathbf{x}) & = U(x_{1}, x_{2}, \\ldots, x_{N} )  \\\\\n                    & = - (x_{1} - \\mu_{1} )^2 - (x_{2} - \\mu_{2})^2 - \\ldots - (x_{N} - \\mu_{N})^{2} \\\\\n    & = -\\sum_{j=1}^{N} (x_{j} - \\mu_{j} )^{2}\n\\end{aligned}                           \n\\]Suppose \\(\\mathbf{\\mu} = (\\mu_{1}, \\mu_{2}, \\ldots, \\mu_{N} ) = (0, 0, \\ldots, 0)\\). Evaluate legislator’s utility policy proposal \\(\\mathbf{m} = (1, 1, \\ldots, 1)\\)\\[\n\\begin{aligned}\nU(\\mathbf{m} ) & = U(1, 1, \\ldots, 1) \\\\\n                          & = - (1 - 0)^2 - (1- 0) ^2 - \\ldots - (1- 0) ^2 \\\\\n                & = -\\sum_{j=1}^{N} 1 = - N   \\\\\n\\end{aligned} \n\\]Example 5.4  (Regression models randomized treatments) Often administer randomized experiments. recent wave interest began voter mobilization, wonders individual \\(\\) turns vote, \\(\\text{Vote}_{}\\)\\(T = 1\\) (treated): voter receives mobilization\\(T = 0\\) (control): voter receive mobilizationSuppose find following regression model, \\(x_{2}\\) participant’s age:\\[\n\\begin{aligned}\nf(T,x_2) & = \\Pr(\\text{Vote}_{} = 1 | T, x_{2} ) \\\\\n    & =   \\beta_{0} + \\beta_{1} T + \\beta_{2} x_{2} \n\\end{aligned}\n\\]can calculate effect experiment :\\[\n\\begin{aligned}\nf(T = 1, x_2) - f(T=0, x_2) & = \\beta_{0} + \\beta_{1} 1  + \\beta_{2} x_{2} - (\\beta_{0} + \\beta_{1}  0 + \\beta_{2} x_{2}) \\\\\n& = \\beta_{0} - \\beta_{0}  + \\beta_{1}(1 - 0) + \\beta_{2}(x_{2} - x_{2} ) \\\\\n    & = \\beta_{1} \n\\end{aligned}                                       \n\\]","code":""},{"path":"multivariable-differentiation.html","id":"multivariate-derivatives","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.3 Multivariate derivatives","text":"happens ’s one variable changing?Suppose function \\(f\\) now two () variables want determine rate change relative one variables. , find partial derivative, defined similar derivative function one variable.Definition 5.2  (Partial derivative) Let \\(f\\) function variables \\((x_1,\\ldots,x_n)\\). partial derivative \\(f\\) respect \\(x_i\\) \\[\\frac{\\partial f}{\\partial x_i} (x_1,\\ldots,x_n) = \\lim\\limits_{h\\0} \\frac{f(x_1,\\ldots,x_i+h,\\ldots,x_n)-f(x_1,\\ldots,x_i,\\ldots,x_n)}{h}\\]\\(\\)th variable changes — others treated constants.can take higher-order partial derivatives, like functions single variable, except now higher-order partials can respect multiple variables.Notice can take partials regard different variables.Suppose \\(f(x,y)=x^2+y^2\\). \\[\n\\begin{aligned}\n\\frac{\\partial f}{\\partial x}(x,y) &= 2x \\\\\n\\frac{\\partial f}{\\partial y}(x,y) &= 2y\\\\\n\\frac{\\partial^2 f}{\\partial x^2}(x,y) &= 2\\\\\n\\frac{\\partial^2 f}{\\partial x \\partial y}(x,y) &= 0\n\\end{aligned}\n\\]Let \\(f(x,y)=x^3 y^4 +e^x -\\log y\\). following partial derivaitves?\\[\n\\begin{aligned}\n\\frac{\\partial f}{\\partial x}(x,y) &= 3x^2y^4 + e^x\\\\\n\\frac{\\partial f}{\\partial y}(x,y) &=4x^3y^3 - \\frac{1}{y}\\\\\n\\frac{\\partial^2 f}{\\partial x^2}(x,y) &= 6xy^4 + e^x\\\\\n\\frac{\\partial^2 f}{\\partial x \\partial y}(x,y) &= 12x^2y^3\n\\end{aligned}\n\\]Example 5.5  (Rate change, linear regression) Suppose regress \\(\\text{Approval}_{}\\) rate Trump month \\(\\) \\(\\text{Employ}_{}\\) \\(\\text{Gas}_{}\\). obtain following model:\\[\\text{Approval}_{} = 0.8  -0.5 \\text{Employ}_{}  -0.25 \\text{Gas}_{}\\]modeling \\(\\text{Approval}_{} = f(\\text{Employ}_{}, \\text{Gas}_{} )\\). partial derivative respect employment?\\[\\frac{\\partial f(\\text{Employ}_{}, \\text{Gas}_{} ) }{\\partial \\text{Employ}_{} } = -0.5\\]","code":""},{"path":"multivariable-differentiation.html","id":"multivariate-optimization","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.4 Multivariate optimization","text":"Just want optimize functions single variable, often wish opimize functions multiple variables.Parameters \\(\\mathbf{\\beta} = (\\beta_{1}, \\beta_{2}, \\ldots, \\beta_{n} )\\) \\(f(\\mathbf{\\beta}| \\mathbf{X}, \\mathbf{Y})\\) maximizedParameters \\(\\mathbf{\\beta} = (\\beta_{1}, \\beta_{2}, \\ldots, \\beta_{n} )\\) \\(f(\\mathbf{\\beta}| \\mathbf{X}, \\mathbf{Y})\\) maximizedPolicy \\(\\mathbf{x} \\\\Re^{n}\\) maximizes \\(U(\\mathbf{x})\\)Policy \\(\\mathbf{x} \\\\Re^{n}\\) maximizes \\(U(\\mathbf{x})\\)Weights \\(\\mathbf{\\pi} = (\\pi_{1}, \\pi_{2}, \\ldots, \\pi_{K})\\) weighted average forecasts \\(\\mathbf{f} = (f_{1} , f_{2}, \\ldots, f_{k})\\) minimum loss\n\\[\\min_{\\mathbf{\\pi}} = - (\\sum_{j=1}^{K} \\pi_{j} f_{j}  - y ) ^ 2\\]Weights \\(\\mathbf{\\pi} = (\\pi_{1}, \\pi_{2}, \\ldots, \\pi_{K})\\) weighted average forecasts \\(\\mathbf{f} = (f_{1} , f_{2}, \\ldots, f_{k})\\) minimum loss\\[\\min_{\\mathbf{\\pi}} = - (\\sum_{j=1}^{K} \\pi_{j} f_{j}  - y ) ^ 2\\], consider analytic computational approaches.","code":""},{"path":"multivariable-differentiation.html","id":"differences-from-single-variable-optimization-procedure","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.4.1 Differences from single variable optimization procedure","text":"basic approach, except multiple parameters interest. requires explicit knowledge linear algebra track components optimize multidimensional spaceLet \\(\\mathbf{x} \\\\Re^{n}\\) let \\(\\delta >0\\). Define neighborhood \\(\\mathbf{x}\\), \\(B(\\mathbf{x}, \\delta)\\), set points ,\\[B(\\mathbf{x}, \\delta) = \\{ \\mathbf{y} \\\\Re^{n} : ||\\mathbf{x} - \\mathbf{y}||< \\delta \\}\\], \\(B(\\mathbf{x}, \\delta)\\) set points vector \\(\\mathbf{y}\\) vector n-dimensional space vector norm \\(\\mathbf{x} - \\mathbf{y}\\) less \\(\\delta\\)neighborhood \\(\\delta\\) bigNow suppose \\(f:X \\rightarrow \\Re\\) \\(X \\subset \\Re^{n}\\). vector \\(\\mathbf{x}^{*} \\X\\) global maximum , \\(\\mathbf{x} \\X\\)\\[f(\\mathbf{x}^{*}) > f(\\mathbf{x} )\\]vector \\(\\mathbf{x}^{\\text{local}}\\) local maximum neighborhood around \\(\\mathbf{x}^{\\text{local}}\\), \\(Q \\subset X\\) , \\(x \\Q\\),\\[f(\\mathbf{x}^{\\text{local} }) > f(\\mathbf{x} )\\]maximum minimum values function \\(f:X \\rightarrow \\Re\\) real number line (n-dimensional space) fall somewhere along \\(X\\). saw previously, except now \\(X\\) scalar value - vector \\(\\mathbf{X}\\).","code":""},{"path":"multivariable-differentiation.html","id":"first-derivative-test-gradient","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.4.2 First derivative test: Gradient","text":"Suppose \\(f:X \\rightarrow \\Re^{n}\\) \\(X \\subset \\Re^{1}\\) differentiable function. Define gradient vector \\(f\\) \\(\\mathbf{x}_{0}\\), \\(\\nabla f(\\mathbf{x}_{0})\\) \\[\\nabla f (\\mathbf{x}_{0}) = \\left(\\frac{\\partial f (\\mathbf{x}_{0}) }{\\partial x_{1} }, \\frac{\\partial f (\\mathbf{x}_{0}) }{\\partial x_{2} }, \\frac{\\partial f (\\mathbf{x}_{0}) }{\\partial x_{3} }, \\ldots, \\frac{\\partial f (\\mathbf{x}_{0}) }{\\partial x_{n} } \\right)\\]first partial derivatives variable \\(x_n\\) stored vectorGradient points direction function increasing fastest directionSo \\(\\mathbf{} \\X\\) local extremum, ,\\[\n\\begin{aligned}\n\\nabla f(\\mathbf{}) &= \\mathbf{0}  \\\\\n                                    &= (0, 0, \\ldots, 0)                \n\\end{aligned}\n\\], root(s) gradient \\(f(\\mathbf{})\\) equals \\(\\mathbf{0}\\) \\(n\\)-dimensional space.Example 5.6  \\[\n\\begin{aligned}\nf(x,y) &= x^2+y^2 \\\\\n\\nabla f(x,y) &= (2x, \\, 2y)\n\\end{aligned}\n\\]Example 5.7  \\[\n\\begin{aligned}\nf(x,y) &= x^3 y^4 +e^x -\\log y \\\\\n\\nabla f(x,y) &= (3x^2 y^4 + e^x, \\, 4x^3y^3 - \\frac{1}{y})\n\\end{aligned}\n\\]","code":""},{"path":"multivariable-differentiation.html","id":"critical-points-2","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.4.2.1 Critical points","text":"can critical points:MaximumMinimumSaddle pointIn order know maximum/minimum/saddle point, need perform second derivative test.","code":""},{"path":"multivariable-differentiation.html","id":"second-derivative-test-hessian","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.4.3 Second derivative test: Hessian","text":"Suppose \\(f:X \\rightarrow \\Re^{1}\\) , \\(X \\subset \\Re^{n}\\), \\(f\\) twice differentiable function. define Hessian matrix matrix second derivatives \\(\\mathbf{x}^{*} \\X\\),\\[\n\\mathbf{H}(f)(\\mathbf{x}^{*} ) = \\begin{bmatrix} \n        \\frac{\\partial^{2} f }{\\partial x_{1} \\partial x_{1} } (\\mathbf{x}^{*} ) & \\frac{\\partial^{2} f }{\\partial x_{1} \\partial x_{2} } (\\mathbf{x}^{*} ) & \\ldots & \\frac{\\partial^{2} f }{\\partial x_{1} \\partial x_{n} } (\\mathbf{x}^{*} ) \\\\\n        \\frac{\\partial^{2} f }{\\partial x_{2} \\partial x_{1} } (\\mathbf{x}^{*} ) & \\frac{\\partial^{2} f }{\\partial x_{2} \\partial x_{2} } (\\mathbf{x}^{*} ) & \\ldots & \\frac{\\partial^{2} f }{\\partial x_{2} \\partial x_{n} } (\\mathbf{x}^{*} ) \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        \\frac{\\partial^{2} f }{\\partial x_{n} \\partial x_{1} } (\\mathbf{x}^{*} ) & \\frac{\\partial^{2} f }{\\partial x_{n} \\partial x_{2} } (\\mathbf{x}^{*} ) & \\ldots & \\frac{\\partial^{2} f }{\\partial x_{n} \\partial x_{n} } (\\mathbf{x}^{*} ) \\\\\n\\end{bmatrix}  \n\\]Hessians symmetric, describe curvature function (think, bended). calculate hessian, must differentiate entire gradient respect \\(x_n\\).Example 5.8  \\[\n\\begin{aligned}\nf(x,y) &= x^2+y^2 \\\\\n\\nabla f(x,y) &= (2x, \\, 2y) \\\\\n\\mathbf{H}(f)(x,y) &= \\begin{bmatrix}\n2 & 0 \\\\\n0 & 2\n\\end{bmatrix}\n\\end{aligned}\n\\]Example 5.9  \\[\n\\begin{aligned}\nf(x,y) &= x^3 y^4 +e^x -\\log y \\\\\n\\nabla f(x,y) &= (3x^2 y^4 + e^x, \\, 4x^3y^3 - \\frac{1}{y}) \\\\\n\\mathbf{H}(f)(x,y) &= \\begin{bmatrix}\n6xy^4 + e^x & 12x^2y^3 \\\\\n12x^2y^3 & 12x^3y^2 + \\frac{1}{y^2}\n\\end{bmatrix}\n\\end{aligned}\n\\]","code":""},{"path":"multivariable-differentiation.html","id":"definiteness-of-a-matrix","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.4.3.1 Definiteness of a matrix","text":"Consider \\(n \\times n\\) matrix \\(\\mathbf{}\\). , \\(\\mathbf{x} \\\\Re^{n}\\) \\(\\mathbf{x} \\neq \\mathbf{0}\\):\\[\n\\begin{aligned}\n\\mathbf{x}^{'} \\mathbf{} \\mathbf{x} &> 0, \\quad \\mathbf{} \\text{ positive definite} \\\\\n\\mathbf{x}^{'} \\mathbf{} \\mathbf{x} &< 0, \\quad \\mathbf{} \\text{ negative definite } \n\\end{aligned}\n\\]\\(\\mathbf{x}^{'} \\mathbf{} \\mathbf{x} >0\\) \\(\\mathbf{x}\\) \\(\\mathbf{x}^{'} \\mathbf{} \\mathbf{x}<0\\) \\(\\mathbf{x}\\), say \\(\\mathbf{}\\) indefinite.","code":""},{"path":"multivariable-differentiation.html","id":"second-derivative-test-1","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.4.3.2 Second derivative test","text":"\\(\\mathbf{H}(f)(\\mathbf{})\\) positive definite \\(\\mathbf{}\\) local minimumIf \\(\\mathbf{H}(f)(\\mathbf{})\\) negative definite \\(\\mathbf{}\\) local maximumIf \\(\\mathbf{H}(f)(\\mathbf{})\\) indefinite \\(\\mathbf{}\\) saddle point","code":""},{"path":"multivariable-differentiation.html","id":"use-the-determinant-to-assess-definiteness","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.4.3.3 Use the determinant to assess definiteness","text":"measure definiteness now \\(\\mathbf{x}\\) vector? can use determinant Hessian \\(f\\) critical value \\(\\mathbf{}\\):\\[\n\\mathbf{H}(f)(\\mathbf{}) = \\begin{bmatrix} \n    & B \\\\\n    B & C \\\\\n\\end{bmatrix} \n\\]determinant \\(2 \\times 2\\) matrix can easily calculated using known formula \\(AC - B^2\\).\\(AC - B^2> 0\\) \\(>0\\) \\(\\leadsto\\) positive definite \\(\\leadsto\\) \\(\\mathbf{}\\) local minimum\\(AC - B^2> 0\\) \\(<0\\) \\(\\leadsto\\) negative definite \\(\\leadsto\\) \\(\\mathbf{}\\) local maximum\\(AC - B^2<0\\) \\(\\leadsto\\) indefinite \\(\\leadsto\\) saddle point\\(AC- B^2 = 0\\) inconclusive","code":""},{"path":"multivariable-differentiation.html","id":"basic-procedure-summarized","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.4.4 Basic procedure summarized","text":"Calculate gradientSet equal zero, solve system equationsCalculate HessianAssess Hessian critical valuesBoundary values? (relevant)","code":""},{"path":"multivariable-differentiation.html","id":"a-simple-optimization-example","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.5 A simple optimization example","text":"Suppose \\(f:\\Re^{2} \\rightarrow \\Re\\) \\[f(x_{1}, x_{2}) = 3(x_1 + 2)^2  + 4(x_{2}  + 4)^2  \\]Calculate gradient:\\[\n\\begin{aligned}\n\\nabla f(\\mathbf{x}) &= (6 x_{1} + 12 , 8x_{2} + 32 )  \\\\\n\\mathbf{0} &= (6 x_{1}^{*} + 12 , 8x_{2}^{*} + 32 )  \n\\end{aligned}\n\\]now solve system equations yield\\[x_{1}^{*}  = - 2, \\quad x_{2}^{*}  = -4\\]\\[\n\\textbf{H}(f)(\\mathbf{x}^{*}) = \\begin{bmatrix}\n6 & 0 \\\\\n0 & 8 \\\\\n\\end{bmatrix} \n\\]\\(\\det(\\textbf{H}(f)(\\mathbf{x}^{*}))\\) = 48 \\(6>0\\) \\(\\textbf{H}(f)(\\mathbf{x}^{*})\\) positive definite. \\(\\mathbf{x^{*}}\\) local minimum.","code":""},{"path":"multivariable-differentiation.html","id":"maximum-likelihood-estimation-for-a-normal-distribution","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.6 Maximum likelihood estimation for a normal distribution","text":"Suppose draw independent identically distributed random sample \\(n\\) observations normal distribution,\\[\n\\begin{aligned}\nY_{} &\\sim \\text{Normal}(\\mu, \\sigma^2)  \\\\  \n\\mathbf{Y} &= (Y_{1}, Y_{2}, \\ldots, Y_{n} )   \n\\end{aligned}\n\\]task:Obtain likelihood (summary estimator)Derive maximum likelihood estimators \\(\\mu\\) \\(\\sigma^2\\)\\[\n\\begin{aligned}\nL(\\mu, \\sigma^2 | \\mathbf{Y} ) &\\propto \\prod_{=1}^{n} f(Y_{}|\\mu, \\sigma^2) \\\\  \n&\\propto  \\prod_{=1}^{N} \\frac{\\exp[ - \\frac{ (Y_{} - \\mu)^2 }{2\\sigma^2} ]}{\\sqrt{2 \\pi \\sigma^2}} \\\\\n&\\propto \\frac{\\exp[ -\\sum_{=1}^{n} \\frac{(Y_{} - \\mu)^2}{2\\sigma^2}  ]}{ (2\\pi)^{n/2} \\sigma^{2n/2} }\n \\end{aligned}\n\\]Taking logarithm, \\[l(\\mu, \\sigma^2|\\mathbf{Y} ) = -\\sum_{=1}^{n} \\frac{(Y_{} - \\mu)^2}{2\\sigma^2} - \\frac{n}{2} \\log(2 \\pi) - \\frac{n}{2} \\log (\\sigma^2)\\]Let’s find \\(\\widehat{\\mu}\\) \\(\\widehat{\\sigma}^{2}\\) maximizes log-likelihood.\\[\n\\begin{aligned}\nl(\\mu, \\sigma^2|\\mathbf{Y} ) &=  -\\sum_{=1}^{n} \\frac{(Y_{} - \\mu)^2}{2\\sigma^2} - \\frac{n}{2} \\log (\\sigma^2) \\\\\n\\frac{\\partial l(\\mu, \\sigma^2)|\\mathbf{Y} )}{\\partial \\mu }  &= \\sum_{=1}^{n} \\frac{2(Y_{} - \\mu)}{2\\sigma^2} \\\\\n\\frac{\\partial l(\\mu, \\sigma^2)|\\mathbf{Y})}{\\partial \\sigma^2} &=  -\\frac{n}{2\\sigma^2}  + \\frac{1}{2\\sigma^4} \\sum_{=1}^{n} (Y_{} - \\mu)^2\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\n0 &= -\\sum_{=1}^{n} \\frac{2(Y_{} - \\widehat{\\mu})}{2\\widehat{\\sigma}^2} \\\\\n0 &=  -\\frac{n}{2\\widehat{\\sigma}^2 }  + \\frac{1}{2\\widehat{\\sigma}^4} \\sum_{=1}^{n} (Y_{} - \\mu^{*})^2 \n\\end{aligned}\n\\]Solving \\(\\widehat{\\mu}\\) \\(\\widehat{\\sigma}^2\\) yields,\\[\n\\begin{aligned}\n\\widehat{\\mu} &= \\frac{\\sum_{=1}^{n} Y_{} }{n} \\\\\n\\widehat{\\sigma}^{2} &= \\frac{1}{n} \\sum_{=1}^{n} (Y_{} - \\overline{Y})^2\n\\end{aligned}\n\\]\\[\n\\textbf{H}(f)(\\widehat{\\mu}, \\widehat{\\sigma}^2)  = \n \\begin{bmatrix} \n\\frac{\\partial^{2} l(\\mu, \\sigma^2|\\mathbf{Y} )}{\\partial \\mu^{2}} & \\frac{\\partial^{2} l(\\mu, \\sigma^2|\\mathbf{Y} )}{\\partial \\sigma^{2} \\partial \\mu} \\\\\n\\frac{\\partial^{2} l(\\mu, \\sigma^2|\\mathbf{Y} )}{\\partial \\sigma^{2} \\partial \\mu} & \\frac{\\partial^{2} l(\\mu, \\sigma^2|\\mathbf{Y} )}{\\partial^{2} \\sigma^{2}} \n\\end{bmatrix}\n\\]Taking derivatives evaluating MLE’s yields,\\[\n\\textbf{H}(f)(\\widehat{\\mu}, \\widehat{\\sigma}^2) = \\begin{bmatrix} \\frac{-n}{\\widehat{\\sigma}^2} & 0 \\\\\n0 & \\frac{-n}{2(\\widehat{\\sigma}^2)^2}  \\\\\n\\end{bmatrix}\n\\]\\(\\text{det}(\\textbf{H}(f)(\\widehat{\\mu}, \\widehat{\\sigma}^2)) = \\dfrac{n^2}{2(\\widehat{\\sigma}^2)^3} > 0\\) \\(= \\dfrac{-n}{\\widehat{\\sigma}^2} < 0\\) \\(\\leadsto\\) maximumDeterminant greater 0 \\(\\) less zero - local maximum","code":""},{"path":"multivariable-differentiation.html","id":"computational-optimization-procedures-1","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.7 Computational optimization procedures","text":"previous example suggests, analytical approaches can difficult impossible many multivariate functions. Computational approaches simplify problem.","code":""},{"path":"multivariable-differentiation.html","id":"multivariate-newton-raphson","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.7.1 Multivariate Newton-Raphson","text":"Suppose \\(f:\\Re^{n} \\rightarrow \\Re\\). Suppose guess \\(\\mathbf{x}_{t}\\). update :\\[\\mathbf{x}_{t+1} = \\mathbf{x}_{t} - [\\textbf{H}(f)(\\mathbf{x}_{t})]^{-1} \\nabla f(\\mathbf{x}_{t})\\]Approximate function tangent planeFind value \\(x_{t+1}\\) makes plane equal zeroUpdate ","code":""},{"path":"multivariable-differentiation.html","id":"drawbacks","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.7.1.1 Drawbacks","text":"Expensive calculate (requires inverting Hessian)sensitive starting points","code":""},{"path":"multivariable-differentiation.html","id":"grid-search-1","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.7.2 Grid search","text":"Example: MLE normal distributionIn R, drew 10,000 realizations \\(Y_{} \\sim \\text{Normal}(0.25, 100)\\)Used realized values \\(y_{}\\) evaluate \\(l(\\mu, \\sigma^2| \\mathbf{y} )\\) across range valuesComputationally inefficient - try large number combinations parameters","code":""},{"path":"multivariable-differentiation.html","id":"gradient-descent-1","chapter":"Day 5 Functions of several variables and optimization with several variables","heading":"5.7.3 Gradient descent","text":"approach , now derivative vector (.e. gradient, hence name approach “gradient” descent).\\[f(x, y) = x^2 + 2y^2\\]","code":""},{"path":"integral-calculus.html","id":"integral-calculus","chapter":"Day 6 Integration and integral calculus","heading":"Day 6 Integration and integral calculus","text":"","code":""},{"path":"integral-calculus.html","id":"learning-objectives-5","chapter":"Day 6 Integration and integral calculus","heading":"Learning objectives","text":"Summarize areas, slices, integralsApply common rules calculate definite integralsDefine fundamental theorem calculusCalculate antiderivatives integrals using integration substitution integration partsEvaluate improper integralsEvaluate multivariate integrals","code":""},{"path":"integral-calculus.html","id":"supplemental-readings-5","chapter":"Day 6 Integration and integral calculus","heading":"Supplemental readings","text":"Chapters 19-20, Pemberton Rau (2011)OpenStax Calculus: Volume 2, ch 1-2, 3.1, 3.7","code":""},{"path":"integral-calculus.html","id":"prepare-for-the-journey","chapter":"Day 6 Integration and integral calculus","heading":"6.1 Prepare for the journey","text":"\nFigure 6.1: Differentiation Integration. Source: xkcd\n","code":""},{"path":"integral-calculus.html","id":"indefinite-integration","chapter":"Day 6 Integration and integral calculus","heading":"6.2 Indefinite integration","text":"far, ’ve interested finding derivative \\(f=F'\\) function \\(F\\). However, sometimes ’re interested exactly reverse: finding function \\(F\\) \\(f\\) derivative. refer \\(F\\) antiderivative \\(f\\).example antiderivative \\(f(x) = \\frac{1}{x^2}\\)?\\[\n\\begin{aligned}\n\\int \\frac{1}{x^2} \\,dx &= -\\frac{1}{x} + c \\\\\n\\frac{d}{dx} \\left[ -\\frac{1}{x} + c \\right] &= \\frac{1}{x^2}\n\\end{aligned}\n\\]know derivatives manipulate \\(F\\) get \\(f\\). express procedure manipulate \\(f\\) get \\(F\\)? , need new symbol, call indefinite integration. indefinite integral \\(f(x)\\) written\\[F(x) = \\int f(x) \\,dx \\]","code":""},{"path":"integral-calculus.html","id":"many-possible-antiderivatives","chapter":"Day 6 Integration and integral calculus","heading":"6.2.1 Many possible antiderivatives","text":"Draw function \\(f(x)\\) indefinite integral, \\(\\int\\limits f(x) \\,dx\\)\\[f(x) = (x^2-4)\\]indefinite integral function \\(f(x) = (x^2-4)\\) can, example, \\(F(x) = \\frac{1}{3}x^3 - 4x\\). can also \\(F(x) = \\frac{1}{3}x^3 - 4x + 1\\), constant 1 disappears taking derivative.\nFigure 6.2: Many Indefinite Integrals Function\nNotice examples single derivative function, multiple antiderivatives: one arbitrary constant \\(c\\). \\(c\\) just shifts curve \\(y\\)-axis. information present antiderivative — e.g., passes particular point — can solve specific value \\(c\\).","code":""},{"path":"integral-calculus.html","id":"common-rules-of-integration","chapter":"Day 6 Integration and integral calculus","heading":"6.2.2 Common rules of integration","text":"common rules integrals follow virtue inverse derivative.Constants allowed slip : \\(\\int f(x)\\,dx = \\int f(x)\\,dx\\)Integration sum sum integrations: \\(\\int [f(x)+g(x)]\\,dx=\\int f(x)\\,dx + \\int g(x)\\,dx\\)Reverse Power-rule: \\(\\int x^n \\,dx = \\frac{1}{n+1} x^{n+1} + c\\)Exponents still exponents: \\(\\int e^x \\,dx = e^x +c\\)Recall derivative \\(\\log(x)\\) one \\(x\\), : \\(\\int \\frac{1}{x} \\,dx = \\log x + c\\)Reverse chain-rule: \\(\\int e^{f(x)}f^\\prime(x)\\,dx = e^{f(x)}+c\\)generally: \\(\\int [f(x)]^n f'(x)\\,dx = \\frac{1}{n+1}[f(x)]^{n+1}+c\\)Remember derivative log function: \\(\\int \\frac{f^\\prime(x)}{f(x)}\\,dx=\\log f(x) + c\\)","code":""},{"path":"integral-calculus.html","id":"practice-integrating-functions","chapter":"Day 6 Integration and integral calculus","heading":"6.2.3 Practice integrating functions","text":"Simplify following indefinite integrals:\\(\\int 3x^2 \\,dx\\)\nClick solution\n\nFactor constant \\(3\\) reverse power rule.\n\\[\n   \\begin{aligned}\n   \\int 3x^2 \\,dx &= 3 \\int x^2 \\,dx \\\\\n   &= 3 \\left( \\frac{x^3}{3} + c \\right) \\\\\n   &= x^3 + c\n   \\end{aligned}\n   \\]\n\n\\(\\int 3x^2 \\,dx\\)Click solution\nFactor constant \\(3\\) reverse power rule.\\[\n   \\begin{aligned}\n   \\int 3x^2 \\,dx &= 3 \\int x^2 \\,dx \\\\\n   &= 3 \\left( \\frac{x^3}{3} + c \\right) \\\\\n   &= x^3 + c\n   \\end{aligned}\n   \\]\\(\\int (2x+1) \\,dx\\)\nClick solution\n\nIntegrate sum term term factor constants.\n\\[\n   \\begin{aligned}\n   \\int (2x+1) \\,dx &= 2 \\int x \\,dx + \\int 1 \\,dx \\\\\n   &= x^2 + c + \\int 1 \\,dx \\\\\n   &= x^2 + x + c\n   \\end{aligned}\n   \\]\n\n\\(\\int (2x+1) \\,dx\\)Click solution\nIntegrate sum term term factor constants.\\[\n   \\begin{aligned}\n   \\int (2x+1) \\,dx &= 2 \\int x \\,dx + \\int 1 \\,dx \\\\\n   &= x^2 + c + \\int 1 \\,dx \\\\\n   &= x^2 + x + c\n   \\end{aligned}\n   \\]\\(\\int e^x e^{e^x} \\,dx\\)\nClick solution\n\nReverse chain-rule knowledge antiderivative \\(e^x\\).\n\\[\\int e^x e^{e^x} \\,dx = e^{e^x} + c\\]\n\n\\(\\int e^x e^{e^x} \\,dx\\)Click solution\nReverse chain-rule knowledge antiderivative \\(e^x\\).\\[\\int e^x e^{e^x} \\,dx = e^{e^x} + c\\]","code":""},{"path":"integral-calculus.html","id":"the-definite-integral-area-under-the-curve","chapter":"Day 6 Integration and integral calculus","heading":"6.3 The definite integral: area under the curve","text":"indefinite integral, must definite integral. Indeed , notion definite integrals comes different objective: finding area function.Suppose want determine area \\((R)\\) region \\(R\\) defined curve \\(f(x)\\) interval \\(\\le x \\le b\\).\nFigure 6.3: Riemann Integral Sum Evaluations\nOne way calculate area divide interval \\(\\le x\\le b\\) \\(n\\) subintervals length \\(\\Delta x\\) approximate region series rectangles, base rectangle \\(\\Delta x\\) height \\(f(x)\\) midpoint interval. \\((R)\\) approximated area union rectangles, given \\[S(f,\\Delta x)=\\sum\\limits_{=1}^n f(x_i)\\Delta x\\]called Riemann sum.decrease size subintervals \\(\\Delta x\\), making rectangles “thinner,” expect approximation area region become closer true area. allows us express area limit series:\\[(R)=\\lim\\limits_{\\Delta x\\0}\\sum\\limits_{=1}^n f(x_i)\\Delta x\\]define “Definite” Integral.","code":""},{"path":"integral-calculus.html","id":"the-definite-integral-riemann","chapter":"Day 6 Integration and integral calculus","heading":"6.3.1 The definite integral (Riemann)","text":"given function \\(f\\) Riemann sum approaches limit \\(\\Delta x \\0\\), limit called Riemann integral \\(f\\) \\(\\) \\(b\\). express \\(\\int\\), symbol, write\\[\\lim\\limits_{\\Delta x\\0} \\sum\\limits_{=1}^n f(x_i)\\Delta x = \\int\\limits_a^b f(x) \\,dx\\]read\\[\\int\\limits_a^b f(x) \\,dx\\]definite integral \\(f\\) \\(\\) \\(b\\), area “curve” \\(f(x)\\) point \\(x=\\) \\(x=b\\).Theorem 6.1  (Continuous functions) Suppose \\(f:[,b] \\rightarrow \\Re\\) continuous function. \\(f\\) integrable.Theorem 6.2  (Monotonic functions) Suppose \\(f:[,b]\\rightarrow \\Re\\) monotonic function. \\(f\\) integrable.","code":""},{"path":"integral-calculus.html","id":"counterexamples","chapter":"Day 6 Integration and integral calculus","heading":"6.3.2 Counterexamples","text":"Example 6.1  Suppose \\(f:[0,1]\\rightarrow \\frac{1}{x}\\)\\[\\int_{0}^{1} \\frac{1}{x} \\,dx\\]\\(\\frac{1}{x}\\) integrable \\([,b]\\) area integral represent infinite.Example 6.2  \\[\n\\begin{aligned}\nf(x) &= 1 \\text{ } x \\text{ rational}  \\\\\n&= 0 \\text{ } x \\text{ irrational}\n\\end{aligned}\n\\]integrable, every interval contain discontinuous jump.","code":""},{"path":"integral-calculus.html","id":"fundamental-theorem-of-calculus","chapter":"Day 6 Integration and integral calculus","heading":"6.3.3 Fundamental theorem of calculus","text":"fundamental theorem calculus shows us Riemann sum , fact, antiderivative. Let function \\(f\\) bounded \\([,b]\\) continuous \\((,b)\\). , use symbol \\(F(x)\\) denote definite integral \\(\\) \\(x\\):\\[F(x)=\\int\\limits_a^x f(t) \\,dt, \\quad \\le x\\le b\\]\\(F(x)\\) derivative point \\((,b)\\) \\[F^\\prime(x)=f(x), \\quad <x<b\\], definite integral function \\(f\\) one antiderivatives \\(f\\).long way saying differentiation inverse integration. now, ’ve covered definite integrals.second theorem gives us simple way computing definite integral function indefinite integrals. Let function \\(f\\) bounded \\([,b]\\) continuous \\((,b)\\). Let \\(F\\) function continuous \\([,b]\\) \\(F'(x)=f(x)\\) \\((,b)\\). \\[\\int\\limits_a^bf(x)\\,dx = F(b)-F()\\]procedure calculate simple definite integral \\(\\int\\limits_a^b f(x)\\,dx\\) thenFind indefinite integral \\(F(x)\\).Evaluate \\(F(b)-F()\\).","code":""},{"path":"integral-calculus.html","id":"uniform-distribution","chapter":"Day 6 Integration and integral calculus","heading":"6.3.3.1 Uniform distribution","text":"Suppose \\(f:\\Re \\rightarrow \\Re\\), \\[\n\\begin{aligned}\nf(x) &=  1 \\text{  } x \\[0,1]  \\\\\nf(x) &= 0 \\text{ otherwise }\n\\end{aligned}\n\\]area \\(f(x)\\) \\([0, 1/2]\\)?\\[\n\\begin{aligned}\n\\int_{0}^{1/2}  f(x)\\,dx &= \\int_{0}^{1/2} 1 \\,dx \\\\\n    &= x|_{0}^{1/2}  \\\\\n    &= (1/2) - (0 )  \\\\\n                            &= 1/2 \n\\end{aligned}\n\\]call \\(f(x) = 1\\) uniform distribution.","code":""},{"path":"integral-calculus.html","id":"area-under-a-line","chapter":"Day 6 Integration and integral calculus","heading":"6.3.3.2 Area under a line","text":"Suppose \\(f:\\Re \\rightarrow \\Re\\), \\[f(x) = x\\]Evaluate \\(\\int_{2}^{t}f(x)\\,dx\\).\\[\n\\begin{aligned}\n\\int_{2}^{t}f(x)\\,dx &= \\int_{2}^{t} x \\,dx  \\\\\n    &= \\frac{x^{2} }{2} |_{2}^{t}   \\\\\n        &= \\frac{t^2}{2} - \\frac{2^2}{2}  \\\\\n    &= \\frac{t^2}{2} - \\frac{4}{2} = \\frac{t^2}{2} - 2\n\\end{aligned}                   \n\\]","code":""},{"path":"integral-calculus.html","id":"area-under-a-curve","chapter":"Day 6 Integration and integral calculus","heading":"6.3.3.3 Area under a curve","text":"Solve \\(\\int\\limits_1^3 3x^2 \\,dx\\)\\[\n\\begin{aligned}\nf(x) &= 3x^2 \\\\\nF(x) &= x^3 + c \\\\\n\\int\\limits_1^3 3x^2 \\,dx &= F(3) - F(1) \\\\\n&= (3^3 + c) - (1^3 + c) \\\\\n&= 27 + c - 1 - c \\\\\n&= 26\n\\end{aligned}\n\\]","code":""},{"path":"integral-calculus.html","id":"common-rules-for-definite-integrals","chapter":"Day 6 Integration and integral calculus","heading":"6.3.4 Common rules for definite integrals","text":"area-interpretation definite integral provides rules simplification.area point:\n\\[\\int\\limits_a^f(x)\\,dx=0\\]area point:\\[\\int\\limits_a^f(x)\\,dx=0\\]Reversing limits changes sign integral:\n\\[\\int\\limits_a^b f(x)\\,dx=-\\int\\limits_b^f(x)\\,dx\\]Reversing limits changes sign integral:\\[\\int\\limits_a^b f(x)\\,dx=-\\int\\limits_b^f(x)\\,dx\\]Sums can separated integrals:\n\\[\\int\\limits_a^b [\\alpha f(x)+\\beta g(x)]\\,dx = \\alpha \\int\\limits_a^b f(x)\\,dx + \\beta \\int\\limits_a^b g(x)\\,dx\\]Sums can separated integrals:\\[\\int\\limits_a^b [\\alpha f(x)+\\beta g(x)]\\,dx = \\alpha \\int\\limits_a^b f(x)\\,dx + \\beta \\int\\limits_a^b g(x)\\,dx\\]Areas can combined long limits linked:\n\\[\\int\\limits_a^b f(x) \\,dx +\\int\\limits_b^c f(x)\\,dx = \\int\\limits_a^c f(x)\\,dx\\]Areas can combined long limits linked:\\[\\int\\limits_a^b f(x) \\,dx +\\int\\limits_b^c f(x)\\,dx = \\int\\limits_a^c f(x)\\,dx\\]","code":""},{"path":"integral-calculus.html","id":"practice-solving-definite-integrals","chapter":"Day 6 Integration and integral calculus","heading":"6.3.5 Practice solving definite integrals","text":"Simplify following definite intergrals.\\(\\int\\limits_1^1 3x^2 \\,dx\\)\nClick solution\n\n\\(0\\). Area point 0.\n\n\\(\\int\\limits_1^1 3x^2 \\,dx\\)Click solution\n\\(0\\). Area point 0.\\(\\int\\limits_0^4 (2x+1) \\,dx\\)\nClick solution\n\nIntegrate sum term factor constants.\n\\[\n   \\begin{aligned}\n   \\int\\limits_0^4 (2x+1) \\,dx &= 2 \\int_0^4 2x \\,dx + \\int_0^4 1 \\,dx \\\\\n   &= x^2|_0^4 + x |_0^4 \\\\\n   &= (4^2 - 0^2) + (4 - 0) \\\\\n   &= 16 + 4 \\\\\n   &= 20\n   \\end{aligned}\n   \\]\n\n\\(\\int\\limits_0^4 (2x+1) \\,dx\\)Click solution\nIntegrate sum term factor constants.\\[\n   \\begin{aligned}\n   \\int\\limits_0^4 (2x+1) \\,dx &= 2 \\int_0^4 2x \\,dx + \\int_0^4 1 \\,dx \\\\\n   &= x^2|_0^4 + x |_0^4 \\\\\n   &= (4^2 - 0^2) + (4 - 0) \\\\\n   &= 16 + 4 \\\\\n   &= 20\n   \\end{aligned}\n   \\]\\(\\int\\limits_{-2}^0 e^x e^{e^x} dx + \\int\\limits_0^2 e^x e^{e^x} \\,dx\\)\nClick solution\n\nLimits linked, combine together use definition antiderivative calculated earlier.\n\\[\n   \\begin{aligned}\n   \\int\\limits_{-2}^0 e^x e^{e^x} dx + \\int\\limits_0^2 e^x e^{e^x} \\,dx &= \\int\\limits_{-2}^2 e^x e^{e^x} \\,dx \\\\\n   &= e^{e^{x}} |_{-2}^2 \\\\\n   &= e^{e^{2}} - e^{e^{-2}} \\\\\n   &\\approx 1617\n   \\end{aligned}\n   \\]\n\n\\(\\int\\limits_{-2}^0 e^x e^{e^x} dx + \\int\\limits_0^2 e^x e^{e^x} \\,dx\\)Click solution\nLimits linked, combine together use definition antiderivative calculated earlier.\\[\n   \\begin{aligned}\n   \\int\\limits_{-2}^0 e^x e^{e^x} dx + \\int\\limits_0^2 e^x e^{e^x} \\,dx &= \\int\\limits_{-2}^2 e^x e^{e^x} \\,dx \\\\\n   &= e^{e^{x}} |_{-2}^2 \\\\\n   &= e^{e^{2}} - e^{e^{-2}} \\\\\n   &\\approx 1617\n   \\end{aligned}\n   \\]","code":""},{"path":"integral-calculus.html","id":"integration-by-substitution","chapter":"Day 6 Integration and integral calculus","heading":"6.3.6 Integration by substitution","text":"second fundamental theorem calculus, know quick way get definite integral first find indefinite integral, just plug bounds.Sometimes integrand (thing trying take integral ) doesn’t appear integrable using common rules antiderivatives. method one might try integration substitution, related Chain Rule.Suppose want find indefinite integral \\[\\int g(x)\\,dx\\] \\(g(x)\\) complex none formulas seen far seem apply immediately. trick come new function \\(u(x)\\) \\[g(x)=f[u(x)]u'(x).\\]introduction yet another function end simplifying things? Let’s refer antiderivative \\(f\\) \\(F\\). chain rule tells us \\[\\frac{d}{dx} F[u(x)]=f[u(x)]u'(x)\\], \\(F[u(x)]\\) antiderivative \\(g\\). can write\\[\\int g(x) \\,dx= \\int f[u(x)]u'(x)\\,dx = \\int \\frac{d}{dx} F[u(x)]\\,dx = F[u(x)]+c\\]summarize, procedure determine indefinite integral \\(\\int g(x)\\,dx\\) method substitution:Identify part \\(g(x)\\) might simplified substituting single variable \\(u\\) (function \\(x\\)).Determine \\(g(x)\\,dx\\) can reformulated terms \\(u\\) \\(du\\).Solve indefinite integral.Substitute back \\(x\\)Example 6.3  Consider\\[\\int \\frac{x}{x^2 + 1} \\,dx\\]particularly nasty solve given just rules learned . However, multiply expression \\(\\frac{2}{2}\\) pull constant denominator outside integral:\\[\\frac{1}{2} \\int \\frac{2x}{x^2 + 1} \\,dx\\]see derivative \\(x^2 + 1\\) \\(2x\\), contained numerator. \\[\n\\begin{aligned}\nu &= x^2 + 1 \\\\\ndu &= 2x \\,dx \\\\\n\\int \\frac{x}{x^2 + 1} \\,dx &= \\frac{1}{2} \\int \\frac{2x}{x^2 + 1} \\,dx \\\\\n&= \\frac{1}{2} \\int \\frac{1}{u} \\,du \\\\\n&= \\frac{1}{2} \\log(u) + c \\\\\n&= \\frac{1}{2} \\log(x^2 + 1) + c \\\\\n\\end{aligned}\n\\]Antiderivative \\(\\frac{1}{x}\\) \\(\\log(x)\\) - knownAfter integrating \\(u\\) function, substitute back original \\(x\\) based function","code":""},{"path":"integral-calculus.html","id":"integration-by-parts","chapter":"Day 6 Integration and integral calculus","heading":"6.3.7 Integration by parts","text":"Another useful integration technique integration parts, related product rule differentiation. product rule states \\[\\frac{d}{dx} f(x) g(x) =  f(x) g'(x) + g(x) f'(x)\\]Another way framing function \\(g\\) continuous derivative,\\[\\int_a^b g'(x) dx = g(b) - g()\\]Suppose \\(g(x)\\) can expressed product two functions \\(g(x) = p(x)q(x)\\). Rewriting integral applying product rule expand \\(g'(x)\\) get\\[\\int_a^b (p'(x)q(x) + p(x)q'(x)) dx = p(b)q(b) - p()q()\\]can rearrange equation read \\[\\int_a^b p'(x)q(x) dx = p(b)q(b) - p()q() - \\int_a^b p(x)q'(x) dx\\]goal decompose \\(g(x)\\) product two functions, one easy integrate becomes simpler differentiated. Denote first function \\(p'(x)\\) \\(p(x)\\) easily found, second function \\(q(x)\\).indefinite integrals, integration parts \\[\\int p'(x)q(x) dx = p(x)q(x) - \\int p(x)q'(x) dx\\]Example 6.4  Simplify following integral:\\[\\int x \\log(x) \\,dx\\]\\[\n\\begin{aligned}\np(x) &= \\frac{1}{2}x^2 \\\\\np'(x) &= x \\, dx \\\\\nq(x) &= \\log(x) \\\\\nq'(x) &= \\frac{1}{x} \\,dx \\\\\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\n\\int p'(x)q(x) dx &= p(x)q(x) - \\int p(x)q'(x) dx \\\\\n&= \\frac{1}{2}x^2 \\times \\log(x) - \\int \\frac{1}{2}x^2 \\times \\frac{1}{x} \\,dx \\\\\n&= \\frac{1}{2}x^2\\log(x) - \\int \\frac{1}{2}x \\,dx \\\\\n&= \\frac{1}{2}x^2\\log(x) - \\frac{1}{2} \\int x \\,dx \\\\\n&= \\frac{1}{2}x^2\\log(x) - \\frac{x^2}{4} + c\n\\end{aligned}\n\\]Pull constant integral\\(\\int x \\,dx = \\frac{1}{2}x^2\\) - simple application power rule","code":""},{"path":"integral-calculus.html","id":"infinite-integrals","chapter":"Day 6 Integration and integral calculus","heading":"6.4 Infinite integrals","text":"Consider definite integral\\[\n\\begin{aligned}\n\\int_1^X x^{-2} \\,dx &= -\\frac{1}{x} |_1^X \\\\\n&= -\\frac{1}{X} - -\\frac{1}{1} \\\\\n&= -\\frac{1}{X} + 1 \\\\\n&= 1 - X^{-1}\n\\end{aligned}\n\\]\\(X \\rightarrow \\infty, \\frac{1}{X} \\rightarrow 0\\). Hence\\[\\lim_{X \\rightarrow \\infty} \\int_1^X x^{-2} \\,dx = 1\\]can written compactly \\[\\int_1^\\infty x^{-2} \\,dx = 1\\]example infinite integral. Infinite refers value integral (fact finite number 1) fact integration taking place infinite integral. crucial point shaded region extends infinitely far right, tapers sufficiently sharply area finite number 1.Definition 6.1  Suppose continuous function \\(f(x)\\) defined \\(x \\geq \\), suppose integral \\(\\int_a^X f(x)\\,dx\\) approaches finite limit \\(L\\) \\(X \\rightarrow \\infty\\). write\\[\\int_a^\\infty f(x)\\,dx = L\\]case, say indefinite integral exists converges.contrast, \\(\\int_a^X f(x)\\,dx\\) approach finite limit \\(X \\rightarrow \\infty\\), say infinite integral exist diverges.Example 6.5  \\[f(x) = \\frac{1}{x}\\]\\[\n\\begin{aligned}\n\\int_{1}^{\\infty} \\frac{1}{x} \\,dx &= \\lim_{t \\rightarrow \\infty} \\int_{1}^{t} \\frac{1}{x}  \\,dx \\\\\n                                        &= \\lim_{t \\rightarrow \\infty} (\\log x)|_{1}^{t} \\\\\n                                        &= \\lim_{t \\rightarrow \\infty} (\\log t) - \\lim_{t \\rightarrow \\infty} (\\log 1) \n\\end{aligned}\n\\]converge.","code":""},{"path":"integral-calculus.html","id":"two-sided-infinite-integrals","chapter":"Day 6 Integration and integral calculus","heading":"6.4.1 Two-sided infinite integrals","text":"\\(f(x)\\) continuous function defined \\(x \\leq \\), definite integral \\(\\int_Y^f(x) \\,dx\\) approaches finite limit \\(Y \\rightarrow -\\infty\\), denote limit \\[\\int_{-\\infty}^f(x) \\,dx\\]integrals \\(\\int_{-\\infty}^f(x) \\,dx\\) \\(\\int_a^\\infty f(x) \\,dx\\) exist, denote sum \\[\\int_{-\\infty}^\\infty f(x) \\,dx\\]property extremely important discuss probability distributions, since major property probability density function given PDF \\(f(x)\\),\\[\\int_{-\\infty}^\\infty f(x) \\,dx = 1\\]","code":""},{"path":"integral-calculus.html","id":"improper-integrals","chapter":"Day 6 Integration and integral calculus","heading":"6.4.2 Improper integrals","text":"improper integral integral integrand defined one limits integration. Consider example\\[= \\int_0^1 \\frac{1}{\\sqrt{x}} \\,dx\\]case, lower limit causes problem since \\(\\frac{1}{\\sqrt{x}} \\rightarrow \\infty\\) \\(x \\rightarrow 0\\). Nevertheless, can define evaluate \\(\\) \\[= \\lim_{\\delta \\downarrow0} \\int_\\delta^1 \\frac{1}{\\sqrt{x}} \\,dx\\]\\(\\downarrow\\) means “tends .” fact,\\[\n\\begin{aligned}\n&= \\lim_{\\delta \\downarrow0} \\int_\\delta^1 \\frac{1}{\\sqrt{x}} \\,dx \\\\\n&= \\lim_{\\delta \\downarrow0} \\left[ 2\\sqrt{x} \\right]_\\delta^1 \\\\\n&= \\lim_{\\delta \\downarrow0} (2 - 2\\sqrt{\\delta}) \\\\\n&= 2\n\\end{aligned}\n\\]generally, suppose function \\(f(x)\\) defined continuous \\(< x \\leq b\\), defined \\(x = \\). integral\\[\\int_{+ \\delta}^b f(x) \\,dx\\]tends finite limit \\(\\) \\(\\delta \\downarrow 0\\), say integral \\(\\int_a^b f(x) \\,dx\\) exists value \\(\\). Otherwise, say integral \\(\\int_a^b f(x) \\,dx\\) diverges.","code":""},{"path":"integral-calculus.html","id":"monte-carlo-and-integration","chapter":"Day 6 Integration and integral calculus","heading":"6.5 Monte Carlo and integration","text":"Suppose want compute expected value function \\(g\\) \\(X\\) \\[\\E[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f_X(x) \\,dx\\]\\(f(x)\\) complicated.\\[f(x) = \\frac{\\exp\\left(- \\frac{(x- \\mu)^2}{2\\sigma^2} \\right) }{\\sqrt{2\\pi}}\\]Substituting \\(\\E[g(X)]\\) definite integral\\[\\int_{-\\infty}^{\\infty} x \\times \\frac{\\exp\\left(- \\frac{(x- \\mu)^2}{2\\sigma^2} \\right) }{\\sqrt{2\\pi}} \\,dx\\]\\(f(x)\\) probability density function \\(\\E[g(X)]\\) expected value. ’ll learn next week, now important thing focus \\[\\int_{-\\infty}^{\\infty} x \\times \\frac{\\exp\\left(- \\frac{(x- \\mu)^2}{2\\sigma^2} \\right) }{\\sqrt{2\\pi}} \\,dx\\]going incredibly difficult calculate analytically.Suppose can generate random draws \\(X\\) \\(x_1, \\ldots, x_n\\) computed arithmetic mean \\(g(x)\\) sample, Monte Carlo estimate\\[\\tilde{g_n}(x) = \\frac{1}{n} \\sum_{=1}^n g(x_i)\\]Monte Carlo estimator14 \\(\\E[g(x)]\\).\\(n \\rightarrow \\infty\\), \\(\\tilde{g_n}(x) \\leadsto \\E[g(x)]\\).15 can demonstrate using example \\(\\mu = 0, \\sigma^2 = 1\\). Analytically \\(\\E[g(x)] = 0\\).16 simulate repeatedly, can see increase number draws estimate \\(\\tilde{g_n}(x)\\) converges towards 0.","code":""},{"path":"integral-calculus.html","id":"multivariate-integration","chapter":"Day 6 Integration and integral calculus","heading":"6.6 Multivariate integration","text":"Suppose function \\(f:X \\rightarrow \\Re^{1}\\), \\(X \\subset \\Re^{2}\\). integrate function area. Suppose area, \\(\\), 2-dimensions\\[= \\{x, y : x \\[0,1], y \\[0,1] \\}\\]\\[= \\{x, y: x^2 + y^2 \\leq 1 \\}\\]\\[= \\{ x, y: x< y, x, y \\(0,2) \\}\\]calculate area function regions?Definition 6.2  (Multivariate integration) Suppose \\(f:X \\rightarrow \\Re\\) \\(X \\subset \\Re^{n}\\). say \\(f\\) integrable \\(\\subset X\\) able calculate area refined partitions \\(\\) write integral \\(=\\int_{} f(\\boldsymbol{x}) d\\boldsymbol{}\\).’s horribly abstract. extremely helpful theorem makes manageable.Theorem 6.3  (Fubini's theorem) Suppose \\(= [a_{1}, b_{1}] \\times [a_{2}, b_{2} ] \\times \\ldots \\times [a_{n}, b_{n}]\\) \\(f:\\rightarrow \\Re\\) integrable. \\[\\int_{} f(\\boldsymbol{x}) d\\boldsymbol{} = \\int_{a_{n}}^{b_{n}} \\int_{a_{n-1}}^{b_{n-1}} \\ldots \\int_{a_{2}}^{b_{2}} \\int_{a_{1}}^{b_{1}} f(\\boldsymbol{x})\\,dx_{1} \\,dx_{2} \\ldots \\,dx_{n-1} \\,dx_{n}\\]Start inside integral \\(x_{1}\\) variable, everything else constantWork inside , iteratingAt last step, arrive number","code":""},{"path":"integral-calculus.html","id":"multivariate-uniform-distribution","chapter":"Day 6 Integration and integral calculus","heading":"6.6.0.1 Multivariate uniform distribution","text":"Suppose \\(f:[0,1] \\times [0,1] \\rightarrow \\Re\\) \\(f(x_{1}, x_{2}) = 1\\) \\(x_{1}, x_{2} \\[0,1]\\times[0,1]\\). \n\\(\\int_{0}^{1}\\int_{0}^{1} f(x) \\,dx_{1} \\,dx_{2}\\)?\\[\n\\begin{aligned}\n\\int_{0}^{1}\\int_{0}^{1} f(x) \\,dx_{1} \\,dx_{2} &= \\int_{0}^{1} \\int_{0}^{1} 1 \\,dx_{1} \\,dx_{2} \\nonumber \\\\\n                                                            &= \\int_{0}^{1} x_{1}|_{0}^{1} \\,dx_{2} \\\\\n                                                            &= \\int_{0}^{1} (1 - 0) \\,dx_{2}  \\\\\n                                                            &= \\int_{0}^{1} 1 \\,dx_{2}  \\\\\n                                                            &= x_{2}|_{0}^{1}  \\\\\n                                                            &= 1 \n\\end{aligned}                                                           \n\\]","code":""},{"path":"integral-calculus.html","id":"another-example","chapter":"Day 6 Integration and integral calculus","heading":"6.6.0.2 Another example","text":"Suppose \\(f:[a_{1}, b_{1} ] \\times [a_{2}, b_{2} ] \\rightarrow \\Re\\) given \\[f(x_{1}, x_{2} ) = x_{1} x_{2}\\]Find \\(\\int_{a_{2}}^{b_{2}} \\int_{a_{1}}^{b_{1}} f(x_{1}, x_{2} )\\,dx_{1} \\,dx_{2}\\)\\[\n\\begin{aligned}\n\\int_{a_{2}}^{b_{2}} \\int_{a_{1}}^{b_{1}} f(x_{1}, x_{2} )\\,dx_{1} \\,dx_{2} &= \\int_{a_{2}}^{b_{2}} \\int_{a_{1}}^{b_{1}} x_{1} x_{2} \\,dx_{1} \\,dx_{2} \\\\\n &= \\int_{a_{2}}^{b_{2}} \\frac{x_{1}^2}{2} x_{2} |_{a_{1}}^{b_{1}} \\,dx_{2} \\\\\n &= \\frac{b_{1}^{2} - a_{1}^{2} }{2}  \\int_{a_{2}}^{b_{2}} x_{2} \\,dx_{2} \\\\\n &= \\frac{b_{1}^{2} - a_{1}^{2} }{2} \\left( \\frac{x_{2}^{2} }{2} |_{a_{2}}^{b_{2}} \\right ) \\\\ \n &= \\frac{b_{1}^{2} - a_{1}^{2} }{2} \\frac{b_{2}^{2} - a_{2}^{2} }{2}\n\\end{aligned}\n\\]","code":""},{"path":"integral-calculus.html","id":"exponential-distributions","chapter":"Day 6 Integration and integral calculus","heading":"6.6.0.3 Exponential distributions","text":"Suppose \\(f:\\Re^{2}_{+} \\rightarrow \\Re\\) \\[f(x_{1}, x_{2}) = 2 \\exp(-x_{1}) \\exp(-2 x_{2} )\\]Find \\(\\int_{0}^{\\infty} \\int_{0}^{\\infty} f(x_{1}, x_{2})\\)\\[\n\\begin{aligned}\n\\int_{0}^{\\infty} \\int_{0}^{\\infty} f(x_{1}, x_{2}) &= 2 \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\exp(-x_{1}) \\exp(-2x_{2}) \\,dx_{1} \\,dx_{2} \\\\\n&= 2 \\int_{0}^{\\infty}\\exp(-x_{1}) \\,dx_{1}\\int_{0}^{\\infty}  \\exp(-2x_{2}) \\,dx_{2}  \\\\\n&= 2 ( - \\exp(-x)|^{\\infty}_{0} ) (-\\frac{1}{2} \\exp(-2x_{2} ) |^{\\infty}_{0}  )  \\\\\n&= 2 \\left[  (- \\lim_{x_{1} \\rightarrow \\infty} \\exp(-x_{1} )  + 1 ) (-\\frac{1}{2} \\lim_{x_{2} \\rightarrow \\infty} \\exp(-2x_{2})  + \\frac{1}{2} ) \\right] \\\\\n&=  2 [ \\frac{1}{2} ]  \\\\\n&= 1\n\\end{aligned}\n\\]","code":""},{"path":"integral-calculus.html","id":"more-complicated-bounds-of-integration","chapter":"Day 6 Integration and integral calculus","heading":"6.6.1 More complicated bounds of integration","text":"far, integrated rectangles. often, interested complicated regions.?","code":""},{"path":"integral-calculus.html","id":"example-more-complicated-region","chapter":"Day 6 Integration and integral calculus","heading":"6.6.1.1 Example: more complicated region","text":"Suppose \\(f:[0,1] \\times [0,1] \\rightarrow \\Re\\), \\(f(x_{1}, x_{2}) = x_{1} + x_{2}\\). Find area function \\(x_{1} < x_{2}\\).Trick: need determine bound. \\(x_{1}< x_{2}\\), \\(x_{1}\\) can take value \\(0\\) \\(x_{2}\\).\\[\n\\begin{aligned}\n\\iint_{x_{1}< x_{2}}  f(\\boldsymbol{x}) &= \\int_{0}^{1} \\int_{0}^{x_{2}} x_{1} + x_{2} \\,dx_{1} \\,dx_{2}  \\\\\n&= \\int_{0}^{1} x_{2} x_{1} |_{0}^{x_{2}} \\,dx_{2}  + \\int_{0}^{1} \\frac{x_{1}^{2} }{2} |_{0}^{x_{2} }  \\\\\n&= \\int_{0}^{1} x_{2}^{2} \\,dx_{2}  + \\int_{0}^{1} \\frac{x_{2}^2}{2}  \\\\\n&= \\frac{x_{2}^{3} }{3}|_{0}^{1} + \\frac{x_{2}^{3}}{6}|_{0}^{1}  \\\\\n&= \\frac{1}{3}  + \\frac{1}{6}  \\\\\n&= \\frac{3}{6} = \\frac{1}{2}  \n\\end{aligned}\n\\]Consider function let’s switch bounds.\\[\n\\begin{aligned}\n\\iint_{x_{1}<x_{2}} f(\\boldsymbol{x}) &= \\int_{0}^{1} \\int_{x_{1}}^{1} x_{1} + x_{2} \\,dx_{2} \\,dx_{1} \\\\\n                                                    &= \\int_{0}^{1} x_{1}x_{2}|_{x_{1}}^{1}  +  \\int_{0}^{1} \\frac{x_{2}^{2}}{2} |_{x_{1}}^{1}\\,dx_{1}  \\\\\n                                                    &= \\int_{0}^{1} x_{1} - x_{1}^2 + \\int_{0}^{1} \\frac{1}{2} - \\frac{x_{1}^2}{2} \\,dx_{1}  \\\\\n                                                    &=  \\frac{x_{1}^2}{2}|_{0}^{1}  - \\frac{x_{1}^{3}}{3}|_{0}^{1}  + \\frac{x_{1}}{2}|_{0}^{1}  - \\frac{x_{1}^{3}}{6}|_{0}^{1}  \\\\\n                                                    &= \\frac{1}{2}  - \\frac{1}{3}  + \\frac{1}{2} - \\frac{1}{6}  \\\\\n                                                    &= 1 - \\frac{3}{6}  \\\\\n                                                    &= \\frac{1}{2}  \n\\end{aligned}                                                   \n\\]","code":""},{"path":"integral-calculus.html","id":"acknowledgments","chapter":"Day 6 Integration and integral calculus","heading":"Acknowledgments","text":"materials drawn Harvard Government Math Prefresher","code":""},{"path":"sample-space-probability.html","id":"sample-space-probability","chapter":"Day 7 Sample space and probability","heading":"Day 7 Sample space and probability","text":"","code":""},{"path":"sample-space-probability.html","id":"learning-objectives-6","chapter":"Day 7 Sample space and probability","heading":"Learning objectives","text":"Review set notation operationsDefine probabilistic modelsDescribe conditional probabilityDefine total probability theoremImplement Bayes’ RuleDefine evaluate independence eventsIdentify importance counting possible events","code":""},{"path":"sample-space-probability.html","id":"supplemental-readings-6","chapter":"Day 7 Sample space and probability","heading":"Supplemental readings","text":"Chapter 1, Bertsekas Tsitsiklis (2008)Equivalent reading Bertsekas Tsitsiklis lecture notes","code":""},{"path":"sample-space-probability.html","id":"model-of-probability","chapter":"Day 7 Sample space and probability","heading":"7.1 Model of probability","text":"Sample space - set things happenEvents - subsets sample spaceProbability - chance event","code":""},{"path":"sample-space-probability.html","id":"sample-space","chapter":"Day 7 Sample space and probability","heading":"7.2 Sample space","text":"sample space set things can occur. collect distinct outcomes set \\(\\Omega\\).Example 7.1  (Congressional elections) Members U.S. House Representatives elected every 2 years. consider outcomes incumbent running reelection, two possible outcomes: Win (\\(W\\)) win (\\(N\\)) (.e. lose)One incumbent: \\(\\Omega = \\{W, N\\}\\)Two incumbents: \\(\\Omega = \\{(W,W), (W,N), (N,W), (N,N)\\}\\)435 incumbents: \\(\\Omega = 2^{435}\\) possible outcomes (permutations)Example 7.2  (Number countries signing treaties) \\[\\Omega = \\{0, 1, 2, \\ldots, 194\\}\\]Example 7.3  (Duration parliamentary governments) parliamentary democracy, government defined ruling political part parliament. remains power government collapses vote confidence.17All non-negative real numbers: \\([0, \\infty)\\)\\(\\Omega = \\{x : 0 \\leq x < \\infty\\}\\)\npossible \\(x\\) \\(x\\) 0 infinity\npossible \\(x\\) \\(x\\) 0 infinity","code":""},{"path":"sample-space-probability.html","id":"events","chapter":"Day 7 Sample space and probability","heading":"7.3 Events","text":"Events subset sample space:Example 7.4  (Congressional elections) Consider one incumbent. Possible events \\[\n\\begin{aligned}\nE &= W \\\\\nF &= N\n\\end{aligned}\n\\]Now consider two incumbents. Possible events include\\[\n\\begin{aligned}\nE &= \\{(W, N), (W, W) \\} \\\\\nF &= \\{(N, N)\\}\n\\end{aligned}\n\\]\nNow consider 435 incumbents running election. extraordinarily large possible events occur. Consider just two possible examples:Outcome 2016 election - one eventAll outcomes Dems retake control House - one eventNotation: \\(x\\) element set \\(E\\)\\[\n\\begin{aligned}\nx &\\E \\\\\n\\{N, N\\} &\\E\n\\end{aligned}\n\\]","code":""},{"path":"sample-space-probability.html","id":"event-operations","chapter":"Day 7 Sample space and probability","heading":"7.3.1 Event operations","text":"\\(E\\) set, collection distinct objects. can perform operations sets create new sets. Consider two example sets:\\[\n\\begin{aligned}\nE &= \\{ (W,W), (W,N) \\} \\\\\nF &= \\{ (N, N), (W,N) \\} \\\\\n\\Omega &= \\{(W,W), (W,N), (N,W), (N,N) \\}\n\\end{aligned}\n\\]Operations determine lies new set \\(E^{\\text{new}}\\).Union: \\(\\cup\\)\nobjects appear either set ()\n\\(E^{\\text{new}} = E \\cup F = \\{(W,W), (W,N), (N,N) \\}\\)\nobjects appear either set ()\\(E^{\\text{new}} = E \\cup F = \\{(W,W), (W,N), (N,N) \\}\\)Intersection: \\(\\cap\\)\nobjects appear sets ()\n\\(E^{\\text{new}} = E \\cap F = \\{(W,N)\\}\\)\nobjects appear sets ()\\(E^{\\text{new}} = E \\cap F = \\{(W,N)\\}\\)Complement set \\(E\\): \\(E^{c}\\)\nobjects \\(S\\) \\(E\\)\n\\(E^{c} = \\{(N, W) , (N, N) \\}\\)\n\\(F^{c} = \\{(N, W) , (W, W) \\}\\)\nobjects \\(S\\) \\(E\\)\\(E^{c} = \\{(N, W) , (N, N) \\}\\)\\(F^{c} = \\{(N, W) , (W, W) \\}\\)\\(\\Omega^{c}\\)? - empty set \\(\\emptyset\\). Suppose \\(E = {W}\\), \\(F = {N}\\). \\(E \\cap F = \\emptyset\\) (nothing lies sets).Definition 7.1  (Mutually exclusive) Suppose \\(E\\) \\(F\\) events. \\(E \\cap F = \\emptyset\\) ’ll say \\(E\\) \\(F\\) mutually exclusive.Mutual exclusivity \\(\\neq\\) independence\\(E\\) \\(E^{c}\\) mutually exclusive eventsExample 7.5  Consider act flipping coin can land either\\[\n\\begin{aligned}\nH &= \\text{heads} \\\\\nT &= \\text{tails}\n\\end{aligned}\n\\]Suppose \\(S = \\{H, T\\}\\). \\(E = H\\) \\(F = T\\), \\(E \\cap F = \\emptyset\\)Example 7.6  Suppose\\[\n\\begin{aligned}\nS &= \\{(H, H), (H,T), (T, H), (T,T) \\} \\\\\nE &= \\{(H,H)\\} \\\\\nF &= \\{(H, H), (T,H)\\} \\\\\nG &= \\{(H, T), (T, T) \\}\n\\end{aligned}\n\\]\\(E \\cap F = (H, H)\\)\\(E \\cap G = \\emptyset\\)\\(F \\cap G = \\emptyset\\)Example 7.7  Suppose\\[\n\\begin{aligned}\nS &= \\Re_{+} \\quad \\text{(real positive number)} \\\\\nE &= \\{x: x> 10\\} \\\\\nF &= \\{x: x < 5\\}\n\\end{aligned}\n\\]\\(E \\cap F = \\emptyset\\).Definition 7.2  Suppose events \\(E_{1}, E_{2}, \\ldots, E_{N}\\).\\[\\cup_{=1}^{N} E_{} = E_{1} \\cup E_{2} \\cup E_{3} \\cup \\ldots \\cup E_{N}\\]\\(\\cup_{=1}^{N} E_{}\\) set outcomes occur least \\(E_{1} , \\ldots, E_{N}\\).\\[\\cap_{=1}^{N} E_{} = E_{1} \\cap E_{2} \\cap \\ldots \\cap E_{N}\\]\\(\\cap_{=1}^{N} E_{}\\) set outcomes occur \\(E_{}\\).","code":""},{"path":"sample-space-probability.html","id":"probability-1","chapter":"Day 7 Sample space and probability","heading":"7.4 Probability","text":"Probability chance event occurring. \\(\\Pr\\) function, domain contains events \\(E\\).","code":""},{"path":"sample-space-probability.html","id":"three-axioms","chapter":"Day 7 Sample space and probability","heading":"7.4.1 Three axioms","text":"probability functions \\(\\Pr\\) satisfy three axioms:Nonnegativity: events \\(E\\), \\(0 \\leq \\Pr(E) \\leq 1\\)Nonnegativity: events \\(E\\), \\(0 \\leq \\Pr(E) \\leq 1\\)Normalization: \\(\\Pr(S) = 1\\)Normalization: \\(\\Pr(S) = 1\\)Additivity: sequences mutually exclusive events \\(E_{1}, E_{2}, \\ldots,E_{N}\\) (\\(N\\) can go infinity):\n\\[\\Pr\\left(\\cup_{=1}^{N} E_{}  \\right)  = \\sum_{=1}^{N} \\Pr(E_{} )\\]\ncountable sequence mutually exclusive events can added together generate probability mutually exclusive events occurring\nAdditivity: sequences mutually exclusive events \\(E_{1}, E_{2}, \\ldots,E_{N}\\) (\\(N\\) can go infinity):\\[\\Pr\\left(\\cup_{=1}^{N} E_{}  \\right)  = \\sum_{=1}^{N} \\Pr(E_{} )\\]countable sequence mutually exclusive events can added together generate probability mutually exclusive events occurring","code":""},{"path":"sample-space-probability.html","id":"basic-examples","chapter":"Day 7 Sample space and probability","heading":"7.4.2 Basic examples","text":"Example 7.8  (Coin flipping) Suppose flipping fair coin. \\(\\Pr(H) = \\Pr(T) = 1/2\\).Suppose rolling six-sided die. \\(\\Pr(1) = 1/6\\).Suppose flipping pair fair coins. \\(\\Pr(H, H) = 1/4\\).Example 7.9  (Congressional incumbents) One candidate example:\\(\\Pr(W)\\): probability incumbent wins\\(\\Pr(N)\\): probability incumbent loses (win)Two candidate example:\\(\\Pr(\\{W,W\\})\\): probability incumbents win\\(\\Pr( \\{W,W\\}, \\{W, N\\} )\\): probability incumbent \\(1\\) winsFull House example:\\(\\Pr( \\{ \\text{Democrats Win}\\} )\\)’ll use data infer things.","code":""},{"path":"sample-space-probability.html","id":"rolling-the-dice","chapter":"Day 7 Sample space and probability","heading":"7.4.2.1 Rolling the dice","text":"Consider experiment rolling pair 4-sided dice. assume dice fair, interpret assumption mean sixteen possible outcomes [pairs \\((,j)\\) \\(,j = 1,2,3,4\\)] probability \\(1/16\\). calculate probablity event, must count number elements event divide 16 (total number possible outcomes). event probabilities calculated way:\\[\n\\begin{aligned}\n\\Omega &= \\{(1,1), (1,2), (1,3), (1,4), (2,1), (2,2), (2,3), (2,4), \\\\\n&\\quad (3,1), (3,2), (3,3), (3,4), (4,1), (4,2), (4,3), (4,4) \\}\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\n\\Pr (\\text{sum rolls even}) &= 8/16 = 1/2 \\\\\n\\Pr (\\text{sum rolls odd}) &= 8/16 = 1/2 \\\\\n\\Pr (\\text{first roll equal second}) &= 4/16 = 1/4 \\\\\n\\Pr (\\text{first roll larger second}) &= 6/16 = 3/8 \\\\\n\\Pr (\\text{least one roll equal 4}) &= 7/16\n\\end{aligned}\n\\]","code":""},{"path":"sample-space-probability.html","id":"surprising-probability-facts","chapter":"Day 7 Sample space and probability","heading":"7.4.3 Surprising probability facts","text":"Formalized probabilistic reasoning helps us avoid silly reasoning.“odds happened?!?!?” - perhaps great, neither non-patterns missed.“way candidate 80% chance winning, forecasted vote share 55%” - confuses different events. Chance winning \\(\\neq\\) vote share.“Group higher rate behavior, therefore behavior group ” - confuses two different problems (see Simpson’s Paradox).“low probability event, therefore God designed ”\nEven stipulate low probability event, intelligent design assumption\nLow probability obviously doesn’t imply divine intervention. Take 100 balls let sort 2 bins. ’ll get result, probability result \\(= 1/(10^{29} \\times \\text{Number Atoms Universe})\\). , vast number possible combinations 100 balls one specific combination highly unlikely.\nEven stipulate low probability event, intelligent design assumptionLow probability obviously doesn’t imply divine intervention. Take 100 balls let sort 2 bins. ’ll get result, probability result \\(= 1/(10^{29} \\times \\text{Number Atoms Universe})\\). , vast number possible combinations 100 balls one specific combination highly unlikely.","code":""},{"path":"sample-space-probability.html","id":"birthday-problem","chapter":"Day 7 Sample space and probability","heading":"7.4.3.1 Birthday problem","text":"Suppose room full \\(N\\) people. probability least 2 people birthday?Assuming leap year counts, \\(N = 367\\) guarantees least two people birthday. \\(N< 367?\\)?Actually low probability. need 23 people least \\(.5\\) probability two individuals possessing birthday.","code":""},{"path":"sample-space-probability.html","id":"e-harmony-problem","chapter":"Day 7 Sample space and probability","heading":"7.4.3.2 E-Harmony problem","text":"Also known curse dimensionality online dating.Suppose (example) 29 dimensions binary (0,1) independent.\\[\\Pr(\\text{2 people agree}) = 0.5\\]\\[\n\\begin{aligned}\n\\Pr\\text{(Exact)} &= \\Pr\\text{(Agree)}_{1} \\times \\Pr\\text{(Agree)}_{2}\\times \\ldots \\times \\Pr\\text{(Agree)}_{29}  \\\\\n&= 0.5 \\times 0.5 \\times \\ldots \\times 0.5  \\\\\n&= 0.5^{29} \\\\\n&\\approx 1.8 \\times 10^{-9} \n\\end{aligned}\n\\]probability exact match across 29 dimensions 1 536,870,912. Across many “variables” (events) agreement harder. Many approaches therefore approximate match handful dimensions. instance, \\(k\\)-nearest neighbors machine learning algorithm relaxes requirement agreement across variables instead looks observations closest many dimensions possible.","code":""},{"path":"sample-space-probability.html","id":"conditional-probability","chapter":"Day 7 Sample space and probability","heading":"7.5 Conditional probability","text":"Social scientists almost always examine conditional relationshipsGiven low-interest rates, probability high inflation?Given “economic anxiety,” probability voting Donald Trump?Given opposite political party identification, probability obtaining date Tinder?intuition event occurred: outcome realized. knowledge outcome already happened, probability something another set happens?Definition 7.3  (Conditional probability) Suppose two events, \\(E\\) \\(F\\), \\(\\Pr(F)>0\\). ,\\[\\Pr(E|F) = \\frac{\\Pr(E\\cap F ) } {\\Pr(F) }\\]\\(\\Pr(E \\cap F)\\): \\(E\\) \\(F\\) must occur\\(\\Pr(F)\\) normalize: know \\(\\Pr(F)\\) already occurred","code":""},{"path":"sample-space-probability.html","id":"examples-1","chapter":"Day 7 Sample space and probability","heading":"7.5.1 Examples","text":"Example 7.10  Suppose\\(F = \\{\\text{Democrats Win} \\}\\)\\(E = \\{\\text{Nancy Pelosi Wins (D-CA)} \\}\\)\\(F\\) occurs \\(E\\) occur, \\(\\Pr(E|F) = 1\\)Example 7.11  Suppose\\(F = \\{\\text{Democrats Win} \\}\\)\\(E = \\{ \\text{Louie Gohmert Wins (R-TX)} \\}\\)\\(F \\cap E = \\emptyset \\Rightarrow \\Pr(E|F) = \\frac{\\Pr(F \\cap E) }{\\Pr(F)} = \\frac{\\Pr(\\emptyset)}{\\Pr(F)} = 0\\)Example 7.12  (Incumbency advantage) Suppose\\(= \\{ \\text{Candidate incumbent} \\}\\)\\(D = \\{ \\text{Candidate Defeated} \\}\\)\\(\\Pr(D|) = \\frac{\\Pr(D \\cap )}{\\Pr() }\\)probability candidate defeated given candidate incumbent equal probability defeated incumbent divided probability incumbent","code":""},{"path":"sample-space-probability.html","id":"difference-between-prab-and-prba","chapter":"Day 7 Sample space and probability","heading":"7.5.2 Difference between \\(\\Pr(A|B)\\) and \\(\\Pr(B|A)\\)","text":"\\[\n\\begin{aligned}\n\\Pr(|B) & = \\frac{\\Pr(\\cap B)}{\\Pr(B)} \\\\\n\\Pr(B|) & = \\frac{\\Pr(\\cap B) } {\\Pr()}\n\\end{aligned}\n\\]values. Consider Less Serious Example \\(\\leadsto\\) type person attends football games:\nFigure 7.1: football.\n\nFigure 7.2: football.\n\\[\n\\begin{aligned}\n\\Pr(\\text{Attending football game}| \\text{Drunk}) & = 0.01  \\\\\n\\Pr(\\text{Drunk}| \\text{Attending football game}) & \\approx 1\n\\end{aligned}\n\\]","code":""},{"path":"sample-space-probability.html","id":"law-of-total-probability","chapter":"Day 7 Sample space and probability","heading":"7.6 Law of total probability","text":"Suppose set events \\(F_{1}, F_{2}, \\ldots, F_{N}\\) events mutually exclusive together comprise entire sample space \\(\\cup_{=1}^{N} F_{} = \\Omega\\). , event \\(E\\)\\[\\Pr(E) = \\sum_{=1}^{N} \\Pr(E | F_{} ) \\times \\Pr(F_{})\\]Example 7.13  (Voter mobilization) Infer \\(\\Pr(\\text{vote})\\) mobilization campaign.\\(\\Pr(\\text{vote}|\\text{mobilized} ) = 0.75\\)\\(\\Pr(\\text{vote}| \\text{mobilized} ) = 0.25\\)\\(\\Pr(\\text{mobilized}) = 0.6 ; \\Pr(\\text{mobilized} ) = 0.4\\)\\(\\Pr(\\text{vote})\\)?Sample space (one person) = \\(\\{\\) (mobilized, vote), (mobilized, vote), (mobilized, vote) , (mobilized, vote) \\(\\}\\)Mobilization partitions space (mutually exclusive exhaustive), can use law total probability:\\[\n\\begin{aligned}\n\\Pr(\\text{vote} ) & = \\Pr(\\text{vote}| \\text{mob.} ) \\times \\Pr(\\text{mob.} ) + \\Pr(\\text{vote} | \\text{mob} ) \\times \\Pr(\\text{mob}) \\\\\n& = 0.75 \\times 0.6  + 0.25 \\times 0.4   \\\\\n& = 0.55\n\\end{aligned}\n\\]Example 7.14  (Chess tournament) enter chess tournament probability winning game \\(0.3\\) half players (type 1), \\(0.4\\) quarter players (type 2), \\(0.5\\) remaining quarter players (type 3). play game randomly chosen opponent. probability winning?Let \\(A_i\\) event playing opponent type \\(\\). \\[\\Pr (A_1) = 0.5, \\quad \\Pr (A_2) = 0.25, \\quad \\Pr (A_3) = 0.25\\]Also, let \\(B\\) event winning. \\[\\Pr (B | A_1) = 0.3, \\quad \\Pr (B | A_2) = 0.4, \\quad \\Pr (B | A_3) = 0.5\\]Thus, total probability theorem, probability winning \\[\n\\begin{aligned}\n\\Pr (B) &= \\Pr (A_1) \\Pr (B | A_1) + \\Pr (A_2) \\Pr (B | A_2) + \\Pr (A_3) \\Pr (B | A_3) \\\\\n&= 0.5 \\times 0.3 + 0.25 \\times 0.4 + 0.25 \\times 0.5 \\\\\n&= 0.375\n\\end{aligned}\n\\]","code":""},{"path":"sample-space-probability.html","id":"bayes-rule","chapter":"Day 7 Sample space and probability","heading":"7.7 Bayes’ Rule","text":"\nFigure 7.3: Modified Bayes’ Theorem\n\\(\\Pr(B|)\\) may easy obtain, whereas \\(\\Pr(|B)\\) may harder determine. Bayes’ rule provides method move \\(\\Pr(B|)\\) \\(\\Pr(|B)\\)Definition 7.4  (Bayes' Rule) two events \\(\\) \\(B\\),\\[\\Pr(|B) = \\frac{\\Pr()\\times \\Pr(B|)}{\\Pr(B)} \\]proof :\\[\n\\begin{aligned}\n\\Pr(|B) & = \\frac{\\Pr(\\cap B) }{\\Pr(B) } \\\\\n& = \\frac{\\Pr(B|)\\Pr() } {\\Pr(B) } \n\\end{aligned}\n\\]Conditional probability allows us replace joint probability \\(\\) \\(B\\) alternative expressionExample 7.15  (Chess tournament redux) Let \\(A_i\\) event playing opponent type \\(\\). \\[\\Pr (A_1) = 0.5, \\quad \\Pr (A_2) = 0.25, \\quad \\Pr (A_3) = 0.25\\]Also, let \\(B\\) event winning. \\[\\Pr (B | A_1) = 0.3, \\quad \\Pr (B | A_2) = 0.4, \\quad \\Pr (B | A_3) = 0.5\\]Suppose win. probability \\(\\Pr (A_1 | B)\\) opponent type 1?Using Bayes’ rule, \\[\n\\begin{aligned}\n\\Pr (A_1 | B) &= \\frac{\\Pr (A_1) \\Pr (B | A_1)}{\\Pr (A_1) \\Pr (B | A_1) + \\Pr (A_2) \\Pr (B | A_2) + \\Pr (A_3) \\Pr (B | A_3)} \\\\\n&= \\frac{0.5 \\times 0.3}{0.5 \\times 0.3 + 0.25 \\times 0.4 + 0.25 \\times 0.5} \\\\\n&= \\frac{0.15}{0.375} \\\\\n&= 0.4\n\\end{aligned}\n\\]Example 7.16  (Identifying racial groups name) identify racial groups lists names? Census Bureau collects information distribution names race. example, Washington “blackest” name America.\\(\\Pr (\\text{black}) = 0.126\\)\\(\\Pr (\\text{black}) = 1 - \\Pr (\\text{black}) = 0.874\\)\\(\\Pr (\\text{Washington} | \\text{black}) = 0.00378\\)\\(\\Pr (\\text{Washington} | \\text{black}) = 0.000060615\\)probability black conditional name “Washington?”Using Bayes’ rule, \\[\n\\begin{aligned}\n\\Pr(\\text{black}|\\text{Wash} ) & = \\frac{\\Pr(\\text{black}) \\Pr(\\text{Wash}| \\text{black}) }{\\Pr(\\text{Wash} ) } \\\\\n & = \\frac{\\Pr(\\text{black}) \\Pr(\\text{Wash}| \\text{black}) }{\\Pr(\\text{black})\\Pr(\\text{Wash}|\\text{black}) + \\Pr(\\text{nb})\\Pr(\\text{Wash}| \\text{nb}) } \\\\\n & = \\frac{0.126 \\times 0.00378}{0.126\\times 0.00378 + 0.874 \\times 0.000060616} \\\\\n & \\approx 0.9  \n \\end{aligned}\n\\]Example 7.17  (False-positive puzzle) test certain rare disease assumed correct 95% time: person disease, test results positive probability \\(0.95\\), person disease, test results negative probability \\(0.95\\). random person drawn certain population probability \\(0.001\\) disease. Given person just tested positive, probability disease?\\(\\) event person disease, \\(B\\) event test results positive\\[\n\\begin{aligned}\n\\Pr () &= 0.001 \\\\\n\\Pr (^c) &= 0.999 \\\\\n\\Pr (B | ) &= 0.95 \\\\\n\\Pr (B | ^c) &= 0.05\n\\end{aligned}\n\\]desired probability \\(\\Pr (|B)\\) \\[\n\\begin{aligned}\n\\Pr (|B) &= \\frac{\\Pr () \\Pr (B|)}{\\Pr () \\Pr (B|) + \\Pr (^c) \\Pr (B | ^c)} \\\\\n&= \\frac{0.001 \\times 0.95}{0.001 \\times 0.95 + 0.999 \\times 0.05} \\\\\n&= 0.0187\n\\end{aligned}\n\\]","code":""},{"path":"sample-space-probability.html","id":"independence-of-probabilities","chapter":"Day 7 Sample space and probability","heading":"7.8 Independence of probabilities","text":"one event provide information another event? Two events \\(E\\) \\(F\\) independent \\[\\Pr(E\\cap F ) = \\Pr(E)\\Pr(F) \\]\\(E\\) \\(F\\) independent, ’ll say dependent. Independence symmetric: \\(F\\) independent \\(E\\), \\(E\\) indepenent \\(F\\).Suppose \\(E\\) \\(F\\) independent. ,\\[\n\\begin{aligned}\n\\Pr(E|F) & = \\frac{\\Pr(E \\cap F) }{\\Pr(F) }  \\\\\n& = \\frac{\\Pr(E)\\Pr(F)}{\\Pr(F)} \\\\\n& = \\Pr(E)  \n\\end{aligned}\n\\]Conditioning event \\(F\\) modify probability \\(E\\). information \\(E\\) \\(F\\).","code":""},{"path":"sample-space-probability.html","id":"rolling-a-4-sided-die","chapter":"Day 7 Sample space and probability","heading":"7.8.1 Rolling a 4-sided die","text":"Consider experiment involving two successive rolls 4-sided die 16 possible outcomes equally likely probability \\(1/16\\).","code":""},{"path":"sample-space-probability.html","id":"part-a","chapter":"Day 7 Sample space and probability","heading":"7.8.1.1 Part A","text":"events\\[A_i = \\{ \\text{1st roll results } \\}, \\quad B_j = \\{ \\text{2nd roll results } j \\}\\]independent? \\[\n\\begin{aligned}\n\\Pr (A_i \\cap B_j) &= \\Pr (\\text{outcome two rolls } (,j)) = \\frac{1}{16} \\\\\n\\Pr (A_i) &= \\frac{\\text{number elements } A_i}{\\text{total number possible outcomes}} = \\frac{4}{16} \\\\\n\\Pr (B_j) &= \\frac{\\text{number elements } B_j}{\\text{total number possible outcomes}} = \\frac{4}{16}\n\\end{aligned}\n\\]observe \\(\\Pr (A_i \\cap B_j) = \\Pr (A_i) \\Pr (B_j)\\), independence \\(A_i\\) \\(B_j\\) verified.","code":""},{"path":"sample-space-probability.html","id":"part-b","chapter":"Day 7 Sample space and probability","heading":"7.8.1.2 Part B","text":"events\\[= \\{ \\text{1st roll 1} \\}, \\quad B = \\{ \\text{sum two rolls 5} \\}\\]independent? answer quite obvious. \\[\\Pr (\\cap B) = \\Pr (\\text{result two rolls } (1,4)) = \\frac{1}{16}\\]also\\[\\Pr () = \\frac{\\text{number elements } }{\\text{total number possible outcomes}} = \\frac{4}{16}\\]event \\(B\\) consists outcomes \\((1,4), (2,3), (3,2), (4,1)\\), \\[\\Pr (B) = \\frac{\\text{number elements } B}{\\text{total number possible outcomes}} = \\frac{4}{16}\\]Thus, see \\(\\Pr (\\cap B) = \\Pr () \\Pr (B)\\), independence \\(A_i\\) \\(B_j\\) verified.","code":""},{"path":"sample-space-probability.html","id":"part-c","chapter":"Day 7 Sample space and probability","heading":"7.8.1.3 Part C","text":"events\\[= \\{ \\text{maximum two rolls 2} \\}, \\quad B = \\{ \\text{minimum two rolls 2} \\}\\]independent? Intuitively, answer “” minimum two rolls conveys information maximum. example, minimum \\(2\\) maximum \\(1\\). precisely, verify \\(\\) \\(B\\) indpendent, calculate\\[\\Pr (\\cap B) = \\Pr (\\text{result two rolls } (2,2)) = \\frac{1}{16}\\]also\\[\n\\begin{aligned}\n\\Pr () &= \\frac{\\text{number elements } A_i}{\\text{total number possible outcomes}} = \\frac{3}{16} \\\\\n\\Pr (B) &= \\frac{\\text{number elements } B_j}{\\text{total number possible outcomes}} = \\frac{5}{16}\n\\end{aligned}\n\\]\\(\\Pr () \\Pr (B) = \\frac{15}{16^2}\\), \\(\\Pr (\\cap B) \\neq \\Pr () \\Pr (B)\\), \\(\\) \\(B\\) independent","code":""},{"path":"sample-space-probability.html","id":"independence-and-causal-inference","chapter":"Day 7 Sample space and probability","heading":"7.8.2 Independence and causal inference","text":"Independence matters great deal conducting observational studies. often want infer effect treatment:Incumbency vote returnCollege education job earningsIn observational studies observe see make inference. problem units (e.g. people) self-select receiving treatment . simple example evaluating effectiveness job training programs. observational study, compare people enroll job training people , measure whether obtain job within six months. problem people choose enroll job training can systematically different people . ,\\[\\Pr (\\text{job} | \\text{training study}) \\neq \\Pr(\\text{job} | \\text{forced training})\\]Selection job training programs independent effect job training program . Background characteristics difference treatment control groups besides act treatment . Observational studies control possible background characteristics. Instead, experiments make background characteristics treatment status independent.","code":""},{"path":"sample-space-probability.html","id":"independence-of-a-collection-of-events","chapter":"Day 7 Sample space and probability","heading":"7.8.3 Independence of a collection of events","text":"say events \\(A_1, A_2, \\ldots, A_n\\) independent \\[\\Pr \\left( \\bigcap_{\\S} A_i \\right) = \\prod_{\\S} \\Pr (A_i),\\quad \\text{every subset } S \\text{ } \\{1,2,\\ldots,n \\}\\]case three events, \\(A_1, A_2, A_3\\), independence amounts satisfying four conditions\\[\n\\begin{aligned}\n\\Pr (A_1 \\cap A_2) &= \\Pr (A_1) \\Pr (A_2) \\\\\n\\Pr (A_1 \\cap A_3) &= \\Pr (A_1) \\Pr (A_3) \\\\\n\\Pr (A_2 \\cap A_3) &= \\Pr (A_2) \\Pr (A_3) \\\\\n\\Pr (A_1 \\cap A_2 \\cap A_3) &= \\Pr (A_1) \\Pr (A_2) \\Pr (A_3)\n\\end{aligned}\n\\]","code":""},{"path":"sample-space-probability.html","id":"independent-trials-and-the-binomial-probabilities","chapter":"Day 7 Sample space and probability","heading":"7.8.4 Independent trials and the binomial probabilities","text":"experiment involves sequence independent identical stages, say sequence independent trials. case two possible results stage, say sequence independent Bernoulli trials.Heads tailsSuccess failRains rainConsider experiment consists \\(n\\) independent tosses coin, probability heads \\(p\\), \\(p\\) number 0 1. context, independence means events \\(A_1, A_2, \\ldots, A_n\\) independent \\(A_i = \\text{th toss heads}\\).Let us consider probability\\[p(k) = \\Pr(k \\text{ heads come } n \\text{-toss sequence})\\]probability given sequence contains \\(k\\) heads \\(p^k (1-p)^{n-k}\\), \\[p(k) = \\binom{n}{k} p^k (1-p)^{n-k}\\]use notation\\[\\binom{n}{k} = \\text{number distinct } n \\text{-toss sequences contain } k \\text{ heads}\\]numbers \\(\\binom{n}{k}\\) (read “\\(n\\) choose \\(k\\)”) known binomial coefficients, probabilities \\(p(k)\\) known binomial probabilities. Using counting argument, can show \\[\\binom{n}{k} = \\frac{n!}{k! (n-k)!}, \\quad k=0,1,\\ldots,n\\]positive integer \\(\\) \\[! = 1 \\times 2 \\times \\cdots \\times (-1) \\times \\], convention, \\(0! = 1\\). Note binomial probabilities \\(p(k)\\) must sum 1, thus showing binomial formula\\[\\sum_{k=0}^n \\binom{n}{k} p^k (1-p)^{n-k} = 1\\]","code":""},{"path":"sample-space-probability.html","id":"reliability-of-an-k-out-of-n-system","chapter":"Day 7 Sample space and probability","heading":"7.8.4.1 Reliability of an \\(k\\)-out-of-\\(n\\) system","text":"system consists \\(n\\) identical components, operational probability \\(p\\), independent components. system operational least \\(k\\) \\(n\\) components operational. probability system operational?Let \\(A_i\\) event exactly \\(\\) components operational. probability system operational probability union \\(\\bigcup_{=k}^n A_i\\), since \\(A_i\\) disjoint, equal \\[\\sum_{=k}^n \\Pr (A_i) = \\sum_{=k}^n p()\\]\\(p()\\) binomial probabilities. Thus, probability operational system \\[\\sum_{=k}^n \\binom{n}{} p^(1-p)^{n-}\\]instance, \\(n=100, k=60, p=0.7\\), probability operational system 0.979.","code":""},{"path":"sample-space-probability.html","id":"counting","chapter":"Day 7 Sample space and probability","heading":"7.9 Counting","text":"Frequently need calculate total number possible outcomes sample space. example, want calculate probability event \\(\\) finite number equally likely outcomes, already known probability \\(p\\), probability \\(\\) given \\[\\Pr () = p \\times (\\text{number elements } )\\]","code":""},{"path":"sample-space-probability.html","id":"counting-principle","chapter":"Day 7 Sample space and probability","heading":"7.9.1 Counting principle","text":"Consider process consists \\(r\\) stages. Suppose :\\(n_1\\) possible results first stage.every possible result first stage, \\(n_2\\) possible results second stage.generally, sequence possible results first \\(-1\\) stages, \\(n_i\\) possible results \\(\\)th stage., total number possible results \\(r\\)-stage process \\[n_1, n_2, \\cdots, n_r\\]","code":""},{"path":"sample-space-probability.html","id":"example---telephone-numbers","chapter":"Day 7 Sample space and probability","heading":"7.9.1.1 Example - telephone numbers","text":"local telephone number 7-digit sequence, first digit different 0 1. many distinct telephone numbers ? can visualize choice sequence sequential process, select one digit time. total 7 stages, choice one 10 elements stage, except first stage 8 choices. Therefore, answer \\[8 \\times 10 \\times 10 \\times 10 \\times 10 \\times 10 \\times 10 = 8 \\times 10^6\\]","code":""},{"path":"sample-space-probability.html","id":"permutations","chapter":"Day 7 Sample space and probability","heading":"7.9.2 Permutations","text":"start \\(n\\) distinct objects, let \\(k\\) positive integer \\(k \\leq n\\). wish count number different ways can pick \\(k\\) \\(n\\) objects arrange sequence (.e. number distinct \\(k\\)-object sequences). number possible sequences, called \\(k\\)-permutations, \\[\n\\begin{aligned}\nn(n-1) \\cdots (n-k-1) &= \\frac{n(n-1) \\cdots (n-k+1) (n-k) \\cdots 2 \\times 1}{(n-k) \\cdots 2 \\times 1} \\\\\n&= \\frac{n!}{(n-k)!}\n\\end{aligned}\n\\]","code":""},{"path":"sample-space-probability.html","id":"examples-2","chapter":"Day 7 Sample space and probability","heading":"7.9.2.1 Examples","text":"Example 7.18  (Counting letters) Let us count number words consist four distinct letters. problem counting number 4-permutations 26 letters alphabet. desired number \\[\\frac{n!}{(n-k)!} = \\frac{26!}{22!} = 26 \\times 25 \\times 24 \\times 23 = 358,800\\]Example 7.19  (De Méré's puzzle) six-sided die rolled three times independently. likely: sum 11 sum 12?sum 11 obtained following 6 combinations:\n\\[(6,4,1) (6,3,2) (5,5,1) (5,4,2) (5,3,3) (4,4,3)\\]sum 11 obtained following 6 combinations:\\[(6,4,1) (6,3,2) (5,5,1) (5,4,2) (5,3,3) (4,4,3)\\]sum 12 obtained following 6 combinations:\n\\[(6,5,1) (6,4,2) (6,3,3) (5,5,2) (5,4,3) (4,4,4)\\]sum 12 obtained following 6 combinations:\\[(6,5,1) (6,4,2) (6,3,3) (5,5,2) (5,4,3) (4,4,4)\\]combination 3 distinct numbers corresponds 6 permutations, \\(k=n\\):\\[3! = 3 \\times 2 \\times 1 = 6\\]combination 3 numbers, two equal, corresponds 3 permutations.Counting number permutations 6 combinations corresponding sum 11, obtain \\(6+6+3+6+3+3 = 27\\) permutations.Counting number permutations 6 combinations corresponding sum 12, obtain \\(6 + 6 + 3 + 3 + 6 + 1 = 25\\) permutations.Since permutations equally likely, sum 11 likely sum 12.","code":""},{"path":"sample-space-probability.html","id":"combinations","chapter":"Day 7 Sample space and probability","heading":"7.9.3 Combinations","text":"\\(n\\) people interested forming committee \\(k\\). many different committees possible? Notice counting problem inside counting problem: need count number \\(k\\)-element subsets given \\(n\\)-element set. combination, ordering selected elements. example, whereas 2-permutations letters \\(, B, C, D\\) \\[AB, BA, AC, CA, AD, DA, BC, CB, BD, DB, CD, DC\\]combinations two four letters \\[AB, AC, AD, BC, BD, CD\\]example, group together duplicates distinct tabulate frequency. generally, can view combination associated \\(k!\\) duplicate \\(k\\)-permutation. Hence, number possible combinations equal \\[\\frac{n!}{k!(n-k)!}\\]","code":""},{"path":"sample-space-probability.html","id":"examples-3","chapter":"Day 7 Sample space and probability","heading":"7.9.3.1 Examples","text":"Example 7.20  (Counting letters redux) number combinations two four letters \\(, B, C, D\\) found letting \\(n=4\\) \\(k=2\\). \\[\\binom{n}{k} = \\binom{4}{2} = \\frac{4!}{2!2!} = 6\\]Example 7.21  (Parking cars) Twenty distinct cars park parking lot every day. Ten cars US-made, ten foreign-made. parking lot exactly twenty spaces, row, cars park side side. However, drivers varying schedules, position car might take certain day random.many different ways can cars line ?\nSince cars distinct, \\(n! = 20!\\) ways line .many different ways can cars line ?Since cars distinct, \\(n! = 20!\\) ways line .probability given day, cars park way alternate (two US-made cars adjacent two foreign-made adjacent?)\nfind probability cars parked alternate, count number “favorable” outcomes, divide total number possible outcomes found part (). count following manner. first arrange US cars ordered sequence (permutation). can \\(10!\\) ways, since \\(10\\) distinct cars. Similarly, arrange foreign cars ordered sequence, can also done \\(10!\\) ways. Finally, interleave two sequences. can done two different ways, since can let first car either US-made foreign. Thus, total \\(2 \\times 10! \\times 10!\\) possibilities, desired probability \n\\[\\frac{2 \\times 10! \\times 10!}{20!}\\]\nNote solved second part problem neglecting fact cars distinct. Suppose foreign cars indistinguishable, also US cars indistinguishable. 20 available spaces, need choose 10 spaces place US cars, thus \\(\\binom{20}{10}\\) possible outcomes. outcomes, two cars alternate, depending whether start US foreign car. Thus, desired probability \\(2 / \\binom{20}{10}\\), coincides earlier answer.probability given day, cars park way alternate (two US-made cars adjacent two foreign-made adjacent?)find probability cars parked alternate, count number “favorable” outcomes, divide total number possible outcomes found part (). count following manner. first arrange US cars ordered sequence (permutation). can \\(10!\\) ways, since \\(10\\) distinct cars. Similarly, arrange foreign cars ordered sequence, can also done \\(10!\\) ways. Finally, interleave two sequences. can done two different ways, since can let first car either US-made foreign. Thus, total \\(2 \\times 10! \\times 10!\\) possibilities, desired probability \\[\\frac{2 \\times 10! \\times 10!}{20!}\\]Note solved second part problem neglecting fact cars distinct. Suppose foreign cars indistinguishable, also US cars indistinguishable. 20 available spaces, need choose 10 spaces place US cars, thus \\(\\binom{20}{10}\\) possible outcomes. outcomes, two cars alternate, depending whether start US foreign car. Thus, desired probability \\(2 / \\binom{20}{10}\\), coincides earlier answer.","code":""},{"path":"discrete-random-variables.html","id":"discrete-random-variables","chapter":"Day 8 Discrete random variables","heading":"Day 8 Discrete random variables","text":"","code":""},{"path":"discrete-random-variables.html","id":"learning-objectives-7","chapter":"Day 8 Discrete random variables","heading":"Learning objectives","text":"Define random variablesDistinguish discrete interval variablesIdentify discrete random variable distributions relevant social scienceReview measures central tendency dispersionDefine expected value varianceDefine cumulative mass functions (CMFs) discrete random variables","code":""},{"path":"discrete-random-variables.html","id":"supplemental-readings-7","chapter":"Day 8 Discrete random variables","heading":"Supplemental readings","text":"Chapter 2.1-.4, Bertsekas Tsitsiklis (2008)Equivalent reading Bertsekas Tsitsiklis lecture notes","code":""},{"path":"discrete-random-variables.html","id":"random-variable","chapter":"Day 8 Discrete random variables","heading":"8.1 Random variable","text":"random variable random process variable numerical outcome. formally, random variable \\(X\\) function sample spaceNumber incumbents winAn indicator whether country defaults loan (1 default, 0 otherwise)Number casualties war (rather possible outcomes)\\[X:\\text{Sample Space} \\rightarrow \\Re\\]Example 8.1  (Treatment assignment) Suppose \\(3\\) units, flipping fair coin (\\(\\frac{1}{2}\\)) assign unit. Assign \\(T=\\)Treatment \\(C=\\)control, \\(X\\) = Number units received treatment. function \\[\nX  = \\left \\{ \\begin{array} {ll}\n0  \\text{   } (C, C, C)  \\\\\n1 \\text{   } (T, C, C) \\text{ } (C, T, C) \\text{ } (C, C, T) \\\\\n2 \\text{  }  (T, T, C) \\text{ } (T, C, T) \\text{ } (C, T, T) \\\\\n3 \\text{ } (T, T, T) \n\\end{array} \\right.\n\\]words:\\[\n\\begin{aligned}\nX( (C, C, C) )  & = 0 \\\\\nX( (T, C, C)) & =  1 \\\\\nX((T, C, T)) & = 2 \\\\\nX((T, T, T)) & = 3 \n\\end{aligned}\n\\]Example 8.2  (Legislative calls) \\(X\\) = Number Calls congressional office period \\(p\\)\\[X(c) = c\\]Example 8.3  (Electoral outcome) Define \\(v\\) proportion vote candidate receives. \\(X = 1\\) \\(v>0.50\\), \\(X = 0\\) \\(v<0.50\\). example, \\(v = 0.48\\), \\(X(v) = 0\\).Example 8.4  (Loan default) indicator whether country defaults loan (1 default, 0 otherwise)random variables result experiment - observational.","code":""},{"path":"discrete-random-variables.html","id":"discrete-random-variables-1","chapter":"Day 8 Discrete random variables","heading":"8.1.1 Discrete random variables","text":"Discrete random variables random variable finite countably infinite range. random variable uncountably infinite number values continuous.","code":""},{"path":"discrete-random-variables.html","id":"probability-mass-functions","chapter":"Day 8 Discrete random variables","heading":"8.2 Probability mass functions","text":"probability mass function (PMF) defines probability values discrete random variable can take.","code":""},{"path":"discrete-random-variables.html","id":"intuition-1","chapter":"Day 8 Discrete random variables","heading":"8.2.1 Intuition","text":"Go back experiment example – probability comes probability outcomes\\[\\Pr(C, T, C) = \\Pr(C)\\Pr(T)\\Pr(C) = \\frac{1}{2}\\frac{1}{2}\\frac{1}{2} = \\frac{1}{8}\\]applies outcomes:\\[\n\\begin{aligned}\np(X = 0) & = \\Pr(C, C, C) = \\frac{1}{8}\\\\\np(X = 1) & = \\Pr(T, C, C) + \\Pr(C, T, C) + \\Pr(C, C, T) = \\frac{3}{8} \\\\\np(X = 2) & = \\Pr(T, T, C)  + \\Pr(T, C, T) + \\Pr(C, T, T) = \\frac{3}{8} \\\\\np(X = 3) & = \\Pr(T, T, T) = \\frac{1}{8} \\\\\np(X = ) &= 0 \\, \\forall \\, \\notin (0, 1, 2, 3)\n\\end{aligned}\n\\]","code":""},{"path":"discrete-random-variables.html","id":"definition-3","chapter":"Day 8 Discrete random variables","heading":"8.2.2 Definition","text":"Definition 8.1  (Probability mass function) discrete random variable \\(X\\), define probability mass function \\(p_X(x)\\) \\[p_X(x) = \\Pr(X = x)\\]Use upper-case letters denote random variables, lower-case letters denote real numbers numerical values random variable. Note \\[\\sum_x p_{X}(x) = 1\\]\\(x\\) ranges possible values \\(X\\) makes sense - probability must sum 1.can also add probabilities smaller sets \\(S\\) possible values \\(X \\S\\).\\[\\Pr(X \\S) = \\sum_{x \\S} p_X (x)\\]example, \\(X\\) number heads obtained two independent tosses fair coin, probability least one head \\[\\Pr (X > 0) = \\sum_{x=1}^2 p_X (x) = \\frac{1}{2} + \\frac{1}{4} = \\frac{3}{4}\\]calculate PMF random variable \\(X\\)possible value \\(x\\) \\(X\\)\nCollect possible outcomes give rise event \\(\\{ X=x \\}\\)\nAdd probabilities obtain \\(p_X (x)\\)\nCollect possible outcomes give rise event \\(\\{ X=x \\}\\)Add probabilities obtain \\(p_X (x)\\)Example 8.5  (Topic models) Topics distinct concepts (war Afghanistan, national debt, fire department grants) found corpus text documents. Mathematically probability mass functions Words (probability using word, discussing topic).Suppose set words:(afghanistan, fire, department, soldier, troop, war, grant)(afghanistan, fire, department, soldier, troop, war, grant)Topic 1 (war)\n\\(\\Pr(\\text{afghanistan}) = 0.3\\); \\(\\Pr(\\text{fire}) = 0.0001\\); \\(\\Pr(\\text{department}) = 0.0001\\); \\(\\Pr(\\text{soldier}) = 0.2\\); \\(\\Pr(\\text{troop}) = 0.2\\); \\(\\Pr(\\text{war})=0.2997\\); \\(\\Pr(\\text{grant})=0.0001\\)\nTopic 1 (war)\\(\\Pr(\\text{afghanistan}) = 0.3\\); \\(\\Pr(\\text{fire}) = 0.0001\\); \\(\\Pr(\\text{department}) = 0.0001\\); \\(\\Pr(\\text{soldier}) = 0.2\\); \\(\\Pr(\\text{troop}) = 0.2\\); \\(\\Pr(\\text{war})=0.2997\\); \\(\\Pr(\\text{grant})=0.0001\\)Topic 2 (fire departments):\n\\(\\Pr(\\text{afghanistan}) = 0.0001\\); \\(\\Pr(\\text{fire}) = 0.3\\); \\(\\Pr(\\text{department}) = 0.2\\); \\(\\Pr(\\text{soldier}) = 0.0001\\); \\(\\Pr(\\text{troop}) = 0.0001\\); \\(\\Pr(\\text{war})=0.0001\\); \\(\\Pr(\\text{grant})=0.2997\\)\nTopic 2 (fire departments):\\(\\Pr(\\text{afghanistan}) = 0.0001\\); \\(\\Pr(\\text{fire}) = 0.3\\); \\(\\Pr(\\text{department}) = 0.2\\); \\(\\Pr(\\text{soldier}) = 0.0001\\); \\(\\Pr(\\text{troop}) = 0.0001\\); \\(\\Pr(\\text{war})=0.0001\\); \\(\\Pr(\\text{grant})=0.2997\\)Topic models take set documents estimate topics.","code":""},{"path":"discrete-random-variables.html","id":"cumulative-mass-function","chapter":"Day 8 Discrete random variables","heading":"8.3 Cumulative mass function","text":"Definition 8.2  (Cumulative mass function) random variable \\(X\\), define cumulative mass function \\(F(x)\\) ,\\[F(x) = \\Pr(X \\leq x)\\]CMF characterizes probability cumulates \\(X\\) gets larger. \\(F(x) \\[0,1]\\), \\(F(x)\\) non-decreasing.","code":""},{"path":"discrete-random-variables.html","id":"three-person-experiment","chapter":"Day 8 Discrete random variables","heading":"8.3.1 Three person experiment","text":"Consider three person experiment: \\(\\Pr(T) = \\Pr(C) = 1/2\\). \\(F(2)\\)?\\[\n\\begin{aligned}\nF(2) & = \\Pr(X = 0) + \\Pr(X = 1) + \\Pr(X = 2) \\\\\n& = \\frac{1}{8} + \\frac{3}{8} + \\frac{3}{8} \\\\\n& = \\frac{7}{8} \n\\end{aligned}\n\\]\\(F(2) - F(1)\\)?\\[\n\\begin{aligned} \nF(2)  - F(1) & = [\\Pr(X = 0) + \\Pr(X = 1) + \\Pr(X = 2)]  \\nonumber \\\\\n& \\quad -[\\Pr(X = 0) + \\Pr(X = 1)] \\\\\nF(2) - F(1) & = \\Pr(X = 2)\n\\end{aligned}\n\\]close relationship PMFs CMFs.Cumulative functions similar integration, discrete set values.","code":""},{"path":"discrete-random-variables.html","id":"famous-discrete-random-variables","chapter":"Day 8 Discrete random variables","heading":"8.4 Famous discrete random variables","text":"","code":""},{"path":"discrete-random-variables.html","id":"bernoulli","chapter":"Day 8 Discrete random variables","heading":"8.4.1 Bernoulli","text":"Definition 8.3  (Bernoulli random variable) Suppose \\(X\\) random variable, \\(X \\\\{0, 1\\}\\) \\(\\Pr(X = 1) = \\pi\\). say \\(X\\) Bernoulli random variable,\\[p_X(k)= \\pi^{k} (1- \\pi)^{1 - k}\\]\\(k \\\\{0,1\\}\\) \\(p_X(k) = 0\\) otherwise.(equivalently) say \\[Y \\sim \\text{Bernoulli}(\\pi)\\]Suppose flip fair coin \\(Y = 1\\) outcome Heads.\\[\n\\begin{aligned}\nY & \\sim \\text{Bernoulli}(1/2) \\nonumber \\\\\np_X(1) & = (1/2)^{1} (1- 1/2)^{ 1- 1} = 1/2 \\nonumber \\\\\np_X(0) & = (1/2)^{0} (1- 1/2)^{1 - 0} = (1- 1/2) \\nonumber \n\\end{aligned}\n\\]\nFigure 8.1: Example Bernoulli probability mass functions\nexamples include:Person healthy sickPerson votes vote","code":""},{"path":"discrete-random-variables.html","id":"binomial","chapter":"Day 8 Discrete random variables","heading":"8.4.2 Binomial","text":"Definition 8.4  (Binomial random variable) Suppose \\(X\\) random variable counts number successes \\(N\\) independent identically distributed Bernoulli trials. \\(X\\) Binomial random variable,\\[p_X(k) = {{N}\\choose{k}}\\pi^{k} (1- \\pi)^{n-k}\\]\\(k \\\\{0, 1, 2, \\ldots, N\\}\\) \\(p_X(k) = 0\\) otherwise, \\(\\binom{N}{k} = \\frac{N!}{(N-k)! k!}\\). Equivalently,\\[Y \\sim \\text{Binomial}(N, \\pi)\\]Binomial random variables can used model count number successes across \\(N\\) trials.","code":""},{"path":"discrete-random-variables.html","id":"example","chapter":"Day 8 Discrete random variables","heading":"8.4.2.1 Example","text":"\nFigure 8.2: Example Binomial probability mass functions\nRecall experiment example:\\(\\Pr(T) = \\Pr(C) = 1/2\\)\\(Z =\\) number units assigned treatment\\[\n\\begin{aligned}\nZ & \\sim  \\text{Binomial}(1/2)\\\\\np_Z(0) & = {{3}\\choose{0}} (1/2)^{0} (1- 1/2)^{3-0} = 1 \\times \\frac{1}{8}\\\\\np_Z(1) & = {{3}\\choose{1}} (1/2)^{1} (1 - 1/2)^{2} = 3 \\times \\frac{1}{8} \\\\\np_Z(2) & = {{3}\\choose{2}} (1/2)^{2} (1- 1/2)^1 = 3 \\times \\frac{1}{8} \\\\\np_Z(3) & = {{3}\\choose{3}} (1/2)^{3} (1 - 1/2)^{0} = 1 \\times \\frac{1}{8}\n\\end{aligned}\n\\]","code":""},{"path":"discrete-random-variables.html","id":"geometric","chapter":"Day 8 Discrete random variables","heading":"8.4.3 Geometric","text":"model count number trials Bernoulli outcome success occurs first timeDefinition 8.5  (Geometric random variable) Suppose \\(X\\) random variable counts number tosses needed head come first time. PMF \\[p_X(k) = (1 - p)^{k-1}p, \\quad k = 1, 2, \\ldots\\]\\((1 - p)^{k-1}p\\) probability sequence consisting \\(k-1\\) successive trials followed head. valid PMF \\[\n\\begin{aligned}\n\\sum_{k=1}^{\\infty} p_X(k) &= \\sum_{k=1}^{\\infty} (1 - p)^{k-1}p \\\\\n&= p \\sum_{k=1}^{\\infty} (1 - p)^{k-1} \\\\&\n= p \\times \\frac{1}{1 - (1-p)} \\\\\n&= 1\n\\end{aligned}\n\\]Geometric random variables can used count number trials Bernoulli outcome success occurs first time.\nFigure 8.3: Example Geometric probability mass functions\nExamples include:Number attempts passing testFinding missing item search","code":""},{"path":"discrete-random-variables.html","id":"poisson","chapter":"Day 8 Discrete random variables","heading":"8.4.4 Poisson","text":"Often social scientists interested counting number events occur:Number wars startedNumber speeches madeNumber bribes offeredNumber people waiting licenseThese generally referred event counts.Definition 8.6  (Poisson random variable) Suppose \\(X\\) random variable takes values \\(X \\\\{0, 1, 2, \\ldots, \\}\\) \\(\\Pr(X = k) = p_X(k)\\) ,\\[p_X(k) = e^{-\\lambda} \\frac{\\lambda^{k}}{k!}, \\quad k = 0,1,2,\\ldots\\]\\(k \\\\{0, 1, \\ldots, \\}\\) \\(0\\) otherwise. say \\(X\\) follows Poisson distribution rate parameter \\(\\lambda\\)\\[X \\sim \\text{Poisson}(\\lambda)\\]\nFigure 8.4: Example Poisson probability mass functions\nExample 8.6  (Presidential threats) Suppose number threats president makes term given \\(X \\sim \\text{Poisson}(5)\\)18 probability president make ten threats?\\[p_X(10) = e^{-\\lambda} \\frac{5^{10}}{10!}\\]","code":""},{"path":"discrete-random-variables.html","id":"approximating-a-binomial-random-variable","chapter":"Day 8 Discrete random variables","heading":"8.4.4.1 Approximating a binomial random variable","text":"Poisson PMF parameter \\(\\lambda\\) good approximation binomial PMF parameters \\(n\\) \\(p\\)\\[e^{-\\lambda} \\frac{\\lambda^{k}}{k!} \\approx {{N}\\choose{k}}\\pi^{k} (1- \\pi)^{n-k}, \\quad \\text{} k \\ll n\\]provided \\(\\lambda = np\\), \\(n\\) large, \\(p\\) small. Sometimes using Poisson PMF results simpler models easier calculations. instance, \\(n = 100\\) \\(p = 0.01\\).Using binomial PMF\\[\\frac{100!}{95! 5!} \\times 0.01^5 (1 - 0.01)^{95} = 0.00290\\]Versus using Poisson PMF \\(\\lambda = np = 100 \\times 0.01 = 1\\)\\[e^{-1} \\frac{1}{5!} = 0.00306\\]","code":""},{"path":"discrete-random-variables.html","id":"functions-of-random-variables","chapter":"Day 8 Discrete random variables","heading":"8.5 Functions of random variables","text":"Given random variable \\(X\\), may wish create new random variable \\(Y\\) using transformations \\(X\\). linear function form\\[Y = g(X) = aX + b\\]\\(\\) \\(b\\) scalars. logarithmic transformation\\[g(X) = \\log(X)\\]\\(Y = g(X)\\) function random variable \\(X\\), \\(Y\\) also random variable. outcomes sample space defined numerical value \\(x\\) \\(X\\) also numerical value \\(y = g(x)\\) \\(Y\\).","code":""},{"path":"discrete-random-variables.html","id":"expectation-mean-and-variance","chapter":"Day 8 Discrete random variables","heading":"8.6 Expectation, mean, and variance","text":"PMF random variable \\(X\\) provides several numbers – probabilities possible values \\(X\\). Often desirable summarize information single representative number: expectation \\(X\\) – weighted average possible values \\(X\\).","code":""},{"path":"discrete-random-variables.html","id":"motivation","chapter":"Day 8 Discrete random variables","heading":"8.6.1 Motivation","text":"Consider spinning wheel fortune many times. spin, one numbers \\(m_1, m_2, \\ldots, m_n\\) comes corresponding probability \\(p_1, p_2, \\ldots, p_n\\), monetary reward spin. amount money “expect” get “per spin?” terms “expect” “per spin” little ambiguous, reasonable interpretation.Suppose spin wheel \\(k\\) times, \\(k_i\\) number times outcome \\(m_i\\). , total amount received \\(m_1 k_1 + m_2 k_2 + \\ldots + m_n k_n\\). amount received per spin simple average:\\[M = \\frac{m_1 k_1 + m_2 k_2 + \\ldots + m_n k_n}{k}\\]number spins \\(k\\) large interpret probabilities relative frequencies, anticipate \\(m_i\\) comes fraction times roughly equal \\(p_i\\):\\[\\frac{k_i}{k} \\approx p_i, = 1, \\ldots,n\\]Thus, amount money “expect” receive \\[M = \\frac{m_1 k_1 + m_2 k_2 + \\ldots + m_n k_n}{k} \\approx m_1p_1 + m_2p_2 + \\ldots + m_np_n\\]","code":""},{"path":"discrete-random-variables.html","id":"expectation","chapter":"Day 8 Discrete random variables","heading":"8.6.2 Expectation","text":"Definition 8.7  (Expected value) Define expected value (known expectation mean) random variable \\(X\\), PMF \\(p_X\\) \\[\\E[X] = \\sum_{x:p(x)>0} x p(x)\\]\\(\\sum_{x:p(x)>0}\\) values \\(X\\) probability greater 0.words: values \\(x\\) \\(p(x)\\) greater zero, take weighted average values.Example 8.7  Suppose \\(X\\) number units assigned treatment, one previous example.\\[\nX  = \\left \\{ \\begin{array} {ll}\n0  \\text{   } (C, C, C)  \\\\\n1 \\text{   } (T, C, C) \\text{ } (C, T, C) \\text{ } (C, C, T) \\\\\n2 \\text{  }  (T, T, C) \\text{ } (T, C, T) \\text{ } (C, T, T) \\\\\n3 \\text{ } (T, T, T) \n\\end{array} \\right.\n\\]\\(\\E[X]\\)?\\[\n\\begin{aligned}\n\\E[X]  & = 0\\times \\frac{1}{8} + 1 \\times \\frac{3}{8} + 2 \\times \\frac{3}{8} + 3 \\times \\frac{1}{8} \\\\\n& = 1.5 \n\\end{aligned}\n\\]expected value essentially weighted average, average outcome (value middle range). gives us measure central tendency.Example 8.8  (single person poll) Suppose group \\(N\\) people. Suppose \\(M< N\\) people approve Donald Trump’s performance president, \\(N - M\\) disapprove performance.Draw one person \\(\\), \\(\\Pr(\\text{Draw } ) = \\frac{1}{N}\\)\\[\nX  = \\left \\{ \\begin{array} {ll}\n    1  \\text{   person Approves}  \\\\\n    0 \\text{   Disapproves}   \\\\\n\\end{array} \\right. \n\\]\\(\\E[X]\\)?\\[\n\\begin{aligned}\n\\E[X] & = 1 \\times \\Pr(\\text{Approve})  + 0 \\times \\Pr(\\text{Disapprove}) \\\\\n & = 1 \\times \\frac{M}{N} \\\\\n & = \\frac{M}{N} \n \\end{aligned}\n\\]","code":""},{"path":"discrete-random-variables.html","id":"indicator-variables-and-probabilities","chapter":"Day 8 Discrete random variables","heading":"8.6.2.1 Indicator variables and probabilities","text":"Proposition 8.1  Suppose \\(\\) event. Define random variable \\(\\) \\(= 1\\) outcome \\(\\) occurs \\(=0\\) outcome \\(^{c}\\) occurs. ,\\[\\E[] = \\Pr()\\]Proof. \\[\n\\begin{aligned}\n\\E[]  & =  1 \\times \\Pr() + 0 \\times \\Pr(^{c}) \\\\\n& = \\Pr() \n\\end{aligned} \n\\]","code":""},{"path":"discrete-random-variables.html","id":"variance-moments-and-the-expected-value-rule","chapter":"Day 8 Discrete random variables","heading":"8.6.3 Variance, moments, and the expected value rule","text":"quantities care include variance moments.","code":""},{"path":"discrete-random-variables.html","id":"moments","chapter":"Day 8 Discrete random variables","heading":"8.6.3.1 Moments","text":"1st moment: \\(\\E[X^1] = \\E[X]\\) - , mean2nd moment: \\(\\E[X^2]\\)\\(n\\)th moment: \\(\\E[X^n]\\)","code":""},{"path":"discrete-random-variables.html","id":"variance","chapter":"Day 8 Discrete random variables","heading":"8.6.3.2 Variance","text":"Expected value measure central tendency. spread dispersion?value, might measure distance center. example, Euclidean distance squared \\(d(x, E[X])^{2} = (x - E[X])^2\\), might take weighted average distances\\[\n\\begin{aligned} \n\\E[(X - \\E[X])^2] & = \\sum_{x:p_X(x)>0} (x  - \\E[X])^2p_X(x) \\\\\n& = \\sum_{x:p_X(x)>0} \\left(x^2 p_X(x)\\right)  - 2 \\E[X]\\sum_{x:p_X(x)>0} \\left(x p_X(x)\\right) + \\E[X]^2\\sum_{x:p_X(x)>0} p_X(x)  \\\\\n& =  \\E[X^2] - 2\\E[X]^2 + \\E[X]^2 \\\\\n& = \\E[X^2] - \\E[X]^2 \\\\\n& = \\text{Var}(X)\n\\end{aligned}\n\\]Defined expected value random variable \\((X - \\E[X])^2\\)\\[\n\\begin{aligned}\n\\Var(X) &= \\E[(X - \\E[X])^2] \\\\\n&= \\E[X^2] - \\E[X]^2\n\\end{aligned}\n\\]Since \\((X - \\E[X])^2\\) can take non-negative values, \\(\\Var(X) \\geq 0\\)measure dispersion \\(X\\) around mean. define standard deviation \\(X\\), \\(\\sigma_X = \\sqrt{\\Var(X)}\\). Standard deviation easier interpret sometimes since units \\(X\\).","code":""},{"path":"discrete-random-variables.html","id":"calculating-variance-of-a-random-variable","chapter":"Day 8 Discrete random variables","heading":"8.6.3.2.1 Calculating variance of a random variable","text":"generate PMF random variable \\((X - \\E[X])^2\\), calculate expectation function – just linear function.Instead, can take shortcut: expected value rule functions random variables. Let \\(X\\) random variable PMF \\(p_X\\), let \\(g(X)\\) function \\(X\\). , expected value random variable \\(g(X)\\) given \\[\\E[g(X)] = \\sum_{x} g(x) p_X(x)\\]allows us rewrite variance formula:\\[\n\\begin{align}\n\\Var(X) &= \\E[(X - \\E[X])^2] \\\\\n\\Var(X) &= \\E[X^2] - \\E[X]^2\n\\end{align}\n\\]just need first second moments calculate variance.","code":""},{"path":"discrete-random-variables.html","id":"practice-calculating-expectation-and-variance","chapter":"Day 8 Discrete random variables","heading":"8.6.4 Practice calculating expectation and variance","text":"","code":""},{"path":"discrete-random-variables.html","id":"bernoulli-variable","chapter":"Day 8 Discrete random variables","heading":"8.6.4.1 Bernoulli variable","text":"Suppose \\(Y \\sim \\text{Bernoulli}(\\pi)\\)\n\\[\n  \\begin{aligned} \n  \\E[Y] & = 1 \\times \\Pr(Y = 1) + 0 \\times \\Pr(Y = 0) \\nonumber \\\\\n  & = \\pi + 0 (1 - \\pi) \\nonumber  = \\pi \\\\\n  \\Var(Y) & = \\E[Y^2] - \\E[Y]^2 \\nonumber  \\\\\n  \\E[Y^2] & = 1^{2} \\Pr(Y = 1) + 0^{2} \\Pr(Y = 0) \\nonumber \\\\\n  & = \\pi \\nonumber \\\\ \n  \\Var(Y) & = \\pi - \\pi^{2} \\nonumber \\\\\n  & = \\pi(1 - \\pi ) \\nonumber\n  \\end{aligned}\n  \\]Suppose \\(Y \\sim \\text{Bernoulli}(\\pi)\\)\\[\n  \\begin{aligned} \n  \\E[Y] & = 1 \\times \\Pr(Y = 1) + 0 \\times \\Pr(Y = 0) \\nonumber \\\\\n  & = \\pi + 0 (1 - \\pi) \\nonumber  = \\pi \\\\\n  \\Var(Y) & = \\E[Y^2] - \\E[Y]^2 \\nonumber  \\\\\n  \\E[Y^2] & = 1^{2} \\Pr(Y = 1) + 0^{2} \\Pr(Y = 0) \\nonumber \\\\\n  & = \\pi \\nonumber \\\\ \n  \\Var(Y) & = \\pi - \\pi^{2} \\nonumber \\\\\n  & = \\pi(1 - \\pi ) \\nonumber\n  \\end{aligned}\n  \\]\\(\\E[Y] = \\pi\\)\\(\\E[Y] = \\pi\\)\\(\\Var(Y) = \\pi(1- \\pi)\\)\\(\\Var(Y) = \\pi(1- \\pi)\\)maximum variance?\n\\[\n  \\begin{aligned} \n  \\Var(Y) & = \\pi - \\pi^{2} \\nonumber \\\\\n  & = 0.5(1 - 0.5 ) \\\\\n  & = 0.25\n  \\end{aligned}\n  \\]maximum variance?\\[\n  \\begin{aligned} \n  \\Var(Y) & = \\pi - \\pi^{2} \\nonumber \\\\\n  & = 0.5(1 - 0.5 ) \\\\\n  & = 0.25\n  \\end{aligned}\n  \\]","code":""},{"path":"discrete-random-variables.html","id":"binomial-1","chapter":"Day 8 Discrete random variables","heading":"8.6.4.2 Binomial","text":"\\[Z = \\sum_{=1}^{N} Y_{} \\text{ } Y_{} \\sim \\text{Bernoulli}(\\pi)\\]\\[\n\\begin{aligned}\n\\E[Z] & = \\E[Y_{1} + Y_{2} + Y_{3} + \\ldots + Y_{N} ] \\\\\n& = \\sum_{=1}^{N} \\E[Y_{} ] \\\\\n& = N \\pi \\\\\n\\Var(Z) & = \\sum_{=1}^{N} \\Var(Y_{}) \\\\\n& = N \\pi (1-\\pi)\n\\end{aligned}\n\\]","code":""},{"path":"discrete-random-variables.html","id":"decision-making-using-expected-values","chapter":"Day 8 Discrete random variables","heading":"8.6.5 Decision making using expected values","text":"can use expected values optimizes choice several candidate decisions result random rewards. View expected reward decision average payoff large number trials, choose decision maximum expected reward.Example 8.9  (Going war) Suppose country \\(1\\) engaged conflict can either win lose. Define \\(Y = 1\\) country wins \\(Y = 0\\) otherwise. ,\\[Y \\sim \\text{Bernoulli}(\\pi)\\]Suppose country \\(1\\) deciding whether fight war. Engaging war cost country \\(c\\). win, country \\(1\\) receives \\(B\\). \\(1\\)’s expected utility fighting war?\\[\n\\begin{aligned}\n\\E[U(\\text{war})] & = U(\\text{war} | \\text{win}) \\times \\Pr(\\text{win}) + U(\\text{war} | \\text{lose}) \\times \\Pr(\\text{lose}) \\\\\n& = (B - c) \\Pr(Y = 1) + (- c) \\Pr(Y = 0 ) \\\\\n& = B \\times \\Pr(Y = 1)  - c(\\Pr(Y = 1)  + \\Pr(Y = 0)) \\\\\n& = B \\times \\pi - c \n\\end{aligned}\n\\]Based beliefs appropriate values \\(B, \\pi, c\\), can decide whether engage warIf expected utility greater 0, decide go warIf expected utility less 0, decide go war","code":""},{"path":"discrete-random-variables.html","id":"cumulative-mass-function-redux","chapter":"Day 8 Discrete random variables","heading":"8.7 Cumulative mass function, redux","text":"cumulative mass function (CMF) defines cumulative probability \\(F_X(x)\\) value \\(x\\). discrete random variable \\(X\\), define CMF \\(F_X\\) provides probability \\(\\Pr (X \\leq x)\\). every \\(x\\)\\[F_X(x) = \\Pr (X \\leq x) = \\sum_{k \\leq x} p_X(k)\\]random variable associated given probability model CMF. Basic properties CMFs discrete random variables :\\(F_X\\) monotonically non-decreasing – \\(x \\leq y\\), \\(F_X(x) \\leq F_X(y)\\)\\(F_X\\) monotonically non-decreasing – \\(x \\leq y\\), \\(F_X(x) \\leq F_X(y)\\)\\(F_X(x)\\) tends \\(0\\) \\(x \\rightarrow -\\infty\\), \\(1\\) \\(x \\rightarrow \\infty\\)\\(F_X(x)\\) tends \\(0\\) \\(x \\rightarrow -\\infty\\), \\(1\\) \\(x \\rightarrow \\infty\\)\\(F_X(x)\\) piecewise constant function \\(x\\)\\(F_X(x)\\) piecewise constant function \\(x\\)\\(X\\) discrete takes integer values, PMF CMF can obtained summing differencing:\n\\[F_X(k) = \\sum_{= -\\infty}^k p_X(),\\]\n\\[p_X(k) = \\Pr (X \\leq k) - \\Pr (X \\leq k-1) = F_X(k) - F_X(k-1)\\]\nintegers \\(k\\)\\(X\\) discrete takes integer values, PMF CMF can obtained summing differencing:\\[F_X(k) = \\sum_{= -\\infty}^k p_X(),\\]\n\\[p_X(k) = \\Pr (X \\leq k) - \\Pr (X \\leq k-1) = F_X(k) - F_X(k-1)\\]integers \\(k\\)","code":""},{"path":"discrete-random-variables.html","id":"common-cmfs","chapter":"Day 8 Discrete random variables","heading":"8.7.1 Common CMFs","text":"\nFigure 8.5: Example Bernoulli cumulative mass functions\n\nFigure 8.6: Example Binomial cumulative mass functions\n\nFigure 8.7: Example Geometric cumulative mass functions\n\nFigure 8.8: Example Poisson cumulative mass functions\n","code":""},{"path":"general-random-variables.html","id":"general-random-variables","chapter":"Day 9 General random variables","heading":"Day 9 General random variables","text":"","code":""},{"path":"general-random-variables.html","id":"learning-objectives-8","chapter":"Day 9 General random variables","heading":"Learning objectives","text":"Define continuous random variableIdentify continuous random variable distributions relevant social scienceDefine expected value variance continuous random variablesRelate continuous random variables discrete random variablesDefine cumulative distribution functions (CDFs) continuous random variables compare discrete random variablesEstimate probability events using probability density functions (PDFs) cumulative distribution functions (CDFs)","code":""},{"path":"general-random-variables.html","id":"supplemental-readings-8","chapter":"Day 9 General random variables","heading":"Supplemental readings","text":"Chapter 3.1-.3 Bertsekas Tsitsiklis (2008)Equivalent reading Bertsekas Tsitsiklis lecture notes","code":""},{"path":"general-random-variables.html","id":"continuous-random-variables","chapter":"Day 9 General random variables","heading":"9.1 Continuous random variables","text":"Continuous random variables random variables discrete. Examples include:Approval ratingsGDPWait time wars: \\(X(t) = t\\) \\(t\\)Proportion vote received: \\(X(v) = v\\) \\(v\\)Continuous random variables many analogues discrete probability distributions, instead need calculus answer questions probability.","code":""},{"path":"general-random-variables.html","id":"probability-density-function","chapter":"Day 9 General random variables","heading":"9.2 Probability density function","text":"area curve \\(f(x)\\) \\(.5\\) \\(2\\)?\\[\\int_{1/2}^{2} f(x)\\,dx = F(2) - F(1/2)\\]","code":""},{"path":"general-random-variables.html","id":"definition-4","chapter":"Day 9 General random variables","heading":"9.2.1 Definition","text":"::: {.definition echo=TRUE name=“Probability density function”}\n\\(X\\) continuous random variable exists nonnegative function defined \\(x \\\\Re\\) property (measurable) set real numbers \\(B\\),\\[\\Pr(X \\B) = \\int_{B} f_X(x)\\,dx\\]Non-negative meaning \\(f(x)\\) never negative\\(\\Pr(X \\B)\\): probability \\(X\\) element \\(B\\)\n’ll call \\(f(\\cdot)\\) probability density function (PDF) \\(X\\):::probability value \\(X\\) falls within interval \\[\\Pr (\\leq X \\leq b) = \\int_a^b f_X(x) \\,dx\\]can interpreted area graph PDF. single value \\(\\), \\(\\Pr (X = ) = \\int_a^f_X(x) \\,dx = 0\\). Note qualify PDF, function \\(f_X\\) must nonnegative, .e. \\(f_X(x) \\geq 0\\) every \\(x\\), must also normalization property\\[\\int_{-\\infty}^{\\infty} f_X(x) \\,dx = \\Pr (-\\infty \\leq X \\leq \\infty) = 1\\]","code":""},{"path":"general-random-variables.html","id":"example-uniform-random-variable","chapter":"Day 9 General random variables","heading":"9.2.2 Example: Uniform Random Variable","text":"\\[X \\sim \\text{Uniform}(0,1)\\]\\[\nf_X(x) = \\left\\{\n    \\begin{array}{ll}\n        c & \\quad \\text{} 0 \\leq x \\leq 1 \\\\\n        0 & \\quad \\text{otherwise}\n    \\end{array}\n\\right.\n\\]constant \\(c\\). constant can determined normalization property\\[1 = \\int_{-\\infty}^{\\infty} f_X(x)\\,dx = \\int_0^1 c \\,dx = c \\int_0^1 \\,dx = c\\]\\(c=1\\).Example 9.1  \\[\n\\begin{aligned}\n\\Pr(X \\[0.2, 0.5]) & = \\int_{0.2}^{0.5} 1 \\,dx \\\\\n& = X |^{0.5}_{0.2} \\\\\n& = 0.5  - 0.2  \\\\\n& = 0.3 \n\\end{aligned}\n\\]Example 9.2  \\[\n\\begin{aligned}\n\\Pr(X \\[0, 1] ) & = \\int_{0}^{1} 1 \\,dx \\\\\n& = X |^{1}_{0} \\\\\n& = 1 - 0 \\\\\n& = 1 \n\\end{aligned}\n\\]Example 9.3  \\[\n\\begin{aligned}\n\\Pr(X \\[0.5, 0.5]) & = \\int_{0.5}^{0.5} 1\\,dx \\\\\n& = X|^{0.5}_{0.5} \\\\\n& = 0.5 - 0.5 \\\\\n& = 0 \n\\end{aligned}\n\\]Example 9.4  \\[\n\\begin{aligned}\n\\Pr(X \\\\{[0, 0.2]\\cup[0.5, 1]\\}) & = \\int_{0}^{0.2} 1\\,dx + \\int_{0.5}^{1} 1\\,dx \\\\\n & = X_{0}^{0.2} + X_{0.5}^{1} \\\\\n & = 0.2 - 0 + 1 - 0.5 \\\\\n & = 0.7 \n\\end{aligned}\n\\]generally, PDF uniform random variable form\\[\nf_X(x) = \\left\\{\n    \\begin{array}{ll}\n        \\frac{1}{b-} & \\quad \\text{} \\leq x \\leq b \\\\\n        0 & \\quad \\text{otherwise}\n    \\end{array}\n\\right.\n\\]summarize\\(\\Pr(X = ) = 0\\)\\(\\Pr(X \\(-\\infty, \\infty) ) = 1\\)\\(F\\) antiderivative \\(f\\), \\(\\Pr(X \\[c,d]) = F(d) - F(c)\\)19","code":""},{"path":"general-random-variables.html","id":"expectation-continuous","chapter":"Day 9 General random variables","heading":"9.2.3 Expectation","text":"expected value continuous random variable \\(X\\) defined \\[\\E[X] = \\int_{-\\infty}^{\\infty} x f_X(x) \\,dx\\]similar discrete case except instead summation operation, use integration calculate expected value. \\(X\\) continuous random variable given PDF, real-valued function \\(Y = g(X)\\) also random variable. mean \\(g(X)\\) satisfies expected value rule:\\[\\E[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f_X(x) \\,dx\\]\\(n\\)th moment defined \\(\\E[X^n]\\), expected value random variable \\(X^n\\)variance, denoted \\(\\Var(X)\\) defined expected value random variable \\((X - \\E[X])^2 = \\E[X^2] - (\\E[X])^2\\)Example 9.5  (Uniform random variable) Consider uniform PDF interval \\([,b]\\):\\[\n\\begin{align}\n\\E[X] = \\int_{-\\infty}^{\\infty} x f_X(x) \\,dx &= \\int_a^b x \\times \\frac{1}{b-} \\,dx \\\\\n&= \\frac{1}{b-} \\times \\frac{1}{2}x^2 \\Big|_a^b \\\\\n&= \\frac{1}{b-} \\times \\frac{b^2 - ^2}{2} \\\\\n&= \\frac{+b}{2}\n\\end{align}\n\\]obtain variance, first calculate second moment. \\[\n\\begin{align}\n\\E[X^2] = \\int_a^b x^2 \\times \\frac{1}{b-} \\,dx &= \\frac{1}{b-} \\int_a^b x^2 \\,dx \\\\\n&= \\frac{1}{b-} \\times \\frac{1}{3}x^3 \\Big|_a^b \\\\\n&= \\frac{b^3 - ^3}{3(b-)} \\\\\n&= \\frac{^2 + ab + b^2}{3}\n\\end{align}\n\\]Thus, variance \\[\n\\begin{align}\n\\Var(X) = \\E[X^2] - (\\E[X])^2 = \\frac{^2 + ab + b^2}{3} - \\left( \\frac{+b}{2} \\right)^2 = \\frac{(b-)^2}{12}\n\\end{align}\n\\]","code":""},{"path":"general-random-variables.html","id":"exponential-random-variable","chapter":"Day 9 General random variables","heading":"9.2.4 Exponential random variable","text":"exponential random variable PDF form\\[\nf_X(x) = \\left\\{\n    \\begin{array}{ll}\n        \\lambda e^{-\\lambda x} & \\quad \\text{} x \\geq 0 \\\\\n        0 & \\quad \\text{otherwise}\n    \\end{array}\n\\right.\n\\]\\(\\lambda\\) positive parameter characterizing PDF.\\[\\E[X] = \\frac{1}{\\lambda}, \\quad \\Var(X) = \\frac{1}{\\lambda^2}\\]frequently used model phenomena continuous nature time arrivals (e.g. time customers arriving restaraunt) distance occurrences (e.g. distance defects plate glass window). closely associated Poisson discrete random variable, return later.","code":""},{"path":"general-random-variables.html","id":"cumulative-distribution-function","chapter":"Day 9 General random variables","heading":"9.3 Cumulative distribution function","text":"probability density function (\\(f\\)) characterizes distribution continuous random variable. Equivalently, cumulative distribution function characterizes continuous random variables. continuous random variable \\(X\\) define cumulative distribution function (CDF) \\(F_X(x)\\) ,\\[F_X(x) = \\Pr(X \\leq x) = \\int_{-\\infty} ^{x} f_X(t) \\,dt\\]Example 9.6  (Uniform distribution) Suppose \\(X \\sim \\text{Uniform}(0,1)\\), \\[\n\\begin{aligned} \nF_X(x) & = \\Pr(X\\leq x)  \\\\\n& = 0 \\text{, }x< 0 \\\\\n & = 1 \\text{, }x >1 \\\\\n & = x \\text{, } x \\[0,1]\n\\end{aligned}\n\\]","code":""},{"path":"general-random-variables.html","id":"properties-of-cdfs","chapter":"Day 9 General random variables","heading":"9.3.1 Properties of CDFs","text":"\\(F_X\\) monotonically nodecreasing – \\(x \\leq y\\), \\(F_X(x) \\leq F_X(y)\\)\\(F_X\\) monotonically nodecreasing – \\(x \\leq y\\), \\(F_X(x) \\leq F_X(y)\\)\\(F_X(x)\\) tends \\(0\\) \\(x \\rightarrow -\\infty\\), \\(1\\) \\(x \\rightarrow \\infty\\)\\(F_X(x)\\) tends \\(0\\) \\(x \\rightarrow -\\infty\\), \\(1\\) \\(x \\rightarrow \\infty\\)\\(F_X(x)\\) continuous function \\(x\\)\\(F_X(x)\\) continuous function \\(x\\)\\(X\\) continuous, PDF CDF can obtained integration differentiation\n\\[F_X(x) = \\int_{-\\infty}^x f_X(t) \\,dt, \\quad f_X(x) = \\frac{dF_X}{dx} (x)\\]\\(X\\) continuous, PDF CDF can obtained integration differentiation\\[F_X(x) = \\int_{-\\infty}^x f_X(t) \\,dt, \\quad f_X(x) = \\frac{dF_X}{dx} (x)\\]","code":""},{"path":"general-random-variables.html","id":"normal-distribution","chapter":"Day 9 General random variables","heading":"9.4 Normal distribution","text":"Suppose \\(X\\) random variable \\(X \\\\Re\\) density\\[f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\\]\\(\\mu\\) \\(\\sigma\\) two scalar parameters characterizing PDF, \\(\\sigma\\) assumed positive. \\(X\\) normally distributed random variable parameters \\(\\mu\\) \\(\\sigma^2\\).Equivalently, ’ll write\\[X \\sim \\text{Normal}(\\mu, \\sigma^2)\\]","code":""},{"path":"general-random-variables.html","id":"expected-valuevariance-of-normal-distribution","chapter":"Day 9 General random variables","heading":"9.4.1 Expected value/variance of normal distribution","text":"\\(Z\\) standard normal distribution \\[Z \\sim \\text{Normal}(0,1)\\]’ll call cumulative distribution function \\(Z\\),\\[F_{Z}(x) = \\frac{1}{\\sqrt{2\\pi} }\\int_{-\\infty}^{x} \\exp(-z^2/2) \\,dz\\]Suppose \\(Z \\sim \\text{Normal}(0,1)\\)\\[\n\\begin{aligned}\nY &= 2Z + 6 \\\\\nY &\\sim \\text{Normal}(6, 4)\n\\end{aligned}\n\\]Scale/Location: \\(Z \\sim N(0,1)\\), \\(X = aZ + b\\) ,\\[X \\sim \\text{Normal} (b, ^2)\\]Assume know:\\[\n\\begin{aligned}\n\\E[Z]  & = 0 \\\\\n\\Var(Z) & = 1 \n\\end{aligned}\n\\]implies , \\(Y \\sim \\text{Normal}(\\mu, \\sigma^2)\\)\\[\n\\begin{aligned} \n\\E[Y] & = \\E[\\sigma Z + \\mu] \\\\\n& = \\sigma \\E[Z] + \\mu \\\\\n& = \\mu \\\\\n\\Var(Y) & = \\Var(\\sigma Z + \\mu) \\\\\n & = \\sigma^2 \\Var(Z) + \\Var(\\mu) \\\\\n & = \\sigma^2 + 0 \\\\\n & = \\sigma^2 \n\\end{aligned}\n\\]illustrates key property normal random variables. \\(X\\) normal random variable mean \\(\\mu\\) variance \\(\\sigma^2\\), \\(\\neq 0, b\\) scalars, random variable\\[Y = aX + b\\]also normal, mean variance\\[\\E[Y] = \\mu + b, \\quad \\Var(Y) = ^2 \\sigma^2\\]","code":""},{"path":"general-random-variables.html","id":"why-rely-on-the-standard-normal-distribution","chapter":"Day 9 General random variables","heading":"9.4.2 Why rely on the standard normal distribution","text":"normal distribution commonly used statistical analysis. Many random variables can approximated normal distribution. Standardizing makes easier make comparisons across variables different ranges/variancesExample 9.7  (GRE scores) Consider GRE scores. verbal quantitative scores different variability, one easily determine 159 verbal better worse 159 quantitative section. standard normal distribution unitless, random variable can compared another random variable.also saves time calculus don’t need recalculate integral calculating cumulative probability based unique \\(\\mu\\) \\(\\sigma^2\\). Instead, store info lookup table. made calculating probabilities (relatively) easy computers.::: {.example name=“Support President Trump”}Suppose interested modeling presidential approval. Let \\(Y\\) represent random variable “proportion population approves job president .” Individual responses (constitute proportion) independent identically distributed (sufficient, necessary) take average individual responses. observe many responses (\\(N\\rightarrow \\infty\\)), Central Limit Theorem20 \\(Y\\) Normally distributed, \\[\n\\begin{aligned}\nY & \\sim \\text{Normal}(\\mu, \\sigma^2) \\\\\nf_Y(y) & = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2} \\right)\n\\end{aligned}\n\\]Suppose \\(\\mu = 0.39\\) \\(\\sigma^2 = 0.0025\\). probability isn’t bad? , \\(\\Pr(Y\\geq 0.45)\\)?\\[\n\\begin{aligned} \n\\Pr(Y \\geq 0.45)  & = 1 - \\Pr(Y \\leq 0.45 ) \\\\\n& = 1 - \\Pr(0.05 Z + 0.39 \\leq  0.45) \\\\\n& = 1 - \\Pr(Z \\leq \\frac{0.45-0.39 }{0.05} ) \\\\\n& = 1 - \\frac{1}{\\sqrt{2\\pi} } \\int_{-\\infty}^{6/5} \\exp(-z^2/2) \\,dz \\\\\n& = 1 - F_{Z} (\\frac{6}{5} ) \\\\\n& = 0.1150697\n\\end{aligned}\n\\]","code":""},{"path":"general-random-variables.html","id":"gamma-distribution","chapter":"Day 9 General random variables","heading":"9.5 Gamma distribution","text":"Definition 9.1  (Gamma function) Suppose \\(\\alpha>0\\). define \\(\\Gamma(\\alpha)\\) \\[\n\\begin{aligned}\n\\Gamma(\\alpha) &= \\int_{0}^{\\infty} y^{\\alpha- 1} e^{-y} \\,dy \\\\\n&= (\\alpha- 1)! \\, \\forall \\alpha \\\\{1, 2, 3, \\ldots\\}\n\\end{aligned}\n\\]Suppose \\(\\Gamma(\\alpha)\\),\\[\n\\begin{aligned}\n\\frac{\\Gamma(\\alpha)}{\\Gamma(\\alpha)} & = \\frac{\\int_{0}^{\\infty} y^{\\alpha-1} e^{-y} dy}{\\Gamma(\\alpha)} \\\\\n1 & = \\int_{0}^{\\infty} \\frac{1}{\\Gamma(\\alpha)} y^{\\alpha-1} e^{-y} \\,dy \n\\end{aligned}\n\\]\\(\\frac{\\Gamma(\\alpha)}{\\Gamma(\\alpha)}\\) integrates 1 full domain (.e. non-negative real numbers), also PDF.Set \\(X = Y/\\beta\\)\\[\n\\begin{aligned}\nF(x) = \\Pr(X \\leq x) & = \\Pr(Y/\\beta \\leq x ) \\\\\n& = \\Pr(Y \\leq x \\beta ) \\\\\n& = F_{Y} (x \\beta) \\\\\n\\frac{\\partial F_{Y} (x \\beta) }{\\partial x} & = f_{Y} (x \\beta) \\beta\n\\end{aligned}\n\\]result :\\[f(x|\\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha - 1} e^{-x\\beta}\\]Definition 9.2  (Gamma distribution) Suppose \\(X\\) continuous random variable, \\(X \\geq 0\\). pdf \\(X\\) \\[f(x|\\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha - 1} e^{-x\\beta}\\]\\(x\\geq 0\\) \\(0\\) otherwise, say \\(X\\) Gamma distribution.\\[X \\sim \\text{Gamma}(\\alpha, \\beta)\\]Suppose \\(X \\sim \\text{Gamma}(\\alpha, \\beta)\\)\\[\n\\begin{aligned}\n\\E[X] & = \\frac{\\alpha}{\\beta} \\\\\n\\Var(X) & = \\frac{\\alpha}{\\beta^2}\n\\end{aligned}\n\\]Suppose \\(\\alpha = 1\\) \\(\\beta = \\lambda\\). \\[\n\\begin{aligned}\nX & \\sim \\text{Gamma}(1, \\lambda) \\\\\nf(x|1, \\lambda ) & = \\lambda e^{- x \\lambda}\n\\end{aligned}\n\\]say\\[X \\sim \\text{Exponential}(\\lambda)\\]","code":""},{"path":"general-random-variables.html","id":"properties-of-gamma-distributions","chapter":"Day 9 General random variables","heading":"9.5.1 Properties of Gamma distributions","text":"Proposition 9.1  Suppose sequence independent random variables, \\[X_{} \\sim \\text{Gamma}(\\alpha_{}, \\beta)\\]\\[Y = \\sum_{=1}^{N} X_{}\\]\\[Y \\sim \\text{Gamma}(\\sum_{=1}^{N} \\alpha_{} , \\beta)\\]","code":""},{"path":"general-random-variables.html","id":"importance-of-the-gamma-distribution","chapter":"Day 9 General random variables","heading":"9.5.2 Importance of the Gamma distribution","text":"exponential \\(\\chi^2\\) distributions special cases Gamma distribution. Gamma distribution also commonly used Bayesian statistics conjugate prior various types inverse scale (rate) parameters.\\(\\lambda\\) exponential Poisson distributions\\(\\beta\\) Gamma distribution ","code":""},{"path":"general-random-variables.html","id":"chi2-distribution","chapter":"Day 9 General random variables","heading":"9.6 \\(\\chi^2\\) distribution","text":"Suppose \\(Z \\sim \\text{Normal}(0,1)\\). Consider \\(X = Z^2\\)\\[\n\\begin{aligned}\nF_{X}(x)   & =  \\Pr(X \\leq x) \\\\\n& = \\Pr(Z^2 \\leq x ) \\\\\n& = \\Pr(-\\sqrt{x} \\leq Z \\leq \\sqrt{x}) \\\\\n& = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\sqrt{x}}^{\\sqrt{x} } e^{-\\frac{z^2}{2}} \\,dz\\\\\n& = F_{Z} (\\sqrt{x}) - F_{Z} (-\\sqrt{x})\n\\end{aligned}\n\\]PDF \\[\n\\begin{aligned}\n\\frac{\\partial F_{X}(x) }{\\partial x }  & = f_{Z} (\\sqrt{x}) \\frac{1}{2\\sqrt{x}} + f_{Z}(-\\sqrt{x}) \\frac{1}{2\\sqrt{x}} \\\\\n& = \\frac{1}{\\sqrt{x}}\\frac{1}{2 \\sqrt{2\\pi}} ( 2e^{-\\frac{x}{2}}) \\\\\n& = \\frac{1}{\\sqrt{x}}\\frac{1}{\\sqrt{2\\pi}} ( e^{-\\frac{x}{2}}) \\\\\n& = \\frac{(\\frac{1}{2})^{1/2}}{\\Gamma(\\frac{1}{2})}\\left(x^{1/2 - 1} e^{-\\frac{x}{2}}\\right)\n\\end{aligned}\n\\]\\(X \\sim \\text{Gamma}(1/2, 1/2)\\)\\(X = \\sum_{=1}^{N} Z^2\\), \\(X \\sim \\text{Gamma}(n/2, 1/2)\\)Definition 9.3  (Chi-Squared distribution) Suppose \\(X\\) continuous random variable \\(X\\geq 0\\), PDF\\[f(x) = \\frac{1}{2^{n/2} \\Gamma(n/2) } x^{n/2 - 1} e^{-x/2}\\]say \\(X\\) \\(\\chi^2\\) distribution \\(n\\) degrees freedom. Equivalently,\\[X \\sim \\chi^{2}(n)\\]","code":""},{"path":"general-random-variables.html","id":"chi2-properties","chapter":"Day 9 General random variables","heading":"9.6.1 \\(\\chi^2\\) properties","text":"Suppose \\(X \\sim \\chi^2(n)\\)\\[\n\\begin{aligned}\n\\E[X] & = \\E\\left[\\sum_{=1}^{N} Z_{}^2\\right] \\\\\n & = \\sum_{=1}^{N} \\E[Z_{}^{2} ] \\\\\n\\Var(Z_{} ) & = \\E[Z_{}^2] - \\E[Z_{}]^2\\\\\n1 & = \\E[Z_{}^2]- 0 \\\\\n\\E[X] & = n \n\\end{aligned}\n\\]\\[\n\\begin{aligned}\n\\Var(X) & = \\sum_{=1}^{N} \\Var(Z_{}^2) \\\\\n& = \\sum_{=1}^{N} \\left(\\E[Z_{}^{4} ] - \\E[Z_{}]^{2}  \\right) \\\\\n& = \\sum_{=1}^{N} \\left(3 - 1\\right ) = 2n \n\\end{aligned}\n\\]use \\(\\chi^2\\) across statistics.","code":""},{"path":"general-random-variables.html","id":"students-t-distribution","chapter":"Day 9 General random variables","heading":"9.7 Student’s \\(t\\) distribution","text":"Definition 9.4  (Student's t-distribution) Suppose \\(Z \\sim \\text{Normal}(0, 1)\\) \\(U \\sim \\chi^2(n)\\). Define random variable \\(Y\\) ,\\[Y = \\frac{Z}{\\sqrt{\\frac{U}{n}}}\\]\\(Z\\) \\(U\\) independent \\(Y \\sim t(n)\\), PDF\\[t(n) = \\frac{\\Gamma(\\frac{n+1}{2})}{\\sqrt{\\pi n } \\Gamma(\\frac{n}{2})}\\left(1 + \\frac{x^2}{n}\\right)^{-\\frac{n+1}{2}}\\]use t-distribution extensively test statistics.\\(n\\) number degrees freedom\\(\\Gamma\\) Gamma function \\(\\Gamma(n) = (n-1)!\\)","code":""},{"path":"general-random-variables.html","id":"history-of-students-t","chapter":"Day 9 General random variables","heading":"9.7.1 History of Student’s \\(t\\)","text":"\nFigure 9.1: William Sealy Gosset, creator Student’s \\(t\\) distribution. Source: Wikipedia\n\nFigure 9.2: pint Guinness. really preferred type beer, favorite brother--law. Source: Wikipedia\nWilliam Sealy Gosset researcher worked Guinness brewery. Gosset part revolution applying statistics beermaking.chemical properties barley effect beer taste?fertilizer produces best crop yield?Gosset’s problem experiments typically \\(N\\) low 3. properties normal distribution break low sample sizes - Gosset determine estimated mean statistically distinguishable zero?order solve problem, Gosset applied new distribution accounts sample size. worked Guinness, Gosset publish distribution real name. resulting paper published pseudonym “Student,” became known Student’s \\(t\\)-distribution","code":""},{"path":"general-random-variables.html","id":"differences-from-the-normal-distribution","chapter":"Day 9 General random variables","heading":"9.7.2 Differences from the Normal Distribution","text":"normal distribution always shape. \\(Z\\)-scores \\(-1.96\\) \\(+1.96\\) always mark boundaries 95% confidence interval.However shape student’s \\(t\\)-distribution changes depending sample size. sample sizes low, student’s \\(t\\)-distribution expands boundaries random sampling error, creating larger confidence interval. avoids overconfident sample statistic. sample size increases, confidence bounds shrink. sample size approaches infinite size, student’s \\(t\\)-distribution takes shape normal distribution.","code":""},{"path":"multivar-distribs.html","id":"multivar-distribs","chapter":"Day 10 Multivariate distributions","heading":"Day 10 Multivariate distributions","text":"","code":""},{"path":"multivar-distribs.html","id":"learning-objectives-9","chapter":"Day 10 Multivariate distributions","heading":"Learning objectives","text":"Define joint probability density functionCondition PDFs random variablesIdentify independence two random variablesDefine covariance correlationExamine sums random variablesDefine multivariate normal distribution","code":""},{"path":"multivar-distribs.html","id":"supplemental-readings-9","chapter":"Day 10 Multivariate distributions","heading":"Supplemental readings","text":"Chapters 2.5-.7, 3.4-5, 4.2, 4.5, Bertsekas Tsitsiklis (2008)Equivalent reading Bertsekas Tsitsiklis lecture notes","code":""},{"path":"multivar-distribs.html","id":"multivariate-distribution","chapter":"Day 10 Multivariate distributions","heading":"10.1 Multivariate distribution","text":"Definition 10.1  (Multivariate distribution) say \\(X\\) \\(Y\\) jointly continuous , \\(x\\\\Re\\) \\(y\\\\Re\\), exists function \\(f(x,y)\\) \\(C \\subset \\Re^{2}\\),\\[\\Pr\\{(X, Y) \\C \\}  = \\iint_{(x,y)\\C} f(x,y)\\, dx\\, dy\\]\\(C \\subset R^{2}\\)?\\[R^{2} = R \\underbrace{\\times}_{\\text{Cartesian Product}} R\\]2-d plane (piece paper). \\(C\\) subset 2-d plane.\\[C = \\{x, y: x \\[0,1] , y\\[0,1] \\}\\]\\[C = \\{x, y: x^2 + y^2 \\leq 1 \\}\\]\\[C = \\{ x, y: x> y, x,y\\(0,2)\\}\\], letting \\(C\\) entire two-dimensional plane, obtain normalization property\\[\\iint_{(x,y)\\C} f(x,y)\\, dx\\, dy = 1\\]generally,\\[\n\\begin{aligned}\nC &= \\{ x, y: x \\, y \\B \\} \\\\\n\\Pr\\{(X,Y) \\C \\} &= \\int_{B} \\int_{} f(x,y) \\, dx\\, dy\n\\end{aligned}\n\\]","code":""},{"path":"multivar-distribs.html","id":"examples-of-joint-pdfs","chapter":"Day 10 Multivariate distributions","heading":"10.2 Examples of joint PDFs","text":"’re going focus (initially) pdfs two random variables. Consider function \\(f:\\Re \\times \\Re \\rightarrow \\Re\\):Input: \\(x\\) value \\(y\\) value.Output: number real line\\(f(x,y) = \\)","code":""},{"path":"multivar-distribs.html","id":"multivariate-normal-distribution","chapter":"Day 10 Multivariate distributions","heading":"10.2.0.0.1 Multivariate normal distribution","text":"\\[\nf(x,y) =\n      \\frac{1}{2 \\pi  \\sigma_X \\sigma_Y \\sqrt{1-\\rho^2}}\n      \\mathrm{e}^{\n        -\\frac{1}{2(1-\\rho^2)}\\left[\n          \\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)^2 -\n          2\\rho\\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)\\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right) +\n          \\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right)^2 \n        \\right]\n      }\n\\]\\(\\rho\\) Pearson product-moment correlation coefficient \\(X\\) \\(Y\\) \n\\(\\sigma_X>0\\) \\(\\sigma_Y>0\\). case,\\[\n    \\boldsymbol\\mu = \\begin{pmatrix} \\mu_X \\\\ \\mu_Y \\end{pmatrix}, \\quad\n    \\boldsymbol\\Sigma = \\begin{pmatrix} \\sigma_X^2 & \\rho \\sigma_X \\sigma_Y \\\\\n                             \\rho \\sigma_X \\sigma_Y  & \\sigma_Y^2 \\end{pmatrix}.\n\\]3D plot:Contour plot:","code":""},{"path":"multivar-distribs.html","id":"multivariate-uniform-distribution-1","chapter":"Day 10 Multivariate distributions","heading":"10.2.0.0.2 Multivariate uniform distribution","text":"\\(f(x,y) = 1\\) \\(x \\[0,1], y \\[0,1]\\), \\(f(x,y) = 0\\)","code":""},{"path":"multivar-distribs.html","id":"joint-exponential-times-standard-normal-distribution","chapter":"Day 10 Multivariate distributions","heading":"10.2.0.0.3 Joint exponential \\(\\times\\) standard normal distribution","text":"\\(f(x,y) = \\frac{2 \\exp(-2x)}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{(x)^2}{2}\\right)\\) \\(x \\[0,\\infty), y \\\\Re\\), \\(f(x,y) = 0\\) otherwise","code":""},{"path":"multivar-distribs.html","id":"multivariate-cumulative-density-function","chapter":"Day 10 Multivariate distributions","heading":"10.3 Multivariate cumulative density function","text":"Definition 10.2  (Multivariate cumulative density function) jointly continuous random variables \\(X\\) \\(Y\\) define, \\(F(b,)\\) \\[F(b,) = \\Pr\\{ X \\leq b , Y \\leq \\} = \\int_{-\\infty}^{} \\int_{-\\infty}^{b} f(x,y) \\, dx\\, dy\\]","code":""},{"path":"multivar-distribs.html","id":"multivariate-normal","chapter":"Day 10 Multivariate distributions","heading":"10.3.0.0.1 Multivariate normal","text":"\\[\nf(x,y) =\n      \\frac{1}{2 \\pi  \\sigma_X \\sigma_Y \\sqrt{1-\\rho^2}}\n      \\mathrm{e}^{\n        -\\frac{1}{2(1-\\rho^2)}\\left[\n          \\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)^2 -\n          2\\rho\\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)\\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right) +\n          \\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right)^2 \n        \\right]\n      }\n\\]\\(\\rho\\) Pearson product-moment correlation coefficient \\(X\\) \\(Y\\) \n\\(\\sigma_X>0\\) \\(\\sigma_Y>0\\). case,\\[\n    \\boldsymbol\\mu = \\begin{pmatrix} \\mu_X \\\\ \\mu_Y \\end{pmatrix}, \\quad\n    \\boldsymbol\\Sigma = \\begin{pmatrix} \\sigma_X^2 & \\rho \\sigma_X \\sigma_Y \\\\\n                             \\rho \\sigma_X \\sigma_Y  & \\sigma_Y^2 \\end{pmatrix}.\n\\]","code":""},{"path":"multivar-distribs.html","id":"multivariate-uniform-distribution-2","chapter":"Day 10 Multivariate distributions","heading":"10.3.0.0.2 Multivariate uniform distribution","text":"\\(f(x,y) = 1\\) \\(x \\[0,1], y \\[0,1]\\), \\(f(x,y) = 0\\)","code":""},{"path":"multivar-distribs.html","id":"joint-exponential-times-standard-normal-distribution-1","chapter":"Day 10 Multivariate distributions","heading":"10.3.0.0.3 Joint exponential \\(\\times\\) standard normal distribution","text":"\\(f(x,y) = \\frac{2 \\exp(-2x)}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{(x)^2}{2}\\right)\\) \\(x \\[0,\\infty), y \\\\Re\\), \\(f(x,y) = 0\\) otherwise","code":""},{"path":"multivar-distribs.html","id":"marginalization","chapter":"Day 10 Multivariate distributions","heading":"10.4 Marginalization","text":"Definition 10.3  (Moving joint distributions univariate PDFs) Define \\(f_{X}(x)\\) marginal pdf \\(X\\),\\[f_{X}(x) = \\int_{-\\infty}^{\\infty} f(x,y) \\, dy\\]Similarly, define \\(f_{Y}(y)\\) marginal pdf \\(Y\\),\\[f_{Y}(y) = \\int_{-\\infty}^{\\infty} f(x,y)\\, dx\\]Definition 10.4  (Conditional probability distribution function) Suppose \\(X\\) \\(Y\\) continuous random variables joint pdf \\(f(x,y)\\). define conditional probability function \\(f(x|y)\\) \\[f(x|y) = \\frac{f(x, y) }{f_{Y}(y) }\\]","code":""},{"path":"multivar-distribs.html","id":"joint-vs.-conditional-pdf","chapter":"Day 10 Multivariate distributions","heading":"10.4.1 Joint vs. conditional PDF","text":"example using two standard normal variables \\(x\\) \\(y\\):\\[f(x,y) = x \\times y\\]\\(y\\) increases, marginal probability \\(x\\) changes.Note \\(f(x | y)\\) regardless value \\(y\\). , conditional probability \\(x\\) given \\(y\\) depend specific value \\(y\\). ?","code":""},{"path":"multivar-distribs.html","id":"why-does-marginalization-work","chapter":"Day 10 Multivariate distributions","heading":"10.4.2 Why does marginalization work?","text":"Begin discrete case.Consider jointly distributed discrete random variables, \\(X\\) \\(Y\\). ’ll suppose joint PMF,\\[\\Pr(X =x, Y = y) = p(x, y)\\]Suppose distribution allocates mass \\(x_{1}, x_{2}, \\ldots, x_{M}\\) \\(y_{1}, y_{2}, \\ldots, y_{N}\\). Define conditional mass function \\(\\Pr(X= x| Y= y)\\) ,\\[\n\\begin{aligned}\n\\Pr(X=x|Y=y) &\\equiv p(x|y)  \\\\\n& = \\frac{p(x,y)}{p(y)}\n\\end{aligned}\n\\]follows :\\[p(x,y) = p(x|y)p(y)\\]Marginalizing \\(y\\) get \\(p(x)\\) ,\\[p(x_{j}) = \\sum_{=1}^{N} p(x_{j} |y_{})p(y_{} )\\]","code":""},{"path":"multivar-distribs.html","id":"table-setup","chapter":"Day 10 Multivariate distributions","heading":"10.4.2.0.1 Table setup","text":"","code":""},{"path":"multivar-distribs.html","id":"example-probabilities","chapter":"Day 10 Multivariate distributions","heading":"10.4.2.0.2 Example probabilities","text":"","code":""},{"path":"multivar-distribs.html","id":"marginalize-over-columns","chapter":"Day 10 Multivariate distributions","heading":"10.4.2.0.3 Marginalize over columns","text":"\\[\n\\begin{aligned}\np_{X}(0) & = \\Pr(0|y = 0) \\Pr(y= 0) + \\Pr(0|y=1) \\Pr(y=1) \\\\ \n& = \\frac{0.01}{0.26} \\times 0.26 + \\frac{0.05}{0.74} \\times 0.74 \\\\\n& = 0.06\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\np_{X}(1) & = \\Pr(1|y = 0) \\Pr(y= 0) + \\Pr(1|y=1) \\Pr(y=1) \\\\ \n& = \\frac{0.25}{0.26} \\times 0.26 + \\frac{0.69}{0.74} \\times 0.74 \\\\\n& = 0.94\n\\end{aligned}\n\\]","code":""},{"path":"multivar-distribs.html","id":"move-to-the-continuous-case","chapter":"Day 10 Multivariate distributions","heading":"10.4.3 Move to the continuous case","text":"jointly distributed continuous random variables \\(X\\) \\(Y\\) define,\\[f_{X|Y}(x|y) = \\frac{f(x,y)}{f_{Y}(y) }\\], analogously, can define\\[f_{X}(x) = \\int_{-\\infty }^{\\infty} f_{X|Y}(x|y)f_{Y}(y) \\, dy\\]Think \\(f_{X|Y}(x|y)\\) pdf \\(X\\) value \\(Y\\). average pdfs get final pdf \\(X\\) (want densities lots area \\(Y\\) receive lots weight, whereas densities without much area \\(Y\\) receive little weight).","code":""},{"path":"multivar-distribs.html","id":"a-simple-example","chapter":"Day 10 Multivariate distributions","heading":"10.4.4 A (simple) example","text":"Suppose \\(X\\) \\(Y\\) jointly continuous \\[\n\\begin{aligned}\nf_{XY}(x,y) &  = x + \\frac{3}{2}y^2 \\text{ ,  } x \\[0,1], y \\[0,1] \\\\\n& = 0 \\text{ , otherwise } \n\\end{aligned}\n\\]can show function joint PDF since area multivariate curve 1.\\[\n\\begin{aligned}\n\\iint_{-\\infty}^{+ \\infty} \\, dy \\, dx &= \\iint_0^1 x + \\frac{3}{2}y^2 \\, dy \\, dx \\\\\n&= \\int_0^1 \\left[ \\frac{1}{2} x^2 + \\frac{3}{2}y^2x \\right]_0^1 \\, dy \\\\\n&= \\int_0^1 \\frac{1}{2} + \\frac{3}{2}y^2 \\, dy \\\\\n&= \\frac{1}{2} \\int_0^1 1 \\, dy + \\frac{3}{2} \\int_0^1 y^2 \\, dy \\\\\n&= \\left[ \\frac{y}{2} \\right]_0^1 + \\left[\\frac{y^3}{2} \\right]_0^1 \\\\\n&= \\frac{1}{2} + \\frac{1}{2} \\\\\n&= 1\n\\end{aligned}\n\\]want \\(f_{X}(x)\\). Assume \\[f_{Y}(y) = y\\]\\[f_{X|Y}(x|y) = \\frac{x + \\frac{3}{2}y^2}{y}\\]\\[\n\\begin{aligned}\nf_X(x) &= \\int_{0}^{1} f(x|y)f(y)\\, dy \\\\\n&= \\int_0^1 \\frac{x + \\frac{3}{2}y^2}{y} (y) \\, dy \\\\\n&= \\int_0^1 x + \\frac{3}{2}y^2\\, dy \\\\\n&= x \\int_0^1 1 \\, dy + \\frac{3}{2} \\int_0^1 y^2 \\, dy \\\\\n&= x \\left[y \\right]_0^1 + \\frac{3}{2} \\left[\\frac{y^3}{3} \\right]_0^1 \\\\\n&= \\left[xy \\right]_0^1 + \\left[\\frac{y^3}{2} \\right]_0^1 \\\\\n&= x + \\frac{1}{2}\n\\end{aligned}\n\\]","code":""},{"path":"multivar-distribs.html","id":"more-complex-example","chapter":"Day 10 Multivariate distributions","heading":"10.4.5 More complex example","text":"Suppose \\(X\\) \\(Y\\) jointly distributed PDF\\[f(x,y) = 2 \\exp(-x) \\exp(-2y), \\forall \\, x>0, y>0)\\]","code":""},{"path":"multivar-distribs.html","id":"verify-this-is-a-pdf","chapter":"Day 10 Multivariate distributions","heading":"10.4.5.1 Verify this is a PDF","text":"\\[\n\\begin{aligned}\n\\int_{0}^{\\infty} \\int_{0}^{\\infty} f(x, y) & = 2\\int_{0}^{\\infty} \\int_{0}^{\\infty} \\exp(-x) \\exp(-2y) \\, dx\\, dy \\\\\n& = 2 \\int_{0}^{\\infty}\\exp(-2y) \\, dy \\int_{0}^{\\infty} \\exp(-x) \\, dx  \\\\\n& = 2 (-\\frac{1}{2} \\exp(-2y)|_{0}^{\\infty}  ) ( - \\exp(-x)|_{0}^{\\infty} ) \\\\\n& = 2\\left[ (-\\frac{1}{2}(\\lim_{y\\rightarrow\\infty} \\exp(-2y) - 1))(- (\\lim_{x\\rightarrow \\infty} \\exp(-x) - 1) ) \\right] \\\\\n& = 2 \\left[  -\\frac{1}{2} (-1) \\times -1 (-1)   \\right] \\\\\n& = 1 \n\\end{aligned}\n\\]","code":""},{"path":"multivar-distribs.html","id":"calculate-cdf","chapter":"Day 10 Multivariate distributions","heading":"10.4.5.2 Calculate CDF","text":"\\[\n\\begin{aligned}\nF(x,y) \\equiv \\Pr\\{X \\leq b, Y \\leq \\} & = 2 \\int_{0}^{} \\int_{0}^{b} \\exp(-x) \\exp(-2y) \\, dx\\, dy \\\\\n& = 2 (\\int_{0}^{} \\exp(-2y) \\, dy) (\\int_{0}^{b} \\exp(-x) \\, dx) \\\\\n& = 2 \\left[-\\frac{1}{2} (\\exp(-2a) -1 )\\right]\\left[ - (\\exp(-b) - 1) \\right] \\\\\n& = \\left[1 - \\exp(-2a) \\right] \\left[ 1- \\exp(-b) \\right]\n\\end{aligned}\n\\]","code":""},{"path":"multivar-distribs.html","id":"calculate-f_xx-and-f_yy","chapter":"Day 10 Multivariate distributions","heading":"10.4.5.3 Calculate \\(f_{X}(x)\\) and \\(f_{Y}(y)\\)","text":"\\[\n\\begin{aligned}\nf_{X}(x) & = \\int_{0}^{\\infty} 2\\exp(-x) \\exp(-2y) \\, dy \\\\\n& = 2 \\exp(-x) \\int_{0}^{\\infty} \\exp(-2y) \\, dy \\\\\n& = 2 \\exp(-x) \\left[ -\\frac{1}{2}(0 - 1) \\right] \\\\\n& = \\exp(-x)\n\\end{aligned} \n\\]\\[\n\\begin{aligned}\nf_{Y}(y) & = \\int_{0}^{\\infty} 2 \\exp(-x) \\exp(-2y) \\, dx \\\\\n& = 2 \\exp(-2y) \\int_{0}^{\\infty} \\exp(-x) \\, dx \\\\\n& = 2 \\exp(-2y) \\left[-(0 -1) \\right] \\\\\n& = 2 \\exp(-2y)\n\\end{aligned}\n\\]","code":""},{"path":"multivar-distribs.html","id":"conditional-distribution","chapter":"Day 10 Multivariate distributions","heading":"10.5 Conditional distribution","text":"Definition 10.5  Two random variables \\(X\\) \\(Y\\) independent two sets real numbers \\(\\) \\(B\\),\\[\\Pr\\{ X \\, Y \\B \\} = \\Pr\\{X \\\\} \\Pr\\{Y \\B\\}\\]Equivalently say \\(X\\) \\(Y\\) independent ,\\[f(x,y) = f_{X}(x) f_{Y}(y)\\]\\(X\\) \\(Y\\) independent, say dependent.\\(X\\) \\(Y\\) independent, \\[\n\\begin{aligned}\nf_{X|Y} (x|y) & = \\frac{f(x,y)}{f_{Y}(y)} \\\\\n& = \\frac{f_{X}(x)f_{Y}(y)}{f_{Y}(y) } \\\\\n& = f_{X}(x) \n\\end{aligned}\n\\]","code":""},{"path":"multivar-distribs.html","id":"a-simple-example-of-dependence","chapter":"Day 10 Multivariate distributions","heading":"10.5.1 A (simple) example of dependence","text":"Suppose \\(X\\) \\(Y\\) jointly continuous \\[\n\\begin{eqnarray}\nf(x,y) &  = & x + y \\text{ ,  } x \\[0,1], y \\[0,1] \\\\\n& = & 0 \\text{ , otherwise }  \n\\end{eqnarray}\n\\]\\[\n\\begin{eqnarray}\nf_{X}(x) & = & \\int_{0}^{1} \\left(x + y \\right) \\, dy \\\\\n& = & xy  + \\frac{y^2}{2} |^{1}_{0} \\\\\n& = & x + \\frac{1}{2} \\\\\nf_{Y}(y) & = & \\frac{1}{2} + y \n\\end{eqnarray}\n\\]\\[\n\\begin{eqnarray}\nf(x, y)  &= & x + y \\\\\nf_{X}(x) f_{Y}(y) & = & (\\frac{1}{2} + x) (\\frac{1}{2} + y) \\\\\n& = & \\frac{1}{4} + \\frac{x + y}{2} + xy\n\\end{eqnarray}\n\\]Intuition: different levels \\(X\\) distribution \\(Y\\) behaves differently.\n\\(X\\) provides information \\(Y\\).","code":""},{"path":"multivar-distribs.html","id":"expectation-1","chapter":"Day 10 Multivariate distributions","heading":"10.6 Expectation","text":"Definition 10.6  jointly continuous random variables \\(X\\) \\(Y\\) define,\\[\n\\begin{eqnarray}\n\\E[X] & = & \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} x f(x,y) \\, dx\\, dy \\\\\n\\E[Y] & = & \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} y f(x,y) \\, dx\\, dy \\\\\n\\E[XY] & = & \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} x y f(x,y) \\, dx\\, dy \n\\end{eqnarray}\n\\]Proposition 10.1  Suppose \\(g:\\Re^{2} \\rightarrow \\Re\\). \\[\\E[g(X, Y)] = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x, y) f(x,y) \\, dx\\, dy\\]","code":""},{"path":"multivar-distribs.html","id":"covariance-and-correlation","chapter":"Day 10 Multivariate distributions","heading":"10.7 Covariance and correlation","text":"Definition 10.7  (Covariance) jointly continous random variables \\(X\\) \\(Y\\) define, covariance \\(X\\) \\(Y\\) ,\\[\n\\begin{align}\n\\Cov(X, Y)\n&= \\E\\left[\\left(X - \\E\\left[X\\right]\\right) \\left(Y - \\E\\left[Y\\right]\\right)\\right] \\\\\n&= \\E\\left[X Y - X \\E\\left[Y\\right] - \\E\\left[X\\right] Y + \\E\\left[X\\right] \\E\\left[Y\\right]\\right] \\\\\n&= \\E\\left[X Y\\right] - \\E\\left[X\\right] \\E\\left[Y\\right] - \\E\\left[X\\right] \\E\\left[Y\\right] + \\E\\left[X\\right] \\E\\left[Y\\right] \\\\\n&= \\E\\left[X Y\\right] - \\E\\left[X\\right] \\E\\left[Y\\right]\n\\end{align}\n\\]Definition 10.8  (Correlation) Define correlation \\(X\\) \\(Y\\) ,\\[\\Cor(X,Y) =  \\frac{\\Cov(X,Y) }{\\sqrt{\\Var(X) \\Var(Y) } }\\]","code":""},{"path":"multivar-distribs.html","id":"some-observations","chapter":"Day 10 Multivariate distributions","heading":"10.7.1 Some observations","text":"","code":""},{"path":"multivar-distribs.html","id":"variance-is-the-covariance-of-a-random-variable-with-itself","chapter":"Day 10 Multivariate distributions","heading":"10.7.1.1 Variance is the covariance of a random variable with itself","text":"\\[\n\\begin{eqnarray}\n\\Cov(X,X) & = & \\E[X X] - \\E[X]\\E[X] \\\\\n& = & \\E[X^2] - \\E[X]^2\n\\end{eqnarray}\n\\]","code":""},{"path":"multivar-distribs.html","id":"correlation-measures-the-linear-relationship-between-two-random-variables","chapter":"Day 10 Multivariate distributions","heading":"10.7.1.2 Correlation measures the linear relationship between two random variables","text":"Suppose \\(X = Y\\):\\[\n\\begin{eqnarray}\n\\Cor(X,Y) & = & \\frac{\\Cov(X,Y)}{\\sqrt{\\Var(X)\\Var(Y)} } \\\\\n& = & \\frac{\\Var(X)}{\\Var(X)} \\\\\n& = & 1\n\\end{eqnarray}\n\\]Suppose \\(X = -Y\\):\\[\n\\begin{eqnarray}\n\\Cor(X,Y) & = & \\frac{\\Cov(X,Y)}{\\sqrt{\\Var(X)\\Var(Y)} } \\\\\n& =  & \\frac{- \\Var(X)}{\\Var(X)} \\\\\n& = & -1 \n\\end{eqnarray}\n\\]","code":""},{"path":"multivar-distribs.html","id":"correlation-is-between--1-and-1","chapter":"Day 10 Multivariate distributions","heading":"10.7.1.3 Correlation is between -1 and 1","text":"\\[|\\Cor(X,Y)| \\leq 1\\]","code":""},{"path":"multivar-distribs.html","id":"sums-of-random-variables","chapter":"Day 10 Multivariate distributions","heading":"10.8 Sums of random variables","text":"Suppose sequence random variables \\(X_{}\\) , \\(= 1, 2, \\ldots, N\\) joint pdf\\[f(\\boldsymbol{x}) = f(x_{1}, x_{2}, \\ldots, x_{n})\\]\\(\\E[\\sum_{=1}^{N}X_{} ] = \\sum_{=1}^{N} \\E[X_{}]\\)\\(\\Var(\\sum_{=1}^{N} X_{} ) = \\sum_{=1}^{N} \\Var(X_{} ) + 2 \\sum_{<j} \\Cov(X_{}, X_{j})\\)Proposition 10.2  Suppose sequence random variables \\(X_{}\\) , \\(= 1, 2, \\ldots, N\\).Suppose joint pdf,\\[f(\\boldsymbol{x}) = f(x_{1}, x_{2}, \\ldots, x_{n})\\]\\[\\E[\\sum_{=1}^{N} X_{} ] = \\sum_{=1}^{N} \\E[X_{} ]\\]Proof. \\[\n\\begin{eqnarray}\n\\E[\\sum_{=1}^{N} X_{} ] & = &  \\E[X_{1} + X_{2} + \\ldots + X_{N}] \\\\\n& = & \\int_{-\\infty}^{\\infty} \\cdot \\cdot \\cdot \\iint_{-\\infty}^{\\infty} (x_{1} + x_{2} + \\ldots + x_{N}) f(x_{1}, x_{2}, \\ldots, x_{N}) \\, dx_{1}\\, dx_{2}\\ldots \\, dx_{N} \\\\\n& = & \\int_{-\\infty}^{\\infty}x_{1} f_{X_{1}}(x_{1}) \\, dx_{1}  + \\int_{-\\infty}^{\\infty}x_{2} f_{X_{2}}(x_{2}) \\, dx_{2} + \\ldots + \\int_{-\\infty}^{\\infty}x_{N} f_{X_{N}}(x_{N}) \\, dx_{N}  \\\\\n& = & \\E[X_{1} ] + \\E[X_{2}] + \\ldots + \\E[X_{N}] \n\\end{eqnarray}\n\\]Proposition 10.3  Suppose \\(X_{}\\) sequence random variables. \\[\n\\begin{eqnarray}\n\\Var(\\sum_{=1}^{N} X_{} ) & = & \\sum_{=1}^{N} \\Var(X_{} )  + 2 \\sum_{<j} \\Cov(X_{}, X_{j} ) \n\\end{eqnarray}\n\\]Proof. Consider two random variables, \\(X_{1}\\) \\(X_{2}\\). ,\\[\n\\begin{eqnarray}\n\\Var(X_{1} + X_{2} ) & = & \\E[(X_{1} + X_{2})^2] - \\left(\\E[X_{1}] + \\E[X_{2}] \\right)^2 \\\\ \n& = & \\E[X_{1}^2]  + 2 \\E[X_{1}X_{2}]  + \\E[X_{2}^2]  \\\\\n&& - (\\E[X_{1}])^2 - 2 \\E[X_{1}] \\E[X_{2}]  - 2 \\E[X_{2}]^2 \\\\\n& = & \\underbrace{\\E[X_{1}^2] - (\\E[X_{1}])^2}_{\\Var(X_{1}) }   + \\underbrace{\\E[X_{2}^2] - \\E[X_{2}]^{2}}_{\\Var(X_{2})} \\\\\n&&  + 2 \\underbrace{(\\E[X_{1} X_{2} ]  - \\E[X_{1}] \\E[X_{2} ] )}_{\\Cov(X_{1}, X_{2} ) } \\\\\n& = & \\Var(X_{1} ) + \\Var(X_{2} ) + 2 \\Cov(X_{1}, X_{2})\n\\end{eqnarray}\n\\]","code":""},{"path":"multivar-distribs.html","id":"multivariate-normal-distribution-1","chapter":"Day 10 Multivariate distributions","heading":"10.9 Multivariate normal distribution","text":"Definition 10.9  (Multivariate normal distribution) Suppose \\(\\boldsymbol{X} = (X_{1}, X_{2}, \\ldots, X_{N})\\) vector random variables. \\(\\boldsymbol{X}\\) pdf\\[f(\\boldsymbol{x}) = (2 \\pi)^{-N/2} \\text{det}\\left(\\boldsymbol{\\Sigma}\\right)^{-1/2} \\exp\\left(-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu})^{'}\\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu} ) \\right)\\]say \\(\\boldsymbol{X}\\) Multivariate Normal Distribution,\\[\\boldsymbol{X} \\sim \\text{Multivariate Normal} (\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\]regularly used likelihood, Bayesian, parametric inferences.","code":""},{"path":"multivar-distribs.html","id":"bivariate-example","chapter":"Day 10 Multivariate distributions","heading":"10.9.1 Bivariate example","text":"Consider (bivariate) special case \\(\\boldsymbol{\\mu} = (0, 0)\\) \\[\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} \n1 & 0 \\\\\n0 & 1 \\\\\n\\end{pmatrix}\n\\]\\[\n\\begin{eqnarray}\nf(x_{1}, x_{2} ) & = & (2\\pi)^{-2/2} 1^{-1/2} \\exp\\left(-\\frac{1}{2}\\left( (\\boldsymbol{x}  - \\boldsymbol{0} ) ^{'} \\begin{pmatrix} \n1 & 0 \\\\\n0 & 1 \\\\\n\\end{pmatrix}\n(\\boldsymbol{x}  - \\boldsymbol{0} ) \\right) \\right) \\\\\n& = & \\frac{1}{2\\pi} \\exp\\left(-\\frac{1}{2} (x_{1}^{2} + x_{2} ^ 2 )     \\right) \\\\\n& = & \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x_{1}^{2}}{2}  \\right) \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x_{2}^{2}}{2}  \\right)\n\\end{eqnarray}\n\\]\\(\\leadsto\\) product univariate standard normally distributed random variablesDefinition 10.10  (Standard multivariate normal distribution) Suppose \\(\\boldsymbol{Z} = (Z_{1}, Z_{2}, \\ldots, Z_{N})\\) \\[\\boldsymbol{Z} \\sim \\text{Multivariate Normal}(\\boldsymbol{0}, \\boldsymbol{}_{N} )\\]call \\(\\boldsymbol{Z}\\) standard multivariate normal.","code":""},{"path":"multivar-distribs.html","id":"properties-of-the-multivariate-normal-distribution","chapter":"Day 10 Multivariate distributions","heading":"10.9.2 Properties of the multivariate normal distribution","text":"Suppose \\(\\boldsymbol{X} = (X_{1}, X_{2}, \\ldots, X_{N} )\\)\\[\n\\begin{eqnarray}\n\\E[\\boldsymbol{X} ] & = &  \\boldsymbol{\\mu} \\\\\n\\Cov(\\boldsymbol{X} ) & = & \\boldsymbol{\\Sigma} \n\\end{eqnarray}\n\\],\\[\n\\begin{eqnarray}\n\\boldsymbol{\\Sigma}  & = & \\begin{pmatrix} \n\\Var(X_{1}) & \\Cov(X_{1}, X_{2}) & \\ldots & \\Cov(X_{1}, X_{N}) \\\\\n\\Cov(X_{2}, X_{1}) & \\Var(X_{2}) & \\ldots & \\Cov(X_{2}, X_{N} ) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\Cov(X_{N}, X_{1} ) & \\Cov(X_{N}, X_{2} ) & \\ldots & \\Var(X_{N} ) \\\\\n\\end{pmatrix} \n\\end{eqnarray}\n\\]","code":""},{"path":"multivar-distribs.html","id":"independence-and-multivariate-normal","chapter":"Day 10 Multivariate distributions","heading":"10.9.3 Independence and multivariate normal","text":"Proposition 10.4  Suppose \\(X\\) \\(Y\\) independent. \\[\\Cov(X, Y) = 0\\]Proof. Suppose \\(X\\) \\(Y\\) independent.\\[\\Cov(X, Y) = \\E[XY] - \\E[X]\\E[Y]\\]Calculating \\(\\E[XY]\\)\\[\n\\begin{eqnarray}\n\\E[XY] & = & \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} x y f(x,y)\\, dx\\, dy \\\\\n& =& \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} x y f_{X}(x) f_{Y}(y)\\, dx\\, dy \\\\\n& = & \\int_{-\\infty}^{\\infty} x f_{X}(x) \\, dx \\int_{-\\infty}^{\\infty} y f_{Y}(y) \\, dy \\\\\n& = & \\E[X] \\E[Y]\n\\end{eqnarray}\n\\]\\(\\Cov(X,Y) = 0\\).Zero covariance generally imply independence!Example 10.1  (Dependent variables zero covariance) Suppose \\(X \\\\{-1, 1\\}\\) \\(\\Pr(X = 1) = \\Pr(X = -1) = 1/2\\).Suppose \\(Y \\\\{-1, 0,1\\}\\) \\(Y = 0\\) \\(X = -1\\) \\(\\Pr(Y = 1) = \\Pr(Y= -1)\\) \\(X = 1\\).\\[\n\\begin{eqnarray}\n\\E[XY] & = & \\sum_{\\\\{-1, 1\\} } \\sum_{j \\\\{-1, 0, 1\\}} j \\Pr(X = , Y = j) \\\\\n& = & -1 \\times 0 \\times  \\Pr(X = -1, Y = 0) + 1 \\times 1  \\times \\Pr(X = 1, Y = 1)  \\\\\n && - 1 \\times 1 \\times \\Pr(X = 1, Y = -1) \\\\\n &= & 0 + \\Pr(X = 1, Y = 1)  - \\Pr(X = 1, Y = -1 ) \\\\\n  & = &  0.25 - 0.25 = 0  \\\\\n\\E[X] & = &  0 \\\\\n\\E[Y] & = & 0 \n\\end{eqnarray}\n\\]Proposition 10.5  Suppose \\(\\boldsymbol{X} \\sim \\text{Multivariate Normal}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), \\(\\boldsymbol{X}= (X_{1}, X_{2}, \\ldots, X_{N})\\).\\(\\Cov(X_{}, X_{j}) = 0\\), \\(X_{}\\) \\(X_{j}\\) independent.","code":""},{"path":"limits.html","id":"limits","chapter":"Day 11 Properties of random variables and limit theorems","heading":"Day 11 Properties of random variables and limit theorems","text":"","code":""},{"path":"limits.html","id":"learning-objectives-10","chapter":"Day 11 Properties of random variables and limit theorems","heading":"Learning objectives","text":"Define iterated expectationsDemonstrate change coordinates probability density functionDefine moment generating functions transformationsConsider impact limit theorems core statistical techniquesProve weak law large numbersConnect principles convergence estimators central limit theorem","code":""},{"path":"limits.html","id":"supplemental-readings-10","chapter":"Day 11 Properties of random variables and limit theorems","heading":"Supplemental readings","text":"Chapter 5 Bertsekas Tsitsiklis (2008)Equivalent reading Bertsekas Tsitsiklis lecture notes","code":""},{"path":"limits.html","id":"iterated-expectations","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.1 Iterated Expectations","text":"Suppose \\(X\\) \\(Y\\) random variables. \\[\n\\E[X] = \\E[\\E[X|Y]]\n\\]Inner Expectation \\(E[X|Y] = \\int_{-\\infty}^{\\infty} x f_{X|Y} (x|y) dx\\).Outer expectation \\(y\\).Proof (Iterated expectations). \\[\n\\begin{aligned}\n\\E[\\E[X|Y]] & = \\int_{-\\infty}^{\\infty}  \\int_{-\\infty}^{\\infty} x f_{X|Y} (x|y) f_{Y}(y) dx dy \\nonumber \\\\\n&= \\int_{-\\infty}^{\\infty}  \\int_{-\\infty}^{\\infty} x f_{X|Y} (x|y) f_{Y}(y) dy dx \\nonumber \\\\\n& = \\int_{-\\infty}^{\\infty} x \\int_{-\\infty}^{\\infty}  f(x, y) dy dx  \\nonumber \\\\\n& = \\int_{-\\infty}^{\\infty}  x f_{X}(x) dx  \\nonumber \\\\ \n& = \\E[X] \\nonumber\n\\end{aligned}\n\\]Definition 11.1  (Beta distribution) Suppose \\(Y\\) continuous random variable \\(Y \\[0,1]\\) pdf \\(Y\\) given \\[\nf(y) = \\frac{\\Gamma(\\alpha_1 + \\alpha_2)}{\\Gamma(\\alpha_{1} ) \\Gamma(\\alpha_{2})} y^{\\alpha_{1} - 1} (1- y)^{\\alpha_{2} - 1 } \\nonumber \n\\]say \\(Y\\) Beta distribution parameters \\(\\alpha_{1}\\) \\(\\alpha_{2}\\). Equivalently,\\[\nY \\sim \\text{Beta}(\\alpha_{1}, \\alpha_{2} ) \\nonumber \n\\]Beta distribution proportionsBeta special case Dirichlet distribution\\(\\E[Y] = \\frac{\\alpha_{1}}{\\alpha_{1} + \\alpha_{2}}\\)Suppose\\[\n\\begin{aligned}\n\\pi & \\sim \\text{Beta}(\\alpha_{1}, \\alpha_{2}) \\nonumber \\\\\nY|\\pi, n & \\sim \\text{Binomial}(n, \\pi)\\nonumber \n\\end{aligned}\n\\]\\(\\E[Y]\\)?\\[\n\\begin{aligned}\n\\E[Y] & = \\E[\\E[Y| \\pi]] \\nonumber \\\\\n& = \\int_{-\\infty}^{\\infty} \\sum_{j = 0}^{N} {{N}\\choose{j}} j p(j|\\pi) f(\\pi) d\\pi \\nonumber \\\\\n& = \\int_{-\\infty}^{\\infty} N \\pi f(\\pi) d\\pi \\nonumber  \\\\\n& = N \\frac{\\alpha_{1}}{\\alpha_{1} + \\alpha_{2}} \\nonumber\n\\end{aligned}\n\\]","code":""},{"path":"limits.html","id":"change-of-coordinates","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.2 Change of coordinates","text":"Suppose \\(X\\) random variable \\(Y = g(X)\\), \\(g:\\Re \\rightarrow \\Re\\) \\(monotonic\\) function.Define \\(g^{-1}:\\Re \\rightarrow \\Re\\) \\(g^{-1}(g(X)) = X\\) differentiable. ,\\[\n\\begin{aligned}\nf_{Y}(y) & = f_{X}(g^{-1}(y))\\left|\\frac{\\partial g^{-1}(y)}{\\partial y} \\right| \\text{ } y = g(x) \\text{ } x \\nonumber \\\\\n& = 0 \\text{ otherwise } \\nonumber \n\\end{aligned}\n\\]Proof (Change coordinates). Suppose \\(g(\\cdot)\\) monotonically increasing (WLOG)\\[\n\\begin{aligned}\nF_{Y}(y) & = \\Pr(Y \\leq y) \\nonumber \\\\\n& = \\Pr(g(X) \\leq y) \\nonumber \\\\\n& = \\Pr(X \\leq g^{-1}(y) ) \\nonumber \\\\\n& = F_{X}(g^{-1}(y) ) \\nonumber\n\\end{aligned}\n\\]Now differentiating get pdf\\[\n\\begin{aligned}\n\\frac{\\partial F_{Y}(y)}{\\partial y} & = \\frac{\\partial F_{X}(g^{-1}(y) )} {\\partial y } \\nonumber \\\\\n & = f_{X}(g^{-1}(y) ) \\frac{\\partial g^{-1}(y)}{\\partial y }   \\nonumber \n\\end{aligned}\n\\]pdf \\(\\frac{\\partial g^{-1}(Y)}{\\partial y } > 0\\).Suppose \\(X\\) random variable pdf \\(f_{X}(x)\\). Suppose \\(Y = X^{n}\\). Find \\(f_{Y}(y)\\).\\(g^{-1} (x) = x^{1/n}\\).\\[\n\\begin{aligned}\nf_{Y}(y) & = f_{X}(g^{-1}(y)) \\left| \\frac{\\partial g^{-1}(Y)}{\\partial y } \\right| \\nonumber \\\\\n & = f_{X} (y^{1/n} ) \\frac{y^{\\frac{1}{n} - 1}}{n} \\nonumber \n\\end{aligned}\n\\]’ve used derive many pdfs.Normal distributionChi-Squared Distribution","code":""},{"path":"limits.html","id":"moment-generating-functions","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.3 Moment generating functions","text":"Definition 11.2  (Moment) Suppose \\(X\\) random variable pdf \\(f\\). Define,\\[\n\\E[X^{n}] = \\int_{-\\infty}^{\\infty} x^{n} f(x) dx   \\nonumber \n\\]call \\(X^{n}\\) \\(n^{\\text{th}}\\) moment \\(X\\).definition \\(\\Var(X) = \\text{Second Moment} - \\text{First Moment}^{2}\\). assuming integral converges.Proposition 11.1  (Moment generating function) Suppose \\(X\\) random variable pdf \\(f(x)\\). Call \\(M(t) = E[e^{tX}]\\),\\[\n\\begin{aligned}\nM(t) & = \\E[e^{tX}] \\nonumber \\\\\n & = \\int_{-\\infty}^{\\infty} e^{tx} f(x) dx \\nonumber \n\\end{aligned}\n\\]call \\(M(t)\\) moment generating function, :21\\[\n\\frac{\\partial^{n} M (t) }{\\partial^{n} t}|_{0} = E[X^{n}] \\nonumber \n\\]Proof (Moment generating function). Recall Taylor Expansion \\(e^{tX}\\) \\(0\\),\\[\ne^{tX} = 1 + tx + \\frac{t^2 x^2}{2!} + \\frac{t^3 x^3}{3!} + \\ldots \\nonumber\n\\],\\[\n\\E[e^{tX} ] =  1 + t\\E[X] + \\frac{t^2}{2!} \\E[X^2] + \\frac{t^3}{3!} \\E[X^3] + \\ldots \\nonumber\n\\]Differentiate :\\[\n\\begin{aligned}\n\\frac{\\partial M(t)}{\\partial t} & = 0 + \\E[X]  + \\frac{2t}{2!} \\E[X^2] + \\ldots \\nonumber \\\\\nM^{'}(0) & = 0 + \\E[X] + 0 + 0 \\ldots \\nonumber\n\\end{aligned}\n\\]Differentiate \\(n\\) times:\\[\n\\begin{aligned}\n\\frac{\\partial^{n} M(t)}{\\partial^{n} t } & = 0 + 0 + 0 + \\ldots + \\frac{n \\times n-1 \\times \\ldots 2 \\times t^{0} \\E[X^{n}] }{n!}  + \\frac{n!t \\E[X^{n+1}] }{(n+ 1)! } + \\ldots \\nonumber \\\\\n& = \\frac{n! \\E[X^{n}] }{n!}  + \\frac{n!t \\E[X^{n+1}] }{(n+ 1)! } + \\ldots  \\nonumber\n\\end{aligned}\n\\]Evaluated 0, yields \\(M^{n}(0) = \\E[X^{n}]\\).two random variables, \\(X\\) \\(Y\\) moment generating functions, \\(F_{X}(x) = F_{Y}(x)\\) almost \\(x\\).","code":""},{"path":"limits.html","id":"the-moments-of-the-normal-distribution","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.3.1 The moments of the normal distribution","text":"Suppose \\(Z \\sim N(0,1)\\).\\[\n\\E[e^{tX}] = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{tx} e^{-x^2/2} dx \\nonumber\n\\]\\[\ntx - \\frac{1}{2} x^2 = -\\frac{1}{2}\\left( ( x - t)^2 - t^2 \\right)\n\\]\\[\n\\begin{aligned}\n\\E[e^{tX}] & = \\frac{1}{\\sqrt{2\\pi}}  e^{\\frac{t^{2}}{2}}\\int_{-\\infty}^{\\infty} e^{-(x- t)^2/2} dx \\nonumber \\\\\n& = e^{\\frac{t^{2}}{2}} \\nonumber\n\\end{aligned}\n\\]","code":""},{"path":"limits.html","id":"extracting-moments-of-the-normal-distribution","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.3.2 Extracting moments of the normal distribution","text":"\\[\n\\begin{aligned}\nM^{'}(0) & = \\E[X] =  e^{t^2/2} t |_{0} = 0 \\nonumber \\\\\nM^{''} (0 ) & = \\E[X^2] =  e^{t^2/2} (t^2 + 1) |_{0} = 1 \\nonumber \\\\\nM^{'''} (0) & = \\E[X^3] = e^{t^2/2} t (t^2 + 3) |_{0} = 0 \\nonumber \\\\ \nM^{''''} (0) & = \\E[X^4] = e^{t^2/2} (t^4 + 6t^2 + 3)|_{0} = 3 \\nonumber \\\\ \nM^{5} (0) & = \\E[X^5] = e^{t^2/2} t (t^4 + 10t^2 + 15)|_{0} = 0 \\nonumber \\\\\nM^{6} (0) & = \\E[X^6] = e^{t^2/2} (t^6 + 15t^4 + 45t^2 + 15 )|_{0} = 15 \\nonumber\n\\end{aligned}\n\\]","code":""},{"path":"limits.html","id":"sequences-of-independent-random-variables","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.4 Sequences of independent random variables","text":"Suppose \\(X_{}\\) sequence independent random variables. Define\\[\nY = \\sum_{=1}^{N} X_{} \\nonumber \n\\]\\[\nM_{Y}(t) = \\prod_{=1}^{N} M_{X_{}}(t) \\nonumber \n\\]Proof. \\[\n\\begin{aligned}\nM_{Y}(t) & = \\E[e^{tY}] \\nonumber \\\\  \n& = \\E[e^{t\\sum_{=1}^{N} X_{} } ] \\nonumber \\\\ \n& = \\E[e^{tX_{1} + tX_{2} + \\ldots tX_{N} } ] \\nonumber \\\\\n& = \\E[e^{tX_{1} }]E[e^{tX_{2} }] \\ldots \\E[e^{tX_{N} }] \\text{ (independence) } \\nonumber \\\\\n& = \\prod_{=1}^{N} \\E[e^{tX_{}}] \\nonumber\n\\end{aligned}\n\\]","code":""},{"path":"limits.html","id":"inequalities-and-limit-theorems","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.5 Inequalities and limit theorems","text":"","code":""},{"path":"limits.html","id":"limit-theorems","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.5.1 Limit theorems","text":"happens consider long sequence random variables? can reasonably infer data?Laws large numbers: averages random variables converge expected value?Central Limit Theorems: sum random variables normal distribution?’ll focus intuition , ’ll prove stuff .","code":""},{"path":"limits.html","id":"weak-law-of-large-numbers","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.5.2 Weak law of large numbers","text":"Proof plan:Markov’s InequalityChebyshev’s InequalityWeak Law Large Numbers","code":""},{"path":"limits.html","id":"markovs-inequality","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.5.2.1 Markov’s inequality","text":"Suppose \\(X\\) random variable takes non-negative values. , \\(>0\\),\\[\n\\Pr(X\\geq ) \\leq \\frac{\\E[X]}{} \\nonumber \n\\]Proof (Markov's inequality). \\(>0\\),\\[\n\\begin{aligned}\n\\E[X] & = \\int_{0}^{\\infty} x f(x) dx \\nonumber \\\\\n& = \\int_{0}^{} x f(x) dx + \\int_{}^{\\infty} x f(x) dx \\nonumber\n\\end{aligned}\n\\]\\(X\\geq 0\\),\\[\n\\begin{aligned}\n\\E[X] \\geq \\int_{}^{\\infty} x f(x) dx  \\geq \\int_{}^{\\infty} f(x)dx = P(X \\geq )\\nonumber \\\\ \n\\frac{\\E[X]}{} \\geq P(X \\geq )\\nonumber\n\\end{aligned}\n\\]","code":""},{"path":"limits.html","id":"chebyshevs-inequality","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.5.2.2 Chebyshev’s inequality","text":"\\(X\\) random variable mean \\(\\mu\\) variance \\(\\sigma^2\\), , value \\(k>0\\),\\[\n\\Pr(|X - \\mu| \\geq k) \\leq \\frac{\\sigma^2}{k^2} \\nonumber \n\\]Proof (Chebyshev's inequality). Define random variable\\[\nY = (X - \\mu)^2 \\nonumber \n\\]\\(\\mu = E[X]\\).know \\(Y\\) non-negative random variable. Set \\(= k^2\\).Applying inequality:\\[\n\\begin{aligned}\n\\Pr(Y \\geq k^2) \\leq \\frac{\\E[Y]}{k^2} \\nonumber \\\\\n\\Pr( (X- \\mu)^2 \\geq k^2) \\leq \\frac{\\E[(X - \\mu)^2]}{k^2} \\nonumber \\\\\n\\Pr( (X- \\mu)^2 \\geq k^2) \\leq \\frac{\\sigma^2}{k^2}\\nonumber\n\\end{aligned}\n\\]know \\[\n(X - \\mu)^2  \\geq k^2 \\nonumber \n\\]Implies \\[\n|X - \\mu| \\geq k \\nonumber \n\\]Thus, shown\\[\n\\Pr( |X- \\mu| \\geq k) \\leq \\frac{\\sigma^2}{k^2}\\nonumber\n\\]","code":""},{"path":"limits.html","id":"sequence-of-random-variables","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.6 Sequence of random variables","text":"Sequence independent identically distributed (..d.) random variables.Sequence: \\(X_{1}, X_{2}, \\ldots, X_{n}, \\ldots\\)Sequence: \\(X_{1}, X_{2}, \\ldots, X_{n}, \\ldots\\)Think sequence sampled data:\nSuppose drawing sample \\(N\\) observations\nobservation random variable, say \\(X_{}\\)\nrealization \\(x_{}\\)\nThink sequence sampled data:Suppose drawing sample \\(N\\) observationsEach observation random variable, say \\(X_{}\\)realization \\(x_{}\\)","code":""},{"path":"limits.html","id":"meanvariance-of-sample-mean","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.6.1 Mean/variance of sample mean","text":"Let \\(X_{1}, X_{2}, \\ldots, X_{n}\\) random sample distribution mean \\(\\mu\\) variance \\(\\sigma^2\\). Let \\(\\bar{X}_{n}\\) sample mean. \\[\n\\E[\\bar{X}_{n}] = \\mu \\text{ } \\Var(\\bar{X}_{n}) = \\frac{\\sigma^2}{n}\n\\]Proof. \\[\n\\begin{aligned}\n\\E[\\bar{X}_{n}] & = \\frac{1}{n}\\sum_{=1}^{n} \\E[X_{}] \\nonumber \\\\\n& = \\frac{1}{n} n \\mu  = \\mu \\nonumber \n\\end{aligned}\n\\]\\[\n\\begin{aligned}\n\\Var(\\bar{X}_{n}) & = \\frac{1}{n^2} \\Var(\\sum_{=1}^{n} X_{}) \\nonumber \\\\\n&= \\frac{1}{n^2} \\sum_{=1}^{n} \\Var(X_{}) \\nonumber \\\\\n& = \\frac{1}{n^2} n \\sigma^2 = \\frac{\\sigma^2}{n} \\nonumber\n\\end{aligned}\n\\]","code":""},{"path":"limits.html","id":"weak-law-of-large-numbers-1","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.6.2 Weak law of large numbers","text":"Suppose \\(X_{1}, X_{2}, \\ldots, X_{n}\\) random sample distribution mean \\(\\mu\\) \\(Var(X_{})= \\sigma^2\\). , \\(\\epsilon >0\\),\\[\n\\Pr\\left\\{ \\left| \\frac{X_{1} + X_{2} + \\ldots + X_{n} }{n} -\\mu \\right| \\geq \\epsilon \\right\\} \\rightarrow 0 \\text{ } n \\rightarrow \\infty \\nonumber \n\\]Proof. previous proposition\\[\n\\frac{\\text{E}[X_{1} + X_{2} + \\cdots + X_{n} ]}{n} = \\frac{\\sum_{=1}^{n} E[X_{}] }{n}  = \\mu \\nonumber\n\\],\\[\n\\E[ (\\frac{\\sum_{=1}^{n} X_{} - \\mu}{n} )^2] = \\frac{\\Var(X_{1} + X_{2} + \\cdots + X_{n} )}{n^2} = \\frac{ \\sum_{=1}^{n} \\Var(X_{}) }{n^2} = \\frac{\\sigma^2}{n} \\nonumber\n\\]Apply Chebyshev’s Inequality:\\[\n\\Pr\\left\\{ \\left| \\frac{X_{1} + X_{2} + \\ldots + X_{n} }{n} -\\mu \\right| \\geq \\epsilon \\right\\}\\leq \\frac{\\sigma^2}{n \\epsilon^2} \\nonumber\n\\]Suppose \\(X_{1}, X_{2}, \\ldots\\) ..d. normal distributions,\\[\nX_{} \\sim \\text{Normal}(0, 10) \\nonumber \n\\]\\[\n\\Pr\\left\\{ \\left| \\frac{X_{1} + X_{2} + \\ldots + X_{n} }{n} -\\mu \\right| \\geq 0.1 \\right\\} \\text{ } n \\rightarrow \\infty\\nonumber \n\\]Suppose want guarantee \\(0.01\\) probability \\(0.1\\) away true \\(\\mu\\). big need \\(n\\)?\\[\n\\begin{aligned}\n0.01 & = \\frac{10}{n (0.1^2) } \\nonumber \\\\\nn & = \\frac{1000}{0.01} \\nonumber \\\\\nn & = 100,000 \n\\end{aligned}\n\\]","code":""},{"path":"limits.html","id":"sequences-and-convergence","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.7 Sequences and convergence","text":"Sequence (refresher):\\[\n\\left\\{a_{} \\right\\}_{=1}^{\\infty} = \\left\\{a_{1}, a_{2}, a_{3}, \\ldots, a_{n}, \\ldots, \\right\\} \\nonumber \n\\]Definition 11.3  say sequence \\(\\left\\{a_{} \\right\\}_{=1}^{\\infty}\\) converges real number \\(\\) \\(\\epsilon>0\\) positive integer \\(N\\) \\(n\\geq N\\), \\(|a_{n} - | < \\epsilon\\)Sequence functions:\\[\n\\left\\{ f_{} \\right\\}_{=1}^{\\infty}  = \\left\\{f_{1}, f_{2}, f_{3}, \\ldots, f_{n}, \\ldots, \\right\\}\\nonumber \n\\]Definition 11.4  (Pointwise convergence) Suppose \\(f_{}: X \\rightarrow \\Re\\) \\(\\). \\(\\left\\{ f_{} \\right\\}_{=1}^{\\infty}\\) converges pointwise \\(f\\) , \\(x \\X\\) \\(\\epsilon> 0\\), \\(N\\) \\(n\\geq N\\),\\[\n|f_{n} (x)  - f(x)|<\\epsilon \\nonumber \n\\]strong statement ’re likely make statistics.","code":""},{"path":"limits.html","id":"convergence-definitions","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.7.1 Convergence definitions","text":"Define \\(\\widehat{\\theta}_{n}\\) estimator \\(\\theta\\) based \\(n\\) observations.Sequence estimators: increasing sample size\\[\n\\left\\{\\widehat{\\theta}_{}\\right\\}_{=1}^{n} = \\left\\{\\widehat{\\theta}_{1}, \\widehat{\\theta}_{2},    \\widehat{\\theta}_{3}, \\ldots, \\widehat{\\theta}_{n} \\right\\} \\nonumber\n\\]Question: can say \\(\\left\\{\\widehat{\\theta}_{}\\right\\}_{=1}^{n}\\) \\(n\\rightarrow \\infty\\)?probability \\(\\widehat{\\theta}_{n}\\) differs \\(\\theta\\)?probability \\(\\left\\{\\widehat{\\theta}_{}\\right\\}_{=1}^{n}\\) converges \\(\\theta\\)?sampling distribution \\(\\widehat{\\theta}_{n}\\) \\(n \\rightarrow \\infty\\) ?","code":""},{"path":"limits.html","id":"convergence-in-probability","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.7.2 Convergence in probability","text":"Definition 11.5  (Convergence probability) say sequence \\(\\widehat{\\theta}_{n}\\) converges probability \\(\\theta\\) (perhaps non-degenerate RV) ,\\[\n\\lim_{n\\rightarrow\\infty} \\Pr(|\\widehat{\\theta}_{n} - \\theta | > \\epsilon ) = 0 \\nonumber \n\\]\\(\\epsilon>0\\)\\(\\epsilon\\) tolerance parameter: much error around \\(\\theta\\)?limit, convergence probability implies sampling distribution collapses spike \\(\\theta\\)\\(\\left\\{\\widehat{\\theta}_{}\\right\\}\\) need actually converge \\(\\theta\\), (\\(\\Pr|\\theta_{n}\\) - \\(\\theta| > \\epsilon) = 0\\)Example 11.1  Suppose \\(S \\sim\\) Uniform(0,1). Define \\(X(s) = s\\).Suppose \\(X_{n}\\) defined follows:\\[\n\\begin{aligned}\nX_{1}(s) = s + (s \\[0,1]) & ,  X_{2} (s)  = s +  (s \\[0,1/2]) \\nonumber \\\\\nX_{3}(s)  = s + (s \\[1/2, 1])& ,  X_{4}(s) = s + (s \\[0,1/3]) \\nonumber \\\\\nX_{5} (s)  =  s + (s \\[1/3,2/3]) & ,  X_{6}(s) = s + (s \\[2/3, 1]) \\nonumber\n\\end{aligned}\n\\]\\(X_{n}(s)\\) pointwise converge \\(X(s)\\)?\\(X_{n}(s)\\) converge probability \\(X(s)\\)?\\[\n\\Pr(|X_{n} - X | > \\epsilon)  = \\Pr ( s \\[l_{n}, u_{n} ] ) \\nonumber\n\\]Length \\([l_{n}, u_{n}] \\rightarrow 0 \\Rightarrow P ( s \\[L_{n}, U_{n} ] ) = 0\\).","code":""},{"path":"limits.html","id":"almost-sure-convergence","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.7.3 Almost sure convergence","text":"Definition 11.6  say sequence \\(\\widehat{\\theta}_{n}\\) converges almost surely \\(\\theta\\) ,\\[\n\\Pr(\\lim_{n \\rightarrow \\infty} |\\widehat{\\theta}_{n} - \\theta| > \\epsilon ) = 0 \\nonumber\n\\]Stronger: says sequence converges \\(\\theta\\) (almost everywhere) )Stronger: says sequence converges \\(\\theta\\) (almost everywhere) )Think definition random variable: \\(\\widehat{\\theta}_{n}\\) function sample space real line.Think definition random variable: \\(\\widehat{\\theta}_{n}\\) function sample space real line.Almost sure says , outcomes (\\(s\\)) sample space (\\(S\\)) \\(s \\S\\),\n\\[\n \\widehat{\\theta}_{n}(s) \\rightarrow \\theta(s) \\nonumber\n \\]Almost sure says , outcomes (\\(s\\)) sample space (\\(S\\)) \\(s \\S\\),\\[\n \\widehat{\\theta}_{n}(s) \\rightarrow \\theta(s) \\nonumber\n \\]Except subset \\(\\mathcal{N} \\subset S\\) \\(\\Pr(\\mathcal{N}) = 0\\).Except subset \\(\\mathcal{N} \\subset S\\) \\(\\Pr(\\mathcal{N}) = 0\\).","code":""},{"path":"limits.html","id":"example-7","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.7.3.1 Example","text":"Suppose \\(S \\sim\\) Uniform(0,1). Suppose \\(X_{n}\\) defined follows:\\[\n\\begin{aligned}\nX_{1}(s) = s + (s \\[0,1]) & ,  X_{2} (s)  = s +  (s \\[0,1/2]) \\nonumber \\\\\n X_{3}(s)  = s + (s \\[1/2, 1])& ,  X_{4}(s) = s + (s \\[0,1/3]) \\nonumber \\\\\n X_{5} (s)  =  s + (s \\[1/3,2/3]) & ,  X_{6}(s) = s + (s \\[2/3, 1]) \\nonumber \n\\end{aligned}\n\\]\\(X_{n}(s)\\) converge almost surely \\(X(s) = s\\)?! – sequence doesn’t converge \\(s\\). value \\(s\\) sequence varies \\(s\\) \\(s + 1\\) infinitely often.","code":""},{"path":"limits.html","id":"convergence-in-distribution","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.7.4 Convergence in distribution","text":"’ve talked \\(\\widehat{\\theta}_{n}\\)’s sampling distribution converging normal distribution. convergence distribution.Definition 11.7  (Convergence distribution) \\(\\widehat{\\theta}_{n}\\), cdf \\(F_{n}(x)\\), converges distribution random variable \\(Y\\) cdf \\(F(x)\\) \\[\n\\lim_{n\\rightarrow \\infty} |F_{n} (x) - F(x) | = 0 \\nonumber \n\\]\\(x \\\\Re\\) \\(F(x)\\) continuous.Weakest form convergence almost sure \\(\\rightarrow\\) probability \\(\\rightarrow\\) distributionSays cdfs equal, says nothing convergence underlying random variableUseful justifying use sampling distributions","code":""},{"path":"limits.html","id":"convergence-in-distribution-not-rightarrow-convergence-in-probability","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.7.5 Convergence in distribution \\(\\not \\Rightarrow\\) convergence in probability","text":"Define \\(X \\sim N(0,1)\\) \\(X_{n} = - X\\). :\\(X_{n} \\sim N(0,1)\\) \\(n\\) \\(X_{n}\\) trivially converges \\(X\\). ,\\[\n\\begin{aligned}\n\\Pr(|X_{n} - X| > \\epsilon ) & = \\Pr(|X + X| > \\epsilon) \\nonumber \\\\\n                            & = \\Pr( |2X| > \\epsilon) \\nonumber \\\\\n                            & =  \\Pr(|X| > \\epsilon/2) \\\\leadsto 0\n\\end{aligned}\n\\]","code":""},{"path":"limits.html","id":"central-limit-theorem","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.7.6 Central limit theorem","text":"Let \\(X_{1}\\), \\(X_{2}, \\ldots\\) sequence independent random variables mean \\(\\mu\\) variance \\(\\sigma^2\\). Let \\(X_{}\\) cdf \\(P(X_{} \\leq x) = F(x)\\) moment generating function \\(M(t) = \\E[e^{tX_{}}]\\) . Let \\(S_{n} = \\sum_{=1}^{n} X_{}\\). \\[\n\\lim_{n\\rightarrow \\infty} P\\left( \\frac{S_{n} - \\mu n }{\\sigma\\sqrt{n}} \\leq x \\right)  =  \\frac{1}{\\sqrt{2\\pi} } \\int_{-\\infty}^{x} \\exp\\left( -\\frac{z^{2} }{2} \\right) dz \\nonumber \n\\]Proof plan:Rely fact convergence MGFs \\(\\leadsto\\) convergence CDFsShow MGFs, limit, converge normal MGFLet \\(F_{n}\\) sequence cumulative distribution functions corresponding moment generating functions \\(M_{n}\\). \\(F\\) cdf moment generating functions \\(M\\). \\(\\lim_{n\\rightarrow \\infty} M_{n}(t) \\rightarrow M(t)\\) \\(t\\) interval, \\(F_{n}(x) \\leadsto F(x)\\) \\(x\\) (\\(F\\) continuous).Suppose \\(\\lim_{n\\rightarrow \\infty} a_{n} \\rightarrow \\), \\[\n\\lim_{n\\rightarrow \\infty} \\left( 1 + \\frac{a_{n}}{n}\\right)^{n}  = e^{} \\nonumber \n\\]Suppose \\(M(t)\\) moment generating function random variable \\(X\\). \\(M(0) = 1\\).Proof (Central limit theorem). Suppose \\(X_{1}, \\ldots, X_{n}\\) ..d. variables \\(\\E[X] = 0\\), variance \\(\\sigma^{2}_{x}\\), Moment Generating Function (MGF) \\(M_{x}(t)\\).Let \\(S_{n} = \\sum_{=1}^{n} X_{}\\) \\(Z_{n} = \\frac{S_{n}}{\\sigma_{x} \\sqrt{n}}\\).\\(M_{S_{n}} = (M_{x}(t))^{n}\\) \\(M_{Z_{n}} (t) = \\left(M_{x} \\left(\\frac{t}{\\sigma_{x} \\sqrt{n}} \\right) \\right)^{n}\\)Using Taylor’s Theorem can write\\[\nM_{x} (s) = M_{x} (0)  + s M^{'}_{x}(0) + \\frac{1}{2} s^2 M_{x}^{''}(0) + e_{s} \\nonumber \n\\]\\(e_{s}/s^2 \\rightarrow 0\\) \\(s\\rightarrow 0\\).Filling values \\[\nM_{x} (s) = 1  + 0 + \\frac{\\sigma_{x}^{2}}{2} s^2  + \\underbrace{e_{s}}_{\\text{Goes zero}} \\nonumber \n\\]Set \\(s = \\frac{t}{\\sigma_{x} \\sqrt{n}}\\) \\(\\lim_{n\\rightarrow \\infty} s \\rightarrow 0\\). \\[\n\\begin{aligned}\nM_{Z_{n}}(t) & = \\left(1 + \\frac{\\sigma_{x}^{2}}{2}\\left( \\frac{t}{\\sigma_{x} \\sqrt{n}} \\right)^{2} \\right)^{n} \\nonumber \\\\\n& = \\left( 1 + \\frac{t^{2}/2}{n} \\right)^{n} \\nonumber \\\\\n\\lim_{n\\rightarrow \\infty} M_{Z_{n}}(t) & = e^{\\frac{t^2}{2}}\\nonumber\n\\end{aligned}\n\\]","code":""},{"path":"limits.html","id":"why-this-matters","chapter":"Day 11 Properties of random variables and limit theorems","heading":"11.7.6.1 Why this matters","text":"central limit theorem wide range applications statistics. importantly, central limit theorem eliminates need detailed probabilistic model extended calculations PMFs PDFs. Instead, many probabilities can calculated using normal CDF table based purely knowledge means variances. core techniques classical statistical inference derived theorem.","code":""},{"path":"classic-inference.html","id":"classic-inference","chapter":"Day 12 Classical statistical inference","heading":"Day 12 Classical statistical inference","text":"","code":""},{"path":"classic-inference.html","id":"learning-objectives-11","chapter":"Day 12 Classical statistical inference","heading":"Learning objectives","text":"Define classical statistical inferenceSummarize core concepts point estimates, confidence sets, hypothesis testingDefine parametric inference identify use casesSummarize point estimatesDefine hypothesis testing \\(p\\)-valueDefine Wald testSummarize \\(\\chi^2\\) test significance","code":""},{"path":"classic-inference.html","id":"supplemental-readings-11","chapter":"Day 12 Classical statistical inference","heading":"Supplemental readings","text":"Chapter 9 Bertsekas Tsitsiklis (2008)Wasserman (2013)\nCh 6 - Models, Statistical Inference Learning\nCh 10 - Hypothesis Testing p-values\nCh 6 - Models, Statistical Inference LearningCh 10 - Hypothesis Testing p-values","code":""},{"path":"classic-inference.html","id":"statistical-inference","chapter":"Day 12 Classical statistical inference","heading":"12.1 Statistical inference","text":"Statistical inference process using data infer probability distribution/random variable generated data. Given sample \\(X_1, \\ldots, X_n \\sim F\\), infer \\(F\\)? Sometimes want infer features/parameters \\(F\\), sometimes need subset features/parameters.","code":""},{"path":"classic-inference.html","id":"parametric-models","chapter":"Day 12 Classical statistical inference","heading":"12.2 Parametric models","text":"statistical model \\(\\xi\\) set distributions (densities regression functions). parametric model set \\(\\xi\\) can parameterized finite number parameters. seen many examples parametric models - major types random variables ’ve explored defined terms fixed number parameters. instance, assume data generated Normal distribution, model \\[\\xi \\equiv f(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left[ -\\frac{1}{2\\sigma^2} (x - \\mu)^2 \\right], \\quad \\mu \\\\Re, \\sigma > 0\\]example two-parameter model. density \\(f(x; \\mu, \\sigma)\\) indicates \\(x\\) value random variable \\(X\\), whereas \\(\\mu\\) \\(\\sigma\\) parameters define model.general, parametric model takes form\\[\\xi \\equiv f(x; \\theta) : \\theta \\\\Theta\\]\\(\\theta\\) unknown parameter (vector parameters) can take values parameter space \\(\\Theta\\). \\(\\theta\\) vector interested one component \\(\\theta\\), call remaining parameters nuisance parameters.","code":""},{"path":"classic-inference.html","id":"examples-of-parametric-models","chapter":"Day 12 Classical statistical inference","heading":"12.2.1 Examples of parametric models","text":"Example 12.1  (One-dimensional parametric estimation) Let \\(X_1, \\ldots, X_n\\) independent observations drawn Bernoulli random variable probability \\(\\pi\\) success. problem estimate parameter \\(\\pi\\).Example 12.2  (Two-dimensional parametric estimation) Suppose \\(X_1, \\ldots, X_n \\sim F\\) assume PDF \\(f \\\\xi\\) \\[\\xi \\equiv f(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left[ -\\frac{1}{2\\sigma^2} (x - \\mu)^2 \\right], \\quad \\mu \\\\Re, \\sigma > 0\\]case, two parameters, \\(\\mu\\) \\(\\sigma\\). goal estimate parameters data. interested estimating \\(\\mu\\) (generally case inferential methods linear regression), \\(\\mu\\) parameter interest \\(\\sigma\\) nuisance parameter.","code":""},{"path":"classic-inference.html","id":"point-estimates","chapter":"Day 12 Classical statistical inference","heading":"12.3 Point estimates","text":"Point estimation refers providing single “best guess” quantity interest. quantity interest parameter parametric model, CDF \\(F\\), PDF \\(f\\), regression function \\(r\\), prediction future value \\(Y\\) random variable.denote point estimate \\(\\theta\\) \\(\\hat{\\theta}\\) \\(\\hat{\\theta}_n\\). Remember \\(\\theta\\) fixed, unknown quantity. estimate \\(\\hat{\\theta}\\) depends data, \\(\\hat{\\theta}\\) random variable. formally, let \\(X_1, \\ldots, X_n\\) \\(n\\) IID (independently identically drawn) data points distribution \\(F\\). point estimator \\(\\hat{\\theta}_n\\) parameter \\(\\theta\\) function \\(X_1, \\ldots, X_n\\):\\[\\hat{\\theta}_n = g(X_1, \\ldots, X_n)\\]","code":""},{"path":"classic-inference.html","id":"properties-of-point-estimates","chapter":"Day 12 Classical statistical inference","heading":"12.3.1 Properties of point estimates","text":"bias estimator defined \\[\\text{bias}(\\hat{\\theta}_n) = \\E_\\theta (\\hat{\\theta_n}) - \\theta\\]\\(\\E (\\hat{\\theta_n}) - \\theta = 0\\), say \\(\\hat{\\theta_n}\\) unbiased. Many estimators statistical inference unbiased – modern approaches, sometimes justified. see examples Perspectives Computational Modeling. preferable requirement estimator consistency: number observations \\(n\\) increases, estimator converge towards true parameter \\(\\theta\\).distribution \\(\\hat{\\theta}_n\\) called sampling distribution. standard deviation \\(\\hat{\\theta}_n\\) called standard error:\\[\\se = \\sd(\\hat{\\theta}_n) = \\sqrt{\\Var (\\hat{\\theta}_n)}\\]Frequently standard error depends unknown \\(F\\). cases, usually estimate . estimated standard error denoted \\(\\widehat{\\se}\\).quality point estimate sometimes assessed mean squared error (MSE) defined \\[\n\\begin{align}\n\\text{MSE} &= \\E_\\theta [(\\hat{\\theta}_n - \\theta)^2] \\\\\n&= \\text{bias}^2(\\hat{\\theta}_n) + \\Var_\\theta (\\hat{\\theta}_n)\n\\end{align}\n\\]Remember \\(\\E_\\theta (\\cdot)\\) refers expectation respect distribution \\(f(x_1, \\ldots, x_n; \\theta)\\) generated data. \\(\\theta\\) distribution - fixed, unknown, value.Many estimators turn , approximately, Normal distribution – another reason continuous distribution important statistical inference.\\[\\frac{\\hat{\\theta}_n - \\theta}{\\se} \\leadsto N(0,1)\\]Example 12.3  (Bernoulli distributed random variable) Let \\(X_1, \\ldots, X_n ~ \\text{Bernoulli}(\\pi)\\) let \\(\\hat{\\pi}_n = \\frac{1}{n} \\sum_{=1}^n X_i\\). \\[\\E(\\hat{\\pi}_n) = \\frac{1}{n} \\sum_{=1}^n \\E(X_i) = \\pi\\]\\(\\hat{\\pi}_n\\) unbiased. standard error \\[\\se = \\sqrt{\\Var (\\hat{\\pi}_n)} = \\sqrt{\\frac{\\pi (1 - \\pi)}{n}}\\]can estimated \\[\\widehat{\\se} = \\sqrt{\\frac{\\hat{\\pi} (1 - \\hat{\\pi})}{n}}\\]Additionally, \\(\\E_\\pi (\\hat{\\pi}_n) = \\pi\\) \\(\\text{bias} = \\pi - \\pi = 0\\)\\[\n\\begin{align}\n\\text{bias}(\\hat{\\pi}_n) &= \\E_\\pi (\\hat{\\pi}) - \\pi \\\\\n&= \\pi - \\pi \\\\\n&= 0\n\\end{align}\n\\]\\[\\se = \\sqrt{\\frac{\\pi (1 - \\pi)}{n}} \\rightarrow 0\\]\\(n\\) increases. Hence, \\(\\hat{\\pi}_n\\) consistent estimator \\(\\pi\\).","code":""},{"path":"classic-inference.html","id":"confidence-sets","chapter":"Day 12 Classical statistical inference","heading":"12.4 Confidence sets","text":"\\(1 - \\alpha\\) confidence interval parameter \\(\\theta\\) interval \\(C_n = (,b)\\) \\(= (X_1, \\ldots, X_n)\\) \\(b = b(X_1, \\ldots, X_n)\\) functions data \\[\\Pr_{\\theta} (\\theta \\C_n) \\geq 1 - \\alpha, \\quad \\forall \\, \\theta \\\\Theta\\]words, \\((,b)\\) traps \\(\\theta\\) probability \\(1- \\alpha\\). call \\(1 - \\alpha\\) coverage confidence interval.","code":""},{"path":"classic-inference.html","id":"caution-interpreting-confidence-intervals","chapter":"Day 12 Classical statistical inference","heading":"12.4.1 Caution interpreting confidence intervals","text":"\\(C_n\\) random \\(\\theta\\) fixed. core assumption statistical inference especially critical frequentist inference. Commonly people use 95% confidence intervals corresponding \\(\\alpha = 0.05\\). \\(\\theta\\) vector use confidence set (sphere ellipse) instead interval.confidence interval probability statement \\(\\theta\\) since \\(\\theta\\) fixed quantity, random variable. Either \\(\\theta\\) interval probability \\(1\\). better definition :Definition 12.1  (Confidence interval) day 1, collect data construct 95% confidence interval parameter \\(\\theta_1\\). day 2, collect new data construct 95% confidence interval parameter \\(\\theta_2\\). continue way constructing confidence intervals sequence unrelated parameters \\(\\theta_1, \\theta_2, \\ldots\\). 95% intervals trap true parameter value.","code":""},{"path":"classic-inference.html","id":"constructing-confidence-intervals","chapter":"Day 12 Classical statistical inference","heading":"12.4.2 Constructing confidence intervals","text":"point estimators approximate Normal distribution, can use Normal distribution construct confidence intervals relatively easily point estimates relying directly Normal distribution.Suppose \\(\\hat{\\theta}_n \\approx N(\\theta, \\widehat{\\se}^2)\\). Let \\(\\Phi\\) CDF standard Normal distribution let\\[z_{\\frac{\\alpha}{2}} = \\Phi^{-1} \\left(1 - \\frac{\\alpha}{2} \\right)\\],\\[\\Pr (Z > \\frac{\\alpha}{2}) = \\frac{\\alpha}{2}\\]\\[\\Pr (-z_{\\frac{\\alpha}{2}} \\leq Z \\leq z_{\\frac{\\alpha}{2}}) = 1 - \\alpha\\]\\(Z \\sim N(0,1)\\). Let\\[C_n = (\\hat{\\theta}_n - z_{\\frac{\\alpha}{2}} \\widehat{\\se}, \\hat{\\theta}_n + z_{\\frac{\\alpha}{2}} \\widehat{\\se})\\]\\[\n\\begin{align}\n\\Pr_\\theta (\\theta \\C_n) &= \\Pr_\\theta (\\hat{\\theta}_n - z_{\\frac{\\alpha}{2}} \\widehat{\\se} < \\theta < \\hat{\\theta}_n + z_{\\frac{\\alpha}{2}} \\widehat{\\se}) \\\\\n&= \\Pr_\\theta (- z_{\\frac{\\alpha}{2}} < \\frac{\\hat{\\theta}_n - \\theta}{\\widehat{\\se}} < z_{\\frac{\\alpha}{2}}) \\\\\n&\\rightarrow \\Pr ( - z_{\\frac{\\alpha}{2}} < Z < z_{\\frac{\\alpha}{2}}) \\\\\n&= 1 - \\alpha\n\\end{align}\n\\]95% confidence intervals, \\(\\alpha = 0.05\\) \\(z_{\\frac{\\alpha}{2}} = 1.96 \\approx 2\\) leading approximate 95% confidence interval \\(\\hat{\\theta}_n \\pm 2 \\widehat{\\se}\\).","code":""},{"path":"classic-inference.html","id":"hypothesis-testing","chapter":"Day 12 Classical statistical inference","heading":"12.5 Hypothesis testing","text":"hypothesis testing, start default theory – called null hypothesis – ask data provide sufficient evidence reject theory. , fail reject null hypothesis.Formally, suppose partition parameter space \\(\\Theta\\) two disjoint sets \\(\\Theta_0\\) \\(\\Theta_1\\) wish test\\[H_0: \\theta \\\\Theta_0 \\quad \\text{versus} \\quad H_1: \\theta \\\\Theta_1\\]\\(H_0\\) - null hypothesis\\(H_1\\) - alternative hypothesisLet \\(X\\) random variable let \\(\\chi\\) range \\(X\\). test hypothesis finding appropriate subset outcomes \\(R \\subset \\chi\\) called rejection region. \\(X \\subset R\\) reject null hypothesis, otherwise reject null hypothesis. Usually rejection region \\(R\\) form\\[R = \\left\\{ x: T(x) > c \\right\\}\\]\\(T\\) test statistic \\(c\\) critical value. Hypothesis testing requires us find appropriate test statistic \\(T\\) appropriate critical value \\(c\\) test given hypothesis. Different hypotheses require different test statistics.","code":""},{"path":"classic-inference.html","id":"types-of-errors","chapter":"Day 12 Classical statistical inference","heading":"12.5.1 Types of errors","text":"\nFigure 12.1: Stereotypical example hypothesis testing errors.\nHypothesis testing error-proof. start assumption \\(H_0\\) true unless strong evidence reject \\(H_0\\). Rejecting \\(H_0\\) \\(H_0\\) true type error (false positive), retaining \\(H_0\\) \\(H_1\\) true called type II error (false negative).","code":""},{"path":"classic-inference.html","id":"power-function","chapter":"Day 12 Classical statistical inference","heading":"12.5.2 Power function","text":"power function test rejection region \\(R\\) defined \\[\\beta(\\theta) = \\Pr_\\theta (X \\R)\\]size test defined \\[\\alpha = \\text{sup}_{\\theta \\\\Theta_0} \\beta(\\theta)\\]test said level \\(\\alpha\\) size less equal \\(\\alpha\\).","code":""},{"path":"classic-inference.html","id":"sided-tests","chapter":"Day 12 Classical statistical inference","heading":"12.5.3 Sided tests","text":"test form\\[H_0: \\theta = \\theta_0 \\quad \\text{versus} \\quad H_1: \\theta \\neq \\theta_0\\]called two-sided test, simple hypothesis. test form\\[H_0: \\theta \\leq \\theta_0 \\quad \\text{versus} \\quad H_1: \\theta > \\theta_0\\]\\[H_0: \\theta \\geq \\theta_0 \\quad \\text{versus} \\quad H_1: \\theta < \\theta_0\\]called one-sided test, composite hypothesis.","code":""},{"path":"classic-inference.html","id":"example-hypothesis-test","chapter":"Day 12 Classical statistical inference","heading":"12.5.4 Example hypothesis test","text":"Let \\(X_1, \\ldots, X_n \\sim N(\\mu, \\sigma^2)\\) \\(\\sigma\\) known. want test \\(H_0: \\mu \\leq 0\\) versus \\(H_1: \\mu > 0\\). Hence, \\(\\Theta_0 = (-\\infty, 0]\\) \\(\\Theta_1 = (0, \\infty]\\). Consider test\\[\\text{reject } H_0 \\text{ } T>c\\]\\(T = \\bar{X}\\). rejection region \\[R = \\left\\{(x_1, \\ldots, x_n): T(x_1, \\ldots, x_n) > c \\right\\}\\]Let \\(Z\\) denote standard Normal random variable. power function \\[\n\\begin{align}\n\\beta(\\mu) &= \\Pr_\\mu (\\bar{X} > c) \\\\\n&= \\Pr_\\mu \\left(\\frac{\\sqrt{n} (\\bar{X} - \\mu)}{\\sigma} > \\frac{\\sqrt{n} (c - \\mu)}{\\sigma} \\right) \\\\\n&= \\Pr_\\mu \\left(Z > \\frac{\\sqrt{n} (c - \\mu)}{\\sigma} \\right) \\\\\n&= 1 - \\Phi \\left( \\frac{\\sqrt{n} (c - \\mu)}{\\sigma} \\right)\n\\end{align}\n\\]function increasing \\(\\mu\\):Hence\\[\\alpha = \\text{sup}_{\\mu \\leq 0} \\beta(\\mu) = \\beta(0) = 1 - \\Phi \\left( \\frac{\\sqrt{n} (c)}{\\sigma} \\right)\\]size \\(\\alpha\\) test, set equal \\(\\alpha\\) solve \\(c\\) get\\[c = \\frac{\\sigma \\Phi^{-1} (1 - \\alpha)}{\\sqrt{n}}\\]reject \\(H_0\\) \\[\\bar{X} > \\frac{\\sigma \\Phi^{-1} (1 - \\alpha)}{\\sqrt{n}}\\]Equivalently, reject \\[\\frac{\\sqrt{n}(\\bar{X} - 0)}{\\sigma} > z_\\alpha\\]\\(z_\\alpha = \\Phi^{-1} (1 - \\alpha)\\).Ideally find test highest power \\(H_1\\) among size \\(\\alpha\\) tests. practice, use many commonly used tests.","code":""},{"path":"classic-inference.html","id":"wald-test","chapter":"Day 12 Classical statistical inference","heading":"12.5.5 Wald test","text":"Let \\(\\theta\\) scalar parameter, let \\(\\hat{\\theta}\\) estimate \\(\\theta\\), let \\(\\widehat{\\se}\\) estimated standard error \\(\\hat{\\theta}\\). Consider testing\\[H_0: \\theta = \\theta_0 \\quad \\text{versus} \\quad H_1: \\theta \\neq \\theta_0\\]Assume \\(\\hat{\\theta}\\) asymptotically Normal:\\[\\frac{\\hat{\\theta} - \\theta_0}{\\widehat{\\se}} \\leadsto N(0,1)\\]size \\(\\alpha\\) Wald test : reject \\(H_0\\) \\(|W| > z_{\\alpha / 2}\\) \\[W = \\frac{\\hat{\\theta} - \\theta_0}{\\widehat{\\se}}\\]test statistic follows Normal distribution.","code":""},{"path":"classic-inference.html","id":"power-of-the-wald-test","chapter":"Day 12 Classical statistical inference","heading":"12.5.5.1 Power of the Wald test","text":"Suppose true value \\(\\theta\\) \\(\\theta_* \\neq \\theta_0\\). power \\(\\beta(\\theta_*)\\) – probability correctly rejecting null hypothesis – given (approximately) \\[1 - \\Phi \\left( \\frac{\\hat{\\theta} - \\theta_0}{\\widehat{\\se}} + z_{\\alpha/2} \\right) + \\Phi \\left( \\frac{\\hat{\\theta} - \\theta_0}{\\widehat{\\se}} - z_{\\alpha/2} \\right)\\]Remember two-tailed test. Essentially collecting probability mass center standard normal distribution subtracting 1, get area tails distribution. Hence, two-tailed test.Recall \\(\\widehat{\\se}\\) tends 0 sample size increases. can note :power large \\(\\theta_*\\) far \\(\\theta_0\\)power large sample size largeExample 12.4  (Comparing two means) Let \\(X_1, \\ldots, X_m\\) \\(Y_1, \\ldots, Y_n\\) two independent samples populations means \\(\\mu_1, \\mu_2\\) respectively. Let’s test null hypothesis \\(\\mu_1 = \\mu_2\\). Write \\[H_0: \\delta = 0 \\quad \\text{versus} \\quad H_1: \\delta \\neq 0\\]\\(\\delta = \\mu_1 - \\mu_2\\). estimate \\(\\delta\\) \\(\\hat{\\delta} = \\bar{X} - \\bar{Y}\\) estimated standard error\\[\\widehat{\\se} = \\sqrt{\\frac{s_1^2}{m} + \\frac{s_2^2}{n}}\\]\\(s_1^2\\) \\(s_2^2\\) sample variances. size \\(\\alpha\\) Wald test rejects \\(H_0\\) \\(|W| > z_{\\alpha / 2}\\) \\[W = \\frac{\\hat{\\delta} - 0}{\\widehat{\\se}} = \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\frac{s_1^2}{m} + \\frac{s_2^2}{n}}}\\]","code":""},{"path":"classic-inference.html","id":"wald-or-t-test","chapter":"Day 12 Classical statistical inference","heading":"12.5.6 Wald or \\(t\\)-test?","text":"test \\(H_0: \\mu = \\mu_0\\) \\(\\mu = \\E[X_i]\\) mean, can use Wald test. data assumed Normal sample size small, common use \\(t\\)-test based Student’s \\(t\\) distribution.Definition 12.2  (Degrees freedom) Generally defined number observations minus number estimated parameters.","code":""},{"path":"classic-inference.html","id":"relationship-to-confidence-intervals","chapter":"Day 12 Classical statistical inference","heading":"12.5.7 Relationship to confidence intervals","text":"relationship Wald test \\(1 - \\alpha\\) asymptotic confidence interval \\(\\hat{\\theta} \\pm \\widehat{\\se} z_{\\alpha/2}\\). size \\(\\alpha\\) Wald test rejects \\(H_0: \\theta = \\theta_0 \\quad \\text{versus} \\quad \\theta \\neq \\theta_0\\) \\(\\theta_0 \\notin C\\) \\[C = (\\hat{\\theta} - \\widehat{\\se}z_{\\alpha / 2}, \\hat{\\theta} + \\widehat{\\se}z_{\\alpha / 2})\\]Thus, testing hypothesis equivalent checking whether null value confidence interval.","code":""},{"path":"classic-inference.html","id":"statistical-vs.-scientific-significance","chapter":"Day 12 Classical statistical inference","heading":"12.5.8 Statistical vs. scientific significance","text":"\nFigure 12.2: Difference statistical scientific significance. Courtesy Carl Sagan.\nRejecting \\(H_0\\) indicates result statistically significant. , strong evidence reject \\(H_0\\). result effect size can still small test powerful. situation, statistical significance necessarily scientific/substantive/practical significance. always concerned types significance. Statistical significance alone necessarily useful informative finding.","code":""},{"path":"classic-inference.html","id":"p-values","chapter":"Day 12 Classical statistical inference","heading":"12.6 \\(p\\)-values","text":"use fine-grained measure evidence null hypothesis. Generally, test rejects level \\(\\alpha\\) also reject level \\(\\alpha' > \\alpha\\). Hence, smallest \\(\\alpha\\) test rejects call number \\(p\\)-value. Informally, smaller \\(p\\)-value, stronger evidence \\(H_0\\). Remember \\(\\alpha\\) function power test, magnitude difference \\(\\theta_*\\) \\(\\theta_0\\) sample size influence value.","code":""},{"path":"classic-inference.html","id":"interpreting-p-values","chapter":"Day 12 Classical statistical inference","heading":"12.6.1 Interpreting \\(p\\)-values","text":"values informal standards. rhyme reason soA large \\(p\\)-value strong evidence favor \\(H_0\\)\n\\(H_0\\) true\n\\(H_0\\) false test low power\n\\(H_0\\) true\\(H_0\\) false test low power\\(p\\)-value \\(\\Pr (H_0 | \\text{Data})\\). \\(p\\)-value probability null hypothesis true","code":""},{"path":"classic-inference.html","id":"calculating-p-values","chapter":"Day 12 Classical statistical inference","heading":"12.6.2 Calculating \\(p\\)-values","text":"Suppose size \\(\\alpha\\) test form\\[\\text{reject } H_0 \\text{ } T(X_n) \\geq c_\\alpha\\],\\[\\text{p-value} = \\text{sup}_{\\theta \\\\Theta_0} \\Pr_\\theta (T(X^n) \\geq T(x^n))\\]\\(x^n\\) observed value \\(X^n\\). \\(\\Theta_0 = \\{ \\theta_0 \\}\\) \\[\\text{p-value} = \\Pr_{\\theta_0} (T(X^n) \\geq T(x^n))\\]Informally, \\(p\\)-value probability \\(H_0\\) observing value test statistic extreme actually observed.","code":""},{"path":"classic-inference.html","id":"p-value-for-wald-test","chapter":"Day 12 Classical statistical inference","heading":"12.6.2.1 \\(p\\)-value for Wald test","text":"Let\\[w = \\frac{\\hat{\\theta} - \\theta_0}{\\widehat{\\se}}\\]denote observed value Wald statistic \\(W\\). \\(p\\)-value given \\[\\text{p-value} = \\Pr_{\\theta_0} (|W| > |w|) \\approx \\Pr (|Z| > |w| = 2 \\Phi(-|w|)\\]\\(Z \\sim N(0,1)\\).Example 12.5  (Cholesterol data) Consider set 371 individuals health study examining cholesterol levels (mg/dl). 320 individuals narrowing arteries, 51 patients evidence heart disease. mean cholesterol different two groups?Let estimated mean cholesterol levels first group \\(\\bar{X} = 216.2\\) second group \\(\\bar{Y} = 195.3\\). Let estimated standard error group \\(\\widehat{\\se}(\\hat{\\mu}_1) = 5.0\\) \\(\\widehat{\\se}(\\hat{\\mu}_2) = 2.4\\). Wald test statistic \\[W = \\frac{\\hat{\\delta} - 0}{\\widehat{\\se}} = \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\widehat{\\se}_1^2 + \\widehat{\\se}_2^2}} = \\frac{216.2 - 195.3}{\\sqrt{5^2 + 2.4^2}} = 3.78\\]compute \\(p\\)-value, let \\(Z \\sim N(0,1)\\) denote standard Normal random variable. \\[\\text{p-value} = \\Pr (|Z| > 3.78) = 2 \\Pr(Z < -3.78) = 0.0002\\]strong evidence null hypothesis.","code":""},{"path":"classic-inference.html","id":"pearsons-chi2-test-for-multinomial-data","chapter":"Day 12 Classical statistical inference","heading":"12.6.3 Pearson’s \\(\\chi^2\\) test for multinomial data","text":"Pearson’s \\(\\chi^2\\) test used multinomial data. Recall \\(X = (X_1, \\ldots, X_k)\\) multinomial \\((n,p)\\) distribution, MLE \\(p\\) \\(\\hat{p} = (\\hat{p}_1, \\ldots, \\hat{p}_k) = (x_1 / n, \\ldots, x_k / n)\\).Let \\(p_0 = (p_{01}, \\ldots, p_{0k})\\) fixed vector suppose want test\\[H_0: p = p_0 \\quad \\text{versus} \\quad H_1: p \\neq p_0\\]Pearson’s \\(\\chi^2\\) statistic \\[T = \\sum_{j=1}^k \\frac{(X_j - np_{0j})^2}{np_{0j}} = \\sum_{j=1}^k \\frac{(X_j - \\E[X_j])^2}{\\E[X_j]}\\]\\(\\E[X_j] = \\E[X_j] = np_{0j}\\) expected value \\(H_0\\).","code":""},{"path":"classic-inference.html","id":"example-attitudes-towards-abortion","chapter":"Day 12 Classical statistical inference","heading":"12.6.3.1 Example: Attitudes towards abortion","text":"\\(H_A\\) - comparison individuals, liberals likely favor allowing woman obtain abortion reason conservatives\\(H_0\\) - difference support liberals conservatives allowing woman obtain abortion reason. difference result random sampling error.Say null hypothesis correct - differences ideological groups attitudes towards abortion. table look like?22In truth, table actually look like?can test differences statistically significant? , test see can reject null hypothesis?Calculating test statistic\n\\(\\chi^2=\\sum{\\frac{(X_j - \\E[X_j])^2}{\\E[X_j]}}=145.27\\)\n\\(\\text{Degrees freedom} = (\\text{number rows}-1)(\\text{number columns}-1)=2\\)\n\\(\\chi^2=\\sum{\\frac{(X_j - \\E[X_j])^2}{\\E[X_j]}}=145.27\\)\\(\\text{Degrees freedom} = (\\text{number rows}-1)(\\text{number columns}-1)=2\\)Calculating \\(p\\)-value\n\\(\\text{p-value} = \\Pr (\\chi_2^2 > 145.27) = 0\\)\nprobability null hypothesis true observed frequencies result random sampling error less 1 quintillion. Extremely extremely unlikely null hypothesis true.\n\\(\\text{p-value} = \\Pr (\\chi_2^2 > 145.27) = 0\\)probability null hypothesis true observed frequencies result random sampling error less 1 quintillion. Extremely extremely unlikely null hypothesis true.","code":""},{"path":"mle-ols.html","id":"mle-ols","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"Day 13 Maximum likelihood estimation and linear regression","text":"","code":""},{"path":"mle-ols.html","id":"learning-objectives-12","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"Learning objectives","text":"Define maximum likelihood estimation (MLE)Review properties maximum likelihood estimatorDemonstrate MLE basic estimatorsDefine ordinary least squares (OLS) estimationIdentify OLS estimator best linear unbiased estimatorIdentify key assumptions OLS modelsEvaluate methods test violations assumptionsConsider alleviate violations","code":""},{"path":"mle-ols.html","id":"supplemental-readings-12","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"Supplemental readings","text":"Chapter 9 Bertsekas Tsitsiklis (2008)Wasserman (2013)\nCh 9 - Parametric Inference\nCh 13 - Linear Logistic Regression\nCh 9 - Parametric InferenceCh 13 - Linear Logistic Regression","code":""},{"path":"mle-ols.html","id":"maximum-likelihood","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.1 Maximum likelihood","text":"common method estimating parameters parametric model maximum likelihood method. Let \\(X_1, \\ldots, X_n\\) IID PDF \\(f(x; \\theta)\\). likelihood function defined \\[\\Lagr_n(\\theta) = \\prod_{=1}^n f(X_i; \\theta)\\]log-likelihood function defined \\(\\lagr_n (\\theta) = \\log \\Lagr_n(\\theta)\\). likelihood function joint density data, except treat function parameter \\(\\theta\\). However likelihood function density function – likelihood function. general, true \\(\\Lagr_n(\\theta)\\) integrates 1 (respect \\(\\theta\\)).maximum likelihood estimator (MLE), denoted \\(\\hat{\\theta}_n\\), value \\(\\theta\\) maximizes \\(\\Lagr_n(\\theta)\\). maximum \\(\\lagr_n(\\theta)\\) occurs place maximum \\(\\Lagr_n(\\theta)\\), maximizing log-likelihood leads answer maximizing likelihood. Often, just easier work log-likelihood.multiply \\(\\Lagr_n(\\theta)\\) positive constant \\(c\\) (depending \\(\\theta\\)) change MLE. Thus shall drop constants likelihood function.Example 13.1  (Bernoulli distribution) Suppose \\(X_1, \\ldots, X_n \\sim \\text{Bernoulli} (\\pi)\\). probability function \\[f(x; \\pi) = \\pi^x (1 - \\pi)^{1 - x}\\]\\(x = 0,1\\). unknown parameter \\(\\pi\\). ,\\[\n\\begin{align}\n\\Lagr_n(\\pi) &= \\prod_{=1}^n f(X_i; \\pi) \\\\\n&= \\prod_{=1}^n \\pi^{X_i} (1 - \\pi)^{1 - X_i} \\\\\n&= \\pi^S (1 - \\pi)^{n - S}\n\\end{align}\n\\]\\(S = \\sum_{} X_i\\). log-likelihood function therefore\\[\\lagr_n (\\pi) = S \\log(\\pi) + (n - S) \\log(1 - \\pi)\\]analytically solve \\(\\hat{\\pi}_n\\), take derivative \\(\\lagr_n (\\pi)\\), set equal 0, solve \\(\\hat{\\pi}_n = \\frac{S}{n}\\).Example 13.2  (Normal distribution) Let \\(X_1, \\ldots, X_n \\sim N(\\mu, \\sigma^2)\\). parameter \\(\\theta = (\\mu, \\sigma)\\) likelihood function (ignoring constants) :\\[\n\\begin{align}\n\\Lagr_n (\\mu, \\sigma) &= \\prod_i \\frac{1}{\\sigma} \\exp \\left[ - \\frac{1}{2\\sigma^2} (X_i - \\mu)^2 \\right] \\\\\n&= \\frac{1}{\\sigma^n} \\exp \\left[ - \\frac{1}{2\\sigma^2} \\sum_i (X_i - \\mu)^2 \\right] \\\\\n&= \\frac{1}{\\sigma^n} \\exp \\left[ \\frac{n S^2}{2 \\sigma^2} \\right] \\exp \\left[ - \\frac{n (\\bar{X} - \\mu)^2}{2 \\sigma^2} \\right]\n\\end{align}\n\\]\\(\\bar{X} = \\frac{1}{n} \\sum_i X_i\\) sample mean \\(S^2 = \\frac{1}{n} \\sum_i (X_i - \\bar{X})^2\\). log-likelihood \\[\\lagr_n (\\mu, \\sigma) = -n \\log \\sigma - \\frac{nS^2}{2\\sigma^2} - \\frac{n(\\bar{X} - \\mu)^2}{2\\sigma^2}\\]Calculating first derivatives respect \\(\\mu\\) \\(\\sigma\\), setting equal 0, solving \\(\\hat{\\mu}, \\hat{\\sigma}\\) leads \\(\\hat{\\mu} = \\bar{X} = \\E [X]\\) \\(\\hat{\\sigma} = S = \\sqrt{\\Var[X]}\\). mean variance/standard deviation normal distribution also maximum likelihood estimators.","code":""},{"path":"mle-ols.html","id":"properties-of-maximum-likelihood-estimators","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.1.1 Properties of maximum likelihood estimators","text":"certain conditions, maximum likelihood estimator \\(\\hat{\\theta}_n\\) possesses many properties make appealing choice estimatory. main properties :ConsistencyEquivariantAsymptotically NormalAsymptotically optimal efficientThese properties generally hold true random variables large sample sizes smooth conditions \\(f(x; \\theta)\\). requirements met, MLE may good estimator parameter interest.","code":""},{"path":"mle-ols.html","id":"consistency","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.1.1.1 Consistency","text":"MLE consistent, \\(\\hat{\\theta}_n \\xrightarrow{P} \\theta_*\\), \\(\\theta_*\\) denotes true value parameter \\(\\theta\\). Consistency means MLE converges probability true value number observations increases.","code":""},{"path":"mle-ols.html","id":"equivariance","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.1.1.2 Equivariance","text":"Equivariance indicates \\(\\hat{\\theta}_n\\) MLE \\(\\theta\\), \\(g(\\hat{\\theta}_n)\\) MLE \\(g(\\theta)\\). Basically, MLE estimator random variable transformed function \\(g(x)\\) also MLE estimator new random variable \\(g(x)\\). example, let \\(X_1, \\ldots, X_n \\sim N(\\theta,1)\\). MLE \\(\\theta\\) \\(\\hat{\\theta}_n = \\bar{X}_n\\). Let \\(\\tau = e^\\theta\\). MLE \\(\\tau\\) \\(\\hat{\\tau} = e^{\\hat{\\theta}} = e^{\\bar{X}}\\).","code":""},{"path":"mle-ols.html","id":"asymptotic-normality","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.1.1.3 Asymptotic normality","text":"Asymptotic normality indicates distribution MLE estimator asymptotically normal. , let \\(\\se = \\sqrt{\\Var (\\hat{\\sigma}_n)}\\).\\[\\frac{\\hat{\\theta}_n - \\theta_*}{\\se} \\leadsto N(0,1)\\]distribution true standard error \\(\\hat{\\theta}_n\\) approximately standard Normal distribution. Since typically estimate standard error data, also holds true \\[\\frac{\\hat{\\theta}_n - \\theta_*}{\\widehat{\\se}} \\leadsto N(0,1)\\]proof property book. Informally, means distribution MLE can approximated \\(N(\\theta, \\widehat{\\se}^2)\\). allows us construct confidence intervals point estimates like saw previously. long sample size sufficiently large \\(f(x; \\theta)\\) sufficiently smooth, property holds true need estimate actual confidence intervals MLE – Normal approximation sufficient.","code":""},{"path":"mle-ols.html","id":"optimality","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.1.1.4 Optimality","text":"Suppose \\(X_1, \\ldots, X_n \\sim N(\\theta, \\sigma^2)\\). MLE \\(\\hat{\\sigma}_n = \\bar{X}_n\\). Another reasonable estimator \\(\\theta\\) sample median \\(\\tilde{\\theta}_n\\). MLE satisfies\\[\\sqrt{n} (\\hat{\\theta}_n - \\theta) \\leadsto N(0, \\sigma^2)\\]can shown median satisfies\\[\\sqrt{n} (\\tilde{\\theta}_n - \\theta) \\leadsto N \\left(0, \\sigma^2 \\frac{\\pi}{2} \\right)\\]means median converges right value larger variance MLE.generally, consider two estimators \\(T_n\\) \\(U_n\\), suppose \\[\n\\begin{align}\n\\sqrt{n} (T_n - \\theta) &\\leadsto N(0, t^2) \\\\\n\\sqrt{n} (U_n - \\theta) &\\leadsto N(0, u^2) \\\\\n\\end{align}\n\\]define asymptotic relative efficiency \\(U\\) \\(T\\) \\(\\text{}(U, T) = \\frac{t^2}{u^2}\\). Normal example, \\(\\text{}(\\tilde{\\theta}_n, \\hat{\\theta}_n) = \\frac{2}{\\pi} \\approx .63\\). interpretation use median, effectively using fraction data estimate optimal mean.Fundamentally, \\(\\hat{\\theta}_n\\) MLE \\(\\tilde{\\theta}_n\\) estimator, \\[\\text{} (\\tilde{\\theta}_n, \\hat{\\theta}_n) \\leq 1\\]Thus, MLE smallest (asymptotic) variance say MLE efficient asymptotically optimal.Example 13.3  (Calculating MLE mean Normal variable) presume response variable \\(Y\\) drawn Gaussian (normal) distribution mean \\(\\mu\\) variance \\(\\sigma^2\\):\\[\\Pr(X_i = x_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left[\\frac{(X_i - \\mu)^2}{2\\sigma^2}\\right]\\]density, probability density function (PDF) variable \\(Y\\).probability , one observation \\(\\), \\(Y\\) take particular value \\(y\\).function \\(\\mu\\), expected value distribution, \\(\\sigma^2\\), variability distribution around mean.want generate estimates parameters \\(\\hat{\\mu}_n\\) \\(\\hat{\\sigma}_n^2\\) based data. normal distribution, log-likelihood function :\\[\n\\begin{align}\n\\lagr_n(\\mu, \\sigma^2 | X) &= \\log \\prod_{= 1}^{N}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left[\\frac{(X_i - \\mu)^2}{2\\sigma^2}\\right]} \\\\\n&= \\sum_{=1}^{N}{\\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left[\\frac{(X_i - \\mu)^2}{2\\sigma^2}\\right]\\right)} \\\\\n&= -\\frac{N}{2} \\log(2\\pi) - \\left[ \\sum_{= 1}^{N} \\log{\\sigma^2 - \\frac{1}{2\\sigma^2}} (X_i - \\mu)^2 \\right]\n\\end{align}\n\\]Suppose sample assistant professor salaries:Table 13.1: Salaries assistant professorsIf want explain distribution possible assistant professor salaries given data points, use maximum-likelihood estimation find \\(\\hat{\\mu}\\) maximizes likelihood data. testing different values \\(\\mu\\) see optimizes function. regressors predictors, \\(\\hat{\\mu}\\) constant. Furthermore, treat \\(\\sigma^2\\) nuisance parameter hold constant \\(\\sigma^2 = 1\\). log-likelihood curve look like :maximum 60, mean 5 sample observations. Notice choice value \\(\\sigma^2\\) doesn’t change estimate \\(\\hat{\\mu}_n\\).","code":""},{"path":"mle-ols.html","id":"least-squares-regression","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.2 Least squares regression","text":"Regression method studying relationship response variable \\(Y\\) covariate \\(X\\) (also known predictor variable feature). One way summarize relationship \\(X\\) \\(Y\\) regression function:\\[r(x) = \\E (Y | X = x) = \\int y f(y|x) dy\\]goal estimate regression function \\(r(x)\\) data form\\[(Y_1, X_1), \\ldots, (Y_n, X_n) \\sim F_{X,Y}\\]","code":""},{"path":"mle-ols.html","id":"simple-linear-regression","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.2.1 Simple linear regression","text":"simplest form regression \\(X_i\\) simple (one-dimensional) \\(r(x)\\) assumed linear:\\[r(x) = \\beta_0 + \\beta_1 x\\]model called simple linear regression model. make assumption \\(\\Var (\\epsilon_i | X = x) = \\sigma^2\\) depend \\(x\\). Thus linear regression model :\\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\]\\(\\E (\\epsilon_i | X_i) = 0\\) \\(\\Var (\\epsilon_i | X_i) = \\sigma^2\\). unknown parameters model intercept \\(\\beta_0\\) slope \\(\\beta_1\\) variance \\(\\sigma^2\\). Let \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\) denote estimates \\(\\beta_0\\) \\(\\beta_1\\). fitted line \\[\\hat{r}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\]predicted values fitted values \\[\\hat{Y}_i = \\hat{r}(X_i)\\]residuals defined \\[\\hat{\\epsilon}_i = Y_i - \\hat{Y}_i = Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x)\\]residual sum squares RSS measures well line fits data. defined \\[RSS = \\sum_{=1}^n \\hat{\\epsilon}_i^2\\]","code":""},{"path":"mle-ols.html","id":"estimation-strategy","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.2.2 Estimation strategy","text":"appropriate way estimate \\(\\beta\\)s? fit many lines data, better others.seek estimators set desired qualities. Classically, two desired qualities estimator unbiasedness efficiency.Definition 13.1  (Unbiasedness) \\(\\E(\\hat{\\beta}) = \\beta\\), estimator “gets right” vis--vis \\(\\beta\\).Definition 13.2  (Efficiency) \\(\\min(\\Var(\\hat{\\beta}))\\). get right, given sample used generate model never want far “right.”","code":""},{"path":"mle-ols.html","id":"least-squares-estimator","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.2.3 Least squares estimator","text":"least squares estimates values \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\) minimize RSS.\\[\\min(RSS)\\]requires bit calculus solve.\\[\n\\begin{aligned}\nRSS &= \\sum_{=1}^n \\hat{\\epsilon}_i^2 \\\\\n\\sum_{=1}^n (\\hat{\\epsilon}_i)^2 &= \\sum_{=1}^n (Y_i - (\\beta_0 + \\beta_1 X_i))^2\\\\\nf(\\beta_0, \\beta_1 | x_i, y_i) & = \\sum_{=1}^n (Y_i - \\beta_0 - \\beta_1 X_i )^2\\\\\n\\dfrac{\\partial{ f(\\beta_0, \\beta_1 | x_i, y_i)}}{\\partial \\beta_0} & = -2 (\\sum_{=1}^n (Y_i - \\beta_0 - \\beta_1 X_i))\\\\\n& = \\sum_{=1}^n -2Y_i + 2\\beta_0 + 2\\beta_1 X_i\\\\\n0 & = \\sum_{=1}^n -2Y_{} + 2\\beta_0 + 2\\beta_1 X_i\\\\\n0 & = -2 \\sum_{=1}^n Y_{} +  2\\sum_{=1}^n \\beta_0 +  2\\beta_1 \\sum_{=1}^n X_i\\\\\n0 & = -2 \\sum_{=1}^n Y_{} +  (n \\times 2\\beta_0) +  2\\beta_1 \\sum_{=1}^n X_i\\\\\nn \\times 2\\beta_0 & = 2 \\sum_{=1}^n Y_i - 2\\beta_1 \\sum_{=1}^n X_i\\\\\n\\hat \\beta_0 & = \\dfrac{2 \\sum_{=1}^n Y_i}{2n} - \\dfrac{2\\beta_1 \\sum_{=1}^n X_i}{2n}\\\\\n& =  \\dfrac{\\sum_{=1}^n Y_i}{n} - \\beta_1\\dfrac{ \\sum_{=1}^n X_i}{n}\\\\\n \\hat \\beta_0 & = \\bar{Y}_n - \\beta_1 \\bar{X}_n\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\n\\dfrac{\\partial{ f(\\beta_0, \\beta_1 | x_i, y_i)}}{\\partial \\beta_1} & = \\sum_{=1}^n -2X_i(Y_i - \\beta_0 - \\beta_1 X_i) \\\\\n& =  \\sum_{=1}^n -2Y_iX_i + 2\\beta_0X_i + 2\\beta_1 X_i^2\\\\\n0 & =  \\sum_{=1}^n -2Y_iX_i + 2\\beta_0 \\sum_{=1}^nX_i + 2\\beta_1  \\sum_{=1}^n X_i^2\\\\\n& =  \\sum_{=1}^n -2Y_iX_i + 2 (\\bar{Y}_n - \\beta_1 \\bar{X}_n) \\sum_{=1}^nX_i + 2\\beta_1  \\sum_{=1}^n X_i^2\\\\\n& = \\sum_{=1}^n -2Y_iX_i + 2\\bar{Y}_n \\sum_{=1}^nX_i - 2\\beta_1 \\bar{X}_n\\sum_{=1}^nX_i + 2\\beta_1  \\sum_{=1}^n X_i^2\\\\\n2\\beta_1  \\sum_{=1}^n X_i^2 - 2\\beta_1 \\bar{X}_n\\sum_{=1}^nX_i  & = \\sum_{=1}^n 2Y_iX_i  - 2\\bar{Y}_n \\sum_{=1}^nX_i\\\\\n\\beta_1 ( \\sum_{=1}^n X_i^2 - \\bar{X}_n\\sum_{=1}^nX_i ) & = \\sum_{=1}^n Y_iX_i  - \\bar{Y}_n \\sum_{=1}^nX_i\\\\\n\\hat \\beta_1 & = \\dfrac{ \\sum_{=1}^n Y_iX_i  - \\bar{Y}_n \\sum_{=1}^nX_i}{ \\sum_{=1}^n X_i^2 - \\bar{X}_n\\sum_{=1}^nX_i}\\\\\n \\hat \\beta_0 & = \\bar{Y}_n - \\hat{\\beta}_1 \\bar{X}_n\n\\end{aligned}\n\\]Recall also need estimate \\(\\sigma^2\\). unbiased estimate turns \\[\\hat{\\sigma}^2 = \\left( \\frac{1}{n - 2} \\right) \\sum_{=1}^n \\hat{\\epsilon}_i^2\\]","code":""},{"path":"mle-ols.html","id":"maximum-likelihood-estimation","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.2.4 Maximum likelihood estimation","text":"Suppose add assumption \\(\\epsilon_i | X_i \\sim N(0, \\sigma^2)\\), ,\\[Y_i | X_i \\sim N(\\mu_i, \\sigma^2)\\]\\(\\mu_i = \\beta_0 + \\beta_1 X_i\\). means \\(\\)th observation systematic mean varies based value \\(X_i\\). likelihood function \\[\n\\begin{align}\n\\prod_{=1}^n f(X_i, Y_i) &= \\prod_{=1}^n f_X(X_i) f_{Y | X} (Y_i | X_i) \\\\\n&= \\prod_{=1}^n f_X(X_i) \\times \\prod_{=1}^n f_{Y | X} (Y_i | X_i) \\\\\n&= \\Lagr_1 \\times \\Lagr_2\n\\end{align}\n\\]\\[\n\\begin{align}\n\\Lagr_1 &= \\prod_{=1}^n f_X(X_i) \\\\\n\\Lagr_2 &= \\prod_{=1}^n f_{Y | X} (Y_i | X_i)\n\\end{align}\n\\]\\(\\Lagr_1\\) involve parameters \\(\\beta_0, \\beta_1\\). Instead can focus second term \\(\\Lagr_2\\) called conditional likelihood, given \\[\n\\begin{align}\n\\Lagr_2 &\\equiv \\Lagr(\\beta_0, \\beta_1, \\sigma^2) \\\\\n&= \\prod_{=1}^n f_{Y | X}(Y_i | X_i) \\\\\n&\\propto \\frac{1}{\\sigma} \\exp \\left\\{ -\\frac{1}{2\\sigma^2} \\sum_{=1}^n (Y_i - \\mu_i)^2 \\right\\}\n\\end{align}\n\\]conditional log-likelihood \\[\\lagr(\\beta_0, \\beta_1, \\sigma^2) = -n \\log(\\sigma) - \\frac{1}{2\\sigma^2} \\left( Y_i - (\\beta_0 + \\beta_1 X_i) \\right)^2\\]find MLE \\((\\beta_0, \\beta_1)\\), maximize \\(\\lagr(\\beta_0, \\beta_1, \\sigma^2)\\). equivalent minimizing RSS\\[RSS = \\sum_{=1}^n \\hat{\\epsilon}_i^2 = \\sum_{=1}^n \\left( Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x) \\right)\\]Therefore, assumption residuals distributed normally, least squares estimator also maximum likelihood estimator.","code":""},{"path":"mle-ols.html","id":"properties-of-the-least-squares-estimator","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.2.5 Properties of the least squares estimator","text":"regression problems, usually focus properties estimators conditional \\(X^n = (X_1, \\ldots, X_n)\\). Thus state means variances conditional means variances.Let \\(\\hat{\\beta}^T = (\\hat{\\beta}_0, \\hat{\\beta}_1)^T\\) denote least squares estimators (\\(^T\\)) simply indicates vector transposed column vector. \\[\n\\begin{align}\n\\E (\\hat{\\beta} | X^n) &= \\begin{pmatrix}\n  \\beta_0 \\\\\n  \\beta_1\n \\end{pmatrix} \\\\\n \\Var (\\hat{\\beta} | X^n) &= \\frac{\\sigma^2}{n s_X^2}  \\begin{pmatrix}\n  \\frac{1}{n} \\sum_{=1}^n X_i^2 & -\\bar{X}^n \\\\\n  -\\bar{X}^n & 1\n \\end{pmatrix}\n\\end{align}\n\\]\\[s_X^2 = \\frac{1}{n} \\sum_{=1}^n (X_i - \\bar{X}_n)^2\\]estimated standard errors \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\) obtained taking square roots corresponding diagonal terms \\(\\Var (\\hat{\\beta} | X^n)\\) inserting estimate \\(\\hat{\\sigma}\\) \\(\\sigma\\). Thus,\\[\n\\begin{align}\n\\widehat{\\se} (\\hat{\\beta}_0) &= \\frac{\\hat{\\sigma}}{s_X \\sqrt{n}} \\sqrt{\\frac{ \\sum_{=1}^n X_i^2}{n}} \\\\\n\\widehat{\\se} (\\hat{\\beta}_1) &= \\frac{\\hat{\\sigma}}{s_X \\sqrt{n}}\n\\end{align}\n\\]appropriate conditions, estimators meet criteria maximum likelihood estimators.","code":""},{"path":"mle-ols.html","id":"assumptions-of-linear-regression-models","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.3 Assumptions of linear regression models","text":"Basic linear regression follows functional form:\\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\]\\(Y_i\\) value response variable \\(Y\\) \\(\\)th observation, \\(X_i\\) value explanatory variable \\(X\\) \\(\\)th observation. coefficients \\(\\beta_0\\) \\(\\beta_1\\) population regression coefficients - goal estimate population parameters given observed data. \\(\\epsilon_i\\) error representing aggregated omitted causes \\(Y\\), explanatory variables included model, measurement error \\(Y\\), inherently random component \\(Y\\).key assumptions linear regression concern behavior errors.","code":""},{"path":"mle-ols.html","id":"linearity","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.3.1 Linearity","text":"expectation error 0:\\[\\E(\\epsilon_i) \\equiv E(\\epsilon_i | X_i) = 0\\]allows us recover expected value response variable linear function explanatory variable:\\[\n\\begin{aligned}\n\\mu_i \\equiv E(Y_i) \\equiv E(Y | X_i) &= E(\\beta_0 + \\beta_1 X_i + \\epsilon_i) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 X_i + E(\\epsilon_i) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 X_i + 0 \\\\\n\\mu_i &= \\beta_0 + \\beta_1 X_i\n\\end{aligned}\n\\]","code":""},{"path":"mle-ols.html","id":"constant-variance","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.3.2 Constant variance","text":"variance errors regardless values \\(X\\):\\[\\Var(\\epsilon_i | X_i) = \\sigma^2\\]","code":""},{"path":"mle-ols.html","id":"normality","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.3.3 Normality","text":"errors assumed normally distributed:\\[\\epsilon_i \\mid X_i \\sim N(0, \\sigma^2)\\]","code":""},{"path":"mle-ols.html","id":"independence","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.3.4 Independence","text":"Observations sampled independently one another. pair errors \\(\\epsilon_i\\) \\(\\epsilon_j\\) independent \\(\\neq j\\). Simple random sampling large population ensure assumption met. However data collection procedures frequently (explicitly) violate assumption (e.g. time series data, panel survey data).","code":""},{"path":"mle-ols.html","id":"fixed-x-or-x-measured-without-error-and-independent-of-the-error","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.3.5 Fixed \\(X\\), or \\(X\\) measured without error and independent of the error","text":"\\(X\\) assumed fixed measured without error independent error. fixed \\(X\\), researcher controls precise value \\(X\\) given observation (think experimental design treatment/control). observational study, assume \\(X\\) measured without error explanatory variable error independent population sample drawn.\\[\\epsilon_i \\sim N(0, \\sigma^2), \\text{} = 1, \\dots, n\\]","code":""},{"path":"mle-ols.html","id":"x-is-not-invariant","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.3.6 \\(X\\) is not invariant","text":"\\(X\\) fixed, must vary (.e. ’s values ). \\(X\\) random, population \\(X\\) must vary. estimate regression line invariant \\(X\\).","code":""},{"path":"mle-ols.html","id":"handling-violations-of-assumptions","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.3.7 Handling violations of assumptions","text":"assumptions violated, conducting inference linear regression becomes tricky, biased, inefficient, /error prone. move robust inferential method nonparametric regression, decision trees, support vector machines, etc., methods tricky generate inference explanatory variables. Instead, can attempt diagnose assumption violations impose solutions still constraining linear regression framework.","code":""},{"path":"mle-ols.html","id":"unusual-and-influential-data","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.4 Unusual and influential data","text":"Outliers observations somehow unusual, either value \\(Y_i\\), one \\(X_i\\)s, combination thereof. Outliers potential disproportionate influence regression model.","code":""},{"path":"mle-ols.html","id":"terms","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.4.1 Terms","text":"Outlier - observation unusual value dependent variable \\(Y\\) given particular combination values \\(X\\)Outlier - observation unusual value dependent variable \\(Y\\) given particular combination values \\(X\\)Leverage - degree potential influence coefficient estimates given observation can (necessarily ) haveLeverage - degree potential influence coefficient estimates given observation can (necessarily ) haveDiscrepancy - extent observation “unusual” “different” rest dataDiscrepancy - extent observation “unusual” “different” rest dataInfluence - much effect particular observation’s value(s) \\(Y\\) \\(X\\) coefficient estimates. Influence function leverage discrepancy:\n\\[\\text{Influence} = \\text{Leverage} \\times \\text{Discrepancy}\\]Influence - much effect particular observation’s value(s) \\(Y\\) \\(X\\) coefficient estimates. Influence function leverage discrepancy:\\[\\text{Influence} = \\text{Leverage} \\times \\text{Discrepancy}\\]Dino observation high leverage low discrepancy (close regression line defined Betty, Fred, Wilma). Therefore little impact regression line (long dashed line); influence low discrepancy low.Barney high leverage (though lower Dino) high discrepancy, substantially influences regression results (short-dashed line).","code":""},{"path":"mle-ols.html","id":"measuring-leverage","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.4.2 Measuring leverage","text":"Leverage typically assessed using leverage (hat) statistic:\\[h_i = \\frac{1}{n} + \\frac{(X_i - \\bar{X})^2}{\\sum_{j=1}^{n} (X_{j} - \\bar{X})^2}\\]Measures contribution observation \\(Y_i\\) fitted value \\(\\hat{Y}_j\\) (values dataset)solely function \\(X\\)Larger values indicate higher leverage\\(\\frac{1}{n} \\leq h_i \\leq 1\\)\\(\\bar{h} = \\frac{(p + 1)}{n}\\)Observations leverage statistic greater average high leverage.","code":""},{"path":"mle-ols.html","id":"measuring-discrepancy","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.4.3 Measuring discrepancy","text":"Residuals natural way look discrepant outlying observations (discrepant observations typically large residuals, differences actual fitted values \\(y_i\\).) problem variability errors \\(\\hat{\\epsilon}_i\\) equal variances, even actual errors \\(\\epsilon_i\\) equal variances:\\[\\Var(\\hat{\\epsilon}_i) = \\sigma^2 (1 - h_i)\\]High leverage observations tend small residuals, makes sense pull regression line towards . Alternatively can calculate standardized residual parses variability \\(X_i\\) \\(\\hat{\\epsilon}_i\\):\\[\\hat{\\epsilon}_i ' \\equiv \\frac{\\hat{\\epsilon}_i}{S_{E} \\sqrt{1 - h_i}}\\]\\(S_E\\) standard error regression:\\[S_E = \\sqrt{\\frac{\\hat{\\epsilon}_i^2}{(n - k - 1)}}\\]problem numerator denominator independent - contain \\(\\hat{\\epsilon}_i\\), \\(\\hat{\\epsilon}_i '\\) follow \\(t\\)-distribution. Instead, can modify measure calculating \\(S_{E(-)}\\); , refit model deleting \\(\\)th observation, estimating standard error regression \\(S_{E(-)}\\) based remaining \\(-1\\) observations. calculate studentized residual:\\[\\hat{\\epsilon}_i^{\\ast} \\equiv \\frac{\\hat{\\epsilon}_i}{S_{E(-)} \\sqrt{1 - h_i}}\\]now independent numerator denominator follows \\(t\\)-distribution \\(n-k-2\\) degrees freedom. common scale expect roughly 95% studentized residuals fall within interval \\([-2,2]\\).","code":""},{"path":"mle-ols.html","id":"measuring-influence","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.4.4 Measuring influence","text":"described previously, influence combination observation’s leverage discrepancy. words, influence effect particular observation coefficient estimates. simple measure influence difference coefficient estimate without observation question:\\[D_{ij} = \\hat{\\beta_1j} - \\hat{\\beta}_{1j(-)}, \\text{} =1, \\dots, n \\text{ } j = 0, \\dots, k\\]measure called \\(\\text{DFBETA}_{ij}\\). Since coefficient estimates scaled differently depending variables scaled, can rescale \\(\\text{DFBETA}_{ij}\\) coefficient’s standard error account fact:\\[D^{\\ast}_{ij} = \\frac{D_{ij}}{SE_{-}(\\beta_{1j})}\\]measure called \\(\\text{DFBETAS}_{ij}\\).Positive values \\(\\text{DFBETAS}_{ij}\\) correspond observations decrease estimate \\(\\hat{\\beta}_{1j}\\)Negative values \\(\\text{DFBETAS}_{ij}\\) correspond observations increase estimate \\(\\hat{\\beta}_{1j}\\)Frequently \\(\\text{DFBETA}\\)s used construct summary statistics observation’s influence regression model. Cook’s D based theory one conduct \\(F\\)-test observation hypothesis \\(\\beta_{1j} = \\hat{\\beta}_{1k(-)} \\forall j \\J\\). formula measure :\\[D_i = \\frac{\\hat{\\epsilon}^{'2}_i}{k + 1} \\times \\frac{h_i}{1 - h_i}\\]\\(\\hat{\\epsilon}^{'2}_i\\) squared standardized residual, \\(k\\) number parameters model, \\(\\frac{h_i}{1 - h_i}\\) hat value. look values \\(D_i\\) stand rest.","code":""},{"path":"mle-ols.html","id":"visualizing-leverage-discrepancy-and-influence","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.4.5 Visualizing leverage, discrepancy, and influence","text":"example, results basic model number federal laws struck U.S. Supreme Court Congress, based :Age - mean age members Supreme CourtTenure - mean tenure members CourtUnified - dummy variable indicating whether Congress controlled party periodA major concern regression analysis data results driven outliers data.74th Congress (1935-36), New Deal/Court-packing crisis associated abnormally large number laws struck court. determine whether observation driving results.combining three variables “bubble plot,” can visualize three variables simultaneously.observation’s leverage (\\(h_i\\)) plotted \\(x\\) axisEach observation’s discrepancy (.e. Studentized residual) plotted \\(y\\) axisEach symbol drawn proportional observation’s Cook’s \\(D_i\\)bubble plot tells us several things:size/color symbols proportional Cook’s D, turn multiplicative function square Studentized residuals (Y axis) leverage (X axis), observations farther away \\(Y=0\\) /higher values \\(X\\) larger symbols.plot tells us whether large influence observation due high discrepancy, high leverage, \n104th Congress relatively low leverage discrepant\n74th 98th Congresses demonstrate high discrepancy high leverage\n104th Congress relatively low leverage discrepantThe 74th 98th Congresses demonstrate high discrepancy high leverage","code":"## # A tibble: 4 x 5\n##   term        estimate std.error statistic    p.value\n##   <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n## 1 (Intercept) -12.1       2.54       -4.76 0.00000657\n## 2 age           0.219     0.0448      4.88 0.00000401\n## 3 tenure       -0.0669    0.0643     -1.04 0.300     \n## 4 unified       0.718     0.458       1.57 0.121"},{"path":"mle-ols.html","id":"numerical-rules-of-thumb","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.4.6 Numerical rules of thumb","text":"hard fast rules rigorously defended mathematical proofs; simply potential rules thumb follow interpreting statistics.","code":""},{"path":"mle-ols.html","id":"hat-values","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.4.6.1 Hat-values","text":"Anything exceeding twice average \\(\\bar{h} = \\frac{k + 1}{n}\\) noteworthy. example following observations:","code":"## # A tibble: 9 x 10\n##   Congress congress nulls   age tenure unified  year    hat student  cooksd\n##   <chr>       <dbl> <dbl> <dbl>  <dbl>   <dbl> <dbl>  <dbl>   <dbl>   <dbl>\n## 1 1st             1     0  49.8  0.800       1  1789 0.0974   0.330 0.00296\n## 2 3rd             3     0  52.8  4.20        0  1793 0.113    0.511 0.00841\n## 3 12th           12     0  49    6.60        1  1811 0.0802   0.669 0.00980\n## 4 17th           17     0  59   16.6         1  1821 0.0887  -0.253 0.00157\n## 5 20th           20     0  61.7 17.4         1  1827 0.0790  -0.577 0.00719\n## 6 23rd           23     0  64   18.4         1  1833 0.0819  -0.844 0.0159 \n## 7 34th           34     0  64   14.6         0  1855 0.0782  -0.561 0.00671\n## 8 36th           36     0  68.7 17.8         0  1859 0.102   -1.07  0.0326 \n## 9 99th           99     3  71.9 16.7         0  1985 0.0912   0.295 0.00221"},{"path":"mle-ols.html","id":"studentized-residuals","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.4.6.2 Studentized residuals","text":"Anything outside range \\([-2,2]\\) discrepant.","code":"## # A tibble: 7 x 10\n##   Congress congress nulls   age tenure unified  year    hat student cooksd\n##   <chr>       <dbl> <dbl> <dbl>  <dbl>   <dbl> <dbl>  <dbl>   <dbl>  <dbl>\n## 1 67th           67     6  66     9          1  1921 0.0361    2.14 0.0415\n## 2 74th           74    10  71.1  14.2        1  1935 0.0514    4.42 0.223 \n## 3 90th           90     6  64.7  13.3        1  1967 0.0195    2.49 0.0292\n## 4 91st           91     6  65.1  13          1  1969 0.0189    2.42 0.0269\n## 5 92nd           92     5  62     9.20       1  1971 0.0146    2.05 0.0150\n## 6 98th           98     7  69.9  14.7        0  1983 0.0730    3.02 0.165 \n## 7 104th         104     8  60.6  12.5        1  1995 0.0208    4.48 0.0897"},{"path":"mle-ols.html","id":"influence","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.4.6.3 Influence","text":"\\[D_i > \\frac{4}{n - k - 1}\\]\\(n\\) number observations \\(k\\) number coefficients regression model.","code":"## # A tibble: 4 x 10\n##   Congress congress nulls   age tenure unified  year    hat student cooksd\n##   <chr>       <dbl> <dbl> <dbl>  <dbl>   <dbl> <dbl>  <dbl>   <dbl>  <dbl>\n## 1 67th           67     6  66      9         1  1921 0.0361    2.14 0.0415\n## 2 74th           74    10  71.1   14.2       1  1935 0.0514    4.42 0.223 \n## 3 98th           98     7  69.9   14.7       0  1983 0.0730    3.02 0.165 \n## 4 104th         104     8  60.6   12.5       1  1995 0.0208    4.48 0.0897"},{"path":"mle-ols.html","id":"how-to-treat-unusual-observations","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.4.7 How to treat unusual observations","text":"","code":""},{"path":"mle-ols.html","id":"mistakes","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.4.7.1 Mistakes","text":"data just wrong (miscoded, mismeasured, misentered, etc.), either fix error, impute plausible value observation, omit offending observation.","code":""},{"path":"mle-ols.html","id":"weird-observations","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.4.7.2 Weird observations","text":"data particular observation just strange, may want ask “strange?”data strange something unusual/weird/singular happened data point\n“something” important theory tested, may want respecify model\nanswer , can drop offending observation analysis\n“something” important theory tested, may want respecify modelIf answer , can drop offending observation analysisThe data strange apparent reason\nreally good answer . Try digging history observation find going .\nDropping observation judgment call\nalways rerun model omitting observation including results footnote (.e. robustness check)\nreally good answer . Try digging history observation find going .Dropping observation judgment callYou always rerun model omitting observation including results footnote (.e. robustness check)example, let’s re-estimate SCOTUS model omit observations commonly identified outliers:23Not much changed original model\nEstimate age bit smaller, well smaller standard error\nTenure also smaller, fractionally\nUnified bit larger smaller standard error\nEstimate age bit smaller, well smaller standard errorTenure also smaller, fractionallyUnified bit larger smaller standard error\\(R^2\\) larger omitted observation model, RMSE smallerThese three observations mostly influenced precision estimates (.e. standard errors), accuracy ","code":"## [1] 0.232\n## [1] 0.258\n## [1] 1.68\n## [1] 1.29"},{"path":"mle-ols.html","id":"non-normally-distributed-errors","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.5 Non-normally distributed errors","text":"Recall OLS assumes errors distributed normally:\\[\\epsilon_i | X_i \\sim N(0, \\sigma^2)\\]However according central limit theorem, inference based least-squares estimator approximately valid broad conditions.24 validity estimates robust violating assumption, efficiency estimates robust. Recall efficiency guarantees us smallest possible sampling variance therefore smallest possible mean squared error (MSE). Heavy-tailed skewed distributions errors therefore give rise outliers (just recognized problem). Alternatively, interpret least-squares fit conditional mean \\(Y | X\\). arithmetic means good measures center highly skewed distribution.","code":""},{"path":"mle-ols.html","id":"detecting-non-normally-distributed-errors","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.5.1 Detecting non-normally distributed errors","text":"Graphical interpretations easiest detect non-normality errors. Consider regression model using survey data 1994 wave Statistics Canada’s Survey Labour Income Dynamics (SLID), explaining hourly wages outcome sex, education, age:figure quantile-comparison plot, graphing observation studentized residual \\(y\\) axis corresponding quantile \\(t\\)-distribution \\(x\\) axis. dashed lines indicate 95% confidence intervals calculated assumption errors normally distributed. observations fall outside range, indication assumption violated. Clearly, case.density plot studentized residuals, can also see residuals positively skewed.","code":"## # A tibble: 3,997 x 4\n##      age sex    compositeHourlyWages yearsEducation\n##    <dbl> <chr>                 <dbl>          <dbl>\n##  1    40 Male                  10.6              15\n##  2    19 Male                  11                13\n##  3    46 Male                  17.8              14\n##  4    50 Female                14                16\n##  5    31 Male                   8.2              15\n##  6    30 Female                17.0              13\n##  7    61 Female                 6.7              12\n##  8    46 Female                14                14\n##  9    43 Male                  19.2              18\n## 10    17 Male                   7.25             11\n## # … with 3,987 more rows\n## # A tibble: 4 x 5\n##   term           estimate std.error statistic   p.value\n##   <chr>             <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)      -8.12    0.599       -13.6 5.27e- 41\n## 2 sexMale           3.47    0.207        16.8 4.04e- 61\n## 3 yearsEducation    0.930   0.0343       27.1 5.47e-149\n## 4 age               0.261   0.00866      30.2 3.42e-180\n## [1]  967 3911"},{"path":"mle-ols.html","id":"fixing-non-normally-distributed-errors","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.5.2 Fixing non-normally distributed errors","text":"Power log transformations typically used correct problem. , trial error reveals log transforming wage variable, distribution residuals becomes much symmetric:","code":"## # A tibble: 4 x 5\n##   term           estimate std.error statistic   p.value\n##   <chr>             <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)      1.10    0.0380        28.9 1.97e-167\n## 2 sexMale          0.224   0.0131        17.1 2.16e- 63\n## 3 yearsEducation   0.0559  0.00217       25.7 2.95e-135\n## 4 age              0.0182  0.000549      33.1 4.50e-212\n## [1] 2760 3267"},{"path":"mle-ols.html","id":"non-constant-error-variance","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.6 Non-constant error variance","text":"Recall linear regression assumes error terms \\(\\epsilon_i\\) constant variance, \\(\\text{Var}(\\epsilon_i) = \\sigma^2\\). called homoscedasticity. Remember standard errors directly rely upon estimate value:\\[\\widehat{\\se}(\\hat{\\beta}_{1j}) = \\sqrt{\\hat{\\sigma}^{2} (X'X)^{-1}_{jj}}\\]variances error terms non-constant (aka heteroscedastic), estimates parameters \\(\\hat{\\beta}_1\\) still unbiased depend \\(\\sigma^2\\). However estimates standard errors inaccurate - either inflated deflated, leading incorrect inferences statistical significance predictor variables.","code":""},{"path":"mle-ols.html","id":"detecting-heteroscedasticity","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.6.1 Detecting heteroscedasticity","text":"","code":""},{"path":"mle-ols.html","id":"graphically","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.6.1.1 Graphically","text":"can uncover homo- heteroscedasticity use residual plot. data generated process:\\[Y_i = 2 + 3X_i + \\epsilon\\]\\(\\epsilon_i\\) random error distributed normally \\(N(0,1)\\).Compare linear model fit data generating process:\\[Y_i = 2 + 3X_i + \\epsilon_i\\]\\(\\epsilon_i\\) random error distributed normally \\(N(0,\\frac{X}{2})\\). Note variance error term observation \\(\\epsilon_i\\) constant, function \\(X\\).see distinct funnel-shape relationship predicted values residuals. assuming variance constant, substantially underestimate actual response \\(Y_i\\) \\(X_i\\) increases.","code":""},{"path":"mle-ols.html","id":"statistical-tests","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.6.1.2 Statistical tests","text":"formal statistical tests check heteroscedasticity. One test Breusch-Pagan test. procedure :Estimate OLS model obtain squared residuals \\(\\hat{\\epsilon}^2\\)Regress \\(\\hat{\\epsilon}^2\\) :\n\\(k\\) variables think might causing heteroscedasticity\ndefault, include explanatory variables original model\n\\(k\\) variables think might causing heteroscedasticityBy default, include explanatory variables original modelCalculate coefficient determination (\\(R^2_{\\hat{\\epsilon}^2}\\)) residual model multiply number observations \\(n\\)\nresulting statistic follows \\(\\chi^2_{(k-1)}\\) distribution\nRejecting null hypothesis indicates heteroscedasticity present\nresulting statistic follows \\(\\chi^2_{(k-1)}\\) distributionRejecting null hypothesis indicates heteroscedasticity presentThe lmtest library contains function Breusch-Pagan test:","code":"## \n##  studentized Breusch-Pagan test\n## \n## data:  sim_homo_mod\n## BP = 0.08, df = 1, p-value = 0.8\n## \n##  studentized Breusch-Pagan test\n## \n## data:  sim_hetero_mod\n## BP = 208, df = 1, p-value <2e-16"},{"path":"mle-ols.html","id":"accounting-for-heteroscedasticity","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.6.2 Accounting for heteroscedasticity","text":"","code":""},{"path":"mle-ols.html","id":"weighted-least-squares-regression","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.6.2.1 Weighted least squares regression","text":"Instead assuming errors constant variance \\(\\text{Var}(\\epsilon_i) = \\sigma^2\\), instead can assume errors independent normally distributed mean zero different variances \\(\\epsilon_i \\sim N(0, \\sigma_i^2)\\):\\[\n\\begin{bmatrix}\n    \\sigma_1^2       & 0 & 0 & 0 \\\\\n    0       & \\sigma_2^2 & 0 & 0 \\\\\n    0       & 0 & \\ddots & 0 \\\\\n    0       & 0 & 0 & \\sigma_n^2 \\\\\n\\end{bmatrix}\n\\]can define reciprocal variance \\(\\sigma_i^2\\) weight \\(w_i = \\frac{1}{\\sigma_i^2}\\), let matrix \\(\\mathbf{W}\\) diagonal matrix containing weights:\\[\n\\mathbf{W} =\n\\begin{bmatrix}\n    \\frac{1}{\\sigma_1^2}       & 0 & 0 & 0 \\\\\n    0       & \\frac{1}{\\sigma_2^2} & 0 & 0 \\\\\n    0       & 0 & \\ddots & 0 \\\\\n    0       & 0 & 0 & \\frac{1}{\\sigma_n^2} \\\\\n\\end{bmatrix}\n\\]rather following traditional linear regression estimator\\[\\hat{\\mathbf{\\beta_1}} = (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'\\mathbf{y}\\]can substitute weighting matrix \\(\\mathbf{W}\\):\\[\\hat{\\mathbf{\\beta_1}} = (\\mathbf{X}' \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{W} \\mathbf{y}\\]\\[\\sigma_{}^2 = \\frac{\\sum(w_i \\hat{\\epsilon}_i^2)}{n}\\]equivalent minimizing weighted sum squares, according greater weight observations smaller variance.estimate weights \\(W_i\\)?Use residuals preliminary OLS regression obtain estimates error variance within different subsets observations.Model weights function observable variables model.example, using first approach original SLID model:see mild changes estimated parameters, drastic reductions standard errors. problem reduction potentially biased estimated covariance matrix sampling error estimates reflect additional source uncertainty, explicitly accounted just basing original residuals. Instead, better model weights function relevant explanatory variables.25","code":"## # A tibble: 4 x 5\n##   term           estimate std.error statistic   p.value\n##   <chr>             <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)      -8.12    0.599       -13.6 5.27e- 41\n## 2 sexMale           3.47    0.207        16.8 4.04e- 61\n## 3 yearsEducation    0.930   0.0343       27.1 5.47e-149\n## 4 age               0.261   0.00866      30.2 3.42e-180\n## # A tibble: 4 x 5\n##   term           estimate std.error statistic p.value\n##   <chr>             <dbl>     <dbl>     <dbl>   <dbl>\n## 1 (Intercept)      -8.10   0.0160       -506.       0\n## 2 sexMale           3.47   0.00977       356.       0\n## 3 yearsEducation    0.927  0.00142       653.       0\n## 4 age               0.261  0.000170     1534.       0"},{"path":"mle-ols.html","id":"corrections-for-the-variance-covariance-estimates","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.6.2.2 Corrections for the variance-covariance estimates","text":"Alternatively, instead attempt correct heteroscedasticity standard error estimates. produces estimated parameters, adjusts standard errors account violation constant error variance assumption (won’t falsely believe estimates precise really .) One major estimation procedure Huber-White standard errors (also called robust standard errors) can recovered using car::hccm() function:26Notice new standard errors bit larger original model, accounting increased uncertainty parameter estimates due heteroscedasticity.","code":"## # A tibble: 4 x 6\n##   term           estimate std.error statistic   p.value std.error.rob\n##   <chr>             <dbl>     <dbl>     <dbl>     <dbl>         <dbl>\n## 1 (Intercept)      -8.12    0.599       -13.6 5.27e- 41       0.636  \n## 2 sexMale           3.47    0.207        16.8 4.04e- 61       0.207  \n## 3 yearsEducation    0.930   0.0343       27.1 5.47e-149       0.0385 \n## 4 age               0.261   0.00866      30.2 3.42e-180       0.00881"},{"path":"mle-ols.html","id":"non-linearity-in-the-data","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.7 Non-linearity in the data","text":"assuming average error \\(\\E (\\epsilon_i)\\) 0 everywhere implies regression line (surface) accurately reflects relationship \\(X\\) \\(Y\\). Violating assumption means model fails capture systematic relationship response explanatory variables. Therefore , term nonlinearity mean couple different things:relationship \\(X_1\\) \\(Y\\) nonlinear - , constant monotonicThe relationship \\(X_1\\) \\(Y\\) conditional \\(X_2\\) - , relationship interactive rather purely additiveDetecting nonlinearity can tricky higher-dimensional regression models multiple explanatory variables.","code":""},{"path":"mle-ols.html","id":"partial-residual-plots","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.7.1 Partial residual plots","text":"Define partial residual \\(j\\)th explanatory variable:\\[\\hat{\\epsilon}_i^{(j)} = \\hat{\\epsilon}_i + \\hat{\\beta}_j X_{ij}\\]essence, calculate least-squares residual (\\(\\hat{\\epsilon}_i\\)) add linear component partial relationship \\(Y\\) \\(X_j\\). Finally, can plot \\(X_j\\) versus \\(\\hat{\\epsilon}_i^{(j)}\\) assess relationship. instance, consider results logged wage model earlier:solid lines generalized additive models (GAMs), dashed lines linear least-squares fits. age, partial relationship logged wages linear - transformation age necessary correct . education, relationship approximately linear except discrepancy individual low education levels.can correct adding squared polynomial term age, square education term. resulting regression model :\\[\\log(\\text{Wage}) = \\beta_0 + \\beta_1(\\text{Male}) + \\beta_2 \\text{Age} + \\beta_3 \\text{Age}^2 + \\beta_4 \\text{Education}^2\\]model now nonlinear age education, need rethink draw partial residuals plot. easiest approach plot partial residuals age education original explanatory variable. age, \\[\\hat{\\epsilon}_i^{\\text{Age}} = 0.083 \\times \\text{Age}_i -0.0008524 \\times \\text{Age}^2_i + \\hat{\\epsilon}_i\\]education,\\[\\hat{\\epsilon}_i^{\\text{Education}} = 0.002 \\times \\text{Education}^2_i + \\hat{\\epsilon}_i\\]graph, also plot partial fits two explanatory variables:\\[\\hat{Y}_i^{(\\text{Age})} = 0.083 \\times \\text{Age}_i -0.0008524 \\times \\text{Age}^2_i\\]education,\\[\\hat{Y}_i^{(\\text{Education})} = 0.002 \\times \\text{Education}^2_i\\]graphs, solid lines represent partial fits dashed lines represent partial residuals. two lines overlap significantly, revised model good job accounting nonlinearity.","code":"## # A tibble: 5 x 5\n##   term                 estimate std.error statistic   p.value\n##   <chr>                   <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)          0.397    0.0578         6.87 7.62e- 12\n## 2 sexMale              0.221    0.0124        17.8  3.21e- 68\n## 3 I(yearsEducation^2)  0.00181  0.0000786     23.0  1.19e-109\n## 4 age                  0.0830   0.00319       26.0  2.93e-138\n## 5 I(age^2)            -0.000852 0.0000410    -20.8  3.85e- 91"},{"path":"mle-ols.html","id":"collinearity","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.8 Collinearity","text":"Collinearity (multicollinearity) state model explanatory variables correlated one another.","code":""},{"path":"mle-ols.html","id":"perfect-collinearity","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.8.1 Perfect collinearity","text":"Perfect collinearity incredibly rare, typically involves using transformed versions variable model along original variable. example, let’s estimate regression model explaining mpg function displ, wt, cyl:Now let’s say want recode displ centered around ’s mean re-estimate model:Oops. ’s problem? disp disp_mean perfectly correlated :perfectly explain , estimate linear regression model contains variables.27 Fortunately R automatically drops second variable can estimate model. , perfect multicollinearity rarely problematic social science.","code":"## \n## Call:\n## lm(formula = mpg ~ disp + wt + cyl, data = mtcars)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -4.403 -1.403 -0.495  1.339  6.072 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 41.10768    2.84243   14.46  1.6e-14 ***\n## disp         0.00747    0.01184    0.63   0.5332    \n## wt          -3.63568    1.04014   -3.50   0.0016 ** \n## cyl         -1.78494    0.60711   -2.94   0.0065 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.59 on 28 degrees of freedom\n## Multiple R-squared:  0.833,  Adjusted R-squared:  0.815 \n## F-statistic: 46.4 on 3 and 28 DF,  p-value: 5.4e-11## \n## Call:\n## lm(formula = mpg ~ disp + wt + cyl + disp_mean, data = mtcars)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -4.403 -1.403 -0.495  1.339  6.072 \n## \n## Coefficients: (1 not defined because of singularities)\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 41.10768    2.84243   14.46  1.6e-14 ***\n## disp         0.00747    0.01184    0.63   0.5332    \n## wt          -3.63568    1.04014   -3.50   0.0016 ** \n## cyl         -1.78494    0.60711   -2.94   0.0065 ** \n## disp_mean         NA         NA      NA       NA    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.59 on 28 degrees of freedom\n## Multiple R-squared:  0.833,  Adjusted R-squared:  0.815 \n## F-statistic: 46.4 on 3 and 28 DF,  p-value: 5.4e-11"},{"path":"mle-ols.html","id":"less-than-perfect-collinearity","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.8.2 Less-than-perfect collinearity","text":"Instead consider credit dataset:Age limit strongly correlated one another, estimating linear regression model predict individual’s balance function age limit problem:using individual’s credit card rating instead age? likely good predictor balance well:replacing age rating, developed problem model. problem limit rating strongly correlated one another:regression model, difficult parse independent effects limit rating balance, limit rating tend increase decrease association one another. accuracy estimates parameters reduced, standard errors increase. can see standard error limit much larger second model compared first model.","code":"## # A tibble: 3 x 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept) -173.     43.8         -3.96 9.01e-  5\n## 2 age           -2.29    0.672       -3.41 7.23e-  4\n## 3 limit          0.173   0.00503     34.5  1.63e-121## # A tibble: 3 x 5\n##   term         estimate std.error statistic  p.value\n##   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept) -378.       45.3       -8.34  1.21e-15\n## 2 limit          0.0245    0.0638     0.384 7.01e- 1\n## 3 rating         2.20      0.952      2.31  2.13e- 2"},{"path":"mle-ols.html","id":"detecting-collinearity","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.8.2.1 Detecting collinearity","text":"","code":""},{"path":"mle-ols.html","id":"scatterplot-matrix","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.8.2.1.1 Scatterplot matrix","text":"correlation scatterplot matrix help reveal strongly correlated variables:clear limit rating strongly correlated one another.","code":""},{"path":"mle-ols.html","id":"variance-inflation-factor-vif","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.8.2.1.2 Variance inflation factor (VIF)","text":"Unfortunately correlation matrices may sufficient detect collinearity correlation exists three variables (aka multicollinearity) existing two pairs variables. Instead, can calculate variance inflation factor (VIF) ratio variance \\(\\hat{\\beta}_{1j}\\) fitting full model divided variance \\(\\hat{\\beta}_{1j}\\) fit model. can use car::vif() function R calculate statistic coefficient. good rule thumb VIF statistic greater 10 indicates potential multicollinearity model. Applied credit regression models :","code":"##   age limit \n##  1.01  1.01\n##  limit rating \n##    160    160"},{"path":"mle-ols.html","id":"fixing-multicollinearity","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.8.3 Fixing multicollinearity","text":"","code":""},{"path":"mle-ols.html","id":"what-not-to-do","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.8.3.1 What not to do","text":"Drop one collinear variables modelThis good idea, even makes results “significant.” omitting variable, completely re-specifying model direct contradiction theory. theory suggests variable can dropped, go ahead. , don’t .","code":""},{"path":"mle-ols.html","id":"what-you-could-do-instead","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.8.3.2 What you could do instead","text":"","code":""},{"path":"mle-ols.html","id":"add-data","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.8.3.2.1 Add data","text":"observations, better. least decrease standard errors give precise estimates. add “odd” unusual observations, also reduce degree multicollinearity.","code":""},{"path":"mle-ols.html","id":"transform-the-covariates","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.8.3.2.2 Transform the covariates","text":"variables indicators underlying concept, can combine index variable. additive index sum comparable covariates binary indicators. Alternatively, create index via principal components analysis.","code":""},{"path":"mle-ols.html","id":"shrinkage-methods","chapter":"Day 13 Maximum likelihood estimation and linear regression","heading":"13.8.3.2.3 Shrinkage methods","text":"Shrinkage methods involve fitting model involving \\(p\\) predictors shrinking estimated coefficients towards zero. shrinkage reduces variance model. multicollinearity high, variance estimator \\(\\hat{\\beta}_1\\) also high. shrinking estimated coefficient towards zero, may increase bias exchange smaller variance estimates.","code":""},{"path":"bayesian-inference.html","id":"bayesian-inference","chapter":"Day 14 Bayesian inference","heading":"Day 14 Bayesian inference","text":"","code":""},{"path":"bayesian-inference.html","id":"learning-objectives-13","chapter":"Day 14 Bayesian inference","heading":"Learning objectives","text":"Define Bayesian philosophy distinguish frequentist inferenceDefine core concepts Bayesian methodsDiscuss importance simulation estimate density functionsAssess methods defining priorsIdentify strengths weaknesses Bayesian inference","code":""},{"path":"bayesian-inference.html","id":"supplemental-readings-13","chapter":"Day 14 Bayesian inference","heading":"Supplemental readings","text":"Chapter 8.1 Bertsekas Tsitsiklis (2008)Wasserman (2013)\nCh 11 - Bayesian Inference\nCh 11 - Bayesian Inference","code":""},{"path":"bayesian-inference.html","id":"bayesian-philosophy","chapter":"Day 14 Bayesian inference","heading":"14.1 Bayesian philosophy","text":"Frequentist methods major methods ’ve employed thus far. frequentist point view based following postulates:Probability refers limiting relative frequencies. Probabilities objective properties real world.Parameters fixed, unknown constants. fluctuating, useful probability statements can made parameters.Statistical procedures designed well-defined long run frequency properties. example, 95% confidence interval trap true value parameter limiting frequency least 95 percent.alternative approach inference called Bayesian inference. Bayesian approach based following postulates:Probability describes degree belief, limiting frequency. , can make probability statements lots things, just data subject random variables. example, might say “probability Donald Trump offended someone November 25, 2018” \\(0.99\\). refer limiting frequency. reflects strength belief proposition true.can make probability statements parameters, even though fixed constants.make inferences parameter \\(\\theta\\) producing probability distribution \\(\\theta\\). Inferences, point estimates interval estimates, may extracted distribution.","code":""},{"path":"bayesian-inference.html","id":"bayes-theorem","chapter":"Day 14 Bayesian inference","heading":"14.2 Bayes’ theorem","text":"Bayes’ theorem fundamental component probability statistics central understanding differences frequentist Bayesian inference. two events \\(\\) \\(B\\), Bayes’ theorem states :\\[\\Pr(B|) = \\frac{\\Pr(|B) \\times \\Pr(B)}{\\Pr()}\\]Bayes’ rule tells us invert conditional probabilities. , find \\(\\Pr(B|)\\) \\(\\Pr(|B)\\).Example 14.1  (Coin tossing) Toss coin 5 times. Let \\(H_1 =\\) “first toss heads” let \\(H_A =\\) “5 tosses heads.” Therefore \\(\\Pr(H_1 | H_A) = 1\\) (five tosses heads, first one must definition also heads) \\(\\Pr(H_A | H_1) = \\frac{1}{16}\\) (\\(\\frac{1}{2^4} = \\frac{1}{16}\\)).However can also use Bayes’ theorem calculate \\(\\Pr(H_1 | H_A)\\) using \\(\\Pr(H_A | H_1)\\). terms need :\\(\\Pr(H_A | H_1) = \\frac{1}{16}\\)\\(\\Pr(H_1) = \\frac{1}{2}\\)\\(\\Pr(H_A) = \\frac{1}{32}\\),\\[\\Pr(H_A | H_1) = \\frac{\\Pr(H_A | H_1) \\times \\Pr(H_1)}{\\Pr(H_A)} = \\frac{\\frac{1}{16} \\times \\frac{1}{2}}{\\frac{1}{32}} = 1\\]Example 14.2  (False positive fallacy) test certain rare disease assumed correct 95% time:person disease, test results positive probability \\(0.95\\)person disease, test results negative probability \\(0.95\\)random person drawn certain population probability \\(0.001\\) disease. Given person just tested positive, probability disease?\\(= {\\text{person disease}}\\)\\(B = {\\text{test result positive disease}}\\)\\(\\Pr() = 0.001\\)\\(\\Pr(B | ) = 0.95\\)\\(\\Pr(B | = 0) = 0.05\\)\\[\n\\begin{align}\n\\Pr(\\text{person disease} | \\text{test positive}) &= \\Pr(|B) \\\\\n& = \\frac{\\Pr() \\times \\Pr(B|)}{\\Pr(B)} \\\\\n& = \\frac{\\Pr() \\times \\Pr(B|)}{\\Pr() \\times \\Pr(B|) + \\Pr(= 0) \\times(B | = 0)} \\\\\n& = \\frac{0.001 \\times 0.95}{0.001 \\times 0.95 + 0.999 \\times 0.05} \\\\\n& = 0.0187\n\\end{align}\n\\]Even though test fairly accurate, person tested positive still unlikely (less 2%) disease. base rate disease population low, vast majority people taking test healthy even accurate test positives healthy people.28","code":""},{"path":"bayesian-inference.html","id":"bayesian-method","chapter":"Day 14 Bayesian inference","heading":"14.3 Bayesian method","text":"Bayesian inference usually carried following way:Choose probability density \\(f(\\theta)\\) – called prior distribution – expresses beliefs parameter \\(\\theta\\) see data.Choose statistical model \\(f(x|\\theta)\\) reflects beliefs \\(x\\) given \\(\\theta\\). Note now write \\(f(x|\\theta)\\), \\(f(x; \\theta)\\).observing data \\(X_1, \\ldots, X_n\\), update beliefs calculate posterior distribution \\(f(\\theta | X_1, \\ldots, X_n)\\).calculate posterior, suppose \\(\\theta\\) discrete single, discrete observation \\(X\\). use capital letter denote parameter since now treat like random variable, let \\(\\Theta\\) denote parameter. discrete setting,\\[\n\\begin{align}\n\\Pr(\\Theta = \\theta | X = x) &= \\frac{\\Pr(X = x, \\Theta = \\theta)}{\\Pr(X = x)} \\\\\n&= \\frac{\\Pr(X = x | \\Theta = \\theta) \\Pr(\\Theta = \\theta)}{\\sum_\\theta \\Pr (X = x| \\Theta = \\theta) \\Pr (\\Theta = \\theta)}\n\\end{align}\n\\]basic application Bayes’ theorem. version continuous variables obtained using density functions\\[f(\\theta | x) = \\frac{f(x | \\theta) f(\\theta)}{\\int f(x | \\theta) f(\\theta) d\\theta}\\]\\(n\\) IID observations \\(X_1, \\ldots, X_n\\), replace \\(f(x | \\theta)\\) \\[f(x_1, \\ldots, x_n | \\theta) = \\prod_{= 1}^n f(x_i | \\theta) = \\Lagr_n(\\theta)\\]now write \\(X^n\\) mean \\((X_1, \\ldots, X_n)\\) \\(x^n\\) mean \\((x_1, \\ldots, x_n)\\). Now,\\[\n\\begin{align}\nf(\\theta | x^n) &= \\frac{f(x^n | \\theta) f(\\theta)}{\\int f(x^n | \\theta) f(\\theta) d\\theta} \\\\\n&= \\frac{\\Lagr_n(\\theta) f(\\theta)}{c_n} \\\\\n&\\propto \\Lagr_n(\\theta) f(\\theta)\n\\end{align}\n\\]\\[c_n = \\int f(x^n | \\theta) f(\\theta) d\\theta\\]called normalizing constant. can summarize stating posterior proportional Likelihood times Prior:\\[f(\\theta | x^n) \\propto \\Lagr_n(\\theta) f(\\theta)\\]Since \\(c_n\\) depend \\(\\theta\\), can safely ignore point, fact can recover constant later need .posterior distribution, can get point estimate summarizing center posterior. Typically mean mode posterior. posterior mean \\[\\bar{\\theta}_n = \\int \\theta f(\\theta | x^n) d\\theta = \\frac{\\int \\theta \\Lagr_n(\\theta) f(\\theta)}{\\int \\Lagr_n(\\theta) f(\\theta) d\\theta}\\]can also obtain Bayesian interval estimate. find \\(\\) \\(b\\) \\[\\int_{-\\infty}^f(\\theta | x^n) d\\theta = \\int_b^\\infty f(\\theta | x^n) d\\theta = \\frac{\\alpha}{2}\\]Let \\(C = (,b)\\). \\[\\Pr (\\theta \\C | x^n) = \\int_a^b f(\\theta | x^n) d\\theta = 1 - \\alpha\\]\\(C\\) \\(1 - \\alpha\\) posterior (credible) interval.","code":""},{"path":"bayesian-inference.html","id":"example-coin-tossing","chapter":"Day 14 Bayesian inference","heading":"14.3.1 Example: coin tossing","text":"three types coins different probabilities landing heads tossed.Type \\(\\) coins fair, \\(p = 0.5\\) headsType \\(B\\) coins bent, \\(p = 0.6\\) headsType \\(C\\) coins bent, \\(p = 0.9\\) headsSuppose drawer containing 5 coins: 2 type \\(\\), 2 type \\(B\\), 1 type \\(C\\). reach drawer pick coin random. Without showing coin flip get heads. probability type \\(\\)? Type \\(B\\)? Type \\(C\\)?","code":""},{"path":"bayesian-inference.html","id":"terminology","chapter":"Day 14 Bayesian inference","heading":"14.3.1.1 Terminology","text":"Let \\(\\), \\(B\\), \\(C\\) event chosen coin respective type. Let \\(D\\) event toss heads. problem asks us find:\\[\\Pr(|D), \\Pr(B|D), \\Pr(C|D)\\]applying Bayes’ theorem, need define things:Experiment - pick coin drawer random, flip , record resultExperiment - pick coin drawer random, flip , record resultData - result experiment. , \\(D = \\text{heads}\\). \\(D\\) data provides evidence hypothesisData - result experiment. , \\(D = \\text{heads}\\). \\(D\\) data provides evidence hypothesisHypotheses - testing three hypotheses: coin type \\(\\), \\(B\\), \\(C\\)Hypotheses - testing three hypotheses: coin type \\(\\), \\(B\\), \\(C\\)Prior probability - probability hypothesis prior tossing coin (collecting data). Since drawer 2 coins type \\(\\), 2 type \\(B\\), 1 type \\(C\\), :\n\\[\\Pr() = 0.4, \\Pr(B) = 0.4, \\Pr(C) = 0.2\\]Prior probability - probability hypothesis prior tossing coin (collecting data). Since drawer 2 coins type \\(\\), 2 type \\(B\\), 1 type \\(C\\), :\\[\\Pr() = 0.4, \\Pr(B) = 0.4, \\Pr(C) = 0.2\\]Likelihood - likelihood function \\(\\Pr(D|H)\\), probability data assuming hypothesis true. often consider data fixed let hypothesis vary. example, \\(\\Pr(D|) =\\) probability heads coin type \\(\\). case, likelihoods :\n\\[\\Pr(D|) = 0.5, \\Pr(D|B) = 0.6, \\Pr(D|C) = 0.9\\]\ncan think parameters series Bernoulli distributions.Likelihood - likelihood function \\(\\Pr(D|H)\\), probability data assuming hypothesis true. often consider data fixed let hypothesis vary. example, \\(\\Pr(D|) =\\) probability heads coin type \\(\\). case, likelihoods :\\[\\Pr(D|) = 0.5, \\Pr(D|B) = 0.6, \\Pr(D|C) = 0.9\\]can think parameters series Bernoulli distributions.Posterior probability - probability (posterior ) hypothesis given data tossing coin:\n\\[\\Pr(|D), \\Pr(B|D), \\Pr(C|D)\\]\nposterior probabilities want find.Posterior probability - probability (posterior ) hypothesis given data tossing coin:\\[\\Pr(|D), \\Pr(B|D), \\Pr(C|D)\\]posterior probabilities want find.can now use Bayes’ theorem compute posterior probabilities. theorem says:\\[\\Pr(|D) = \\frac{\\Pr(D|) \\times \\Pr()}{\\Pr(D)}\\]\n\\[\\Pr(B|D) = \\frac{\\Pr(D|B) \\times \\Pr(B)}{\\Pr(D)}\\]\n\\[\\Pr(C|D) = \\frac{\\Pr(D|C) \\times \\Pr(C)}{\\Pr(D)}\\]\\(\\Pr(D)\\) can computed using law total probability:\\[\n\\begin{align}\n\\Pr(D) & = \\Pr(D|) \\times \\Pr() + \\Pr(D|B) \\times \\Pr(B) + \\Pr(D|C) \\times \\Pr(C) \\\\\n& = 0.5 \\times 0.4 + 0.6 \\times 0.4 + 0.9 \\times 0.2 = 0.62\n\\end{align}\n\\]posterior probabilities :\\[\\Pr(|D) = \\frac{\\Pr(D|) \\times \\Pr()}{\\Pr(D)} = \\frac{0.5 \\times 0.4}{0.62} = \\frac{0.2}{0.62}\\]\\[\\Pr(B|D) = \\frac{\\Pr(D|B) \\times \\Pr(B)}{\\Pr(D)} = \\frac{0.6 \\times 0.4}{0.62} = \\frac{0.24}{0.62}\\]\\[\\Pr(C|D) = \\frac{\\Pr(D|C) \\times \\Pr(C)}{\\Pr(D)} = \\frac{0.9 \\times 0.2}{0.62} = \\frac{0.18}{0.62}\\]Notice total probability \\(\\Pr(D)\\) denominators sum three numerators.Bayes numerator product prior likelihood. posterior probability obtained dividing Bayes numerator \\(\\Pr(D) = 0.625\\).process going prior probability \\(\\Pr(H)\\) posterior \\(\\Pr(H|D)\\) called Bayesian updating. Bayesian updating uses data alter understanding probability hypothesis.","code":""},{"path":"bayesian-inference.html","id":"things-to-notice","chapter":"Day 14 Bayesian inference","heading":"14.3.1.2 Things to notice","text":"posterior probabilities hypothesis last column. Coin \\(B\\) probable, even decrease prior posterior. \\(C\\) increased 0.2 0.29.Bayes numerator determines posterior probability. compute posterior probability, simply rescale Bayes numerator sums 1.care finding likely hypothesis, Bayes numerator works well normalized posterior.posterior probability represents outcome tug--war likelihood prior. calculating posterior, large prior may deflated small likelihood, small prior may inflated large likelihood.Therefore can express Bayes’ theorem :\\[\\Pr(\\text{hypothesis}| \\text{data}) = \\frac{\\Pr(\\text{data} | \\text{hypothesis}) \\times \\Pr(\\text{hypothesis})}{\\Pr(\\text{data})}\\]\\[\\Pr(H|D) = \\frac{\\Pr(D | H) \\times \\Pr(H)}{\\Pr(D)}\\]data fixed, denominator \\(\\Pr(D)\\) just serves normalize total posterior probability 1. express Bayes’ theorem statement proportionality two functions \\(H\\):\\[\\Pr(\\text{hypothesis}| \\text{data}) \\propto \\Pr(\\text{data} | \\text{hypothesis}) \\times \\Pr(\\text{hypothesis})\\]\n\\[\\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior}\\]Example 14.3  (Bernoulli random variable) Let \\(X_1, \\ldots, X_n \\sim \\text{Bernoulli} (p)\\). Suppose take uniform distribution \\(f(p) = 1\\) prior. Bayes’ theorem, posterior form\\[\n\\begin{align}\nf(p | x^n) &\\propto f(p) \\Lagr_n(p) \\\\\n&= p^s (1 - p)^{n - s} \\\\\n&= p^{s + 1 - 1} (1 - p)^{n - s + 1 - 1}\n\\end{align}\n\\]\\(s = \\sum_{=1}^n x_i\\) number successes. Importantly, random variable Beta distribution parameters \\(\\alpha\\) \\(\\beta\\) density \\[f(p; \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}p^{\\alpha - 1} (1 - p)^{\\beta - 1}\\]can see posterior \\(p\\) Beta distribution parameters \\(s + 1\\) \\(n - s + 1\\). ,\\[f(p | x^n) = \\frac{\\Gamma(n + 2)}{\\Gamma(s + 1) \\Gamma(n - s + 1)}p^{(s + 1) - 1} (1 - p)^{(n - s + 1) - 1}\\]write \\[p | x^n \\sim \\text{Beta} (s + 1, n - s + 1)\\]Notice figured normalizing constant \\(c_n = \\frac{\\Gamma(n + 2)}{\\Gamma(s + 1) \\Gamma(n - s + 1)}\\) without actually integral \\(\\int \\Lagr_n(p) f(p) dp\\). mean \\(\\text{Beta}(\\alpha, \\beta)\\) distribution \\(\\frac{\\alpha}{\\alpha + \\beta}\\), Bayes estimator \\[\\bar{p} = \\frac{s + 1}{n + 2}\\]can rewrite estimator \\[\\bar{p} = \\lambda_n \\hat{p} + (1 - \\lambda_n) \\tilde{p}\\]\\(\\hat{p} = \\frac{s}{n}\\) MLE, \\(\\tilde{p} = \\frac{1}{2}\\) prior mean, \\(\\lambda_n = \\frac{n}{n + 2} \\approx 1\\). can think MLE estimate \\(p\\) flat prior. non-flat prior, estimate \\(\\bar{p}\\) weighted average prior MLE.95% credible interval can obtained numerically finding \\(\\) \\(b\\) \\(\\int_a^b f(p | x^n) dp = 0.95\\).Suppose instead uniform prior, use prior \\(p \\sim \\text{Beta} (\\alpha, \\beta)\\). repeat calculations , see \\(p | x^n \\sim \\text{Beta} (\\alpha + s, \\beta + n - s)\\). flat prior special case \\(\\alpha = \\beta = 1\\). posterior mean \\[\\bar{p} = \\frac{\\alpha + s}{\\alpha + \\beta + n} = \\left( \\frac{n}{\\alpha + \\beta + n} \\right) \\hat{p} + \\left( \\frac{\\alpha + \\beta}{\\alpha + \\beta + n} \\right) p_0\\]\\(p_0 = \\frac{\\alpha}{\\alpha + \\beta}\\) prior mean.","code":""},{"path":"bayesian-inference.html","id":"updating-your-prior-beliefs","chapter":"Day 14 Bayesian inference","heading":"14.4 Updating your prior beliefs","text":"life continually update beliefs new experience world. Bayesian inference, today’s posterior tomorrow’s prior.Example 14.4  (September 11, 2001) Consider September 11th attacks New York City.29 Say first plane hit, estimate probability terror attack tall buildings Manhattan just 1 20,000, \\(0.00005\\). also assign low probability plane hitting World Trade Center accident: 1 12,500 given day.30 Consider use Bayes’ theorem instance. probability terrorists crashing planes Manhattan skyscrapers given first plane hitting World Trade Center?initial estimate likely terrorists crash planes Manhattan skyscrapers \\(\\Pr(\\text{Terror attack}) = 0.00005\\)Probability plane hitting terrorists attacking Manhattan \\(\\Pr(\\text{Plane hits WTC} | \\text{Terror attack}) = 1\\)Probability plane hitting terrorists attacking Manhattan skyscrapers (.e. accident) \\(\\Pr(\\text{Plane hits WTC} | \\text{terror attack}) = 0.00008\\)posterior probability terror attack, given first plane hitting world trade center, :\\(=\\) terror attack\\(B =\\) plane hitting World Trade Center\\(\\Pr() = 0.00005 =\\) probability terrorists crash plane World Trade Center\\(\\Pr(^C) = 0.99995 =\\) probability terrorists crash plane World Trade Center\\(\\Pr(B|) = 1 =\\) probability plane crashing World Trade Center terrorists attacking World Trade Center\\(\\Pr(B|^C) = 0.00008 =\\) probability plane hitting terrorists attacking World Trade Center (.e. accident)\\[\n\\begin{align}\n\\Pr(|B) &= \\frac{\\Pr() \\times \\Pr(B|)}{\\Pr(B)} \\\\\n &= \\frac{\\Pr() \\times \\Pr(B|)}{ \\Pr() \\times \\Pr(B|) + \\Pr(^C) \\times \\Pr(B| ^C)} \\\\\n& = \\frac{0.00005 \\times 1}{0.00005 \\times 1 + 0.99995 \\times 0.00008} \\\\\n& = 0.385\n\\end{align}\n\\]now estimate posterior probability 38% chance terrorist attack World Trade Center. can continuously update posterior probability new data presents .\\[\n\\begin{align}\n\\Pr(|B) &= \\frac{\\Pr() \\times \\Pr(B|)}{\\Pr(B)} \\\\\n &= \\frac{\\Pr() \\times \\Pr(B|)}{ \\Pr() \\times \\Pr(B|) + \\Pr(^C) \\times \\Pr(B| ^C)} \\\\\n& = \\frac{0.385 \\times 1}{0.385 \\times 1 + 0.615 \\times 0.00008} \\\\\n& \\approx .9998\n\\end{align}\n\\]","code":""},{"path":"bayesian-inference.html","id":"simulation","chapter":"Day 14 Bayesian inference","heading":"14.5 Simulation","text":"posterior can often approximated simulation. Suppose draw \\(\\theta_1, \\ldots, \\theta_B \\sim p(\\theta | x^n)\\). histogram \\(\\theta_1, \\ldots, \\theta_B\\) approximates posterior density \\(p(\\theta | x^n)\\). approximation posterior mean \\(\\bar{\\theta}_n = \\E (\\theta | x^n)\\) \\(\\frac{\\sum_{j=1}^B \\theta_j}{B}\\). posterior \\(1 - \\alpha\\) interval can approximated \\((\\theta_{\\alpha / 2}, \\theta_{1 - \\alpha /2})\\) \\(\\theta_{\\alpha / 2}\\) \\(\\alpha / 2\\) sample quantile \\(\\theta_1, \\ldots, \\theta_B\\).sample \\(\\theta_1, \\ldots, \\theta_B\\) \\(f(\\theta | x^n)\\), let \\(\\tau_i = g(\\theta_i)\\). \\(\\tau_1, \\ldots, \\tau_B\\) sample \\(f(\\tau | x^n)\\). avoids need analytic calculations, especially \\(f(\\theta | x^n)\\) especially complex function.Example 14.5  (Bernoulli random variable) Let \\(X_1, \\ldots, X_n \\sim \\text{Bernoulli} (p)\\) \\(f(p) = 1\\) \\(p | X^n \\sim \\text{Beta} (s + 1, n - s + 1)\\) \\(s = \\sum_{=1}^n x_i\\). Let \\(\\psi = \\log \\left( \\frac{p}{1 - p} \\right)\\) (.e. log-odds). wanted calculate PMF CDF \\(\\psi | x^n\\), lot calculus analytic math solve equations.31 Alternatively, can approximate posterior \\(\\psi\\) without calculus.Draw \\(P_1, \\ldots, P_B \\sim \\text{Beta} (s + 1, n - s + 1)\\).Let \\(\\psi_i = \\log \\left( \\frac{P_i}{1 - P_i} \\right)\\), \\(= 1, \\ldots, B\\)Now \\(\\psi_1, \\ldots, \\psi_B\\) IID draws \\(h(\\psi | x^n)\\). histogram values provides estimate \\(h(\\psi | x^n)\\).","code":""},{"path":"bayesian-inference.html","id":"priors","chapter":"Day 14 Bayesian inference","heading":"14.6 Priors","text":"employ Bayesian inference, one requires prior. get prior \\(f(\\theta)\\)? One approach use subjective prior based subjective opinion \\(\\theta\\) collect data. may possible, impractical many complicated problems (especially many parameters). argue approach also “scientific” inferences objective possible.alternative approach define sort noninformative prior. One obvious choice use flat prior \\(f(\\theta) \\propto\\) constant. example earlier, taking \\(f(p) = 1\\) leads \\(p | X^n \\sim \\text{Beta} (s + 1, n - s + 1)\\) seems reasonable. unfettered use flat priors raises questions.","code":""},{"path":"bayesian-inference.html","id":"improper-priors","chapter":"Day 14 Bayesian inference","heading":"14.6.1 Improper priors","text":"Let \\(X \\sim N(\\theta, \\sigma^2)\\) \\(\\sigma\\) known. Suppose adopt flat prior \\(f(\\theta) \\propto c\\) \\(c > 0\\) constant. Note \\(\\int f(\\theta) d\\theta = \\infty\\), probability density usual sense (otherwise integrate 1). prior called improper prior. However, can still carry Bayes’ theorem compute posterior density multiplying prior likelihood:\\[f(\\theta) \\propto \\Lagr_n(\\theta) f(\\theta) = \\Lagr_n(\\theta)\\]gives \\(\\theta | X^n \\sim N(\\bar{X}, \\sigma^2 / n)\\) resulting point interval estimators agree exactly frequentist counterparts. general, improper priors problem long resulting posterior well-defined probability distribution.","code":""},{"path":"bayesian-inference.html","id":"flat-priors-are-not-invariant","chapter":"Day 14 Bayesian inference","heading":"14.6.2 Flat priors are not invariant","text":"Let \\(X \\sim \\text{Bernoulli} (p)\\) suppose use flat prior \\(f(p) = 1\\). flat prior represents lack knowledge \\(p\\) experiment. Now let \\(\\psi = \\log(p / (1 - p))\\). transformation \\(p\\) can compute resulting distribution \\(\\psi\\)\\[f_\\Psi (\\psi) = \\frac{e^\\psi}{(1 + e^\\psi)^2}\\]flat. ignorant \\(p\\), also ignorant \\(\\psi\\) use flat prior \\(\\psi\\). contradiction. short, notion flat prior well defined flat prior parameter imply flat prior transformed version parameter. Flat priors transformation invariant.","code":""},{"path":"bayesian-inference.html","id":"multiparameter-problems","chapter":"Day 14 Bayesian inference","heading":"14.7 Multiparameter problems","text":"Suppose \\(\\theta = (\\theta_1, \\ldots, \\theta_p)\\). posterior density given \\[f(\\theta | x^n) \\propto \\Lagr_n(\\theta) f(\\theta)\\]However, now need consider extract inferences one parameter. key find marginal posterior density parameter interest. Suppose want make inferences \\(\\theta_1\\). marginal posterior \\(\\theta_1\\) \\[f(\\theta_1 | x^n) = \\int \\cdots \\int f(\\theta_1, \\ldots, \\theta_p | x^n) d\\theta_2 \\cdots d\\theta_p\\]Essentially, calculate integral function parameters except \\(\\theta_1\\). two parameters \\((\\theta_1, \\theta_2)\\), integrate respect \\(\\theta_2\\). number parameters increase, operation gets extremely tricky (impossible) solve analytically. Instead, simulation can used approximate drawing randomly posterior\\[\\theta^1, \\ldots, \\theta^B \\sim f(\\theta | x^n)\\]superscripts index different draws. \\(\\theta^j\\) vector \\(\\theta^j = (\\theta_1^j, \\ldots, \\theta_p^j)\\). Now collect together first component draw\\[\\theta_1^1, \\ldots, \\theta_1^B\\]sample \\(f(\\theta_1 | x^n)\\) avoided integrals.Example 14.6  (Comparing two binomials) Suppose \\(n_1\\) control patients \\(n_2\\) treatment patients \\(X_1\\) control patients survive \\(X_2\\) treatment patients survive. want estimate \\(\\tau = g(p_1, p_2) = p_2 - p_1\\). ,\\[X_1 \\sim \\text{Binomial} (n_1, p_1) \\, \\text{} \\, X_2 \\sim \\text{Binomial} (n_2, p_2)\\]\\(f(p_1, p_2) = 1\\), posterior \\[f(p_1, p_2 | x_1, x_2) \\propto p_1^{x_1} (1 - p_1)^{n_1 - x_1} p_2^{x_2} (1 - p_2)^{n_2 - x_2}\\]Notice \\[f(p_1, p_2 | x_1, x_2) = f(p_1 | x_1) f(p_2 | x_2)\\]\\[f(p_1 | x_1) \\propto p_1^{x_1} (1 - p_1)^{n_1 - x_1} \\, \\text{} \\, f(p_2 | x_2) \\propto p_2^{x_2} (1 - p_2)^{n_2 - x_2}\\]implies \\(p_1\\) \\(p_2\\) independent posterior. Also\\[\n\\begin{align}\np_1 | x_1 &\\sim \\text{Beta} (x_1 + 1, n_1 - x_1 + 1) \\\\\np_2 | x_2 &\\sim \\text{Beta} (x_2 + 1, n_2 - x_2 + 1)\n\\end{align}\n\\]simulate\\[\n\\begin{align}\nP_{1,1}, \\ldots, P_{1,B} &\\sim \\text{Beta} (x_1 + 1, n_1 - x_1 + 1) \\\\\nP_{2,1}, \\ldots, P_{2,B} &\\sim \\text{Beta} (x_2 + 1, n_2 - x_2 + 1)\n\\end{align}\n\\]\\(\\tau_b = P_{2,b} - P_{1,b}, \\, b = 1, \\ldots, B\\) sample \\(f(\\tau | x_1, x_2)\\).","code":""},{"path":"bayesian-inference.html","id":"critiques-and-defenses-of-bayesian-inference","chapter":"Day 14 Bayesian inference","heading":"14.8 Critiques and defenses of Bayesian inference","text":"","code":""},{"path":"bayesian-inference.html","id":"critique-of-bayesian-inference","chapter":"Day 14 Bayesian inference","heading":"14.8.1 Critique of Bayesian inference","text":"subjective prior subjective. single method choosing prior, different (well-intentioned) people produce different priors therefore arrive different posteriors conclusions. accept premise subjective priors, still need good information create well-defined prior distribution.Philosophically, object assigning probabilities hypotheses hypotheses constitute outcomes repeatable experiments one can measure long-term frequency. Rather, hypothesis either true false, regardless whether one knows case.\ncoin either fair unfair\nTreatment 1 either better worse treatment 2\nsun come tomorrow\neither win win lottery\ncoin either fair unfairTreatment 1 either better worse treatment 2The sun come tomorrowI either win win lotteryFor many parametric models large samples, Bayesian frequentist methods give approximately inferences. Since frequentist methods historically common easier estimate, reason go steps Bayesian inference.Bayesian inference depends entirely likelihood function. high dimensional nonparametric methods, likelihood function may yield accurate inferences.","code":""},{"path":"bayesian-inference.html","id":"defense-of-bayesian-inference","chapter":"Day 14 Bayesian inference","heading":"14.8.2 Defense of Bayesian inference","text":"probability hypotheses exactly need make decisions. doctor tells screening test came back positive disease, really want know probability hypothesis “’m sick.”Bayes’ theorem logically rigorous (obtain prior).testing different priors can see sensitive results choice prior.easy communicate result framed terms probabilities hypotheses (try explaining result null hypothesis test layperson).Priors can defended based assumptions made arrive .Evidence derived data independent notions “data extreme” depend exact experimental setup.Data can used comes . don’t wait every contingency planned ahead time.","code":""},{"path":"bayesian-inference.html","id":"acknowledgements-1","chapter":"Day 14 Bayesian inference","heading":"Acknowledgements","text":"Material drawn Statistics Larry Wasserman","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
