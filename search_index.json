[["index.html", "Computational Math Camp Lecture Notes Overview Meeting information Instructional staff Course description Who should take this course Grades Disability services Core texts Course format Course schedule", " Computational Math Camp Lecture Notes 2022-08-29 Overview Meeting information Meeting day: September 6-September 16, MTWRF Time: 9am-1pm CT (week 1); 9am-5pm CT (week 2) Location: 1155 E 60th St (room 142/138) Instructional staff Dr. Yanyan Sheng Email: y.sheng@uchicago.edu Office: Room 223, 1155 E. 60th St. Office hours: [By appointment] Teaching assistants Kaya Borlase John Wang Wenqian Zhang Course description This course surveys mathematical and statistical tools that are foundational to computational social science. Topics to be reviewed include mathematical notation and linear equations, calculus, linear algebra, probability theory, and statistical inference. Students are assumed to have encountered most of these topics previously, so that the camp serves as a refresher rather than teaching entirely new topics. Class sessions will emphasize problem solving and in-class exercises applying these techniques. Students who successfully complete the camp are situated to pass the MACSS math and statistics placement exam and enroll in computationally-enhanced or statistically theoretical course offerings at the University of Chicago without prior introductory coursework. Who should take this course Students in the MACCS program and the MAPSS QMSA concentration MA and PhD students in the social sciences who have significant prior training and experience in mathematics and statistics and seek to complete the Certificate in Computational Social Science Students looking for a slower-paced camp focused specifically on algebra, calculus, and probability should enroll in SOSC 30100 - Mathematics for Social Sciences. This three-week course makes no assumption of prior math/stats training. Grades This course may only be taken for pass/fail (non-credit). Assignments are comprised of daily problem sets. You are encouraged to work in groups, and the instructional staff is available for consultation during class hours. We expect most students should be able to finish the problem sets during class hours. Students must satisfactorily complete a minimum of ten problem sets to pass the camp. Disability services The University of Chicago is committed to diversity and rigorous inquiry from multiple perspectives. The MAPSS, CIR, and Computation programs share this commitment and seek to foster productive learning environments based upon inclusion, open communication, and mutual respect for a diverse range of identities, experiences, and positions. This course is open to all students who meet the academic requirements for participation. Any student who has a documented need for accommodation should contact Student Disability Services (773-702-6000 or disabilities@uchicago.edu) and provide me (Dr. Soltoff) with a copy of your Accommodation Determination Letter as soon as possible. Core texts There are no required textbooks for the camp. Readings will be drawn from course notes. For topics where you have less confidence, supplementary readings are drawn from: Bertsekas, D. P., &amp; Tsitsiklis, J. N. (2008). Introduction to probability, 2nd edition. Belmont, MA: Athena Scientific. This book is only available in hardcopy The authors have provided digital copies of lecture notes published prior to publishing the full textbook. They will be made available in Canvas. Note the lecture notes do not include any of the chapters on frequentist or Bayesian statistical inference, nor do they include the full set of practice exercises from the textbook. Pemberton, M., &amp; Rau, N. (2015). Mathematics for economists: an introductory textbook, 4th edition. Oxford University Press. OpenStax Calculus, Volume 1 Calculus, Volume 2 Calculus, Volume 3 College Algebra Wasserman, Larry. 2013. All of Statistics: A Concise Course in Statistical Inference. Springer Science &amp; Business Media. Course format Each day’s problem set(s) will be released at 5pm Chicago time the night before the date on the course schedule below. I strongly encourage you to complete the required readings that evening. Each day you are invited to come to campus between 9am-1pm in week 1 and between 9am-5pm in week 2 to work on the problem sets in small groups. Myself or a TA will be available to answer any questions you have about the readings or the problem sets. You may leave as soon as you are finished with the problem set, and it should be submitted before 5pm or 8pm that evening. If you have questions or confusions about the material or problem sets, come speak to a TA. I recognize not all students will be able to utilize these hours. You can also post questions on the discussion thread for that day’s material and myself or a TA will respond as soon as possible. I also have office hours available by appointment. You can schedule them by sending an email. Feel free to meet with me to discuss any concepts or questions regarding the material in the math camp, as well as any other questions you have about the MACSS program, the MAPSS QMSA program or upcoming fall quarter. Problem sets Each problem set is designed to take no more than three hours. If you are spending more than three hours on a problem set, you should make sure you are asking questions when you are confused. All problem sets will be submitted via Canvas. For each problem set, write out your answers to each exercise on paper and show your work. You will need to scan your answers to a PDF or Word Doc to upload on Canvas. If you do not have a scanner at home, you can use your phone to scan your work. A couple recommended scanning apps are: For iOS, Evernote is a good choice For Android, Genius Scan works well It is crucial you show your work on your responses, otherwise you will receive an incomplete for the assignment. I care more about the process you follow to get the answer, rather than the answer itself. And please make sure your writing is legible! Course schedule Date Topic 6-Sept Linear equations, inequalities, sets and functions, quadratics, and logarithms 7-Sept Sequences, limits, continuity, derivatives 8-Sep Critical points and approximation 9-Sep Linear algebra 12-Sep Functions of several variables and optimization with several variables; Integration and integral calculus 13-Sep Sample space and probability; Discrete random variables 14-Sep General random variables; Multivariate distributions 15-Sep Properties of random variables and limit theorems; Classical statistical inference 16-Sep Maximum likelihood estimation and linear regression; Bayesian statistical inference "],["sets-functions.html", "Lecture 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms Learning objectives Supplemental readings 1.1 What is computational social science? 1.2 Difference between math, probability, and statistics 1.3 Goals for this camp 1.4 Course logistics 1.5 Mathematical notation 1.6 Sets 1.7 Functions 1.8 Quadratic functions 1.9 Systems of linear equations 1.10 Logarithms and exponential functions 1.11 Bonus content: Computational tools for the future", " Lecture 1 Linear equations, inequalities, sets and functions, quadratics, and logarithms Learning objectives Define computational social science Connect mathematics and statistics to social scientific inquiry Review course logistics Explain the pedagogical approach for the camp Define basic mathematical notation Assess the use of mathematical notation in the rational voter theory Define functions and their properties Define sets Practice root finding Define and linear systems of equations solve linear systems of equations via backsubstitution and Gaussian elimination Define logarithmic and exponential functions Practice simplifying power, logarithmic, and exponential functions Supplemental readings Chapters 1.1-.3, 2.1, 3, 4, Pemberton and Rau (2011) OpenStax Calculus: Volume 1, ch 1 OpenStax College Algebra, ch 7.1-.2 1.1 What is computational social science? Social science is defined as the scientific study of human society and social relationships.1 1.1.1 Disciplines within social science At the University of Chicago, the social sciences are broken into several distinct disciplines:2 Anthropology - science of human societies and cultures and their developments Economics - science of production, consumption, and transfer of wealth History - science of past events, particularly in human affairs Political science - science of who gets what, when, a how3 Psychology - science of behavior and mind Sociology - science of the development, structure, and functioning of human society The Division of the Social Sciences also includes interdisciplinary committees which bridge across multiple disciplines: Comparative Human Development - “research examines issues of central concern to socio-cultural anthropology, medical anthropology, comparative education, behavioral biology, language and thought, cultural and developmental psychology” Conceptual and Historical Studies of Science - “areas concerned with the history, philosophy, and social relations of science” Social Thought - “serious study of many academic topics, and of many philosophical, historical, theological and literary works, is best prepared for by a wide and deep acquaintance with the fundamental issues presupposed in all such studies, that students should learn about these issues by acquainting themselves with a select number of major ancient and modern texts in an inter-disciplinary atmosphere” 1.1.2 Computational social science Computational social science (CSS) is a modern interdisciplinary approach to the study of social scientific phenomena. Researchers in this field use computers to model, simulated, and analyze social phenomena. While historically CSS was limited to the application of numerical methods and simulation (e.g. agent-based modeling) to complex issues of social science research, over the past decade this has evolved to describe the intersection of computer science, math/statistics, and social science Lazer et al. (2009). Figure 1.1: Source: http://giventhedata.blogspot.com/2013/03/data-science-in-businesscomputational.html Computational social scientists leverage “big data” and computational research designs to analyze and study social phenomena. This requires sophistication and training across all three major domains. 1.1.3 Acquiring CSS skills To be a competently trained computational social scientist, one needs to develop training and expertise in computer science, social science, and math/statistics. At the University of Chicago and within MACSS, you will receive this training through a range of courses. Computer science MACS 30121/122/123 sequence MACS 30500 Computer science electives Social science Perspectives sequence (MACS 30000/100/200) Departmental electives Seminars Non-computational courses Math/statistics Probability and statistics used across sciences Start with Computational Math Camp Math/stats electives Machine learning Causal inference Bayesian inference Network analysis Deep learning Spatial data science Natural language processing And more 1.2 Difference between math, probability, and statistics In this camp, we will review fundamental methods across the fields of mathematics, probability, and statistics. While related, these fields are distinct from one another. Key attributes of each are listed below. 1.2.1 Mathematics Mathematics is purely abstract Based on axioms that are independent from the real world If the axioms are accepted, mathematical inferences are certain Language for expressing structure and relationships Generally proof-based 1.2.2 Probability Systematic and rigorous method for treating uncertainty “Mathematical models of uncertain reality” Derivation of “applied mathematics” 1.2.3 Statistics Practice or science of collecting and analyzing numerical data in large quantities, especially for the purpose of inferring proportions in a whole from those in a sample Making inferences from data that are not entirely certain Based on mathematical models, but with deviations from the model (not deterministic) 1.2.4 Their uses All are used to study social scientific phenomena: Mathematical models Game theory Formal theory Much bigger in economics Defining statistical models and relationships Probability/statistics Establishing a structure for relationships between variables using data Inferring relationships and assessing their validity 1.3 Goals for this camp Survey math and statistical tools that are foundational to CSS Review common mathematical notation Apply math/statistics methods Prepare students to pass the MACSS math/stats placement exam and enroll in computationally-enhanced course offerings at UChicago 1.4 Course logistics All course materials can be found on the course Canvas site 1.4.1 Course staff 1.4.1.1 Me (Dr. Yanyan Sheng) Senior Instructional Professor in Computational Social Science Associate Director of the Committee on Quantitative Methods PhD in educational measurement and statistics Current teaching rotation Introductory Statistical Methods Foundations of Statistical Theory Overview of Quantitative Methods in the Social and Behavioral Sciences Principles of Measurement 1.4.2 Teaching assistants Kaya Borlase John Wang Wenqian Zhang 1.4.3 Prerequisites for the math camp No formal prerequisites Who is likely to succeed in this class? You’ll probably have prior training in: Linear algebra Calculus Probability theory Statistical inference (data description, assessing bivariate relationships for continuous and categorical variables, linear/logistic regression, etc.) High school/AP/IB training may be sufficient depending on the depth of the course and how recently you completed it We assume prior exposure to the content covered in this camp - our goal is to refresh material which you have previously learned, not teach it to you fresh * Impossible to teach all this material from scratch in just three weeks 1.4.4 Alternatives to this camp SOSC 30100 - Mathematics for Social Sciences (aka the Hansen math camp) 1.4.5 Evaluation 1.4.5.1 Grades You don’t take this course for a grade Pass/fail (no credit) Show up in class, complete the problem sets satisfactorily (not perfectly), and you’ll pass That said, it should not matter The irony of grad school Grades no longer matter (or should not) Learn as much material as possible If you truly only care about learning material, you’ll get amazing grades 1.4.5.2 Pedagogical approach for the camp We will follow a flipped-classroom design(-ish). Each day you will have course notes provided in lieu of the actual lecture. Read these closely as the problem sets are drawn directly from this material. Supplemental readings from published textbooks will be listed for each day. You may choose to read these as you find necessary. It is not expected that you read them cover-to-cover. Instead, I suggest concentrating on certain techniques or methods you find especially challenging or confusing. Problem sets will be distributed electronically each day, to be submitted via Canvas for evaluation. “Homework” is your reading for the next day The instructional staff (the TAs and myself) are to be your “sherpas” and guide you along your journey. We are here to assist you - please make liberal use of us and your peers as you complete the problem sets. The goal is to provide you with quick feedback so you don’t waste time at home struggling through problems you cannot understand or resolve. Problem sets will be evaluated and returned within 24 hours, and the solution key will be posted for you to review. 1.4.5.3 15 minute rule 15 min rule: when stuck, you HAVE to try on your own for 15 min; after 15 min, you HAVE to ask for help.- Brain AMA pic.twitter.com/MS7FnjXoGH — Rachel Thomas ((math_rachel?)) August 14, 2016 We will follow the 15 minute rule in this class. If you encounter an issue in your problem sets, spend up to 15 minutes troubleshooting the problem on your own. However, if after 15 minutes you still cannot solve the problem, ask for help. The instructional staff will be on site for help during class hours or after class by appointment scheduled via email. 1.4.6 Why are we doing this What is the point of solving all this math/stats by hand when computers can do it for us? Comment from discussion Kids in school used to say “We’ll never need to know this in the real world”. What, in your experience, were they wrong about?. The truth is that math skills translate to other domains. While perhaps overly simplified, computer science is essentially a bunch of math. Applying mathematical techniques and completing the exercises on the problem sets requires focus, determination, and grit. You’ll need all of that to survive grad school. Figure 1.2: Calvin and Hobbes by Bill Watterson for February 15, 2010 1.5 Mathematical notation 1.5.1 Why math is important to social science Math provides a consistent language to communicate ideas in an orderly and systematic way. Science uses highly precise language that is not easily interpretable to outsiders. Consider the definition of the term “theory”: Definition 1.1 (Lay definition of theory) An unproved assumption. Definition 1.2 (Scientific definition of theory) A well-substantiated explanation of some aspect of the natural world, based on a body of facts that have been repeatedly confirmed through observation and experiment (e.g. empirical support, falsifiable, repeatedly tested). This leads to potential confusion with interactions between the scientific community and the general public. Example 1.1 (Climate change) Predominantly in the United States there is an ideological divide in whether or not people believe it to be true. Is it real or just “a theory”? Example 1.2 (COVID-19) Consider any discussion about scientific evidence related to the COVID-19 pandemic and associated conspiracy theories. Mathematics is an effective way to describe our world. Many physical processes follow precise mathematical models. Social phenomena are less so, but we can still learn from mathematical modeling. Mathematical notation lets us convey precision and minimizes the risk of misinterpretation by other scholars 1.5.2 Example: Paradox of voting Consider the classic paradox of voting first introduced in Downs et al. (1957). Downs identifies how rational voters should weigh the rewards vs. costs of voting. The core principle is if it costs more to vote than the rewards gained from the action, then individuals should not vote in elections. The difference between cost and reward is defined as the utility that the person receives from the act (based on an unknown preference scale). How can we codify this model using mathematical notation? \\[R = PB - C\\] \\(R =\\) the utility satisfaction of voting \\(P =\\) the actual probability that the voter will affect the outcome with her particular vote \\(B =\\) the perceived difference in benefits between the two candidates measured in utiles (units of utility) \\(C =\\) the actual cost of voting in utiles (e.g. time, effort, money) 1.5.2.1 Think-pair-share What implications does the model provide? How would you express them mathematically? Click for the solution Some potential implications are: Implication Formal statement If individuals do not get enough benefit from voting, they will abstain The voter will abstain if \\(R &lt; 0\\). Individuals have other things to do on election day (like going to work). If the benefit of voting is not as large as alternative benefits, then individuals will abstain. The voter may still not vote even if \\(R &gt; 0\\) if there exist other competing activities that produce a higher \\(R\\). Most elections have thousands, if not millions, of ballots cast. There is no point to voting since any individual ballot is unlikely to change the outcome of the election. Therefore everyone should abstain. If \\(P\\) is very small, then it is unlikely that this individual will vote. This leads to the paradox of voting - rationally no one should vote, yet many people still do. Why? This is clearly a question worth answering in political science. Downs et al. (1957) has been cited over 32000 times, so we know people care about the question. While a pure mathematical model may not be fully accurate, it allows us to delve deeper into the paradox. How should we measure \\(B\\)? \\(C\\)? 1.6 Sets A set is a collection of objects. Example 1.3 \\[ \\begin{aligned} A &amp; = \\{1, 2, 3\\} \\nonumber \\\\ B &amp; = \\{4, 5, 6\\}\\nonumber \\\\ C &amp; = \\{ \\text{First year cohort} \\} \\\\ D &amp; = \\{ \\text{U of Chicago Lecturers} \\} \\end{aligned} \\] Definition 1.3 If \\(A\\) is a set, we say that \\(x\\) is an element of \\(A\\) by writing, \\(x \\in A\\). If \\(x\\) is not an element of \\(A\\) then, we write \\(x \\notin A\\). Example 1.4 \\[ \\begin{aligned} 1 &amp;\\in \\{ 1, 2, 3\\} \\\\ 4 &amp;\\in \\{4, 5, 6\\} \\\\ \\text{Will} &amp;\\notin \\{ \\text{First year cohort} \\} \\\\ \\text{Benjamin} &amp;\\in \\{ \\text{U of Chicago Lecturers} \\} \\end{aligned} \\] Why do we care about sets? Sets are necessary for probability theory Defining the set is equivalent to choosing population of interest (usually) Definition 1.4 If \\(A\\) and \\(B\\) are sets, then we say that \\(A = B\\) if, for all \\(x \\in A\\) then \\(x \\in B\\) and for all \\(y \\in B\\) then \\(y \\in A\\). A test to determine equality: Take all elements of \\(A\\), see if in \\(B\\) Take all elements of \\(B\\), see if in \\(A\\) Definition 1.5 If \\(A\\) and \\(B\\) are sets, then we say that \\(A \\subset B\\) is, for all \\(x \\in A\\), then \\(x \\in B\\). What is the difference between these definitions? In the first definition, \\(A\\) and \\(B\\) are identical In the second definition, all \\(x \\in A\\) are included in \\(B\\), but all \\(y \\in B\\) are not necessarily in \\(A\\) 1.6.1 Set builder notation Some famous sets: \\(\\mathbb{N} = \\{1, 2, 3, \\ldots \\}\\) \\(\\mathbb{Z} = \\{\\ldots, -2, -1, 0, 1, 2, \\ldots, \\}\\) \\(\\Re = \\mbox{Real numbers}\\) Use set builder notation to identify subsets: \\([a, b] = \\{x: x \\in \\Re \\text{ and } a \\leq x \\leq b \\}\\) \\((a, b] = \\{x: x \\in \\Re \\text{ and } a &lt; x \\leq b \\}\\) \\([a, b) = \\{x: x \\in \\Re \\text{ and } a \\leq x &lt; b \\}\\) \\((a, b) = \\{x: x \\in \\Re \\text{ and } a &lt; x &lt; b \\}\\) \\(\\emptyset\\) - empty or null set 1.6.2 Set operations We can build new sets with set operations. Definition 1.6 (Union) Suppose \\(A\\) and \\(B\\) are sets. Define the Union of sets \\(A\\) and \\(B\\) as the new set that contains all elements in set \\(A\\) or in set \\(B\\). In notation, \\[ \\begin{aligned} C &amp; = A \\cup B \\\\ &amp; = \\{x: x \\in A \\text{ or } x \\in B \\} \\end{aligned} \\] Example 1.5 (Example of unions) \\[ \\begin{aligned} A &amp;= \\{1, 2, 3\\} \\\\ B &amp;= \\{3, 4, 5\\} \\\\ C &amp;= A \\cup B = \\{ 1, 2, 3, 4, 5\\} \\end{aligned} \\] \\[ \\begin{aligned} D &amp;= \\{\\text{First Year Cohort} \\} \\\\ E &amp;= \\{\\text{Me} \\} \\\\ F &amp;= D \\cup E = \\{ \\text{First Year Cohort, Me} \\} \\end{aligned} \\] Definition 1.7 (Intersection) Suppose \\(A\\) and \\(B\\) are sets. Define the Intersection of sets \\(A\\) and \\(B\\) as the new set that contains all elements in set \\(A\\) and set \\(B\\). In notation, \\[ \\begin{aligned} C &amp; = A \\cap B \\\\ &amp; = \\{x: x \\in A \\text{ and } x \\in B \\} \\end{aligned} \\] Example 1.6 (Example of intersections) \\[ \\begin{aligned} A &amp;= \\{1, 2, 3\\} \\\\ B &amp;= \\{3, 4, 5\\} \\\\ C &amp;= A \\cap B = \\{3\\} \\end{aligned} \\] \\[ \\begin{aligned} D &amp;= \\{\\text{First Year Cohort} \\} \\\\ E &amp;= \\{\\text{Me} \\} \\\\ F &amp;= D \\cap E = \\emptyset \\end{aligned} \\] 1.6.3 Some facts about sets \\(A \\cap B = B \\cap A\\) The intersection of \\(A\\) and \\(B\\) is the same as the intersection of \\(B\\) and \\(A\\) \\(A \\cup B = B \\cup A\\) The union of \\(A\\) and \\(B\\) is the same as the intersection of \\(B\\) and \\(A\\) \\((A \\cap B) \\cap C = A \\cap (B \\cap C)\\) Two different ways of describing the set of objects that belongs to all three of \\(A, B, C\\) \\((A \\cup B) \\cup C = A \\cup (B \\cup C)\\) Two different ways of describing the set of objects that belongs to at least one of \\(A, B, C\\) \\(A \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C)\\) Two different ways of describing the set of members of \\(A\\) that are also in either \\(B\\) or \\(C\\) \\(A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C)\\) Two different ways of describing the set that contains either the members of \\(A\\) or the intersection of sets \\(B\\) and \\(C\\) 1.7 Functions 1.7.1 Ordered pairs You’ve seen an ordered pair before, \\[(a, b)\\] Definition 1.8 (Cartesian product) Suppose we have two sets, \\(A\\) and \\(B\\). Define the Cartesian product of \\(A\\) and \\(B\\), \\(A \\times B\\) as the set of all ordered pairs \\((a, b)\\), where \\(a \\in A\\) and \\(b \\in B\\). In other words, \\[ A \\times B = \\{(a, b): a \\in A \\text{ and } b \\in B \\} \\] Example 1.7 (Cartesian product) \\(A = \\{1, 2\\}\\) and \\(B = \\{3, 4\\}\\), then, \\[A \\times B = \\{ (1, 3); (1, 4); (2, 3); (2, 4) \\}\\] 1.7.2 Relation Definition 1.9 (Relation) A relation is a set of ordered pairs. A function \\(F\\) is a relation such that, \\[ (x, y) \\in F ; (x, z) \\in F \\Rightarrow y = z \\] We will commonly write a function as \\(F(x)\\), where \\(x \\in \\mbox{Domain} \\, F\\) and \\(F(x) \\in \\mbox{Codomain} \\, F\\). It is common to see people write, \\[ F:A \\rightarrow B \\] where \\(A\\) is domain and \\(B\\) is codomain. Examples include: 1.7.3 Relation vs. function A mathematical function is a mapping which gives a correspondence from one measure onto exactly one other for that value. This means mapping from one defined space to another, such as \\(F \\colon \\Re \\rightarrow \\Re\\). Think of it as a machine that takes as an input some value, applies a transformation to it, and spits out a new value Example 1.8 \\[F(x) = x^2 - 1\\] Maps \\(x\\) to \\(F(x)\\) by squaring \\(x\\) and subtracting 1. 1.7.3.1 Not a function All functions are relations, but not all relations are functions. Functions have exactly one value returned by \\(F(x)\\) for each value of \\(x\\), whereas relations may have more than one value returned. 1.7.4 Two major properties of functions \\[F(x) = y\\] A function is continuous if it has no gaps in its mapping from \\(x\\) to \\(y\\) A function is invertible if its reverse operation exists: \\[G^{-1}(y) = x, \\text{where } G^{-1}(G(x)) = x\\] Not all functions are continuous and invertible: \\[ F(x) = \\left\\{ \\begin{array}{ll} \\frac{1}{x} &amp; \\quad x \\neq 0 \\text{ and } x \\text{ is rational}\\\\ 0 &amp; \\quad \\text{otherwise} \\end{array} \\right. \\] We want functions to be continuous and invertible. For example, functions must be continuous and invertible to calculate derivatives in calculus. This is important for optimization and solving for parameter values in modeling strategies. With non-continuous functions, we cannot do much to fix them. However non-invertible functions can be made invertible by restricting the domain. 1.8 Quadratic functions Consider the function \\[y = ax^2\\] where \\(a\\) is a real number such that \\(a \\neq 0\\). The graph of the function has one of two possible shapes, depending on whether \\(a\\) is positive or negative: Definition 1.10 (Quadratic function) A function of the form \\[y = ax^2 + bx + c\\] where \\(a,b,c\\) are real numbers with \\(a \\neq 0\\). The graph of such a function still takes the form of a parabola, but the vertex is not necessarily at the origin. 1.8.1 Quadratic equation Definition 1.11 (Quadratic equation) A quadratic equation takes the form \\(f(x) = 0\\) where \\(f\\) is a quadratic function. To solve the equation, we need to determine the value of \\(x\\) which satisfies the equation. Example 1.9 \\[ \\begin{aligned} x^2 - 7 &amp;= 0 \\\\ x^2 &amp;= 7 \\\\ x &amp;= \\pm \\sqrt{7} \\end{aligned} \\] Example 1.10 \\[ \\begin{aligned} -3x^2 + 30x - 27 &amp;= 0 \\\\ -3 (x^2 - 10x + 9) &amp;= 0 \\\\ -3(x - 9)(x - 1) &amp;= 0 \\\\ (x - 9)(x - 1) &amp;= 0 \\\\ x &amp;= 1, 9 \\end{aligned} \\] 1.8.2 Quadratic formula Definition 1.12 (Quadratic formula) A quadratic equation of the general form \\[ax^2 + bx + c = 0\\] where \\(a,b,c\\) are real numbers such that \\(a \\neq 0\\) can be solved using the formula \\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\] Example 1.11 \\[x^2 + x - 12 = 0\\] \\[ \\begin{aligned} x &amp;= \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\\\ &amp;= \\frac{-1 \\pm \\sqrt{1^2 - 4 \\times 1 \\times 12}}{2 \\times 1} \\\\ &amp;= \\frac{-1 \\pm \\sqrt{1 - (-48)}}{2} \\\\ &amp;= \\frac{-1 \\pm \\sqrt{49}}{2} \\\\ &amp;= \\frac{-1 \\pm 7}{2} \\\\ &amp;= 3, -4 \\\\ \\end{aligned} \\] 1.9 Systems of linear equations Extending from the previous examples, often we need to find the solution to systems of linear equations, or multiple equations with overlapping variables such as: \\[ \\begin{matrix} x &amp; - &amp; 3y &amp; = &amp; -3\\\\ 2x &amp; + &amp; y &amp; = &amp; 8 \\end{matrix} \\] More generally, we might have a system of \\(m\\) equations in \\(n\\) unknowns \\[ \\begin{matrix} a_{11}x_1 &amp; + &amp; a_{12}x_2 &amp; + &amp; \\cdots &amp; + &amp; a_{1n}x_n &amp; = &amp; b_1\\\\ a_{21}x_1 &amp; + &amp; a_{22}x_2 &amp; + &amp; \\cdots &amp; + &amp; a_{2n}x_n &amp; = &amp; b_2\\\\ \\vdots &amp; &amp; &amp; &amp; \\vdots &amp; &amp; &amp; \\vdots &amp; \\\\ a_{m1}x_1 &amp; + &amp; a_{m2}x_2 &amp; + &amp; \\cdots &amp; + &amp; a_{mn}x_n &amp; = &amp; b_m \\end{matrix} \\] A solution to a linear system of \\(m\\) equations in \\(n\\) unknowns is a set of \\(n\\) numbers \\(x_1, x_2, \\cdots, x_n\\) that satisfy each of the \\(m\\) equations. Example 1.12 \\(x=3\\) and \\(y=2\\) is the solution to the above \\(2\\times 2\\) linear system. If you graph the two lines, you will find that they intersect at \\((3,2)\\). Does a linear system have one, no, or multiple solutions? For a system of 2 equations with 2 unknowns (i.e., two lines): One solution: The lines intersect at exactly one point. No solution: The lines are parallel. Infinite solutions: The lines coincide. 1.9.1 One solution Here we consider solving the linear system by substitution and elimination of variables. Consider the example \\[ \\begin{aligned} 3x + 2y &amp;= 8 \\\\ 2x + 5y &amp;= 9 \\end{aligned} \\] To start we need to eliminate one unknown from one equation, allowing us to solve for the other unknown. Here we can do so by eliminating \\(x\\) from the second equation by subtracting a suitable multiple of the first equation. Specifically, we can multiply the first equation by \\(2/3\\) (\\(2x + \\frac{4}{3}y = \\frac{16}{3}\\)) to give us the resulting system of equations: \\[ \\begin{aligned} 3x + 2y &amp;= 8 \\\\ (5 - \\frac{4}{3})y &amp;= 9 - \\frac{16}{3} \\end{aligned} \\] The second equation resolves to \\[ \\begin{aligned} \\frac{11}{3}y &amp;= \\frac{11}{3} \\\\ y &amp;= 1 \\end{aligned} \\] We then substitute \\(y = 1\\) back into the first equation and find that \\[ \\begin{aligned} 3x + 2(1) &amp;= 8 \\\\ 3x &amp;= 6 \\\\ x &amp;= 2 \\end{aligned} \\] So the solution to the system is \\[x = 2, y = 1\\] 1.9.2 No solution Consider instead the linear system \\[ \\begin{aligned} 3x + 2y &amp;= 8 \\\\ 6x + 4y &amp;= 9 \\end{aligned} \\] If we try to solve the system of equations by subtracting twice the first equation from the second equation, we end up with a new equation of \\[0 = -7\\] Clearly nonsensical. And graphically we can see these are two parallel lines which will never intersect. 1.9.3 Infinite solutions The final type of outcome is illustrated by the system of equations \\[ \\begin{aligned} 3x + 2y &amp;= 8 \\\\ 6x + 4y &amp;= 16 \\end{aligned} \\] If we subtract twice the first equation from the second equation, we get \\[0 = 0\\] While true, this is somewhat tautological. That is, we really only have one equation. The second equation is just 2 times the first equation. 1.9.4 Three equations in three unknowns We can extend this general method to systems with three linear equations in three unknowns.4 Consider the system of equations \\[ \\begin{aligned} 2x &amp;+ 7y &amp;+ z &amp;= 2 \\\\ &amp;\\quad\\; 3y &amp;- 2z &amp;= 7 \\\\ &amp;&amp; 4z &amp;= 4 \\end{aligned} \\] This is a triangular system because of its unique trianglular pattern of the equations. This relatively easy to solve via back-substitution since we can immediately solve for \\(z\\) with the third equation (\\(z = 1\\)), substitute this value into the second equation to calculate \\[ \\begin{aligned} 3y - 2 (1) &amp;= 7 \\\\ 3y - 2 &amp;= 7 \\\\ 3y &amp;= 9 \\\\ y &amp;= 3 \\end{aligned} \\] and substitute both these values into the first equation \\[ \\begin{aligned} 2x + 7(3) + 1 &amp;= 2 \\\\ 2x + 22 &amp;= 2 \\\\ 2x &amp;= -20 \\\\ x &amp;= -10 \\end{aligned} \\] The solution is therefore \\[x = -10, y = 3, z = 1\\] If systems do not already follow this form, we can eliminate unknowns from equations until we arrive at a triangular system. Consider \\[ \\begin{aligned} 2x &amp;+ 4y &amp;+ z &amp;= 5 \\\\ x &amp;+ y &amp;+ z &amp;= 6 \\\\ 2x &amp;+ 3y &amp;+ 2z &amp;= 6 \\end{aligned} \\] We can leave the first equation and eliminate \\(x\\) from the second and third equations by subtracting suitable multiples of the first equation \\[ \\begin{aligned} 2x &amp;+ 4y &amp;+ z &amp;= 5 &amp; \\\\ &amp;- y &amp;+ \\frac{1}{2}z &amp;= \\frac{7}{2} &amp;\\quad \\text{- 1/2 times the first equation}\\\\ &amp;- y &amp;+ z &amp;= 1 &amp;\\quad \\text{- the first equation}\\\\ \\end{aligned} \\] Finally, eliminate \\(y\\) from the third equation by subtracting the second equation. \\[ \\begin{aligned} 2x &amp;+ 4y &amp;+ z &amp;= 5 \\\\ &amp;- y &amp;+ \\frac{1}{2}z &amp;= \\frac{7}{2} \\\\ &amp; &amp;+ \\frac{1}{2}z &amp;= -\\frac{5}{2} \\end{aligned} \\] Using backsubstitution, we arrive at the solution \\[z = -5, y = \\frac{1}{2}(z - 7) = \\frac{1}{2} \\times (-12) = -6, x = \\frac{1}{2}(5 - z) - 2y = \\frac{1}{2} \\times 10 + 12 = 17\\] \\[x = 17, y = -6, z = -5\\] 1.9.5 Gaussian elimination In addition to adding and subtracting multiples of one equation from another, we can also introduce elementary operations to interchange the order of equations to more easily arrive at a triangular system. This technique is known as Gaussian elimination. Elementary operatons include EO1 - writing the equations in a different order EO2 - subtracting a multiple of one equation from another equation We can alternate the use of these operations as necessary to simplify a system of linear equations to triangular form, and then apply backsubstitution to solve for the unknowns. Example 1.13 Consider the example \\[ \\begin{aligned} &amp; \\quad\\; y &amp;+ 2z &amp;= 2 \\\\ 2 x &amp;&amp;+ z &amp;= -1 \\\\ x &amp;+ 2 y &amp;\\quad &amp;= -1 \\end{aligned} \\] To make the formatting a bit easier, I fill in the missing unknowns with 0 coefficients. \\[ \\begin{aligned} 0x &amp; +y &amp;+ 2z &amp;= 2 \\\\ 2 x &amp;+ 0y &amp;+ z &amp;= -1 \\\\ x &amp;+ 2 y &amp;+ 0z &amp;= -1 \\end{aligned} \\] Swap equation 1 with equation 2: \\[ \\begin{aligned} 2 x &amp;+ 0y &amp;+ z &amp;= -1 \\\\ 0x &amp; +y &amp;+ 2z &amp;= 2 \\\\ x &amp;+ 2 y &amp;+ 0z &amp;= -1 \\end{aligned} \\] Subtract 1/2 × (equation 1) from equation 3: \\[ \\begin{aligned} 2 x &amp;+ 0y &amp;+ z &amp;= -1 \\\\ 0x &amp; +y &amp;+ 2z &amp;= 2 \\\\ 0x &amp;+ 2 y &amp;- \\frac{z}{2} &amp;= -\\frac{1}{2} \\end{aligned} \\] Multiply equation 3 by 2: \\[ \\begin{aligned} 2 x &amp;+ 0y &amp;+ z &amp;= -1 \\\\ 0x &amp; +y &amp;+ 2z &amp;= 2 \\\\ 0x &amp;+ 4 y &amp;- z &amp;= -1 \\end{aligned} \\] Swap equation 2 with equation 3: \\[ \\begin{aligned} 2 x &amp;+ 0y &amp;+ z &amp;= -1 \\\\ 0x &amp;+ 4 y &amp;- z &amp;= -1 \\\\ 0x &amp; +y &amp;+ 2z &amp;= 2 \\\\ \\end{aligned} \\] Subtract 1/4 × (equation 2) from equation 3: \\[ \\begin{aligned} 2 x &amp;+ 0y &amp;+ z &amp;= -1 \\\\ 0x &amp;+ 4 y &amp;- z &amp;= -1 \\\\ 0x &amp; + 0y &amp;+ \\frac{9}{4}z &amp;= \\frac{9}{4} \\\\ \\end{aligned} \\] At which point we could solve via backsubstitution. \\[ \\begin{aligned} \\frac{9}{4}z &amp;= \\frac{9}{4} \\\\ z &amp;= 1 \\end{aligned} \\] \\[ \\begin{aligned} 4 y -z &amp;= -1 \\\\ 4 y -1 &amp;= -1 \\\\ 4y &amp;= 0 \\\\ y &amp;= 0 \\end{aligned} \\] \\[ \\begin{aligned} 2 x + z &amp;= -1 \\\\ 2x + 1 &amp;= -1 \\\\ 2x &amp;= -2 \\\\ x &amp;= -1 \\end{aligned} \\] \\[x = -1, y = 0, z = 1\\] 1.10 Logarithms and exponential functions Important component to many mathematical and statistical methods in social science Definition 1.13 (Exponent) Repeatedly multiply a number by itself Definition 1.14 (Logarithm) Reverse of an exponent 1.10.1 Functions with exponents \\[f(x) = x \\times x = x^2\\] \\[f(x) = x \\times x \\times x = x^3\\] 1.10.2 Common rules of exponents \\(x^0 = 1\\) \\(x^1 = x\\) \\(\\left ( \\frac{x}{y} \\right )^a = \\left ( \\frac{x^a}{y^a}\\right ) = x^a y^{-a}\\) \\((x^a)^b = x^{ab}\\) \\((xy)^a = x^a y^a\\) \\(x^a \\times x^b = x^{a+b}\\) 1.10.3 Logarithms Class of functions \\(\\log_{b}(x) = a \\Rightarrow b^a = x\\) What number \\(a\\) solves \\(b^a = x\\) 1.10.3.1 Commonly used bases 1.10.3.1.1 Base 10 \\[\\log_{10}(100) = 2 \\Rightarrow 10^2 = 100\\] \\[\\log_{10}(0.1) = -1 \\Rightarrow 10^{-1} = 0.1\\] 1.10.3.1.2 Base 2 \\[\\log_{2}(8) = 3 \\Rightarrow 2^3 = 8\\] \\[\\log_{2}(1) = 0 \\Rightarrow 2^0 = 1\\] 1.10.3.1.3 Base \\(e\\) Euler’s number (aka a natural logarithm) \\[\\log_{e}(e) = 1 \\Rightarrow e^1 = e\\] Natural logarithms are incredibly useful in math. Often \\(\\log()\\) is assumed to be a natural log. You may also see it written as \\(\\ln()\\). 1.10.3.2 Rules of logarithms \\(\\log_b(1) = 0\\) \\(\\log(x \\times y) = \\log(x) + \\log(y)\\) This is extremely useful. Maximum likelihood estimation requires multiplying a bunch of really small numbers. Instead, we can take the log of that operation to add a bunch of normal-sized numbers \\(\\log(\\frac{x}{y}) = \\log(x) - \\log(y)\\) \\(\\log(x^y) = y \\log(x)\\) 1.11 Bonus content: Computational tools for the future We will not be learning any programming in this camp. There is simply not enough time. However for those of you who want to learn the appropriate computational tools for CSS, here is a primer of what we teach in MACSS and/or things you should learn on your own. Vision is open-source - software that is free and whose source code is licensed for public usage Shift away from proprietary formats (e.g. SPSS, Stata, SAS) Emphasis on reproducibility Code Results Analysis Publication 1.11.1 Programming languages for statistical learning Python R 1.11.2 Version control (Git) Reproducibility Backup Track changes Assign blame/responsibility 1.11.3 Publishing 1.11.3.1 Move away from WYSIWYG You probably have lots of experience with Microsoft Word or Google Docs. These programs are what you see is what you get (WYSIWYG) – all the formatting is performed via point and click and you see the final version on the screen as you write. This is not a reproducible format. Hard to incorporate changes from data analysis Hard to format scientific notation/equations Difficult to customize appearance and maintain theme across documents Not scripted 1.11.3.2 Reproducible formats 1.11.3.2.1 Notebooks Integrate code, output, and written text Reproducible Rerun the notebook to regenerate all the output Good for prototyping and exploratory data analysis For Python - Jupyter Notebooks For R - R Markdown 1.11.3.2.2 High-quality typesetting system De facto standard for production of technical and scientific documentation Books Journal articles Free software Renders documents as PDFs Makes typesetting easy $$f(x) = \\frac{\\exp(-\\frac{(x - \\mu)^2}{2\\sigma^2} )}{ \\sqrt{2\\pi \\sigma^2}}$$ \\[f(x) = \\frac{\\exp(-\\frac{(x - \\mu)^2}{2\\sigma^2} )}{ \\sqrt{2\\pi \\sigma^2}}\\] Tables/figures/general typesetting/nice presentations - easier in Papers Books Dissertations/theses Slides (beamer) Steep learning curve up front, but leads to big dividends later 1.11.3.2.3 Markdown Lightweight markup language with plain text formatting syntax Easy to convert to HTML, PDF, and more Used commonly on GitHub documentation, Jupyter Notebooks, R Markdown, and more Simplified syntax compared to - also less flexibility Publishing formats HTML PDF Websites Slides Dashboards Word/ODT/RTF 1.11.4 How will you acquire these skills? CAPP 30121/122 Perspectives sequence MACS 30500 On your own References "],["sequences-derivatives.html", "Lecture 2 Sequences, limits, continuity, and derivatives Learning objectives Supplemental readings 2.1 Sequence 2.2 Limits 2.3 Continuity 2.4 What is calculus? 2.5 Derivatives 2.6 Calculating derivatives 2.7 Derivatives for the exponential function and natural logarithms 2.8 Derivatives and properties of functions", " Lecture 2 Sequences, limits, continuity, and derivatives Learning objectives Define sequences Distinguish convergence and divergence Define limits Define continuity Calculate limits of sequences and functions Define the slope of a line Summarize tangent lines, rates of change, and derivatives Define derivative rules for common functions Apply the product, quotient, and chain rules for differentiation Summarize the exponential function and natural logarithms Identify properties of derivatives helpful to statistical methods Supplemental readings Chapter 5-7, 9, Pemberton and Rau (2011) OpenStax Calculus: Volume 1, ch 2-4 OpenStax Calculus: Volume 2, ch 5 2.1 Sequence 2.1.1 Definition Definition 2.1 (Sequence) A sequence is a function whose domain is the set of positive integers We’ll write a sequence as, \\[\\left\\{u_{n} \\right\\}_{n=1}^{\\infty} = (u_{1} , u_{2}, \\ldots, u_{N}, \\ldots )\\] 2.1.2 Examples \\[\\left\\{\\frac{1}{n} \\right\\} = (1, 1/2, 1/3, 1/4, \\ldots, 1/N, \\ldots, )\\] \\[\\left\\{\\frac{1}{n^2} \\right\\} = (1, 1/4, 1/9, 1/16, \\ldots, 1/N^2, \\ldots, ) \\\\\\] \\[\\left\\{\\frac{1 + (-1)^n}{2} \\right\\} = (0, 1, 0, 1, \\ldots, 0,1,0,1 \\ldots, ) \\\\\\] 2.1.3 Arithmetic and geometric progressions Definition 2.2 (Arithmetic progression) An arithmetic progression is a sequence \\(\\{ u_n \\}\\) with the property that the difference between each pair of successive terms is the same: \\(u_{n+1} - u_n\\) is the same for all \\(n\\). The arithmetic progression with first term \\(a\\) and common difference \\(d\\) is \\[a, a + d, a + 2d, a +3d, \\ldots\\] The \\(n\\)th term is given by \\[u_n = a + (n-1)d\\] Definition 2.3 (Geometric progression) A geometric progression is a sequence \\(\\{ u_n \\}\\) in which each term is obtained from the preceding one by multiplication by the same number: the ratio \\(\\frac{u_{n+1}}{u_n}\\) is the same for all \\(n\\). The geometric progression with first term \\(a\\) and common ratio \\(x\\) is \\[a, ax, ax^2, ax^3, \\ldots\\] The \\(n\\)th term is given by \\[u_n = ax^{n-1}\\] Illustrative of a principal of convergence. Some applications of geometric progressions occur in economics (e.g. compounding interest). 2.1.4 Convergence Consider the sequence: \\[\\left\\{\\frac{(-1)^{n} }{n} \\right \\} = (-1, \\frac{1}{2}, \\frac{-1}{3}, \\frac{1}{4}, \\frac{-1}{5}, \\frac{1}{6}, \\frac{-1}{7}, \\frac{1}{8}, \\ldots )\\] Definition 2.4 (Convergence) A sequence \\(\\left\\{u_{n} \\right\\}_{n=1}^{\\infty}\\) converges to a real number \\(A\\) if for each \\(\\epsilon &gt;0\\) there is a positive integer \\(N\\) such that for all \\(n \\geq N\\) we have \\(|u_{n} - A| &lt; \\epsilon\\). If a sequence converges, it converges to one number. We call that \\(A\\). \\(\\epsilon&gt;0\\) is some arbitrary real-valued number. Think about this as our error tolerance. Notice \\(\\epsilon &gt; 0\\). As we will see the \\(N\\) will depend upon \\(\\epsilon\\). Implies the sequence never gets further than \\(\\epsilon\\) away from \\(A\\). Definition 2.5 (Divergence and Bounded) If a sequence, \\(\\left\\{u_{n} \\right\\}\\) converges we’ll call it convergent. If it doesn’t we’ll call it divergent. If there is some number \\(M\\) such that, for all \\(n\\) \\(|u_{n}|&lt;M\\), then we’ll call it bounded. An unbounded sequence \\[\\left\\{ n \\right \\} = (1, 2, 3, 4, \\ldots, N, \\ldots )\\] A bounded sequence that doesn’t converge \\[\\left\\{\\frac{1 + (-1)^n}{2} \\right\\} = (0, 1, 0, 1, \\ldots, 0,1,0,1 \\ldots, )\\] All convergent sequences are bounded. If a sequence is constant, \\(\\left\\{C \\right \\}\\) it converges to \\(C\\). 2.1.5 Algebra of sequences How do we add, multiply, and divide sequences? Theorem 2.1 Suppose \\(\\left\\{a_{n} \\right \\}\\) converges to \\(A\\) and \\(\\left\\{b_{n} \\right\\}\\) converges to \\(B\\). Then, \\(\\left\\{a_{n} + b_{n} \\right\\}\\) converges to \\(A + B\\). \\(\\left\\{a_{n} b_{n} \\right\\}\\) converges to \\(A \\times B\\). Suppose \\(b_{n} \\neq 0 \\forall n\\) and \\(B \\neq 0\\). Then \\(\\left\\{\\frac{a_{n}}{b_{n}} \\right\\}\\) converges to \\(\\frac{A}{B}\\). 2.1.5.1 Think, pair, share Consider the sequence \\(\\left\\{\\frac{1}{n} \\right\\}\\) - what does it converge to? Consider the sequence \\(\\left\\{\\frac{1}{2n} \\right \\}\\) - what does it converge to? Click for the solution 0 - \\(2 \\times\\) a really big number still leads to a really big number in the denominator 0 - as \\(n\\) gets bigger, the fraction continually decreases towards 0 2.1.5.2 Challenge questions What does \\(\\left\\{3 + \\frac{1}{n}\\right\\}\\) converge to? Click for the solution \\[\\lim_{x \\rightarrow \\infty} \\left\\{3 + \\frac{1}{n}\\right\\} = 3\\] &lt;/p&gt; What about \\(\\left\\{ (3 + \\frac{1}{n} ) (100 + \\frac{1}{n^4} ) \\right\\}\\)? Click for the solution \\[ \\begin{aligned} \\lim_{x \\rightarrow \\infty} \\left\\{ (3 + \\frac{1}{n} ) (100 + \\frac{1}{n^4} ) \\right\\} &amp;= \\lim_{x \\rightarrow \\infty} \\left\\{ (3 + \\frac{1}{n} ) \\right\\} \\times \\lim_{x \\rightarrow \\infty} \\left\\{ (100 + \\frac{1}{n^4} ) \\right\\} \\\\ &amp;= 3 \\times 100 \\\\ &amp;= 300 \\end{aligned} \\] &lt;/p&gt; Finally, \\(\\left\\{ \\frac{ 300 + \\frac{1}{n} }{100 + \\frac{1}{n^4}} \\right\\}\\)? Click for the solution \\[ \\begin{aligned} \\lim_{x \\rightarrow \\infty} \\left\\{ \\frac{ 300 + \\frac{1}{n} }{100 + \\frac{1}{n^4}} \\right\\} &amp;= \\frac{300}{100} \\\\ &amp;= 3 \\end{aligned} \\] &lt;/p&gt; 2.2 Limits 2.2.1 Sequences \\(\\leadsto\\) limits of functions Calculus/Real Analysis: study of functions on the real line Limit of a function: how does a function behave as it gets close to a particular point? Relevant to our understanding and application of: Derivatives Asymptotics Game Theory 2.2.2 Limits of functions Definition 2.6 (Limit of a function) Suppose \\(f: \\Re \\rightarrow \\Re\\). We say that \\(f\\) has a limit \\(L\\) at \\(x_{0}\\) if, for \\(\\epsilon&gt;0\\), there is a \\(\\delta&gt;0\\) such that \\[|f(x) - L| &lt; \\epsilon \\, \\forall \\, x \\backepsilon 0 &lt; |x - x_0 | &lt; \\delta\\] \\[|f(x) - L| &lt; \\epsilon \\, \\text{for all} \\, x \\, \\text{such that} \\, 0 &lt; |x - x_0 | &lt; \\delta\\] Limits are about the behavior of functions at points. Here \\(x_{0}\\). As with sequences, we let \\(\\epsilon\\) define an error rate. \\(\\delta\\) defines an area around \\(x_{0}\\) where \\(f(x)\\) is going to be within our error rate. 2.2.3 Examples of limits Theorem 2.2 The function \\(f(x) = x + 1\\) has a limit of \\(1\\) at \\(x_{0} = 0\\). Proof. Without loss of generalization (WLOG) choose \\(\\epsilon &gt;0\\). We want to show that there is \\(\\delta_{\\epsilon}\\) such that \\(|f(x) - 1| &lt; \\epsilon \\, \\text{for all} \\, x \\, \\text{such that} \\, 0 &lt; |x - x_0 | &lt; \\delta\\). In other words, \\[ \\begin{aligned} |(x + 1) - 1| &lt; \\epsilon \\, \\text{for all} \\, x \\, &amp;\\text{such that} \\, 0 &lt; |x - 0 | &lt; \\delta \\\\ |x| &lt; \\epsilon \\, \\text{for all} \\, x \\, &amp;\\text{such that} \\, 0 &lt; |x | &lt; \\delta \\\\ \\end{aligned} \\] But if \\(\\delta_{\\epsilon} = \\epsilon\\) then this holds, we are done. A function can have a limit of \\(L\\) at \\(x_{0}\\) even if \\(f(x_{0} ) \\neq L\\)(!) Theorem 2.3 The function \\(f(x) = \\frac{x^2 - 1}{x - 1}\\) has a limit of \\(2\\) at \\(x_{0} = 1\\). Proof. For all \\(x \\neq 1\\), \\[ \\begin{aligned} \\frac{x^2 - 1}{x - 1} &amp; = \\frac{(x + 1)(x - 1) }{x - 1} \\\\ &amp; = x + 1 \\end{aligned} \\] Choose \\(\\epsilon &gt;0\\) and set \\(x_{0}=1\\). Then, we’re looking for \\(\\delta_{\\epsilon}\\) such that \\[ \\begin{aligned} |(x + 1) -2 | &lt; \\epsilon \\, \\text{for all} \\, x \\, &amp;\\text{such that} \\, 0 &lt; |x - 1 | &lt; \\delta \\\\ |x - 1 | &lt; \\epsilon \\, \\text{for all} \\, x \\, &amp;\\text{such that} \\, 0 &lt; |x - 1 | &lt; \\delta \\\\ \\end{aligned} \\] Again, if \\(\\delta_{\\epsilon} = \\epsilon\\), then this is satisfied. 2.2.4 Not all functions have limits Theorem 2.4 Consider \\(f:(0,1) \\rightarrow \\Re\\), \\(f(x) = \\frac{1}{x}\\). \\(f(x)\\) does not have a limit at \\(x_{0}=0\\) Proof. Choose \\(\\epsilon&gt;0\\). We need to show that there does not exist \\[ \\begin{aligned} |\\frac{1}{x} - L| &lt; \\epsilon \\, \\text{for all} \\, x \\, &amp;\\text{such that} \\, 0 &lt; |x - 0 | &lt; \\delta \\\\ |\\frac{1}{x} - L| &lt; \\epsilon \\, \\text{for all} \\, x \\, &amp;\\text{such that} \\, 0 &lt; |x| &lt; \\delta \\\\ \\end{aligned} \\] But, there is a problem. Because \\[ \\begin{aligned} \\frac{1}{x} - L &amp; &lt; \\epsilon \\\\ \\frac{1}{x} &amp; &lt; \\epsilon + L \\\\ x &amp; &gt; \\frac{1}{L + \\epsilon} \\end{aligned} \\] This implies that there can’t be a \\(\\delta\\), because \\(x\\) has to be bigger than \\(\\frac{1}{L + \\epsilon}\\). 2.2.5 Intuitive definition of a limit Definition 2.7 (Limit) If a function \\(f\\) tends to \\(L\\) at point \\(x_{0}\\) we say it has a limit \\(L\\) at \\(x_{0}\\) we commonly write, \\[\\lim_{x \\rightarrow x_{0}} f(x) = L\\] Definition 2.8 (Right and left-hand limits) If a function \\(f\\) tends to \\(L\\) at point \\(x_{0}\\) as we approach from the right, then we write \\[\\lim_{x \\rightarrow x_{0}^{+} } f(x) = L\\] and call this a right hand limit. If a function \\(f\\) tends to \\(L\\) at point \\(x_{0}\\) as we approach from the left, then we write \\[\\lim_{x \\rightarrow x_{0}^{-} } f(x) = L\\] and call this a left-hand limit. 2.2.6 Algebra of limits Theorem 2.5 Suppose \\(f:\\Re \\rightarrow \\Re\\) and \\(g: \\Re \\rightarrow \\Re\\) with limits \\(A\\) and \\(B\\) at \\(x_{0}\\). Then, \\[ \\begin{aligned} \\text{i.) } \\lim_{x \\rightarrow x_{0} } (f(x) + g(x) ) &amp; = \\lim_{x \\rightarrow x_{0}} f(x) + \\lim_{x \\rightarrow x_{0}} g(x) = A + B \\\\ \\text{ii.) }\\lim_{x \\rightarrow x_{0} } f(x) g(x) &amp; = \\lim_{x \\rightarrow x_{0}} f(x) \\lim_{x\\rightarrow x_{0}} g(x) = A B \\end{aligned} \\] Suppose \\(g(x) \\neq 0\\) for all \\(x \\in \\Re\\) and \\(B \\neq 0\\) then \\(\\frac{f(x)}{g(x)}\\) has a limit at \\(x_{0}\\) and \\[\\lim_{x \\rightarrow x_{0}} \\frac{f(x)}{g(x)} = \\frac{\\lim_{x\\rightarrow x_{0} } f(x) }{\\lim_{x \\rightarrow x_{0} } g(x) } = \\frac{A}{B}\\] 2.3 Continuity In the example above, a limit exists at 1. But there is a hole in the function. The function fails the pencil test, discontinuous at 1. Definition 2.9 (Pencil test) Imagine drawing a whole function with a pencil. If you can do it without lifting the pencil off the paper, the function is continuous. If you have to lift the pencil off, even for one single point, the function is discontinuous.5 2.3.1 Defining continuity Definition 2.10 Suppose \\(f:\\Re \\rightarrow \\Re\\) and consider \\(x_{0} \\in \\Re\\). We will say \\(f\\) is continuous at \\(x_{0}\\) if for each \\(\\epsilon&gt;0\\) there is a \\(\\delta&gt;0\\) such that if, \\[ \\begin{aligned} |x - x_{0} | &amp; &lt; \\delta \\text{ for all } x \\in \\Re \\text{ then } \\nonumber \\\\ |f(x) - f(x_{0})| &amp; &lt; \\epsilon \\nonumber \\end{aligned} \\] Previously \\(f(x_{0})\\) was replaced with \\(L\\). Now \\(f(x)\\) has to converge on itself at \\(x_{0}\\). Continuity is more restrictive than a limit. 2.3.1.1 Examples of continuity 2.3.2 A real-world example of limits: Measuring incumbency advantage Incumbency advantage is the overall causal impact of being the current incumbent party in a district on the votes obtained in the district’s election. In Lee (2008), the unit of analysis is the congressional district for U.S. House of Representatives. In the United States, incumbent parties win at a consistently high rate in elections to the U.S. House (\\(&gt;90\\%\\) win rate). Incumbent candidates also have a high win rate, though a bit smaller due to retirement (\\(\\approx 88\\%\\) probability of running for reelection, \\(\\approx 90\\%\\) probability of winning conditional on running for election). Compare this to the runner-up – only a \\(3\\%\\) chance of winning the next election, and only \\(20\\%\\) chance of running in the next election. Is there an electoral advantage to incumbency? That is, we expect incumbents use privileges and resources of office to gain an “unfair” advantage over potential challengers. Therefore there is an electoral advantage to incumbency – winning has a causal influence on the probability that the candidate will run for office again and eventually win the next election. Can this be proven through observational study? No – we cannot compare incumbent and non-incumbent electoral outcomes. What if all of the difference between win probabilities is a selection effect – incumbents are, by definition, those politicians who were successful in the previous election – and therefore incumbency is not the cause of the advantage? 2.3.2.1 Ideal experiment Randomly assign incumbent parties in a district between Democrats and Republicans Keep all other factors constant Corresponding increase in Democratic/Republican electoral success in the next election would represent the overall electoral benefit due to being the incumbent party in the district Obviously not realistic 2.3.2.2 Regression discontinuity design RDDs - dichotomous treatment that is a deterministic function of a single, continuous covariate Treatment is assigned to those individuals whose score crosses a known threshold. If you know the score, you can reverse-engineer the treatment assignment and assume as-if random assignment in the local neighborhood around a probability of \\(50\\%\\). In the context of incumbency advantage, consider that whether or not the Democrats are the incumbent party in a Congressional district is a deterministic function of their vote share in the prior election. Democrats are the incumbent party whenever their two-party margin of victor is greater than \\(0\\). So it is plausible that within a local range of that value, district assignment to Democrats or Republicans is as-if random. Any differences in the estimated probability of winning the election can be attributed to the effect of incumbent parties. Figure 2.1: Source: Randomized experiments from non-random selection in U.S. House elections. Lee (2008). As apparent from the figure, there is a large discontinuous jump at the 0 point. Democrats who barely win an election are much more likely to run for office and succeed in the next election, compared to Democrats who barely lose. The causal effect is enormous. Nowhere else is there such a large jump as the function is well-behaved and smooth except for at the threshold determining victory or defeat. This discontinuity is key evidence of a causal effect of incumbency advantage on electoral success. 2.3.3 Continuity and limits Theorem 2.6 Let \\(f: \\Re \\rightarrow \\Re\\) with \\(x_{0} \\in \\Re\\). Then \\(f\\) is continuous at \\(x_{0}\\) if and only if \\(f\\) has a limit at \\(x_{0}\\) and that \\(\\lim_{x \\rightarrow x_{0} } f(x) = f(x_{0})\\). Proof. \\((\\Rightarrow)\\). Suppose \\(f\\) is continuous at \\(x_{0}\\). This implies that \\(|f(x) - f(x_0)| &lt; \\epsilon \\, \\text{for all} \\, x \\, \\text{such that} \\, |x - x_0 | &lt; \\delta\\). This is the definition of a limit, with \\(L = f(x_{0})\\). \\((\\Leftarrow)\\). Suppose \\(f\\) has a limit at \\(x_{0}\\) and that limit is \\(f(x_{0})\\). This implies that \\(|f(x) - f(x_0)| &lt; \\epsilon \\, \\text{for all} \\, x \\, \\text{such that} \\, |x - x_0 | &lt; \\delta\\). But this is the definition of continuity. 2.3.4 Algebra of continuous functions Theorem 2.7 Suppose \\(f:\\Re \\rightarrow \\Re\\) and \\(g:\\Re \\rightarrow \\Re\\) are continuous at \\(x_{0}\\). Then, \\(f(x) + g(x)\\) is continuous at \\(x_{0}\\) \\(f(x) g(x)\\) is continuous at \\(x_{0}\\) if \\(g(x_0) \\neq 0\\), then \\(\\frac{f(x) } {g(x) }\\) is continuous at \\(x_{0}\\) 2.4 What is calculus? Calculus is the study of continuous change within functions. Within calculus falls differential calculus (concerning instantaneous rates of change and slopes of curves) and integral calculus (concerning accumulation of quantities and the areas under and between curves), forever intertwined with one another. Calculus has broad applications to mathematical and statistical methods in the social sciences. Calculus is a fundamental part of any type of statistics exercise. Although you may not be taking derivatives and integral in your daily work as an analyst, calculus undergirds many concepts we use: maximization, expectation, and cumulative probability. Within computational social science, calculus is crucial for finding and identifying extreme values: maxima or minima. This is a process known as optimization, and has uses for both empirical studies as well as formal theory: Given data, what is the most likely value of a parameter(s)? Game theory: given another player’s strategy, what is the action that maximizes utility? 2.5 Derivatives 2.5.1 How functions change Derivatives are rates of change in functions. You can think of them as a special type of limit. 2.5.2 The tangent as a limit Say \\(y = f(x)\\) and there is a point \\(P\\) on the curve. Let \\(Q\\) be another point on the curve, and let \\(L\\) be the straight line through \\(P\\) and \\(Q\\). You should think of \\(L\\) as the entire straight line through \\(P\\) and \\(Q\\), extending forever in both directions. Now suppose we move the point \\(Q\\) along the graph in the direction of \\(Q\\). If the curve is reasonably smooth, the slope of \\(L\\) tends to a limit as \\(Q\\) approaches \\(P\\), and the limit is the same whether \\(P\\) is approached from the right or the left. The slope of the curve at \\(P\\) is the limit of the slope of \\(L\\) as \\(Q\\) approaches \\(P\\). The tangent to the curve at \\(P\\), labeled \\(T\\), is the straight line through \\(P\\) whose slope is the slope of the curve at \\(P\\). We denote the coordinates of \\(P\\) and \\(Q\\) by the ordered pairs \\((x_0, y_0)\\) and \\((x_1, y_1)\\) respectively. Then \\[\\text{slope of } L = \\frac{y_1 - y_0}{x_1 - x_0}\\] This is basic rise over run. We can rewrite this equation to emphasize the fact that \\(P\\) and \\(Q\\) both lie on the graph \\(y = f(x)\\). Let \\(h = x_1 - x_0\\). Then \\[x_1 = x_0 + h, \\quad y_0 = f(x_0), \\quad y_1 = f(x_0 + h)\\] and \\[\\text{slope of } L = \\frac{f(x_0 + h) - f(x_0)}{h}\\] To say that \\(Q\\) approaches \\(P\\) along the curve is the same as saying that \\(h\\) approaches \\(0\\). Thus the slope at the point \\(P\\) of the graph \\(y = f(x)\\) is the limit of the right-hand side as \\(h\\) approaches \\(0\\). 2.5.3 Derivative Suppose \\(f:\\Re \\rightarrow \\Re\\). Measure rate of change at a point \\(x_{0}\\) with a function \\(R(x)\\), \\[ R(x) = \\frac{f(x) - f(x_{0}) }{ x- x_{0} } \\] \\(R(x)\\) defines the rate of change. A derivative will examine what happens with a small perturbation at \\(x_{0}\\). Definition 2.11 (Derivative) Let \\(f:\\Re \\rightarrow \\Re\\). If the limit \\[ \\begin{aligned} \\lim_{x\\rightarrow x_{0}} R(x) &amp; = \\frac{f(x) - f(x_{0}) }{x - x_{0}} \\\\ &amp; = f^{&#39;}(x_{0}) \\end{aligned} \\] exists then we say that \\(f\\) is differentiable at \\(x_{0}\\). If \\(f^{&#39;}(x_{0})\\) exists for all \\(x \\in \\text{Domain}\\), then we say that \\(f\\) is differentiable. Let \\(f\\) be a function whose domain includes an open interval containing the point \\(x\\). The derivative of \\(f\\) at \\(x\\) is given by \\[ \\frac{d}{dx}f(x) =\\lim\\limits_{h\\to 0} \\frac{f(x+h)-f(x)}{(x+h)-x} = \\lim\\limits_{h\\to 0} \\frac{f(x+h)-f(x)}{h} \\] There are two main ways to denote a derivate: Leibniz Notation: \\(\\frac{d}{dx}(f(x))\\) Prime or Lagrange Notation: \\(f&#39;(x)\\) If \\(f(x)\\) is a straight line, the derivative is the slope. For a curve, the slope changes by the values of \\(x\\), so the derivative is the slope of the line tangent to the curve at \\(x\\). Figure 2.2: The Derivative as a Slope If \\(f&#39;(x)\\) exists at a point \\(x_0\\), then \\(f\\) is said to be differentiable at \\(x_0\\). That also implies that \\(f(x)\\) is continuous at \\(x_0\\). 2.5.4 Rates of change in a function Another framework is to consider the function \\(y = f(x)\\). As \\(x\\) changes from \\(x_0\\) to \\(x_0 + h\\), the value of the function changes from \\(f(x_0)\\) to \\(f(x_0 + h)\\). Thus the change in \\(x\\) is \\(h\\), the change in \\(f(x)\\) is \\(f(x_0 + h) - f(x_0)\\), and the rate of change of \\(f(x)\\) is defined to be \\[\\frac{f(x_0 + h) - f(x_0)}{h}\\] We define the rate of change of \\(f(x)\\) at \\(x=x_0\\) to be the limit, as \\(h \\rightarrow 0\\), of the rate of change of \\(f(x)\\) as \\(x\\) changes from \\(x_0\\) to \\(x_0 + h\\). This is equivalent to the derivative \\(f&#39;(x_0)\\). Consider an example looking at the relationship between campaign spending and a candidate’s vote share in a congressional election: Rate of change \\(\\leadsto\\) return on vote share on dollars invested Instantaneous rate of change \\(\\leadsto\\) increase in vote share in response to infinitesimally small increase in spending A type of limit 2.5.5 Examples of derivatives Example 2.1 Suppose \\(f(x) = x^2\\) and consider \\(x_{0} = 1\\). Then, \\[ \\begin{aligned} \\lim_{x\\rightarrow 1}R(x) &amp; = \\lim_{x\\rightarrow 1} \\frac{x^2 - 1^2}{x - 1} \\\\ &amp; = \\lim_{x\\rightarrow 1} \\frac{(x- 1)(x + 1) }{ x- 1} \\\\ &amp; = \\lim_{x\\rightarrow 1} x + 1 \\\\ &amp; = 2 \\end{aligned} \\] Example 2.2 Suppose \\(f(x) = |x|\\) and consider \\(x_{0} = 0\\). Then, \\[ \\lim_{x\\rightarrow 0} R(x) = \\lim_{x\\rightarrow 0} \\frac{ |x| } {x} \\] \\(\\lim_{x \\rightarrow 0^{-}} R(x) = -1\\), but \\(\\lim_{x \\rightarrow 0^{+}} R(x) = 1\\). So, not differentiable at \\(0\\). 2.5.6 Continuity and derivatives \\(f(x) = |x|\\) is continuous but not differentiable. This is because the change is too abrupt. This suggests differentiability is a stronger condition. Theorem 2.8 Let \\(f:\\Re \\rightarrow \\Re\\) be differentiable at \\(x_{0}\\). Then \\(f\\) is continuous at \\(x_{0}\\). 2.5.7 What goes wrong? Consider the following piecewise function: \\[ \\begin{aligned} f(x) &amp; = x^{2} \\text{ for all } x \\in \\Re \\setminus 0 \\\\ f(x) &amp; = 1000 \\text{ for } x = 0 \\end{aligned} \\] Consider its derivative at 0. Then, \\[ \\begin{aligned} \\lim_{x \\rightarrow 0 } R(x) &amp; = \\lim_{x \\rightarrow 0 } \\frac{f(x) - 1000}{ x - 0 } \\\\ &amp;= \\lim_{x \\rightarrow 0 } \\frac{x^2}{x} - \\lim_{x \\rightarrow 0 } \\frac{1000}{x} \\end{aligned} \\] \\(\\lim_{x \\rightarrow 0 } \\frac{1000}{x}\\) diverges, so the limit doesn’t exist. 2.6 Calculating derivatives Rarely will we take a limit to calculate a derivative. Rather, rely on rules and properties of derivatives. Our strategy: Algebra theorems Some specific derivatives Work on problems 2.6.1 Derivative rules Suppose \\(a\\) is some constant, \\(f(x)\\) and \\(g(x)\\) are functions: \\[ \\begin{aligned} f(x) &amp;= x &amp; \\quad f^{&#39;}(x) &amp;= 1 \\\\ f(x) &amp;= a x^{k} &amp; \\quad f^{&#39;}(x) &amp;= (a) (k) x ^{k-1} \\\\ f(x) &amp;= e^{x } &amp; \\quad f^{&#39;} (x) &amp;= e^{x} \\\\ f(x) &amp;= \\sin(x) &amp; \\quad f^{&#39;} (x) &amp;= \\cos (x) \\\\ f(x) &amp;= \\cos(x) &amp; \\quad f^{&#39;} (x) &amp;= - \\sin(x) \\\\ \\end{aligned} \\] Suppose that \\(f\\) and \\(g\\) are functions that are differentiable at \\(x\\) and \\(k\\) is a scalar value. The following rules apply: Definition 2.12 (Constant rule) \\[\\left[k f(x)\\right]&#39; = k f&#39;(x)\\] Definition 2.13 (Sum rule) \\[\\left[f(x)\\pm g(x)\\right]&#39; = f&#39;(x)\\pm g&#39;(x)\\] Definition 2.14 (Product rule) \\[\\left[f(x)g(x)\\right]&#39; = f&#39;(x)g(x)+f(x)g&#39;(x)\\] Definition 2.15 (Quotient rule) \\[\\frac{f(x)}{g(x)}&#39; = \\frac{f&#39;(x)g(x)-f(x)g&#39;(x)}{[g(x)]^2}, g(x)\\neq 0\\] Definition 2.16 (Power rule) \\[\\left[x^k\\right]&#39; = k x^{k-1}\\] These “rules” become apparent by applying the definition of the derivative above to each of the things to be “derived”, but these come up so frequently that it is best to repeat until it is muscle memory. 2.6.2 Challenge problems Differentiate the following functions and evaluate at the specified value: \\(f(x)= x^3 + 5 x^2 + 4 x\\), at \\(x_{0} = 2\\) Click for the solution Power rule. \\[ \\begin{aligned} f&#39;(x) &amp;= 3x^2 + 10x + 4 \\\\ f&#39;(2) &amp;= 3 \\times 2^2 + 10 \\times 2 + 4 \\\\ &amp;= 3 \\times 4 + 10 \\times 2 + 4 \\\\ &amp;= 12 + 20 + 4 \\\\ &amp;= 36 \\end{aligned} \\] \\(f(x) = \\sin(x) x^3\\) at \\(x_{0} = 2\\) Click for the solution Application of the product rule and definition of the derivative of \\(\\sin(x)\\). \\[ \\begin{aligned} g(x) &amp;= \\sin(x) &amp;\\quad h(x) &amp;= x^3 \\\\ g&#39;(x) &amp;= \\cos(x) &amp;\\quad h&#39;(x) &amp;= 3x^2 \\end{aligned} \\] \\[ \\begin{aligned} f&#39;(x) &amp;= g&#39;(x) h(x) + g(x) h&#39;(x) \\\\ &amp;= \\cos(x) x^3 + \\sin(x) 3x^2 \\\\ &amp;= x^2 (x \\cos(x) + 3 \\sin(x)) \\\\ f&#39;(2) &amp;= 2^2 (2 \\cos(2) + 3 \\sin(2)) \\\\ &amp;= 4 (2 \\cos(2) + 3 \\sin(2)) \\\\ &amp;= 8 \\cos(2) + 12 \\sin(2) \\\\ &amp;\\approx 7.582 \\end{aligned} \\] \\(h(x) = \\dfrac{e^{x}}{x^3}\\) at \\(x_0 = 2\\) Click for the solution Application of the quotient rule and definition of the derivative of \\(e^x\\). \\[ \\begin{aligned} f(x) &amp;= e^x &amp;\\quad g(x) &amp;= x^3 \\\\ f&#39;(x) &amp;= e^x &amp;\\quad g&#39;(x) &amp;= 3x^2 \\end{aligned} \\] \\[ \\begin{aligned} h&#39;(x) &amp;= \\frac{f&#39;(x)g(x)-f(x)g&#39;(x)}{[g(x)]^2}, g(x)\\neq 0 \\\\ &amp;= \\frac{e^x x^3 - e^x 3x^2}{(x^3)^2} \\\\ &amp;= \\frac{e^x x^2 (x - 3)}{x^6} \\\\ &amp;= \\frac{e^x (x - 3)}{x^4}, g(x)\\neq 0 \\\\ h&#39;(2) &amp;= \\frac{e^2 (2 - 3)}{2^4} \\\\ &amp;= \\frac{-(e^2)}{16} \\\\ &amp;\\approx \\frac{-7.389}{16} \\\\ &amp;\\approx -0.462 \\end{aligned} \\] \\(h(x) = \\log (x) x^3\\) at \\(x_0 = e\\) Click for the solution Requires the product rule combined with power rule and knowledge of the derivative of \\(\\log(x)\\). \\[ \\begin{aligned} f(x) &amp;= \\log(x) &amp;\\quad g(x) &amp;= x^3 \\\\ f&#39;(x) &amp;= \\frac{1}{x} &amp;\\quad g&#39;(x) &amp;= 3x^2 \\end{aligned} \\] \\[ \\begin{aligned} h&#39;(x) &amp;= f&#39;(x)g(x) + f(x)g&#39;(x) \\\\ &amp;= \\frac{1}{x} \\times x^3 + \\log(x) \\times 3x^2 \\\\ &amp;= x^2 + 3x^2 \\log(x) \\\\ &amp;= x^2(1 + 3 \\log(x)) \\\\ h&#39;(e) &amp;= e^2(1 + 3 \\log(e)) \\\\ &amp;= e^2 (1 + 3 * 1) \\\\ &amp;= 4e^2 \\end{aligned} \\] 2.6.3 Composite functions As useful as the above rules are, many functions you’ll see won’t fit neatly in each case immediately. Instead, they will be functions of functions. For example, the difference between \\(x^2 + 1^2\\) and \\((x^2 + 1)^2\\) may look trivial, but the sum rule can be easily applied to the former, while it’s actually not obvious what do with the latter. Composite functions are formed by substituting one function into another and are denoted by \\[f \\circ g=f[g(x)]\\] To form \\(f[g(x)]\\), the range of \\(g\\) must be contained (at least in part) within the domain of \\(f\\). The domain of \\(f\\circ g\\) consists of all the points in the domain of \\(g\\) for which \\(g(x)\\) is in the domain of \\(f\\). For example, let \\(f(x)=\\log x\\) for \\(0&lt;x&lt;\\infty\\) and \\(g(x)=x^2\\) for \\(-\\infty&lt;x&lt;\\infty\\). Then \\[f\\circ g=\\log x^2, -\\infty&lt;x&lt;\\infty - \\{0\\}\\] Also \\[g\\circ f = [\\log x]^2, 0&lt;x&lt;\\infty\\] Notice that \\(f\\circ g\\) and \\(g\\circ f\\) are not the same functions. With the notation of composite functions in place, now we can introduce a helpful additional rule that will deal with a derivative of composite functions as a chain of concentric derivatives. 2.6.4 Chain rule Let \\(y=f\\circ g= f[g(x)]\\). The derivative of \\(y\\) with respect to \\(x\\) is \\[\\frac{d}{dx} \\{ f[g(x)] \\} = f&#39;[g(x)] g&#39;(x)\\] We can read this as: “the derivative of the composite function \\(y\\) is the derivative of \\(f\\) evaluated at \\(g(x)\\), times the derivative of \\(g\\).” The chain rule can be thought of as the derivative of the “outside” times the derivative of the “inside”, remembering that the derivative of the outside function is evaluated at the value of the inside function. 2.6.4.1 Examples of the chain rule Example 2.3 \\[ \\begin{aligned} h(x) &amp;= e^{2x} \\\\ g(x) &amp;= e^{x} \\\\ f(x) &amp;= 2x \\end{aligned} \\] So \\[h(x) = g(f(x)) = g(2x) = e^{2x}\\] Taking derivatives, we have \\[h^{&#39;}(x) = g^{&#39;}(f(x))f^{&#39;}(x) = e^{2x}2\\] Example 2.4 \\[ \\begin{aligned} h(x) &amp;= \\log(\\cos(x) ) \\\\ g(x) &amp;= \\log(x) \\\\ f(x) &amp;= \\cos(x) \\end{aligned} \\] So \\[h(x) = g(f(x)) = g( \\cos(x)) = \\log(\\cos(x))\\] \\[h^{&#39;}(x) = g^{&#39;}(f(x))f^{&#39;}(x) = \\frac{-1}{\\cos(x)} \\sin(x) = -\\tan (x)\\] 2.6.4.1.1 Generalized Power Rule The direct use of a chain rule is when the exponent of is itself a function, so the power rule could not have applied generally: If \\(f(x)=[g(x)]^p\\) for any rational number \\(p\\), \\[f^\\prime(x) =p[g(x)]^{p-1}g^\\prime(x)\\] 2.7 Derivatives for the exponential function and natural logarithms The exponential function is one of the most important functions in mathematics. We previously discussed common rules for exponents and logarithms. Here, we focus on the properties of their derivatives. 2.7.1 Derivative of exponential function The function \\(e^x\\) is continuous and differentiable in its domains, and its first derivative is \\[\\frac{d}{dx}(e^x) = e^x\\] Why is this so? According to the limit definition of a derivative:6 \\[ \\begin{aligned} \\frac{d}{dx}f(x) &amp; = \\lim\\limits_{h\\to 0} \\frac{f(x+h)-f(x)}{h} \\\\ &amp; = \\lim\\limits_{h\\to 0} \\frac{e^{x + h} - e^x}{h} \\end{aligned} \\] By the law of exponents, we can split the addition of exponents into multiplication of the same base: \\[\\frac{d}{dx}f(x) = \\lim\\limits_{h\\to 0} \\frac{e^x e^h - e^x}{h}\\] Factor out \\(e^x\\): \\[\\frac{d}{dx}f(x) = \\lim\\limits_{h\\to 0} \\frac{e^x(e^h - 1)}{h}\\] We can put \\(e^x\\) in front of the limit because it is a multiplicative constant (while it has a variable \\(x\\) term, the limit is as \\(h \\rightarrow 0\\), not \\(x \\rightarrow 0\\)): \\[\\frac{d}{dx}f(x) = e^x \\lim\\limits_{h\\to 0} \\frac{e^h - 1}{h}\\] As \\(h\\) approaches \\(0\\), the limit gets closer to \\(\\frac{0}{0}\\) which is an indeterminant form. If we visually inspect what is happening at that point: We can clearly see that as \\(x\\) approaches \\(0\\), the function is converging towards 1, even if it never actually gets there.7 We can therefore substitute into the equation: \\[ \\begin{aligned} \\frac{d}{dx}f(x) &amp; = e^x \\lim\\limits_{h\\to 0} \\frac{e^h - 1}{h} \\\\ &amp; = e^x (1) \\\\ &amp; = e^x \\end{aligned} \\] Therefore, \\(e^x\\) is itself the derivative of \\(e^x\\). Figure 2.3: Derivative of the Exponential Function 2.7.2 Derivative of the natural logarithm The natural logarithm of \\(x\\) is the logarithm to base \\(e\\) of \\(x\\), where \\(e\\) is defined as Euler’s number (\\(e^1 \\approx 2.7182818\\)): \\[y = \\log_e (x) \\iff x = e^y\\] There is a direct relationship between \\(e^x\\) and \\(\\log_e(x)\\) (aka \\(\\log\\) or \\(\\ln\\)): \\[ \\begin{aligned} e^{\\log(x)} &amp;= x \\, \\mbox{for every positive number} \\, x \\\\ \\log(e^y) &amp;= y \\, \\mbox{for every real number} \\, y \\\\ \\end{aligned} \\] In short, the natural logarithm is the inverse function of the exponential function. Figure 2.4: Exponential function and natural logarithm The derivative of a natural logarithm is \\[\\frac{d}{dx} \\log(x) = \\frac{1}{x}\\] This follows from the inverse function rule which states that for a monotonic function \\(f\\) and its inverse \\(g\\), their derivatives are related to each other by: \\[ \\begin{aligned} g&#39;(y) &amp;= \\frac{1}{f&#39;(x)} \\\\ \\frac{dx}{dy} &amp;= \\frac{1}{\\frac{dy}{dx}} \\end{aligned} \\] As well as from the fact that the exponential function is its own derivative. Let \\(y = \\log(x)\\). Then \\(x = e^y\\), \\(\\frac{dx}{dy} = e^y = x\\), and \\[\\frac{dy}{dx} = \\frac{1}{\\frac{dx}{dy}} = \\frac{1}{x}\\] Figure 2.5: Derivative of the Natural Log 2.7.3 Relevance of exponential functions and natural logarithm The exponential function is popular in economics for growth over time (e.g. compounding interest). Natural logarithms can be used for elasticity models, as well as transforming variables in regression models to appear more normally distributed. 2.8 Derivatives and properties of functions Derivatives are often used to optimize a function (tomorrow). But they also reveal average rates of change or crucial properties of functions. Here we want to introduce ideas, and hopefully make them less shocking when you see them in work. 2.8.1 Relative maxima, minima and derivatives Theorem 2.9 (Rolle's theorem) Suppose \\(f:[a, b] \\rightarrow \\Re\\). Suppose \\(f\\) has a relative maxima or minima on \\((a,b)\\) and call that \\(c \\in (a, b)\\). Then \\(f&#39;(c) = 0\\). Intuition: Proof. Consider (without loss of generalization) a relative maximum \\(c\\). Consider the left-hand and right-hand limits \\[ \\begin{aligned} \\lim_{x \\rightarrow c^{-}} \\frac{f(x) - f(c) }{x - c } &amp; \\geq 0 \\\\ \\lim_{x \\rightarrow c^{+}} \\frac{f(x) - f(c) } {x - c } &amp; \\leq 0 \\end{aligned} \\] But we also know that \\[ \\begin{aligned} \\lim_{x \\rightarrow c^{-}} \\frac{f(x) - f(c ) }{x - c } &amp; = f^{&#39;}(c) \\\\ \\lim_{x \\rightarrow c^{+}} \\frac{f(x) - f(c) } {x - c } &amp; = f^{&#39;}(c) \\end{aligned} \\] The only way, then, that \\(\\lim_{x \\rightarrow c^{-}} \\frac{f(x) - f(c) }{x -c} = \\lim_{x \\rightarrow c^{+}} \\frac{f(x) - f(c) } {x - c}\\) is if \\(f^{&#39;}(c) = 0\\). 2.8.2 Mean value theorem Rolle’s theorem is a special case of the mean value theorem, where \\(f&#39;(c) = 0\\). Theorem 2.10 (Mean value theorem) If \\(f:[a,b] \\rightarrow \\Re\\) is continuous on \\([a,b]\\) and differentiable on \\((a,b)\\), then there is a \\(c \\in (a,b)\\) such that \\[ f^{&#39;}(c) = \\frac{f(b) - f(a) } { b - a} \\] 2.8.3 Applications of the mean value theorem This will come up in a formal theory article. You’ll at least know where to look. It allows us to say lots of powerful stuff about functions, which is especially useful for approximating derivatives. Corollary 2.1 Suppose that \\(f:[a,b] \\rightarrow \\Re\\) is continuous on \\([a,b]\\) and differentiable on \\((a,b)\\). Then, If \\(f^{&#39;}(x) \\neq 0\\) for all \\(x \\in (a,b)\\) then \\(f\\) is 1-1 If \\(f^{&#39;}(x) = 0\\) then \\(f(x)\\) is constant If \\(f^{&#39;}(x)&gt; 0\\) for all \\(x \\in (a,b)\\) then then \\(f\\) is strictly increasing If \\(f^{&#39;}(x)&lt;0\\) for all \\(x \\in (a,b)\\) then \\(f\\) is strictly decreasing Let’s prove these in turn. Why? Because they are just applying ideas. 2.8.3.1 If \\(f^{&#39;}(x) \\neq 0\\) for all \\(x \\in (a,b)\\) then \\(f\\) is 1-1 A one-to-one function is a function for which every element of the range of the function corresponds to exactly one element of the domain. Proof. By way of contradiction, suppose that \\(f\\) is not 1-1. Then there is \\(x, y \\in (a,b)\\) such that \\(f(x) = f(y)\\). Then, \\[f&#39;(c) = \\frac{f(x) - f(y)}{x- y} = \\frac{0}{x -y} = 0\\] This means \\(f&#39; \\neq 0\\) for all \\(x\\)! 2.8.3.2 If \\(f^{&#39;}(x) = 0\\) then \\(f(x)\\) is constant Proof. By way of contradiction, suppose that there is \\(x, y \\in (a,b)\\) such that \\(f(x) \\neq f(y)\\). But then, \\[f&#39;(c) = \\frac{f(x) - f(y) } {x - y} \\neq 0\\] 2.8.3.3 If \\(f^{&#39;}(x)&gt; 0\\) for all \\(x \\in (a,b)\\) then then \\(f\\) is strictly increasing Proof. By way of contradiction, suppose that there is \\(x, y \\in (a,b)\\) with \\(y&lt;x\\) but \\(f(y)&gt;f(x)\\). But then, \\[f&#39;(c) = \\frac{f(x) - f(y) }{x - y } &lt; 0\\] Bonus: proof for strictly decreasing is the reverse of this 2.8.4 Extension to indeterminate form limits The mean value theorem generalizes to a form known as the Cauchy mean value theorem. Theorem 2.11 (Cauchy mean value theorem) Suppose \\(f\\) and \\(g\\) are differentiable functions and \\(a\\) and \\(b\\) are real numbers such that \\(a &lt; b\\). Suppose also that \\(g&#39;(x) \\neq 0\\) for all \\(x\\) such that \\(a &lt; x &lt; b\\). There exists a real number \\(c\\) such that \\(a &lt; c &lt; b\\) and \\[\\frac{f&#39;(c)}{g&#39;(c)} = \\frac{f(b) - f(a)}{g(b) - g(a)}\\] The ordinary mean value theorem is the special case where \\(g(x) = x\\) for all \\(x\\). This is extraordinarily helpful if we want to calculate the limit of a ratio \\[\\lim_{x \\rightarrow a} \\frac{f(x)}{g(x)}\\] where \\(f\\) and \\(g\\) are continuous functions. If \\(g(a) \\neq 0\\) then \\[\\lim_{x \\rightarrow a} \\frac{f(x)}{g(x)} = \\frac{f(a)}{g(a)}\\] If \\(g(a) = 0\\) and \\(f(a) \\neq 0\\), no limit exists. But in the case where \\(f(a) = g(a) = 0\\), we have what is known as an indeterminate form - a limit may or may not exist in this case. Examples of indeterminate forms include \\(\\frac{0}{0}\\) and \\(\\frac{\\infty}{\\infty}\\). We can use L’Hôpital’s Rule (derived from the Cauchy mean value theorem) to simplify the expression and solve for the limit. Theorem 2.12 (L'Hôpital's Rule) Suppose that \\(f(a) = g(a) = 0\\) and \\(g&#39;(x) \\neq 0\\) if \\(x\\) is close but not equal to \\(a\\). Then \\[\\lim_{x \\rightarrow a} \\frac{f(x)}{g(x)} = \\lim_{x \\rightarrow a} \\frac{f&#39;(x)}{g&#39;(x)}\\] provided the limit on the right-hand side exists. This follows from the Cauchy mean value thoerem. Using that theorem and our assumption that \\(f(a) = g(a) = 0\\), we see that given any \\(x\\) that is close but not equal to \\(a\\), there is a number \\(p\\) between \\(a\\) and \\(b\\) such that \\[ \\begin{aligned} \\frac{f(x) - f(a)}{g(x) - g(a)} &amp;= \\frac{f&#39;(p)}{g&#39;(p)} \\\\ \\frac{f(x) - 0}{g(x) - 0} &amp;= \\frac{f&#39;(p)}{g&#39;(p)} \\\\ \\frac{f(x)}{g(x)} &amp;= \\frac{f&#39;(p)}{g&#39;(p)} \\end{aligned} \\] Same as Cauchy mean value theorem, just replace \\(c\\) with \\(p\\). As \\(x\\) approaches \\(a\\), so does \\(p\\). Example 2.5 \\[\\lim_{x \\rightarrow 0} \\frac{(1 + x)^{1/3} - 1}{x - x^2}\\] Denote the numerator of the fraction by \\(f(x)\\) and the denominator by \\(g(x)\\). Then \\(f(0) = g(0) = 0\\), so the form is indeterminate. Now \\[ \\begin{aligned} f&#39;(x) &amp;= \\frac{1}{3} (1 + x)^{-2/3} \\\\ f&#39;(0) &amp;= \\frac{1}{3} (1)^{-2/3} = \\frac{1}{3} (1) = \\frac{1}{3} \\\\ g&#39;(x) &amp;= 1 - 2x \\\\ g&#39;(0) &amp;= 1 - 2(0) = 1 \\end{aligned} \\] \\[\\lim_{x \\rightarrow a} \\frac{f(x)}{g(x)} = \\lim_{x \\rightarrow a} \\frac{f&#39;(x)}{g&#39;(x)} = \\frac{1/3}{1} = \\frac{1}{3}\\] Example 2.6 \\[l = \\lim_{x \\rightarrow 0} \\frac{x - \\log(1 + x)}{x^2}\\] \\[ \\begin{aligned} f(x) &amp;= x - \\log(1 + x) \\\\ f&#39;(x) &amp;= 1 - \\frac{1}{1 + x} \\\\ g(x) &amp;= x^2 \\\\ g&#39;(x) &amp;= 2x \\end{aligned} \\] \\[ \\begin{aligned} L &amp;= \\lim_{x \\rightarrow 0} \\frac{1 - \\frac{1}{1 + x}}{2x} \\\\ &amp;= \\lim_{x \\rightarrow 0} \\frac{1}{2x} - \\frac{\\frac{1}{1 + x}}{2x} \\\\ &amp;= \\lim_{x \\rightarrow 0} \\frac{1}{2x} - \\frac{1}{2x(1 + x)} \\\\ &amp;= \\lim_{x \\rightarrow 0} \\frac{1(1 + x)}{2x(1 + x)} - \\frac{1}{2x(1 + x)} \\\\ &amp;= \\lim_{x \\rightarrow 0} \\frac{1(1 + x) - 1}{2x(1 + x)} \\\\ &amp;= \\lim_{x \\rightarrow 0} \\frac{1 + x - 1}{2x(1 + x)} \\\\ &amp;= \\lim_{x \\rightarrow 0} \\frac{x}{2x(1 + x)} \\\\ &amp;= \\lim_{x \\rightarrow 0} \\frac{1}{2(1 + x)} \\\\ &amp;= \\lim_{x \\rightarrow 0} \\frac{1}{2(1 + 0)} = \\frac{1}{2} \\end{aligned} \\] Letting \\(x \\rightarrow 0\\), we have \\(L = \\frac{1}{2}\\). Note that instead of simplifying the expression, we could use this approach iteratively. \\[\\lim_{x \\rightarrow a} \\frac{f(x)}{g(x)} = \\lim_{x \\rightarrow a} \\frac{f&#39;(x)}{g&#39;(x)} = \\lim_{x \\rightarrow a} \\frac{f&#39;&#39;(x)}{g&#39;&#39;(x)} = \\ldots\\] \\[ \\begin{aligned} f&#39;&#39;(x) &amp;= \\frac{1}{(1 +x)^{2}} \\\\ g&#39;&#39;(x) &amp;= 2 \\\\ \\lim_{x \\rightarrow 0} \\frac{f&#39;&#39;(x)}{g&#39;&#39;(x)} &amp;= \\frac{(1 + x)^{-2}}{2} = \\frac{1^{-2}}{2} = \\frac{1}{2} \\end{aligned} \\] References "],["critical-points.html", "Lecture 3 Critical points and approximation Learning objectives Supplemental readings 3.1 Intuition 3.2 Higher order derivatives 3.3 Critical points 3.4 Extrema 3.5 Framework for analytical optimization 3.6 Computational optimization procedures", " Lecture 3 Critical points and approximation Learning objectives Define critical points Calculate critical points via analytical methods Demonstrate optimization using maximum likelihood estimation Identify need for approximation methods for calculating critical points Explain and demonstrate root finding procedures using Newton-Raphson hill climber Demonstrate comptuational optimization using gradient descent Supplemental readings Chapters 8, 9.1-.2, 10, Pemberton and Rau (2011) OpenStax Calculus: Volume 1, ch 4 3.1 Intuition Recall Rolle’s Theorem: Theorem 3.1 (Rolle's theorem) Suppose \\(f:[a, b] \\rightarrow \\Re\\). Suppose \\(f\\) has a relative maxima or minima on \\((a,b)\\) and call that \\(c \\in (a, b)\\). Then \\(f&#39;(c) = 0\\). Intuition: Rolle’s theorem guarantee’s that, at some point, \\(f^{&#39;}(x) = 0\\). Think about the intuition given this theorem. What happens as we approach from the left? What happens as we approach from the right? 3.2 Higher order derivatives The first derivative is applying the definition of derivatives on the function, and it can be expressed as \\[f&#39;(x), ~~ y&#39;, ~~ \\frac{d}{dx}f(x), ~~ \\frac{dy}{dx}\\] We can keep applying the differentiation process to functions that are themselves derivatives. The derivative of \\(f&#39;(x)\\) with respect to \\(x\\), would then be \\[f&#39;&#39;(x)=\\lim\\limits_{h\\to 0}\\frac{f&#39;(x+h)-f&#39;(x)}{h}\\] and we can therefore call it the Second derivative: \\[f&#39;&#39;(x), ~~ y&#39;&#39;, ~~ \\frac{d^2}{dx^2}f(x), ~~ \\frac{d^2y}{dx^2}\\] Similarly, the derivative of \\(f&#39;&#39;(x)\\) would be called the third derivative and is denoted \\(f&#39;&#39;&#39;(x)\\). And by extension, the nth derivative is expressed as \\(\\frac{d^n}{dx^n}f(x)\\), \\(\\frac{d^ny}{dx^n}\\). \\[ \\begin{aligned} f(x) &amp;=x^3\\\\ f^{\\prime}(x) &amp;=3x^2\\\\ f^{\\prime\\prime}(x) &amp;=6x \\\\ f^{\\prime\\prime\\prime}(x) &amp;=6\\\\ f^{\\prime\\prime\\prime\\prime}(x) &amp;=0\\\\ \\end{aligned} \\] Earlier, we said that if a function is differentiable at a given point, then it must be continuous. Further, if \\(f&#39;(x)\\) is itself continuous, then \\(f(x)\\) is called continuously differentiable. All of this matters because many of our findings about optimization rely on differentiation, and so we want our function to be differentiable in as many layers. A function that is continuously differentiable infinitely is called smooth. Some examples include: \\[ \\begin{aligned} f(x) &amp;= x^2 \\\\ f(x) &amp;= e^x \\end{aligned} \\] 3.3 Critical points One important purpose of derivatives is to find critical points along a function. Critical points are points on the domain of a function where the function is either not differentiable or the derivative is equal to zero. 3.3.1 Inflection point For a given function \\(y = f(x)\\), a point \\((x^∗, y^∗)\\) is called an inflection point if the second derivative immediately on one side of the point is signed oppositely to the second derivative immediately on the other side. Graphically, this occurs if the tangent line switches sides of the function at the inflection point. 3.3.2 Concavity Concave up (convex) - for a segment of a function, every possible chord (line segment connecting points along \\(f(x)\\)) is above the function Concave down (concave) - for a segment of a function, every possible chord is below the function This can be verified both graphically and analytically using the second derivative. Where a function is twice differentiable and concave over some area, then the function is concave down where \\(f&#39;&#39;(x) &lt; 0\\) and concave up where \\(f&#39;&#39;(x) &gt; 0\\). Some functions are strictly concave up or concave down if the second derivative is constantly signed over the entire domain of \\(x\\). 3.3.2.0.1 Exponential function (concave up) \\[ \\begin{aligned} f(x) &amp; = e^{x} \\\\ f^{&#39;}(x) &amp; = e^{x} \\\\ f^{&#39;&#39;}(x) &amp; = e^{x} \\end{aligned} \\] 3.3.2.0.2 Natural log (concave down) \\[ \\begin{aligned} f(x) &amp; = \\log(x) \\\\ f^{&#39;}(x) &amp; = \\frac{1}{x} \\\\ f^{&#39;&#39;}(x) &amp; = -\\frac{1}{x^2} \\end{aligned} \\] 3.4 Extrema Extreme values are values of a function that are either the maximum or minimum value. Theorem 3.2 (Extreme value theorem) Suppose \\(f:[a, b] \\rightarrow \\Re\\). There exists numbers \\(c\\) and \\(d\\) in \\([a, b]\\) such that \\(f(c) \\ge f(x) \\ge f(d)\\quad\\forall x\\in [a,b]\\). That is, \\(f\\) must attain a maximum and a minimum, each at least once. These values can be locally or (potentially) globally across the entire domain of \\(f\\). 3.4.1 Minimum and maximum on the interval \\([0,5]\\) are located at the endpoints 3.4.2 Global maximum is located at \\(x=0\\) 3.4.3 Global minimum is located at \\(x= - \\frac{9}{2}\\) 3.4.4 A bunch of local minima and maxima 3.4.5 \\(x=0\\) is an inflection point that is neither a minimum nor a maximum (\\(f&#39;&#39;(x) = 0\\)) Also known as a saddle point. 3.5 Framework for analytical optimization Here we see how critical points can be used to find extrema and saddle points. Find \\(f&#39;(x)\\) Set \\(f&#39;(x)=0\\) and solve for \\(x\\). Call all \\(x_0\\) such that \\(f&#39;(x_0)=0\\) critical values Find \\(f&#39;&#39;(x)\\). Evaluate at each \\(x_0\\) If \\(f&#39;&#39;(x) &gt; 0\\), concave up, and therefore a local minimum If \\(f&#39;&#39;(x) &lt; 0\\), concave down, and therefore a local maximum If it’s the global maximum/minimum, it will produce the largest/smallest value for \\(f(x)\\) On a closed range along the domain, check the endpoints as well 3.5.1 Example: \\(f(x) = -x^2\\), \\(x \\in [-3, 3]\\) 3.5.1.1 Critical Value \\[ \\begin{eqnarray} f&#39;(x) &amp; = &amp; - 2 x \\\\ 0 &amp; = &amp; - 2 x^{*} \\\\ x^{*} &amp; = &amp; 0 \\end{eqnarray} \\] 3.5.1.2 Second Derivative \\[ \\begin{eqnarray} f^{&#39;}(x) &amp; = &amp; - 2x \\\\ f^{&#39;&#39;}(x) &amp; = &amp; - 2 \\end{eqnarray} \\] \\(f^{&#39;&#39;}(x)&lt; 0\\), local maximum 3.5.2 Example: \\(f(x) = x^3\\), \\(x \\in [-3, 3]\\) 3.5.2.1 Critical Value \\[ \\begin{eqnarray} f&#39;(x) &amp; = &amp; 3 x^2 \\\\ 0 &amp; = &amp; 3 (x^{*})^2 \\\\ x^{*} &amp; = &amp; 0 \\end{eqnarray} \\] 3.5.2.2 Second Derivative \\[ \\begin{eqnarray} f^{&#39;&#39;}(x) &amp; = &amp; 6x \\\\ f^{&#39;&#39;}(0) &amp; = &amp; 0 \\end{eqnarray} \\] Neither a minimum nor a maximum, it is a saddle point 3.5.3 Example: spatial model A large literature in Congress supposes legislators and policies can be situated in a policy space. Suppose legislator \\(i\\) and policies \\(x, i \\in \\Re\\). Define legislator \\(i\\)’s utility as, \\(U:\\Re \\rightarrow \\Re\\), \\[ \\begin{aligned} U_{i} (x) &amp; = - (x - \\mu)^{2} \\\\ U_{i}(x) &amp; = - x^2 + 2 x \\mu - \\mu^2 \\end{aligned} \\] What is \\(i\\)’s optimal policy over the range \\(x \\in [\\mu- 2, \\mu + 2]\\)? 3.5.3.1 First derivative \\[ \\begin{aligned} U_{i}^{&#39;} (x) &amp; = -2 (x - \\mu) \\\\ 0 &amp; = -2x^{*} + 2 \\mu \\\\ x^{*} &amp; = \\mu \\end{aligned} \\] 3.5.3.2 Second derivative test \\[U^{&#39;&#39;}_{i}(x) = -2 &lt;0 \\rightarrow \\text{Concave Down}\\] We call \\(\\mu\\) legislator \\(i\\)’s ideal point \\[ \\begin{aligned} U_{i}(\\mu) &amp; = - (\\mu - \\mu)^2 = 0 \\\\ U_{i}(\\mu - 2) &amp; = - (\\mu - 2 - \\mu)^2 = -4 \\\\ U_{i} (\\mu + 2) &amp; = - (\\mu + 2 - \\mu)^2 = -4 \\end{aligned} \\] The legislator maximizes her utility at \\(\\mu\\). 3.5.4 Example: Maximum likelihood estimation A likelihood function is a function for calculating the parameters of a statistical model, given specific observed data. As we’ll see in a few days, likelihood is related to, but not the same as, probability. Under frequentist inference: Probability is the plausibility of a random outcome occurring, given a model parameter value Likelihood is the plausibility of a model parameter(s) value, given specific observed value Generally in these problems the data is known but the parameters are unknown. In order to estimate the best values for the parameters, we can use optimization to maximize the function to find the values for the function located at the global maximum of the likelihood function. Here is an example likelihood function. We want to find the maximum likelihood estimate for the function:8 \\[ \\begin{aligned} f(\\mu) &amp; = \\prod_{i=1}^{N} \\exp( \\frac{-(Y_{i} - \\mu)^2}{ 2}) \\\\ &amp; = \\exp(- \\frac{(Y_{1} - \\mu)^2}{ 2}) \\times \\ldots \\times \\exp(- \\frac{(Y_{N} - \\mu)^2}{ 2}) \\\\ &amp; = \\exp( - \\frac{\\sum_{i=1}^{N} (Y_{i} - \\mu)^2} {2}) \\end{aligned} \\] \\[\\exp(x) \\equiv e^{x}\\] \\(\\exp()\\) is a more compact notation used commonly when the exponent is a fractional value. \\(\\prod_{i=1}^{N}\\) is the product over all observations. We can rewrite the function as the sum \\(\\sum_{i=1}^{N} (Y_{i} - \\mu)^2\\) because of the multiplicative exponents rule. This value will be very very very small (exponentiation to a negative exponent). Trying to find the maximum when \\(f(\\mu)\\) only has very slight changes across different values of \\(\\mu\\) is extremely difficult. However, suppose \\(f:\\Re \\rightarrow (0, \\infty)\\). If \\(x_{0}\\) maximizes \\(f\\), then \\(x_{0}\\) maximizes \\(\\log(f(x))\\). That is, we don’t need the exact value of the likelihood function, just the value for \\(\\mu\\) where it is maximized. That value \\(\\mu^{*}\\) will be the same for either function, so instead of maximizing the likelihood we’ll maximize the log-likelihood. \\[ \\begin{aligned} \\log f(\\mu) &amp; = \\log \\left( \\exp( - \\frac{\\sum_{i=1}^{N} (Y_{i} - \\mu)^2} {2}) \\right) \\\\ &amp; = - \\frac{\\sum_{i=1}^{N} (Y_{i} - \\mu)^2} {2} \\\\ &amp; = -\\frac{1}{2} \\left(\\sum_{i=1}^{N} Y_{i}^2 - 2\\mu \\sum_{i=1}^{N} Y_{i} + N\\times\\mu^2 \\right) \\\\ \\frac{ \\partial \\log f(\\mu) }{ \\partial \\mu } &amp; = -\\frac{1}{2} \\left( - 2\\sum_{i=1}^{N} Y_{i} + 2 N \\mu \\right) \\end{aligned} \\] \\(\\log{e^{f(x)}} = f(x)\\) Expand terms in the summation, then separate into separate terms \\(\\sum_{i = 1}^N \\mu^1 = N \\times \\mu^2\\) Calculate the derivative \\(\\sum_{i=1}^{N} Y_{i}^2\\) is a constant with respect to \\(\\mu\\) and drops out \\[ \\begin{aligned} 0 &amp; = -\\frac{1}{2} \\left( - 2 \\sum_{i=1}^{N} Y_{i} + 2 N \\mu^{*} \\right) \\\\ 0 &amp; = \\sum_{i=1}^{N} Y_{i} - N \\mu^{*} \\\\ N \\mu^{*} &amp; = \\sum_{i=1}^{N}Y_{i} \\\\ \\mu^{*} &amp; = \\frac{\\sum_{i=1}^{N}Y_{i}}{N} \\\\ \\mu^{*} &amp; = \\bar{Y} \\end{aligned} \\] Apply the second derivative test: \\[ \\begin{aligned} f^{&#39;}(\\mu ) &amp; = -\\frac{1}{2} \\left( - 2\\sum_{i=1}^{N} Y_{i} + 2 N \\mu \\right) \\\\ f^{&#39;}(\\mu ) &amp; = \\sum_{i=1}^{N} Y_{i} - N \\mu \\\\ f^{&#39;&#39;}(\\mu ) &amp; = -N \\end{aligned} \\] \\(-N&lt;0\\), so the function is concave down at this point and therefore it is a maximum. The arithmetic mean of \\(Y\\) is the maximum likelihood estimator for the variable. 3.6 Computational optimization procedures Analytic approaches can be difficult or impossible for most applications in the social sciences. Computational approaches simplify the problem through various forms of approximations. Different algorithms are available with benefits/drawbacks. Some examples include: Newton-Raphson - expensive Grid search - tedious Gradient descent - parallelizable 3.6.1 Newton-Raphson root finding Roots are values of the function \\(f(x)\\) where \\(f(x) = 0\\), where the function crosses the \\(x\\)-axis. Root solving is necessary to optimize a function since we first have to calculate the first derivative \\(f&#39;(x)\\), set it equal to 0, and solve for \\(x^{*}\\). However, this is not always a realistic method or easy to compute value. Instead, we can use general iterative procedures to approximate \\(x^{*}\\), with decent reliability. Newton’s method (also called Newton-Raphson or a Newton-Raphson hill climber) is one such procedure that relies on Taylor series expansion to iterate over a series of possible \\(x^{*}\\) values until converging on the final estimate. 3.6.1.1 Tangent lines Solving for \\(f(x) = 0\\) can be challenging when \\(f(x)\\) is a non-linear function. However, solving for \\(x\\) when \\(f(x)\\) is linear is relatively easy. If we can approximate the function \\(f(x)\\) using linear functions, the problem becomes less difficult. To start, we define the tangent at a point as the line through that point whose slope is the slope of the curve. The formula for a tangent line at \\(x_0\\) is: \\[g(x) = f^{&#39;}(x_{0}) (x - x_{0} ) + f(x_{0})\\] We’ll use the formula for a tangent line to generate updates. 3.6.1.2 Newton-Raphson method Suppose we have some initial guess \\(x_{0}\\). We’re going to approximate \\(f(x)\\) with the tangent line to generate a new guess: \\[ \\begin{aligned} g(x) &amp; = f^{&#39;}(x_{0})(x - x_{0} ) + f(x_{0} ) \\\\ 0 &amp; = f^{&#39;}(x_{0}) (x_{1} - x_{0}) + f(x_{0} ) \\\\ x_{1} &amp; = x_{0} - \\frac{f(x_{0}) }{f^{&#39;}(x_{0})} \\end{aligned} \\] The procedure is algorithmic and iterative because our initial guess for \\(x_1\\) will not be optimal. However, we can use the same procedure multiple times substituting the new value for \\(x_1\\) into the function as \\(x_0\\) and updating \\(x_1\\). Repeat this step sufficiently until \\(f(x_{j+1})\\) is sufficiently close to zero, then stop. 3.6.1.3 Example: \\(y = -x^2\\) Let’s learn Newton’s method using some arbitrary function. The only requirement we want to impose is that this function has to be twice differentiable (you will soon notice why). Say we want to optimize the following arbitrary function: \\[y = -x^2\\] The first derivative of this function is \\(\\frac{\\partial y}{\\partial x} = -2x\\). The second derivative is \\(\\frac{\\partial^2 y}{\\partial x^2} = -2\\) This function looks like this: We want to use Newton-Raphson to find the critical point: 3.6.1.3.1 Implementing Newton-Raphson Recall that given a starting value \\(x_0\\), we determine the next guess of the optimum of our function \\(f(x)\\) using the following equation: \\[x_1 = x_0 - \\frac{f&#39;(x_0)}{f&#39;&#39;(x_0)}\\] Here the base function is \\(f&#39;(x_0)\\) because we want to solve for the roots of \\(f&#39;(x)\\). Or, more generally, for the \\((n+1)^{th}\\) guess, use \\[x_{n+1} = x_n - \\frac{f&#39;(x_n)}{f&#39;&#39;(x_n)}\\] Given our function, \\[x_{n+1} = x_n - \\frac{-2x}{-2}\\] We want to continue looking for new guesses till the difference between a guess and the previous one is sufficiently small. For example, say we want to stick to a guess \\(x_n\\) if \\(\\mid x_n - x_{n-1} \\mid &lt; 0.0001\\). Let’s find the maximum of our function. Let’s start with a relatively high guess of \\(x_0 = 3\\): This gives us the maximum. If we plug it into the second derivative, we see that the value \\(-2\\) is a local maximum because the second derivative evaluated at this point is negative. Moreover, we find that the first derivative evaluated at this point (\\(-2\\)) is (very close to) zero. 3.6.1.4 Example: \\(y = x^3 + 2x^2 - 3x + 4\\) Now let’s consider the function \\[y = x^3 + 2x^2 - 3x + 4\\] The first derivative of this function is \\(\\frac{\\partial y}{\\partial x} = 3x^2 + 4x - 3\\). The second derivative is \\(\\frac{\\partial^2 y}{\\partial x^2} = 6x + 4\\) This function looks like this: We want to use Newton-Raphson to find the following two points (optima): Given our function, \\[x_{n+1} = x_n - \\frac{3x_n^2 + 4x_n - 3}{6x_n + 4}\\] Let’s start with an initial guess \\(x_0 = 10\\): This gives us the first point: \\(x \\approx 0.535\\). If we plug it into the second derivative, we see that the value is a local minimum because the second derivative evaluated at this point is positive (\\(7.211\\)). Moreover, we find that the first derivative evaluated at this point (\\(4.441\\times 10^{-16}\\)) is (very close to) zero. However this function also has a second critical value: a local maximum. Let’s try to find the second optimum by plugging a negative value (\\(x_0 = -10\\)) into our Newton-Raphson function: Again, we can plug in this point into second derivative to confirm that this is a local maximum (\\(-15.706\\)). The key point here is that Newton-Raphson is not guaranteed to find a global minimum/maximum, nor is it even guaranteed to find a specific critical point. When you use this procedure, you should initialize the algorithm with several different \\(x_0\\) to verify if they converge on the same or differing answers. 3.6.2 Grid search A grid search is an exhaustive search algorithm for maximizing/minimizing a function. It works by defining a specified set of \\(x_i\\), calculating \\(f(x_i)\\), and comparing all the resulting values to determine which is largest/smallest. Consider again the function \\[y = -x^2\\] We can evaluate the function for all \\(x \\in \\{ -2, -1.99, -1.98, \\ldots, 1.98, 1.99, 2 \\}\\) and determine which \\(x_i\\) generates the largest value. For a simple example such as this, the method works reasonably well. However it is also computationally inefficient as you have to calculate \\(f(x)\\) for all reasonable values of \\(x\\). If you do not have a reasonable starting point for your search, you have to search over an extremely large range of values. For this reason, grid searches – while simple – also tend to be disfavored. 3.6.3 Gradient descent Let’s start with the function9 \\[f(x) = 1.2 x^2 - 4.8 x + 8\\] We could solve this analytically: \\[ \\begin{aligned} f&#39;(x) &amp;= 2.4x - 4.8 \\\\ 0 &amp;= 2.4x - 4.8 \\\\ 4.8 &amp;= 2.4x \\\\ x &amp;= 2 \\end{aligned} \\] According to the second derivative test, \\[ \\begin{aligned} f&#39;&#39;(x) &amp;= 2.4 \\\\ f&#39;&#39;(2) &amp;= 2.4 \\end{aligned} \\] \\(f&#39;&#39;(2) &gt; 0\\), so \\(x=2\\) is a minimum. This is easy if the function is simple. If the function is complicated, we can use a technique called gradient descent. \\[ x_1 = x_0 - \\alpha f&#39;(x_0) \\] \\(f&#39;(x)\\) - first derivative (gradient) of the function \\(\\alpha\\) - learning rate (set manually) Like Newton’s method, gradient descent is an iterative algorithm. You repeat the process until the algorithm converges on a stable solution. Unlike Newton’s method, the rate of learning is controlled by the first derivative and the learning rate, not the second derivative \\(f&#39;&#39;(x)\\). Imagine gradient descent as when you’re at the top of a mountain and you want to get down to the very bottom, you have to choose two things. First the direction you wish to descend and second the size of the steps you wish to take. After choosing both of these things, you will keep on taking that step size and that direction until you reach the bottom. \\(\\alpha\\) corresponds to the size of the steps you wish to take and \\(f&#39;(x)\\) gives you the direction that you should take for your given formula. Note that in order for the formula to start calculating you will have to assign an initial value for \\(x\\). Compare this same function using gradient descent and a learning rate of \\(\\alpha = 0.1\\): The algorithm converges more slowly the smaller the learning rate. In many contexts, you will use a different learning rate. A learning rate that is too high can cause the algorithm to converge too quickly to a suboptimal solution (such as a local minima as opposed to a global minima), whereas a learning rate that is too small can cause the process to get stuck. Consider the example below with a higher learning rate: References "],["linear-algebra.html", "Lecture 4 Linear algebra Learning objectives Supplemental readings 4.1 Linear algebra 4.2 Points and vectors 4.3 Example: text analysis 4.4 Matricies 4.5 Example: neural networks 4.6 Matrix inversion 4.7 Determinant 4.8 Matrix decomposition Acknowledgements", " Lecture 4 Linear algebra Learning objectives Define vector and matrix Visualize vectors in multiple dimensions Demonstrate applicability of linear algebra to text analysis and cosine similarity Perform basic algebraic operations on vectors and matricies Generalize linear algebra to tensors and neural networks Define matrix inversion Demonstrate how to solve systems of linear equations using matrix inversion Define the determinant of a matrix Define matrix decomposition Explain singular value decomposition and demonstrate the applicability of matrix algebra to real-world problems Supplemental readings Chapters 11-12, 13.1-.3, Pemberton and Rau (2011) OpenStax Calculus: Volume 3, ch 2 OpenStax College Algebra, ch 7.5-.8 4.1 Linear algebra A matrix is a rectangular array of numbers arranged in rows and columns. Figure 4.1: Not that matrix. Many common statistical methods in the social sciences rely on data structured as matricies (e.g. ordinary least squares regression). As computational social science expands and data sources explode in complexity and scope, big data needs to be stored in processed in many higher dimensional spaces. Linear algebra is the algebra of matricies. It allows us to examine the geometry of high dimensional space, and expand calculus into functions with multiple variables. It is very important for regression/machine learning/deep learning methods you will encounter in your coursework and research. 4.2 Points and vectors 4.2.1 Points A point exists in a single dimension (in \\(\\Re^1\\)) \\(1\\) \\(\\pi\\) \\(e\\) An ordered pair exists in two dimensions (\\(\\Re^2 = \\Re \\times \\Re\\)) \\((1,2)\\) \\((0,0)\\) \\((\\pi, e)\\) An ordered triple in three dimensions (\\(\\Re^3 = \\Re \\times \\Re \\times \\Re\\)) \\((3.1, 4.5, 6.1132)\\) An ordered \\(n\\)-tuple in \\(n\\)-dimensions \\(R^n = \\Re \\times \\Re \\times \\ldots \\times \\Re\\) \\((a_{1}, a_{2}, \\ldots, a_{n})\\) 4.2.2 Vectors A point \\(\\mathbf{x} \\in \\Re^{n}\\) is an ordered n-tuple, \\((x_{1}, x_{2}, \\ldots, x_{n})\\). The vector \\(\\mathbf{x} \\in \\Re^{n}\\) is the arrow pointing from the origin \\((0, 0, \\ldots, 0)\\) to \\(\\mathbf{x}\\). 4.2.3 One dimensional example 4.2.4 Two dimensional example 4.2.5 Three dimensional example (Latitude, Longitude, Elevation) \\((1,2,3)\\) \\((0,1,2)\\) 4.2.6 \\(N\\)-dimensional example Individual campaign donation records \\[\\mathbf{x} = (1000, 0, 10, 50, 15, 4, 0, 0, 0, \\ldots, 2400000000)\\] U.S. counties’ proportion of vote for Donald Trump \\[\\mathbf{y} = (0.8, 0.5, 0.6, \\ldots, 0.2)\\] Run experiment, assess feeling thermometer of elected official \\[\\mathbf{t} = (0, 100, 50, 70, 80, \\ldots, 100)\\] 4.2.7 Examples of some basic arithmetic 4.2.7.1 Vector/scalar addition/multiplication Suppose: \\[ \\begin{aligned} \\mathbf{u} &amp; = (1, 2, 3, 4, 5) \\\\ \\mathbf{v} &amp; = (1, 1, 1, 1, 1) \\\\ k &amp; = 2 \\end{aligned} \\] Then, \\[ \\begin{aligned} \\mathbf{u} + \\mathbf{v} &amp; = (1 + 1, 2 + 1, 3+ 1, 4 + 1, 5+ 1) = (2, 3, 4, 5, 6) \\\\ k \\mathbf{u} &amp; = (2 \\times 1, 2 \\times 2, 2 \\times 3, 2 \\times 4, 2 \\times 5) = (2, 4, 6, 8, 10) \\\\ k \\mathbf{v} &amp; = (2 \\times 1,2 \\times 1,2 \\times 1,2 \\times 1,2 \\times 1) = (2, 2, 2, 2, 2) \\end{aligned} \\] 4.2.8 Linear dependence Expressions such as \\(\\mathbf{a} + \\mathbf{b}\\) and \\(2\\mathbf{a} - 3\\mathbf{b}\\) are examples of linear combinations of vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\). Generally, a linear combination of two vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) is a vector of the form \\(\\alpha \\mathbf{a} + \\beta\\mathbf{b}\\), where \\(\\alpha, \\beta\\) are scalars. This form extends to linear combinations of more than two vectors \\[\\alpha \\mathbf{a} + \\beta\\mathbf{b} + \\gamma\\mathbf{c} + \\delta\\mathbf{d} + \\ldots\\] Suppose we have a set of \\(k\\) \\(n\\)-vectors \\(\\mathbf{b}^1, \\mathbf{b}^2, \\ldots \\mathbf{b}^k\\). We say that the vectors \\(\\mathbf{b}^1, \\mathbf{b}^2, \\ldots \\mathbf{b}^k\\) are linearly independent if they are not linearly dependent; thus, \\(\\mathbf{b}^1, \\mathbf{b}^2, \\ldots \\mathbf{b}^k\\) are linearly independent if none of the vectors can be expressed as a linear combination of the others. 4.2.8.1 Example of linear dependence \\[\\mathbf{a} = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{c} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\\] It is obvious that \\(\\mathbf{b} = \\frac{1}{2} (\\mathbf{a} + \\mathbf{c})\\). Therefore the vectors \\(\\mathbf{a}, \\mathbf{b}, \\mathbf{c}\\) are linearly dependent. 4.2.8.2 Detecting linear dependence A simple criterion for linear dependence is the following: \\(\\mathbf{b}^1, \\mathbf{b}^2, \\ldots \\mathbf{b}^k\\) are linearly dependent if and only if there exist scalars \\(\\alpha_1, \\alpha_2, \\ldots, \\alpha_k\\) not all zero such that \\[\\alpha_1 \\mathbf{b}^1 + \\alpha_2 \\mathbf{b}^2 + \\ldots + \\alpha_k \\mathbf{b}^k = \\mathbf{0}\\] 4.2.8.2.1 Example 1 Consider the vectors \\[\\mathbf{a} = \\begin{bmatrix} 2 \\\\ 1 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} 4 \\\\ 1 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{c} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix}\\] Suppose there are scalars \\(\\alpha, \\beta, \\gamma\\) such that \\[\\alpha\\mathbf{a} + \\beta\\mathbf{b} + \\gamma\\mathbf{c} = \\mathbf{0}\\] We can express this as a system of equations: \\[ \\begin{aligned} 2\\alpha &amp;+ 4\\beta &amp;+ \\gamma &amp;= 0 \\\\ \\alpha &amp;+ \\beta &amp;+ \\gamma &amp;= 0 \\\\ 2\\alpha &amp;+ 3\\beta &amp;+ 2\\gamma &amp;= 0 \\end{aligned} \\] We can solve this system of equations by reducing it down until one equation has just a single term remaining. For instance, eliminate \\(\\alpha\\) from the second and third equations by subtracting from those equations suitable multiples of the first equation: \\[ \\begin{aligned} 2\\alpha &amp;+ 4\\beta &amp;+ \\gamma &amp;= 0 \\\\ &amp;- \\beta &amp;+ \\frac{1}{2}\\gamma &amp;= 0 \\\\ &amp;- \\beta &amp;+ \\gamma &amp;= 0 \\end{aligned} \\] Next, eliminate \\(\\beta\\) from the third equation by subtracting a suitable multiple of the second: \\[ \\begin{aligned} 2\\alpha &amp;+ 4\\beta &amp;+ \\gamma &amp;= 0 \\\\ &amp;- \\beta &amp;+ \\frac{1}{2}\\gamma &amp;= 0 \\\\ &amp; &amp;+ \\frac{1}{2} \\gamma &amp;= 0 \\end{aligned} \\] We can now use back-substitution to solve for \\(\\gamma\\), then plug in that value to solve for \\(\\beta\\) and \\(\\alpha\\). Since the output of each equation is \\(0\\), this reduces down to \\(\\gamma = \\beta = \\alpha = 0\\), so the vectors \\(\\mathbf{a}, \\mathbf{b}, \\mathbf{c}\\) are linearly independent. 4.2.8.2.2 Example 2 Consider the vectors \\[\\mathbf{a} = \\begin{bmatrix} 2 \\\\ 1 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix} 4 \\\\ 1 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{c} = \\begin{bmatrix} 2 \\\\ 2 \\\\ 3 \\end{bmatrix}\\] Suppose there are scalars \\(\\alpha, \\beta, \\gamma\\) such that \\[\\alpha\\mathbf{a} + \\beta\\mathbf{b} + \\gamma\\mathbf{c} = \\mathbf{0}\\] Express this as a system of equations: \\[ \\begin{aligned} 2\\alpha &amp;+ 4\\beta &amp;+ 2\\gamma &amp;= 0 \\\\ \\alpha &amp;+ \\beta &amp;+ 2\\gamma &amp;= 0 \\\\ 2\\alpha &amp;+ 3\\beta &amp;+ 3\\gamma &amp;= 0 \\end{aligned} \\] Let’s try to eliminate \\(\\alpha\\) in the last two equations by subtracting multiples of the first equation: \\[ \\begin{aligned} 2\\alpha &amp;+ 4\\beta &amp;+ 2\\gamma &amp;= 0 \\\\ &amp;- \\beta &amp;+ \\gamma &amp;= 0 \\\\ &amp;- \\beta &amp;+ \\gamma &amp;= 0 \\end{aligned} \\] We can cancel out the third equation to be \\(0 = 0\\), leaving us with \\[ \\begin{aligned} 2\\alpha &amp;+ 4\\beta &amp;+ 2\\gamma &amp;= 0 \\\\ &amp;- \\beta &amp;+ \\gamma &amp;= 0 \\end{aligned} \\] We could find the complete solution by assigning an arbitrary value to \\(\\gamma\\) and then finding \\(\\beta\\) and \\(\\alpha\\). However, simply putting \\(\\gamma = 1\\) leads to \\(\\beta = 1\\) and \\(\\alpha = -3\\). We have thus found non-zero values for \\(\\alpha, \\beta, \\gamma\\), not all zero, such that \\(\\alpha\\mathbf{a} + \\beta\\mathbf{b} + \\gamma\\mathbf{c} = \\mathbf{0}\\). Therefore the vectors \\(\\mathbf{a}, \\mathbf{b}, \\mathbf{c}\\) are linearly dependent. 4.2.9 Inner product The inner product, also known as the dot product, of two vectors results in a scalar (a single value). Suppose \\(\\mathbf{u} \\in \\Re^{n}\\) and \\(\\mathbf{v} \\in \\Re^{n}\\). The inner product \\(\\mathbf{u} \\cdot \\mathbf{v}\\) is the sum of the item-by-item products: \\[ \\begin{aligned} \\mathbf{u} \\cdot \\mathbf{v} &amp;= u_{1} v_{1} + u_{2}v_{2} + \\ldots + u_{n} v_{n} \\\\ &amp; = \\sum_{i=1}^{N} u_{i} v_{i} \\end{aligned} \\] Suppose \\(\\mathbf{u} = (1, 2, 3)\\) and \\(\\mathbf{v} = (2, 3, 1)\\). Then: \\[ \\begin{aligned} \\mathbf{u} \\cdot \\mathbf{v} &amp; = 1 \\times 2 + 2 \\times 3 + 3 \\times 1 \\\\ &amp; = 2+ 6 + 3 \\\\ &amp; = 11 \\end{aligned} \\] 4.2.10 Calculating vector length The standard measurement for length of a vector is known as the vector norm. To illustrate, consider a vector in \\(\\Re^2\\): The Pythagorean Theorem states that for a right triangle with sides length \\(a\\) and \\(b\\), the length of the hypotenuse \\(c = \\sqrt{a^2 + b^2}\\). So if \\(c\\) is a vector in \\(\\Re^2\\), we can directly apply the Pythagorean Theorem to calculate its length from the origin \\((0,0)\\). We are guaranteed a right triangle because \\(a\\) and \\(b\\) simply correspond to the first and second scalar values contained in \\(c\\) (i.e. \\((3, 4)\\)). This generalizes to \\(\\Re^n\\) as given by: \\[ \\begin{aligned} \\| \\mathbf{v}\\| &amp; = (\\mathbf{v} \\cdot \\mathbf{v} )^{1/2} \\\\ &amp; = (v_{1}^2 + v_{2}^{2} + v_{3}^{2} + \\ldots + v_{n}^{2} )^{1/2} \\end{aligned} \\] So to calculate the vector norm of a three-dimensional vector \\(\\mathbf{x} = (1,1,1)\\): \\[ \\begin{aligned} \\| \\mathbf{x}\\| &amp; = (\\mathbf{x} \\cdot \\mathbf{x} )^{1/2} \\\\ &amp; = (x_{1}^2 + x_{2}^{2} + x_{3}^{2})^{1/2} \\\\ &amp; = (1 + 1 + 1)^{1/2} \\\\ &amp;= \\sqrt{3} \\end{aligned} \\] 4.3 Example: text analysis Text analysis is a common research method in the social sciences. In order to computationally analyze documents, we need to store them in a meaningful format. This does not mean the format is human-readable, simply interpretable by a computer. In the most simplistic form, documents are represented as vectors, and each value counts the frequency a word appears in a given document. While we throw away information such as word order, we can represent the information in a mathematical fashion using a vector. a abandoned abc ability able about above abroad absorbed absorbing abstract 43 0 0 0 0 10 0 0 0 0 1 Mathematically, this is represented by the vector \\((43,0,0,0,0,10,\\dots)\\) Consider two hypothetical documents: \\[ \\begin{aligned} \\text{Doc1} &amp; = (1, 1, 3, \\ldots, 5) \\\\ \\text{Doc2} &amp; = (2, 0, 0, \\ldots, 1) \\\\ \\textbf{Doc1}, \\textbf{Doc2} &amp; \\in \\Re^{M} \\end{aligned} \\] where each vector is length \\(m\\). Linear algebra provides many potentially useful operations. For example, the inner product between documents is: \\[ \\begin{aligned} \\textbf{Doc1} \\cdot \\textbf{Doc2} &amp; = (1, 1, 3, \\ldots, 5) (2, 0, 0, \\ldots, 1)&#39; \\\\ &amp; = 1 \\times 2 + 1 \\times 0 + 3 \\times 0 + \\ldots + 5 \\times 1 \\\\ &amp; = 7 \\end{aligned} \\] The length of a document is: \\[ \\begin{aligned} \\| \\textbf{Doc1} \\| &amp; \\equiv \\sqrt{ \\textbf{Doc1} \\cdot \\textbf{Doc1} } \\\\ &amp; = \\sqrt{(1, 1, 3, \\ldots , 5) (1, 1, 3, \\ldots, 5)&#39; } \\\\ &amp; = \\sqrt{1^{2} +1^{2} + 3^{2} + 5^{2} } \\\\ &amp; = 6 \\end{aligned} \\] The cosine of the angle between the documents: \\[ \\begin{aligned} \\cos (\\theta) &amp; \\equiv \\left(\\frac{\\textbf{Doc1} \\cdot \\textbf{Doc2}}{\\| \\textbf{Doc1}\\| \\|\\textbf{Doc2} \\|} \\right) \\\\ &amp; = \\frac{7} { 6 \\times 2.24} \\\\ &amp; = 0.52 \\end{aligned} \\] Remember \\(\\cos(\\theta)\\) is the ratio of the length of the sides of a right triangle \\(\\dfrac{a}{c}\\). What is the purpose of this property? What does this value tell us? It tells us the similarity in vector space between the documents. Measuring similarity between documents is very useful. These methods have been used for: Assessing potential plagiarism Comparing similarity of legislative texts What properties should a similarity measure have? The maximum should be the document with itself The minimum should be documents which have no words in common (i.e. orthogonal) Increasing when more of the same words are used Normalized for document length 4.3.1 Measure 1: inner product Consider if we used the inner product to measure similarity. \\[(2,1) \\cdot (1,4) = 6\\] The problem is that the inner product is length dependent. Consider the same method calculated for the vector in purple: \\[(4,2) \\cdot (1,4) = 12\\] We get different measures of similarity, yet is the new document really that different? We want something more consistent that accounts for varying overall document length. 4.3.2 Measure 2: cosine similarity \\[ \\begin{aligned} (4,2) \\cdot (1,4) &amp;= 12 \\\\ \\mathbf{a} \\cdot \\mathbf{b} &amp;= \\|\\mathbf{a} \\| \\times \\|\\mathbf{b} \\| \\times \\cos(\\theta) \\\\ \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a} \\| \\times \\|\\mathbf{b} \\|} &amp;= \\cos(\\theta) \\end{aligned} \\] \\(\\cos(\\theta)\\) removes document length from the similarity measure. \\[ \\begin{aligned} \\cos (\\theta) &amp; \\equiv \\left(\\frac{\\textbf{Doc1} \\cdot \\textbf{Doc2}}{\\| \\textbf{Doc1}\\| \\|\\textbf{Doc2} \\|} \\right) \\\\ &amp; = \\frac{(2, 1) \\cdot (1, 4)} {\\| (2,1)\\| \\| (1,4) \\|} \\\\ &amp; = \\frac{6} {(\\sqrt{2^2 + 1^2}) (\\sqrt{1^2 + 4^2})} \\\\ &amp; = \\frac{6} {(\\sqrt{5}) (\\sqrt{17})} \\\\ &amp; \\approx 0.65 \\end{aligned} \\] \\[ \\begin{aligned} \\cos (\\theta) &amp; \\equiv \\left(\\frac{\\textbf{Doc3} \\cdot \\textbf{Doc2}}{\\| \\textbf{Doc3}\\| \\|\\textbf{Doc2} \\|} \\right) \\\\ &amp; = \\frac{(4,2) \\cdot (1, 4)} {\\| (24,2)\\| \\| (1,4) \\|} \\\\ &amp; = \\frac{12} {(\\sqrt{4^2 + 2^2}) (\\sqrt{1^2 + 4^2})} \\\\ &amp; = \\frac{12} {(\\sqrt{20}) (\\sqrt{17})} \\\\ &amp; \\approx 0.65 \\end{aligned} \\] This measure works for documents of any length with any unique number of words. 4.4 Matricies A matrix is a rectangular arrangement (array) of numbers defined by two axes (colloquially known as dimensions): Rows Columns Dimension is sometimes used interchangably to describe the length of a vector and the number of axes in an array. It can be somewhat confusing. A dimension could refer to the number of entries along a specific axis or the number of axes in the array (tensor). \\[ \\mathbf{A} = \\begin{array}{rrrr} a_{11} &amp; a_{12} &amp; \\ldots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\ldots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\ldots &amp; a_{mn} \\\\ \\end{array} \\] If \\(\\mathbf{A}\\) has \\(m\\) rows \\(n\\) columns we will say that \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix. Suppose \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are \\(m \\times n\\) matrices. Then \\(\\mathbf{X} = \\mathbf{Y}\\) if \\(x_{ij} = y_{ij}\\) for all \\(i\\) and \\(j\\). 4.4.0.1 Simple examples \\[ \\mathbf{X} = \\left[ \\begin{array}{rrr} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 1 &amp; 4 \\\\ \\end{array} \\right] \\] \\(\\mathbf{X}\\) is an \\(2 \\times 3\\) matrix. \\[ \\mathbf{Y} = \\left[ \\begin{array}{rr} 1 &amp; 2 \\\\ 3 &amp; 2 \\\\ 1 &amp; 4 \\\\ \\end{array} \\right] \\] \\(\\mathbf{Y}\\) is an \\(3 \\times 2\\) matrix. \\(\\mathbf{X} \\neq \\mathbf{Y}\\) because the dimensions are different. 4.4.1 Basic arithmetic 4.4.1.1 Addition 4.4.1.1.1 Matrix Suppose \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are \\(m \\times n\\) matrices. Then: \\[ \\begin{aligned} \\mathbf{X} + \\mathbf{Y} &amp; = \\begin{bmatrix} x_{11} &amp; x_{12} &amp; \\ldots &amp; x_{1n} \\\\ x_{21} &amp; x_{22} &amp; \\ldots &amp; x_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{m1} &amp; x_{m2} &amp; \\ldots &amp; x_{mn} \\\\ \\end{bmatrix} + \\begin{bmatrix} y_{11} &amp; y_{12} &amp; \\ldots &amp; y_{1n} \\\\ y_{21} &amp; y_{22} &amp; \\ldots &amp; y_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ y_{m1} &amp; y_{m2} &amp; \\ldots &amp; y_{mn} \\\\ \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} x_{11} + y_{11} &amp; x_{12} + y_{12} &amp; \\ldots &amp; x_{1n} + y_{1n} \\\\ x_{21} + y_{21} &amp; x_{22} + y_{22} &amp; \\ldots &amp; x_{2n} + y_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ x_{m1} + y_{m1} &amp; x_{m2} + y_{m2} &amp; \\ldots &amp; x_{mn} + y_{mn} \\\\ \\end{bmatrix} \\end{aligned} \\] 4.4.1.1.2 Scalar Suppose \\(\\mathbf{X}\\) is an \\(m \\times n\\) matrix and \\(k \\in \\Re\\). Then: \\[ \\begin{aligned} k \\mathbf{X} &amp; = \\begin{bmatrix} k x_{11} &amp; k x_{12} &amp; \\ldots &amp; k x_{1n} \\\\ k x_{21} &amp; k x_{22} &amp; \\ldots &amp; k x_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ k x_{m1} &amp; k x_{m2} &amp; \\ldots &amp; k x_{mn} \\\\ \\end{bmatrix} \\end{aligned} \\] 4.4.2 Transposition Matricies must be conformable in order to perform certain operations. For example, matrix addition requires matricies to possess the same number of rows \\(m\\) and columns \\(n\\) in order to add each element together. If the second matrix instead has \\(n\\) rows and \\(m\\) columns, we can transpose it to flip the dimensionality of the matrix. \\[ \\begin{aligned} \\mathbf{X} &amp; = \\begin{bmatrix} x_{11} &amp; x_{12} &amp; \\ldots &amp; x_{1n} \\\\ x_{21} &amp; x_{22} &amp; \\ldots &amp; x_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{m1} &amp; x_{m2} &amp; \\ldots &amp; x_{mn} \\\\ \\end{bmatrix} \\\\ \\mathbf{X}&#39; &amp; = \\begin{bmatrix} x_{11} &amp; x_{21} &amp; \\ldots &amp; x_{m1} \\\\ x_{12} &amp; x_{22} &amp; \\ldots &amp; x_{m2} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{1n} &amp; x_{2n} &amp; \\ldots &amp; x_{mn} \\end{bmatrix} \\end{aligned} \\] If \\(\\mathbf{X}\\) is an \\(m \\times n\\) then \\(\\mathbf{X}&#39;\\) is \\(n \\times m\\) If \\(\\mathbf{X} = \\mathbf{X}&#39;\\) then we say \\(\\mathbf{X}\\) is symmetric Only square matricies can be symmetric 4.4.3 Multiplication Matrix multiplication is extraordinarily useful for many computational problems, though somewhat tedious to calculate by hand. Suppose we have two matrices: \\[\\mathbf{X} = \\begin{bmatrix} 1 &amp; 1 \\\\ 1&amp; 1 \\\\ \\end{bmatrix} , \\quad \\mathbf{Y} = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\\\ \\end{bmatrix} \\] We will create a new matrix \\(\\mathbf{A}\\) by matrix multiplication: \\[ \\begin{aligned} \\mathbf{A} &amp; = \\mathbf{X} \\mathbf{Y} \\\\ &amp; = \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\\\ \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} 1 \\times 1 + 1 \\times 3 &amp; 1 \\times 2 + 1 \\times 4 \\\\ 1 \\times 1 + 1 \\times 3 &amp; 1 \\times 2 + 1 \\times 4\\\\ \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} 4 &amp; 6 \\\\ 4 &amp; 6 \\end{bmatrix} \\end{aligned} \\] 4.4.3.1 Algebraic properties Matricies must be conformable - that is, the number of columns in \\(\\mathbf{A}\\) must equal the number of rows in \\(\\mathbf{B}\\). If \\(\\mathbf{AB}\\) exists then we call the matricies conformable. Associative property: \\((\\mathbf{XY})\\mathbf{Z} = \\mathbf{X}(\\mathbf{YZ})\\) Additive distributive property: \\((\\mathbf{X} + \\mathbf{Y})\\mathbf{Z} = \\mathbf{XZ} + \\mathbf{YZ}\\) Zero property: \\(\\mathbf{X0} = 0\\) Order matters: \\(\\mathbf{XY} \\neq \\mathbf{YX}\\) Different from scalar multiplication: \\(xy = yx\\) 4.5 Example: neural networks Why should we care about vectors/matricies and arithmetic operations? What use are they to computational social science? I’m glad you asked! Vectors/matricies and linear algebra form the heart of a neural network. Figure 4.2: Source: Wikipedia Deep learning is the name we use for stacked neural networks; that is, networks composed of several layers. The layers are made of nodes. A node is just a place where computation happens, loosely patterned on a neuron in the human brain, which fires when it encounters sufficient stimuli.10 A node combines input from the data with a set of coefficients, or weights, that either amplify or dampen that input, thereby assigning significance to inputs with regard to the task the algorithm is trying to learn; e.g. which input is most helpful is classifying data without error? These input-weight products are summed and then the sum is passed through a node’s so-called activation function, to determine whether and to what extent that signal should progress further through the network to affect the ultimate outcome, say, an act of classification. If the signals passes through, the neuron has been “activated.” 4.5.1 How are neural networks used Self-driving cars Voice activated assistants Automatic machine translation Image recognition Detection of diseases 4.5.2 How are neural networks related to linear algebra? This section draws heavily from Francois (2017). I highly encourage this text (especially chapter 2) for an intuitive introduction to deep learning and its mathematical building blocks. A tensor is the core unit of data in deep learning. It is a generalization of vectors and matricies to higher-dimensions. Scalars (0D tensors) - a tensor containing a single number Vectors (1D tensors) - a tensor with a one-dimensional array of numbers Matricies (2D tensors) - a tensor with a two-dimensional array of numbers 3D tensors and higher-dimensional tensors - array of matricies 4.5.2.1 Tensor operations All transformations inside of a deep neural network are reduced to a handful of tensor operations, or generalizations of matrix operations (e.g. addition, multiplication). The key point is if you can do it with a matrix, you can do it with a tensor. 4.5.2.2 Geometric interpretation Deep learning is a chain of small tensor operations which are geometric transformations of the input data We could interpret it as a very complex geometric transformation in a high-dimensional space, implemented via a long series of simple steps Consider two pieces of paper - one red and one blue Stick them together and crumple them into a small ball The crumpled paper ball is the input data Each sheet of paper is a class of data We want to classify points on the paper as red or blue Deep learning is then the series of steps necessary to uncrumple the ball so that the two classes are cleanly separable again 4.5.2.3 Here is the linear algebra Mathematically, each layer in the network example transforms input data as follows: \\[\\mathbf{Y} = \\text{activation}(\\mathbf{W} \\cdot \\mathbf{X} + \\mathbf{B})\\] \\(\\mathbf{X}\\) - input tensor \\(\\mathbf{Y}\\) - output tensor \\(\\mathbf{W}, \\mathbf{B}\\) - weight/parameter tensors (attributes of the layer, determined through the optimization process) \\(\\text{activation}()\\) Modifies the input in a non-linear fashion to allow for non-linear relationships between features/independent variables in the data Key traits Non-linear Continuously differentiable Fixed range Common activation functions Rectified Linear Units (RELU) \\[R(z) = \\max(0, z)\\] Sigmoid function (aka logistic regression) \\[S(z) = \\frac{1}{1 + e^{-z}}\\] How do we determine the values for the weights and parameters? Come back next week when we discuss optimization. Each layer adjusts the tensor to create new, and ideally more clear, structures of the underlying data. A basic neural network may contain thousands, if not millions, or tensor operations. While you will not complete these by hand, it is important to understand the underlying principles when you seek to implement neural networks. 4.6 Matrix inversion Suppose \\(\\mathbf{X}\\) is an \\(n \\times n\\) matrix. We want to find the matrix \\(\\mathbf{X}^{-1}\\) such that \\[ \\mathbf{X}^{-1} \\mathbf{X} = \\mathbf{X} \\mathbf{X}^{-1} = \\mathbf{I} \\] where \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix. Why is this useful? Matrix inversion is necessary to: Solve systems of equations Perform linear regression Provide intuition about colinearity, fixed effects, and other relevant design matricies for social scientists. 4.6.1 Calculating matrix inversions Consider the following equations: \\[ \\begin{aligned} x_{1} + x_{2} + x_{3} &amp;= 0 \\\\ 0x_{1} + 5x_{2} + 0x_{3} &amp; = 5 \\\\ 0 x_{1} + 0 x_{2} + 3 x_{3} &amp; = 6 \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\mathbf{A} &amp;= \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 5 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 \\end{bmatrix} \\\\ \\mathbf{x} &amp;= (x_{1} , x_{2}, x_{3} ) \\\\ \\mathbf{b} &amp;= (0, 5, 6) \\end{aligned} \\] The system of equations are now, \\[\\mathbf{A}\\mathbf{x} =\\mathbf{b}\\] \\(\\mathbf{A}^{-1}\\) exists if and only if \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) has only one solution. 4.6.1.1 Definition Suppose \\(\\mathbf{X}\\) is an \\(n \\times n\\) matrix. We will call \\(\\mathbf{X}^{-1}\\) the inverse of \\(\\mathbf{X}\\) if \\[ \\mathbf{X}^{-1} \\mathbf{X} = \\mathbf{X} \\mathbf{X}^{-1} = \\mathbf{I} \\] If \\(\\mathbf{X}^{-1}\\) exists then \\(\\mathbf{X}\\) is invertible. If \\(\\mathbf{X}^{-1}\\) does not exist, then we will say \\(\\mathbf{X}\\) is singular. Note the key requirement: only square matricies are invertible. Solved via R: ## [,1] [,2] [,3] ## [1,] 1 -0.2 -0.333 ## [2,] 0 0.2 0.000 ## [3,] 0 0.0 0.333 4.6.2 When do inverses exist Inverses exist when the columns and rows are linearly independent - that is, there is not repeated information in the matrix. 4.6.2.1 Example 1 Consider \\(\\mathbf{v}_{1} = (1, 0, 0)\\), \\(\\mathbf{v}_{2} = (0,1,0)\\), \\(\\mathbf{v}_{3} = (0,0,1)\\) Can we write any of these vectors as a combination of the other vectors? No. 4.6.2.2 Example 2 Consider \\(\\mathbf{v}_{1} = (1, 0, 0)\\), \\(\\mathbf{v}_{2} = (0,1,0)\\), \\(\\mathbf{v}_{3} = (0,0,1)\\), \\(\\mathbf{v}_{4} = (1, 2, 3)\\). Can we write this as a combination of other vectors? \\[\\mathbf{v}_{4} = \\mathbf{v}_{1} + 2 \\mathbf{v}_{2} + 3\\mathbf{v}_{3}\\] So a matrix \\(\\mathbf{V}\\) containing these vectors as columns is not linearly independent, and therefore is noninvertible (also known as singular). 4.6.3 Inverting a \\(2 \\times 2\\) matrix If \\(\\mathbf{A} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix}\\) and \\(ad \\neq bc\\), then \\(\\mathbf{A}\\) is invertible and \\[\\mathbf{A}^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d &amp; -b \\\\ -c &amp; a \\end{bmatrix}\\] For example \\[ \\begin{aligned} \\mathbf{A} &amp;= \\begin{bmatrix} 9 &amp; 7 \\\\ 2 &amp; 1 \\end{bmatrix} \\\\ \\mathbf{A}^{-1} &amp;= \\frac{1}{(-5)} \\begin{bmatrix} 1 &amp; -7 \\\\ -2 &amp; 9 \\end{bmatrix} = \\begin{bmatrix} -0.2 &amp; 1.4 \\\\ 0.4 &amp; -1.8 \\end{bmatrix} \\end{aligned} \\] We can verify by \\[\\mathbf{A}^{-1} \\mathbf{A} = \\begin{bmatrix} 9 &amp; 7 \\\\ 2 &amp; 1 \\end{bmatrix} \\begin{bmatrix} -0.2 &amp; 1.4 \\\\ 0.4 &amp; -1.8 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} = \\mathbf{I}\\] 4.6.4 Inverting an \\(n \\times n\\) matrix We can use the process of Gauss-Jordan elimination to calculate the inverse of an \\(n \\times n\\) matrix. Gauss-Jordan elimination is a procedure to solve a system of equations. Using an augmented matrix, we apply a series of elementary row operations to modify the augmented matrix until we have a diagonal matrix on the left-hand side. Elementary row operations include: Exchanging two rows in the matrix Subtracting a multiple of one row from another row First we setup an augmented matrix \\([\\mathbf{A} \\; \\mathbf{I}]\\) which we reduce to the form \\([\\mathbf{D} \\; \\mathbf{B}]\\), where \\(\\mathbf{D}\\) is a diagonal matrix with no diagonal entry equal to \\(0\\). \\(\\mathbf{A}^{-1}\\) is then found by dividing each row of \\(\\mathbf{B}\\) by the corresponding diagonal entry of \\(\\mathbf{D}\\). 4.6.4.1 Example 1 For example, let us invert \\[\\mathbf{A} = \\begin{bmatrix} 2 &amp; 1 &amp; 2 \\\\ 3 &amp; 1 &amp; 1 \\\\ 3 &amp; 1 &amp; 2 \\end{bmatrix}\\] First setup the augmented matrix: \\[ \\left[ \\begin{array}{rrr|rrr} 2 &amp; 1 &amp; 2 &amp; 1 &amp; 0 &amp; 0 \\\\ 3 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 3 &amp; 1 &amp; 2 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right] \\] Next we substract \\(3/2\\) times the first row from each of the other rows to get: \\[ \\left[ \\begin{array}{rrr|rrr} 2 &amp; 1 &amp; 2 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; -1/2 &amp; -2 &amp; -3/2 &amp; 1 &amp; 0 \\\\ 0 &amp; -1/2 &amp; -1 &amp; -3/2 &amp; 0 &amp; 1 \\end{array} \\right] \\] Our next step is to add twice the second row to the first row, and to subtract the second row from the third row. We obtain: \\[ \\left[ \\begin{array}{rrr|rrr} 2 &amp; 0 &amp; -2 &amp; -2 &amp; 2 &amp; 0 \\\\ 0 &amp; -1/2 &amp; -2 &amp; -3/2 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; -1 &amp; 1 \\end{array} \\right] \\] Finally we add twice the third row to the first and second rows: \\[ \\left[ \\begin{array}{rrr|rrr} 2 &amp; 0 &amp; 0 &amp; -2 &amp; 0 &amp; 2 \\\\ 0 &amp; -1/2 &amp; 0 &amp; -3/2 &amp; -1 &amp; 2 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; -1 &amp; 1 \\end{array} \\right] \\] At this point \\(\\mathbf{B}\\) is a diagonal matrix with all non-zero elements on the diagonal. We obtain \\(\\mathbf{A}^{-1}\\) by dividing the first row of \\(\\mathbf{B}\\) by \\(2\\), the second row by \\(-\\frac{1}{2}\\), and the third row by \\(1\\): \\[\\mathbf{A}^{-1} = \\begin{bmatrix} -1 &amp; 0 &amp; 1 \\\\ 3 &amp; 2 &amp; -4 \\\\ 0 &amp; -1 &amp; 1 \\end{bmatrix}\\] 4.6.4.2 Example 2 Invert the matrix \\[\\mathbf{A} = \\begin{bmatrix} 1 &amp; 3 &amp; 5 \\\\ 1 &amp; 7 &amp; 5 \\\\ 5 &amp; 10 &amp; 15 \\end{bmatrix}\\] First setup the augmented matrix: \\[ \\left[ \\begin{array}{rrr|rrr} 1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 7 &amp; 5 &amp; 0 &amp; 1 &amp; 0 \\\\ 5 &amp; 10 &amp; 15 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right] \\] Subtract row 1 from row 2: \\[ \\left[ \\begin{array}{rrr|rrr} 1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 4 &amp; 0 &amp; -1 &amp; 1 &amp; 0 \\\\ 5 &amp; 10 &amp; 15 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right] \\] Subtract 5 × (row 1) from row 3: \\[ \\left[ \\begin{array}{rrr|rrr} 1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 4 &amp; 0 &amp; -1 &amp; 1 &amp; 0 \\\\ 0 &amp; -5 &amp; -10 &amp; -5 &amp; 0 &amp; 1 \\end{array} \\right] \\] Swap row 2 with row 3: \\[ \\left[ \\begin{array}{rrr|rrr} 1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; -5 &amp; -10 &amp; -5 &amp; 0 &amp; 1 \\\\ 0 &amp; 4 &amp; 0 &amp; -1 &amp; 1 &amp; 0 \\\\ \\end{array} \\right] \\] Add 4/5 × (row 2) to row 3: \\[ \\left[ \\begin{array}{rrr|rrr} 1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; -5 &amp; -10 &amp; -5 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; -8 &amp; -5 &amp; 1 &amp; \\frac{4}{5} \\\\ \\end{array} \\right] \\] Divide row 3 by -8: \\[ \\left[ \\begin{array}{rrr|rrr} 1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; -5 &amp; -10 &amp; -5 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; \\frac{5}{8} &amp; -\\frac{1}{8} &amp; -\\frac{1}{10} \\\\ \\end{array} \\right] \\] Add 10 × (row 3) to row 2: \\[ \\left[ \\begin{array}{rrr|rrr} 1 &amp; 3 &amp; 5 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; -5 &amp; 0 &amp; \\frac{5}{4} &amp; -\\frac{5}{4} &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; \\frac{5}{8} &amp; -\\frac{1}{8} &amp; -\\frac{1}{10} \\\\ \\end{array} \\right] \\] Subtract 5 × (row 3) from row 1: \\[ \\left[ \\begin{array}{rrr|rrr} 1 &amp; 3 &amp; 0 &amp; -\\frac{17}{8} &amp; \\frac{5}{8} &amp; \\frac{1}{2} \\\\ 0 &amp; -5 &amp; 0 &amp; \\frac{5}{4} &amp; -\\frac{5}{4} &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; \\frac{5}{8} &amp; -\\frac{1}{8} &amp; -\\frac{1}{10} \\\\ \\end{array} \\right] \\] Divide row 2 by -5: \\[ \\left[ \\begin{array}{rrr|rrr} 1 &amp; 3 &amp; 0 &amp; -\\frac{17}{8} &amp; \\frac{5}{8} &amp; \\frac{1}{2} \\\\ 0 &amp; 1 &amp; 0 &amp; -\\frac{1}{4} &amp; \\frac{1}{4} &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; \\frac{5}{8} &amp; -\\frac{1}{8} &amp; -\\frac{1}{10} \\\\ \\end{array} \\right] \\] Subtract 3 × (row 2) from row 1: \\[ \\left[ \\begin{array}{rrr|rrr} 1 &amp; 0 &amp; 0 &amp; -\\frac{11}{8} &amp; -\\frac{1}{8} &amp; \\frac{1}{2} \\\\ 0 &amp; 1 &amp; 0 &amp; -\\frac{1}{4} &amp; \\frac{1}{4} &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; \\frac{5}{8} &amp; -\\frac{1}{8} &amp; -\\frac{1}{10} \\\\ \\end{array} \\right] \\] \\[\\mathbf{A}^{-1} = \\begin{bmatrix} -\\frac{11}{8} &amp; -\\frac{1}{8} &amp; \\frac{1}{2} \\\\ -\\frac{1}{4} &amp; \\frac{1}{4} &amp; 0 \\\\ \\frac{5}{8} &amp; -\\frac{1}{8} &amp; -\\frac{1}{10} \\end{bmatrix}\\] We can simplify by factoring out an appropriate term: \\[\\mathbf{A}^{-1} = \\frac{1}{40} \\begin{bmatrix} -55 &amp; -5 &amp; 20 \\\\ -10 &amp; 10 &amp; 0 \\\\ 25 &amp; -5 &amp; -4 \\end{bmatrix}\\] 4.6.5 Application to regression analysis In methods classes you learn about linear regression. For each \\(i\\) (individual) we observe covariates \\(x_{i1}, x_{i2}, \\ldots, x_{ik}\\) and dependent variable \\(Y_{i}\\). Then, \\[ \\begin{aligned} Y_{1} &amp; = \\beta_{0} + \\beta_{1} x_{11} + \\beta_{2} x_{12} + \\ldots + \\beta_{k} x_{1k} \\\\ Y_{2} &amp; = \\beta_{0} + \\beta_{1} x_{21} + \\beta_{2} x_{22} + \\ldots + \\beta_{k} x_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ Y_{i} &amp; = \\beta_{0} + \\beta_{1} x_{i1} + \\beta_{2} x_{i2} + \\ldots + \\beta_{k} x_{ik} \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ Y_{n} &amp; = \\beta_{0} + \\beta_{1} x_{n1} + \\beta_{2} x_{n2} + \\ldots + \\beta_{k} x_{nk} \\end{aligned} \\] \\(\\mathbf{x}_{i} = (1, x_{i1}, x_{i2}, \\ldots, x_{ik})\\) \\(\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1}\\\\\\mathbf{x}_{2}\\\\ \\vdots \\\\ \\mathbf{x}_{n} \\end{bmatrix}\\) \\(\\boldsymbol{\\beta} = (\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{k} )\\) \\(\\mathbf{Y} = (Y_{1}, Y_{2}, \\ldots, Y_{n})\\) Then we can write \\[ \\begin{aligned} \\mathbf{Y} &amp;= \\mathbf{X}\\mathbf{\\beta} \\\\ \\mathbf{X}^{&#39;} \\mathbf{Y} &amp;= \\mathbf{X}^{&#39;} \\mathbf{X} \\mathbf{\\beta} \\\\ (\\mathbf{X}^{&#39;}\\mathbf{X})^{-1} \\mathbf{X}^{&#39;} \\mathbf{Y} &amp;= (\\mathbf{X}^{&#39;}\\mathbf{X})^{-1}\\mathbf{X}^{&#39;} \\mathbf{X} \\mathbf{\\beta} \\\\ (\\mathbf{X}^{&#39;}\\mathbf{X})^{-1} \\mathbf{X}^{&#39;} \\mathbf{Y} &amp;=\\mathbf{\\beta} \\end{aligned} \\] Pre-multiply both sides by \\(\\mathbf{X}&#39;\\) Pre-multiply both sides by \\((\\mathbf{X}^{&#39;}\\mathbf{X})^{-1}\\) \\((\\mathbf{X}^{&#39;}\\mathbf{X})^{-1}\\mathbf{X}^{&#39;} \\mathbf{X} = \\mathbf{I}\\) This depends on \\((\\mathbf{X}^{&#39;}\\mathbf{X})^{-1}\\) being invertible. If this is true, we can calculate the values for \\(\\boldsymbol{\\beta}\\). If not, then we cannot. When might this occur? We’ll see some occurences of this in today’s and future problem sets. 4.6.6 Application to solving systems of equations: tax benefits of charitable contributions Suppose a company earns before-tax profits of $100,000.11 It has agreed to contribute 10% of its after-tax profits to the Red Cross Relief Fund. It must pay a state tax of 5% of its profits (after the Red Cross donation) and a federal tax of 40% of its profits (after the donation and state taxes are paid). How much does the company pay in state taxes, federal taxes, and Red Cross donation? Without a model, this is a difficult problem because each payment takes into consideration the other payments. However, if we write out the linear equations which describe these deductions and payments, then we can understand the relationships between these payments and solve it in a straightforward manner. Let \\(C\\), \\(S\\), and \\(F\\) represent the amounts of the charitable contributin, state tax, and federal tax, respectively. After-profits are \\(\\$100{,}000 - (S + F)\\), so \\(C = 0.10 \\times (100{,}000 - (S + F))\\). We can write this as \\[C + 0.1S + 0.1F = 10{,}000\\] putting all the variables on one side. The statement that state tax is 5% of the profits net of the donation becomes \\(S = 0.05 \\times (100{,}000 - C)\\), which is \\[0.05C + S = 5{,}000\\] Federal taxes are 40% of the profit after deducting \\(C\\) and \\(S\\), this relationship is expressed as \\(F = 0.40 \\times [100{,}000 - (C+S)]\\), or \\[0.4C + 0.4S + F = 40{,}000\\] We can summarize these payments in a single system of linear equations: \\[ \\begin{aligned} C &amp; + &amp; 0.1S &amp; + &amp; 0.1F &amp;= 10{,}000 \\\\ 0.05C &amp; + &amp; S &amp; &amp;&amp;= 5{,}000 \\\\ 0.4C &amp; + &amp; 0.4S &amp; + &amp; F &amp;= 40{,}000 \\end{aligned} \\] We could substitute the middle equation for \\(S\\) in terms of \\(C\\) and solve the resulting system. Or, we can use matrix inversion: ## [1] 5956 4702 35737 4.7 Determinant The determinant of a square matrix is a single number summary. The determinant uses all of the values of a square matrix to provide a summary of structure. Unfortunately it is rather complicated to calculate for larger matricies. First let’s consider how to calculate the determinant of a \\(2 \\times 2\\) matrix, which is the difference in diagonal products. \\[\\det(\\mathbf{X}) = \\mid \\mathbf{X} \\mid = \\left| \\begin{matrix} x_{11} &amp; x_{12} \\\\ x_{21} &amp; x_{22} \\end{matrix} \\right| = x_{11}x_{22} - x_{12}x_{21}\\] Some simple examples include \\[\\left| \\begin{matrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{matrix} \\right| = (1)(4) - (2)(3) = 4 - 6 = -2\\] \\[\\left| \\begin{matrix} 10 &amp; \\frac{1}{2} \\\\ 4 &amp; 1 \\end{matrix} \\right| = (10)(1) - \\left( \\frac{1}{2} \\right)(4) = 10 - 2 = 8\\] \\[\\left| \\begin{matrix} 2 &amp; 3 \\\\ 6 &amp; 9 \\end{matrix} \\right| = (2)(9) - (3)(6) = 18 - 18 = 0\\] The last case, where the determinant is \\(0\\), is an important case which we shall see shortly. Unfortunately calculating determinants gets much more involved with square matricies larger than \\(2 \\times 2\\). First we need to define a submatrix. The submatrix is simply a form achieved by deleting rows and/or columns of a matrix, leaving the remaining elements in their respective places. So for the matrix \\(\\mathbf{X}\\), notice the following submatricies: \\[ \\mathbf{X} = \\begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} &amp; x_{14} \\\\ x_{21} &amp; x_{22} &amp; x_{23} &amp; x_{24} \\\\ x_{31} &amp; x_{32} &amp; x_{33} &amp; x_{34} \\\\ x_{41} &amp; x_{42} &amp; x_{43} &amp; x_{44} \\\\ \\end{bmatrix} \\] \\[ \\mathbf{X}_{[11]} = \\begin{bmatrix} x_{22} &amp; x_{23} &amp; x_{24} \\\\ x_{32} &amp; x_{33} &amp; x_{34} \\\\ x_{42} &amp; x_{43} &amp; x_{44} \\\\ \\end{bmatrix}, \\mathbf{X}_{[24]} = \\begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} \\\\ x_{31} &amp; x_{32} &amp; x_{33} \\\\ x_{41} &amp; x_{42} &amp; x_{43} \\\\ \\end{bmatrix} \\] To generalize for further \\(n \\times n\\) matricies, the determinant can be calculated as \\[\\mid \\mathbf{X} \\mid = \\sum_{j=1}^n (-1)^{i+j} x_{ij} \\mid\\mathbf{X}_{[ij]}\\mid\\] where the \\(ij\\)th minor of \\(\\mathbf{X}\\) for \\(x_{ij}\\), \\(\\mid\\mathbf{X}_{[ij]}\\mid\\), is the determinant of the \\((n - 1) \\times (n - 1)\\) submatrix that results from taking the \\(i\\)th row and \\(j\\)th column out. The cofactor of \\(\\mathbf{X}\\) is the minor signed as \\((-1)^{i+j} x_{ij} \\mid\\mathbf{X}_{[ij]}\\mid\\). To calculate the determinant we cycle recursively through the columns and take sums with a formula that multiplies the cofactor by the determining value. For instance, here is the method applied to a \\(3 \\times 3\\) matrix: \\[ \\begin{aligned} \\mathbf{X} &amp;= \\begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} \\\\ x_{21} &amp; x_{22} &amp; x_{23} \\\\ x_{31} &amp; x_{32} &amp; x_{33} \\\\ \\end{bmatrix} \\\\ \\det(\\mathbf{X}) &amp;= (+1)x_{11} \\left| \\begin{matrix} x_{22} &amp; x_{23} \\\\ x_{32} &amp; x_{33} \\\\ \\end{matrix} \\right| +(-1)x_{12} \\left| \\begin{matrix} x_{21} &amp; x_{23} \\\\ x_{31} &amp; x_{33} \\\\ \\end{matrix} \\right| + (+1)x_{13} \\left| \\begin{matrix} x_{21} &amp; x_{22} \\\\ x_{31} &amp; x_{32} \\\\ \\end{matrix} \\right| \\end{aligned} \\] Now the problem is simplified because the subsequent three determinant calculations are on \\(2 \\times 2\\) matricies. 4.7.1 Relevance of the determinant Remember how we wanted to invert a \\(2 \\times 2\\) matrix previously? \\[\\mathbf{A}^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d &amp; -b \\\\ -c &amp; a \\end{bmatrix}\\] \\(\\frac{1}{ad - bc}\\) is the formula for the determinant of a \\(2 \\times 2\\) matrix! Recall that non-invertible (singular) matricies are square matricies which have columns or rows that are linearly dependent. Well would it surprise you to know that singular matricies also have the unique property whereby their determinant is \\(0\\). This is also important as we move into eigenvectors and diagonalization. 4.8 Matrix decomposition Matrix decomposition is a factorization of a matrix into a product of matricies. That is, a matrix can be decomposed into more efficient matricies depending on the calculations needing to be performed. LU decomposition applies to square matricies: \\[\\mathbf{A} = \\mathbf{L}\\mathbf{U}\\] where \\(\\mathbf{L}\\) is a lower triangular matrix and \\(\\mathbf{U}\\) is an upper triangular matrix. The benefit of this decomposition is for solving a system of linear equations \\(\\mathbf{A}\\mathbf{x} =\\mathbf{b}\\) because they reduce the number of steps necessary in Gauss-Jordan elimination to invert the matrix. Hence, more computationally efficient. LU decomposition only works on square matricies. But there are many other forms of decomposition used for solving systems of linear equations, or more commonly in the social sciences for dimension reduction. 4.8.1 Dimension reduction Dimension reduction refers to decreasing the number of dimensions in your dataset. There are a couple reasons you might do this: You want to visualize the data but you have a lot of variables. You could generate something like a scatterplot matrix, but once you have more than a handful of variables even these become difficult to interpret. You want to use the variables in a supervised learning framework, but reduce the total number of predictors to make the estimation more efficient. In either case, the goal is to reduce the dimensionality of the data by identifying a smaller number of representative variables/vectors/columns that collectively explain most of the variability in the original dataset. There are several methods available for performing such a task. First we will examine an example of applying dimension reduction techniques to summarize roll-call voting in the United States. 4.8.1.1 Application: DW-NOMINATE In the 1990s, dimension reduction techniques revolutionized the study of U.S. legislative politics. Measuring the ideology of legislators prior to this point was difficult because there was no method for locating legislators along an ideological spectrum (liberal-conservative) in a manner that allowed comparisons over time. That is, how liberal was a Democrat in 1870 compared to a Democrat in 1995? Additionally, supposed you wanted to predict how a legislator would vote on a given bill. Roll-call votes record individual legislator behavior, so you could use past votes to predict future ones. But there have been tens of thousands of recorded votes over the course of the U.S. Congress. Even in a given term of Congress, the Senate may cast hundreds of recorded votes. But there are only 100 senators (at present), and you cannot estimate a regression model when your number of predictors \\(p\\) is larger than your number of observations \\(n\\). We need some method for reducing the dimensionality of this data to a handful of variables which explain as much of the variation in roll-call voting as possible. Multidimensional scaling techniques can be used to perform this feat. The technical details of this specific application are beyond the scope of this class, but Keith Poole and Howard Rosenthal developed a specific procedure called NOMINATE to reduce the dimensionality of the data. Rather than using \\(p\\) predictors to explain or predict individual legislator’s roll-call votes, where \\(p\\) is the total number of roll-call votes in the recorded history of the U.S. Congress, Poole and Rosenthal examined the similarity of legislators’ votes in a given session of Congress and over time to identify two major dimensions to roll-call voting in the U.S. Congress. That is, roll-call votes in Congress can generally be explained by two variables that can be estimated for every past and present member of Congress. The two dimensions do not have any inherent substantive interpretation, but by graphically examining the two dimensions, it becomes clear that they represent two specific factors in legislative voting: First dimension - political ideology. This dimension appears to represent political ideology on the liberal-conservative spectrum. Positive values on this dimension refer to increasingly conservative voting patterns, and negative values refer to increasingly liberal voting patterns. Second dimension - “issue of the day”. This dimension appears to pick up on attitudes that are salient at different points in the nation’s history. They could be regional differences (Southern vs. non-Southern states), or attitudes towards specific policy issues (i.e. slavery). This data can be used for a wide range of research questions. For example, we could use it to assess the degree of polarization in the U.S. Congress over time: Figure 4.3: Source: Polarization in Congress Figure 4.4: Source: Polarization in Congress 4.8.2 Singular value decomposition Singular-Value Decomposition, or SVD, is a matrix decomposition method for reducing a matrix to its constitutent parts in order to make subsequent matrix calculations simpler. Unlike LU decomposition, SVD works with any rectangular matrix (not just square matricies). Suppose \\(\\mathbf{M}\\) is an \\(m \\times n\\) matrix. There exists a factorization of the form \\[\\mathbf{M} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^{*}\\] where \\(\\mathbf{U}\\) is an \\(m \\times n\\) matrix \\(\\boldsymbol{\\Sigma}\\) is an \\(n \\times n\\) diagonal matrix \\(\\mathbf{V}^{*}\\) is the transpose of an \\(n \\times n\\) matrix The diagonal entries \\(\\sigma_i\\) of \\(\\boldsymbol{\\Sigma}\\) are known as the singular values of \\(\\mathbf{M}\\). The columns of \\(\\mathbf{U}\\) are called the left-singular vectors of \\(\\mathbf{M}\\), and the columns of V are called the right-singular vectors of \\(\\mathbf{M}\\). 4.8.2.1 Image compression Digital images can be compressed using this technique. The image is treated as a matrix of pixels with corresponding color values and is decomposed into smaller ranks (i.e. columns) that retain only the essential information that comprises the image.12 The picture of the lion can be stored as an 600 by 337 matrix, where each value is a number between 0 and 1 that indicates how white or black the pixel should appear. ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.361 0.369 0.381 0.393 0.403 ## [2,] 0.365 0.373 0.385 0.397 0.407 ## [3,] 0.369 0.377 0.389 0.399 0.411 ## [4,] 0.377 0.385 0.395 0.407 0.420 ## [5,] 0.388 0.391 0.403 0.416 0.424 SVD of this matrix results in 3 new matricies:13 \\(\\mathbf{U}\\) ## [,1] [,2] [,3] [,4] [,5] ## [1,] -0.0398 -0.0291 -0.02032 0.019709 -0.01329 ## [2,] -0.0405 -0.0150 -0.00198 0.000273 -0.00208 ## [3,] -0.0396 -0.0186 -0.01972 0.020905 0.01126 ## [4,] -0.0390 -0.0264 -0.02890 0.039385 0.01012 ## [5,] -0.0398 -0.0300 -0.03199 0.037500 0.00553 Matrix size: \\((600, 337)\\) \\(\\boldsymbol{\\Sigma}\\) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 193 0.0 0.0 0 0.0 ## [2,] 0 29.2 0.0 0 0.0 ## [3,] 0 0.0 16.2 0 0.0 ## [4,] 0 0.0 0.0 15 0.0 ## [5,] 0 0.0 0.0 0 12.2 Length: \\(337\\) \\(\\mathbf{V}^{*}\\) ## [,1] [,2] [,3] [,4] [,5] ## [1,] -0.0556 0.00838 0.0211 0.0377 -0.0119 ## [2,] -0.0558 0.00848 0.0179 0.0391 -0.0131 ## [3,] -0.0560 0.00874 0.0138 0.0405 -0.0146 ## [4,] -0.0561 0.00888 0.0114 0.0405 -0.0159 ## [5,] -0.0561 0.00874 0.0102 0.0394 -0.0159 Matrix size: \\((337, 337)\\) 4.8.2.2 Interesting properties of SVD 4.8.2.2.1 Recovering the data We can recover the original matrix by multiplying the matricies back together: \\[\\mathbf{M} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^{*}\\] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.361 0.369 0.381 0.393 0.403 ## [2,] 0.365 0.373 0.385 0.397 0.407 ## [3,] 0.369 0.377 0.389 0.399 0.411 ## [4,] 0.377 0.385 0.395 0.407 0.420 ## [5,] 0.388 0.391 0.403 0.416 0.424 4.8.2.2.2 Reducing the data The next useful property of SVD is the values in the diagonal matrix \\(\\Sigma\\). Notice how they are sorted in descending order. ## [1] 193.4417 29.1733 16.1600 14.9806 12.1708 11.3756 10.5788 8.9693 ## [9] 8.3404 7.6359 7.4752 6.8798 6.1244 5.9575 5.5327 5.3978 ## [17] 5.1953 4.8511 4.6521 4.6020 4.2501 4.1820 4.0820 4.0382 ## [25] 3.8938 3.8375 3.7173 3.5563 3.5273 3.4986 3.4396 3.4027 ## [33] 3.3417 3.2681 3.2025 3.1409 3.0671 3.0221 3.0124 2.9543 ## [41] 2.8912 2.8365 2.8076 2.7306 2.6768 2.6547 2.6008 2.5562 ## [49] 2.5353 2.5186 2.4892 2.4669 2.3997 2.3361 2.3274 2.2823 ## [57] 2.2424 2.2378 2.1923 2.1692 2.1122 2.0840 2.0704 2.0510 ## [65] 2.0241 2.0196 1.9849 1.9568 1.9305 1.9237 1.9052 1.8737 ## [73] 1.8433 1.8222 1.8107 1.7891 1.7699 1.7554 1.7195 1.7039 ## [81] 1.6870 1.6695 1.6453 1.6310 1.6101 1.5815 1.5727 1.5373 ## [89] 1.5198 1.5105 1.4861 1.4748 1.4609 1.4378 1.4321 1.4016 ## [97] 1.4001 1.3788 1.3624 1.3386 1.3301 1.3169 1.3057 1.2704 ## [105] 1.2593 1.2419 1.2376 1.2065 1.1922 1.1825 1.1741 1.1584 ## [113] 1.1405 1.1314 1.1157 1.1003 1.0921 1.0705 1.0602 1.0480 ## [121] 1.0406 1.0314 1.0191 0.9983 0.9939 0.9919 0.9634 0.9500 ## [129] 0.9434 0.9337 0.9213 0.9153 0.9044 0.8910 0.8777 0.8528 ## [137] 0.8458 0.8419 0.8246 0.8196 0.8005 0.7967 0.7924 0.7866 ## [145] 0.7734 0.7591 0.7564 0.7469 0.7365 0.7283 0.7198 0.7159 ## [153] 0.7118 0.7009 0.6926 0.6874 0.6817 0.6634 0.6552 0.6517 ## [161] 0.6493 0.6352 0.6184 0.6127 0.6073 0.6039 0.6014 0.5949 ## [169] 0.5915 0.5810 0.5767 0.5627 0.5547 0.5456 0.5381 0.5351 ## [177] 0.5310 0.5247 0.5211 0.5139 0.5025 0.4998 0.4966 0.4808 ## [185] 0.4763 0.4725 0.4613 0.4552 0.4529 0.4471 0.4411 0.4374 ## [193] 0.4326 0.4309 0.4232 0.4178 0.4152 0.4047 0.4005 0.3970 ## [201] 0.3884 0.3795 0.3790 0.3770 0.3705 0.3690 0.3597 0.3535 ## [209] 0.3506 0.3465 0.3434 0.3387 0.3341 0.3243 0.3201 0.3183 ## [217] 0.3099 0.3073 0.3020 0.2980 0.2972 0.2953 0.2911 0.2826 ## [225] 0.2787 0.2738 0.2705 0.2644 0.2584 0.2542 0.2533 0.2472 ## [233] 0.2424 0.2397 0.2356 0.2320 0.2300 0.2268 0.2205 0.2187 ## [241] 0.2160 0.2096 0.2077 0.1980 0.1961 0.1930 0.1895 0.1891 ## [249] 0.1853 0.1814 0.1798 0.1772 0.1720 0.1704 0.1681 0.1658 ## [257] 0.1650 0.1617 0.1539 0.1523 0.1483 0.1457 0.1436 0.1424 ## [265] 0.1367 0.1360 0.1332 0.1304 0.1276 0.1265 0.1259 0.1232 ## [273] 0.1201 0.1158 0.1119 0.1112 0.1079 0.1069 0.1044 0.1010 ## [281] 0.0993 0.0980 0.0934 0.0905 0.0900 0.0878 0.0868 0.0847 ## [289] 0.0838 0.0796 0.0763 0.0744 0.0733 0.0710 0.0682 0.0674 ## [297] 0.0671 0.0637 0.0612 0.0595 0.0570 0.0556 0.0537 0.0501 ## [305] 0.0485 0.0446 0.0435 0.0426 0.0401 0.0361 0.0354 0.0336 ## [313] 0.0311 0.0295 0.0286 0.0257 0.0248 0.0238 0.0235 0.0233 ## [321] 0.0224 0.0221 0.0218 0.0208 0.0203 0.0200 0.0195 0.0191 ## [329] 0.0184 0.0181 0.0175 0.0174 0.0170 0.0162 0.0157 0.0155 ## [337] 0.0152 These tell us the relative importance of each column in \\(\\mathbf{U}\\) and \\(\\mathbf{V}^{*}\\). When their values are multiplied by really small numbers (or even 0), then they do not contribute much information. In the original image we have 337 columns. What if we want to represent as much of the original information as possible, in a more compact form? The first column alone explains over 92.967% of the variation in the original matrix. What if we just used the first two columns to redraw the picture? ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.421 0.422 0.424 0.424 0.424 ## [2,] 0.432 0.434 0.435 0.436 0.436 ## [3,] 0.421 0.423 0.424 0.425 0.425 ## [4,] 0.413 0.414 0.416 0.416 0.416 ## [5,] 0.421 0.423 0.424 0.425 0.425 Okay, that doesn’t appear to be enough. What is the fewest we could get away with? “Rank 150” means retaining only the first 150 columns from all of matricies in the SVD. Rank 173 doesn’t look too bad, and Rank 215 looks pretty indistinguishable from the original. The original matrix contained 202,200 different values in the matrix. If we used SVD to compress the image and only use the first 215 columns of each individual matrix, we would shrink the size of the image by 28%. 4.8.3 Principal components analysis Principal components analysis (PCA) is a basic technique for dimension reduction. The goal is to find a low-dimensional representation of the data that contains as much as possible of the variation. So for example, say your original dataset had 30 columns (i.e. variables, dimensions). We want to reduce the number of columns while still maintaining the overall structure of the matrix. This can be helpful for many reasons, including Exploratory data analysis - visualize \\(p\\) dimensions in a simple \\(2\\) dimensional plot Statistical learning - reduce the number of features/independent variables in a statistical learning model to improve efficiency or remove multicollinearity The PCA algorithm is implemented as: Rescale each column to be mean 0 and standard deviation 1. This prevents variables with larger values and variances from dominating the projection. Compute the covariance matrix \\(\\mathbf{S}\\). Here \\(\\mathbf{X}\\) is a data matrix: \\[\\mathbf{S} = \\dfrac{1}{N} \\mathbf{X}&#39; \\mathbf{X}\\] Compute the \\(K\\) largest eigenvectors of \\(\\mathbf{S}\\). These eigenvectors are the principal components of the dataset. Remember that every eigenvector has a corresponding eigenvalue. The eigenvector defines the direction of the line, and the eigenvalue tells you how much variance there is in the data in that direction (essentially how spread out the data is on that line). Computing the covariance matrix is expensive when \\(\\mathbf{X}\\) is very large or when \\(\\mathbf{X}\\) is very small. SVD can be used to make this process more efficient by computing SVD on the original matrix. \\(\\mathbf{V}^{*}\\) contains the principal directions (or the eigenvectors), the columns of \\(\\mathbf{U} \\boldsymbol{\\Sigma}\\) are principal components (scores) for each observation, and the values of the diagonal elements of \\(\\boldsymbol{\\Sigma}\\) are equivalent to the eigenvalues computed from \\(\\mathbf{S}\\) (amount of variance explained by the principal components). The total number of principal components for a given \\(n \\times p\\) data set is \\(\\min(n,p)\\), either the number of observations in the data or the number of variables in the data (whichever is smaller). Once we estimate the principal components, we can plot them against each other in order to produce a low-dimensional visualization of the data. 4.8.3.1 Example: USArrests Let’s look at the use of PCA on the USArrests dataset, reproduced from An Introduction to Statistical Learning. The principal component score vectors have length \\(n=50\\) and the principal component loading vectors have length \\(p=4\\) (in this data set, \\(p &lt; n\\)). The biplot visualizes the relationship between the first two principal components for the dataset, including both the scores and the loading vectors. The first principal component places approximately equal weight on murder, assault, and rape. We can tell this because these vectors’ length on the first principal component dimension are roughly the same, whereas the length for urban population is smaller. Conversely, the second principal component (the vertical axis) places more emphasis on urban population. Intuitively this makes sense because murder, assault, and rape are all measures of violent crime, and it makes sense that they should be correlated with one another (i.e. states with high murder rates are likely to have high rates of rape as well). We can also interpret the plot for individual states based on their positions along the two dimensions. States with large positive values on the first principal component have high crime rates while states with large negative values have low crime rates; states with large positive values on the second principal component have high levels of urbanization while states with large negative values have low levels of urbanization. 4.8.3.2 Example: MNIST data set Figure 4.5: MNIST digits MNIST digits is a classic practice dataset for image classification. Each image is a standardized picture of a handwritten digit. We want to use the image to classify the digit as the actual number 0-9. We use the individual pixels and their intensity of black/white to generate these predictions. Rather than use all \\(28 \\times 28 = 784\\) individual pixels, we can use SVD/PCA to compress the data set to a smaller number of principal components that capture most of the variation in the rows/columns. To verify if this technique would work, we can visualize the observations along their first and second principal components. If those two components alone can distinguish between each of the ten possible digits, we should see unique clusters of observations in the scatterplot. Acknowledgements A Gentle Introduction to Singular-Value Decomposition for Machine Learning Chapter 14.5, Friedman, Hastie, and Tibshirani (2001) Examples of SVD Singular Value Decomposition (SVD): Tutorial Using Examples in R Relationship between SVD and PCA. How to use SVD to perform PCA? Decoding Dimensionality Reduction, PCA and SVD References "],["multivariable-differentiation.html", "Lecture 5 Functions of several variables and optimization with several variables Learning objectives Supplemental readings 5.1 Higher order derivatives 5.2 Multivariate function 5.3 Multivariate derivatives 5.4 Multivariate optimization 5.5 A simple optimization example 5.6 Maximum likelihood estimation for a normal distribution 5.7 Computational optimization procedures", " Lecture 5 Functions of several variables and optimization with several variables Learning objectives Define a partial derivative Identify higher order derivatives and partial derivatives Define notation for calculus performed on vector and matrix forms Demonstrate multivariable calculus methods on social scientific research Calculate critical points, partial derivatives, and double integrals Supplemental readings Chapter 14, Pemberton and Rau (2011) OpenStax Calculus: Volume 3, ch 4 5.1 Higher order derivatives The first derivative is applying the definition of derivatives on the function, and it can be expressed as \\[f&#39;(x), ~~ y&#39;, ~~ \\frac{d}{dx}f(x), ~~ \\frac{dy}{dx}\\] We can keep applying the differentiation process to functions that are themselves derivatives. The derivative of \\(f&#39;(x)\\) with respect to \\(x\\), would then be \\[f&#39;&#39;(x)=\\lim\\limits_{h\\to 0}\\frac{f&#39;(x+h)-f&#39;(x)}{h}\\] and we can therefore call it the Second derivative: \\[f&#39;&#39;(x), ~~ y&#39;&#39;, ~~ \\frac{d^2}{dx^2}f(x), ~~ \\frac{d^2y}{dx^2}\\] Similarly, the derivative of \\(f&#39;&#39;(x)\\) would be called the third derivative and is denoted \\(f&#39;&#39;&#39;(x)\\). And by extension, the nth derivative is expressed as \\(\\frac{d^n}{dx^n}f(x)\\), \\(\\frac{d^ny}{dx^n}\\). \\[ \\begin{aligned} f(x) &amp;=x^3\\\\ f^{\\prime}(x) &amp;=3x^2\\\\ f^{\\prime\\prime}(x) &amp;=6x \\\\ f^{\\prime\\prime\\prime}(x) &amp;=6\\\\ f^{\\prime\\prime\\prime\\prime}(x) &amp;=0\\\\ \\end{aligned} \\] Earlier, we said that if a function is differentiable at a given point, then it must be continuous. Further, if \\(f&#39;(x)\\) is itself continuous, then \\(f(x)\\) is called continuously differentiable. All of this matters because many of our findings about optimization rely on differentiation, and so we want our function to be differentiable in as many layers. A function that is continuously differentiable infinitely is called smooth. Some examples include: \\[ \\begin{aligned} f(x) &amp;= x^2 \\\\ f(x) &amp;= e^x \\end{aligned} \\] 5.2 Multivariate function A multivariate function is a function with more than one argument. 5.2.0.0.1 Example 1 \\[f(x_{1}, x_{2}) = x_{1} + x_{2}\\] 5.2.0.0.2 Example 2 \\[f(x_{1}, x_{2}) = x_{1}^2 + x_{2}^2\\] 5.2.0.0.3 Example 3 \\[f(x_{1}, x_{2}) = \\sin(x_1)\\cos(x_2)\\] 5.2.0.0.4 Example 4 \\[f(x_{1}, x_{2}) = -(x-5)^2 - (y-2)^2\\] 5.2.0.0.5 Example 5 \\[f(x_{1}, x_{2}, x_{3} ) = x_1 + x_2 + x_3\\] 5.2.0.0.6 Example 6 \\[ \\begin{aligned} f(\\mathbf{x} )&amp;= f(x_{1}, x_{2}, \\ldots, x_{N} ) \\\\ &amp;= x_{1} +x_{2} + \\ldots + x_{N} \\\\ &amp;= \\sum_{i=1}^{N} x_{i} \\end{aligned} \\] 5.2.1 Definition Definition 5.1 (Multivariate function) Suppose \\(f:\\Re^{n} \\rightarrow \\Re^{1}\\). We will call \\(f\\) a multivariate function. We will commonly write, \\[f(\\mathbf{x}) = f(x_{1}, x_{2}, x_{3}, \\ldots, x_{n} )\\] \\(\\Re^{n} = \\Re \\underbrace{\\times}_{\\text{cartesian}} \\Re \\times \\Re \\times \\ldots \\Re\\) The function we consider will take \\(n\\) inputs and output a single number (that lives in \\(\\Re^{1}\\), or the real line) 5.2.2 Evaluating multivariate functions Example 5.1 \\[f(x_{1}, x_{2}, x_{3}) = x_1 + x_2 + x_3\\] Evaluate at \\(\\mathbf{x} = (x_{1}, x_{2}, x_{3}) = (2, 3, 2)\\) \\[ \\begin{aligned} f(2, 3, 2) &amp; = 2 + 3 + 2 \\\\ &amp; = 7 \\end{aligned} \\] Example 5.2 \\[f(x_{1}, x_{2} ) = x_{1} + x_{2} + x_{1} x_{2}\\] Evaluate at \\(\\mathbf{w} = (w_{1}, w_{2} ) = (1, 2)\\) \\[ \\begin{aligned} f(w_{1}, w_{2}) &amp; = w_{1} + w_{2} + w_{1} w_{2} \\\\ &amp; = 1 + 2 + 1 \\times 2 \\\\ &amp; = 5 \\end{aligned} \\] Example 5.3 (Preferences for multidimensional policy) Recall that in the spatial model, we suppose policy and political actors are located in a space. Suppose that policy is \\(N\\) dimensional - or \\(\\mathbf{x} \\in \\Re^{N}\\). Suppose that legislator \\(i\\)’s utility is a \\(U:\\Re^{N} \\rightarrow \\Re^{1}\\) and is given by, \\[ \\begin{aligned} U(\\mathbf{x}) &amp; = U(x_{1}, x_{2}, \\ldots, x_{N} ) \\\\ &amp; = - (x_{1} - \\mu_{1} )^2 - (x_{2} - \\mu_{2})^2 - \\ldots - (x_{N} - \\mu_{N})^{2} \\\\ &amp; = -\\sum_{j=1}^{N} (x_{j} - \\mu_{j} )^{2} \\end{aligned} \\] Suppose \\(\\mathbf{\\mu} = (\\mu_{1}, \\mu_{2}, \\ldots, \\mu_{N} ) = (0, 0, \\ldots, 0)\\). Evaluate legislator’s utility for a policy proposal of \\(\\mathbf{m} = (1, 1, \\ldots, 1)\\) \\[ \\begin{aligned} U(\\mathbf{m} ) &amp; = U(1, 1, \\ldots, 1) \\\\ &amp; = - (1 - 0)^2 - (1- 0) ^2 - \\ldots - (1- 0) ^2 \\\\ &amp; = -\\sum_{j=1}^{N} 1 = - N \\\\ \\end{aligned} \\] Example 5.4 (Regression models and randomized treatments) Often we administer randomized experiments. The most recent wave of interest began with voter mobilization, and wonders if individual \\(i\\) turns out to vote, \\(\\text{Vote}_{i}\\) \\(T = 1\\) (treated): voter receives mobilization \\(T = 0\\) (control): voter does not receive mobilization Suppose we find the following regression model, where \\(x_{2}\\) is a participant’s age: \\[ \\begin{aligned} f(T,x_2) &amp; = \\Pr(\\text{Vote}_{i} = 1 | T, x_{2} ) \\\\ &amp; = \\beta_{0} + \\beta_{1} T + \\beta_{2} x_{2} \\end{aligned} \\] We can calculate the effect of the experiment as: \\[ \\begin{aligned} f(T = 1, x_2) - f(T=0, x_2) &amp; = \\beta_{0} + \\beta_{1} 1 + \\beta_{2} x_{2} - (\\beta_{0} + \\beta_{1} 0 + \\beta_{2} x_{2}) \\\\ &amp; = \\beta_{0} - \\beta_{0} + \\beta_{1}(1 - 0) + \\beta_{2}(x_{2} - x_{2} ) \\\\ &amp; = \\beta_{1} \\end{aligned} \\] 5.3 Multivariate derivatives What happens when there’s more than one variable that is changing? If you can do ordinary derivatives, you can do partial derivatives: just hold all the other input variables constant except for the one with respect to which you are differentiating. Suppose we have a function \\(f\\) now of two (or more) variables and we want to determine the rate of change relative to one of the variables. To do so, we would find its partial derivative, which is defined similar to the derivative of a function of one variable. Definition 5.2 (Partial derivative) Let \\(f\\) be a function of the variables \\((x_1,\\ldots,x_n)\\). The partial derivative of \\(f\\) with respect to \\(x_i\\) is \\[\\frac{\\partial f}{\\partial x_i} (x_1,\\ldots,x_n) = \\lim\\limits_{h\\to 0} \\frac{f(x_1,\\ldots,x_i+h,\\ldots,x_n)-f(x_1,\\ldots,x_i,\\ldots,x_n)}{h}\\] Only the \\(i\\)th variable changes — the others are treated as constants. We can take higher-order partial derivatives, like we did with functions of a single variable, except now the higher-order partials can be with respect to multiple variables. Notice that you can take partials with regard to different variables. Suppose \\(f(x,y)=x^2+y^2\\). Then \\[ \\begin{aligned} \\frac{\\partial f}{\\partial x}(x,y) &amp;= 2x \\\\ \\frac{\\partial f}{\\partial y}(x,y) &amp;= 2y\\\\ \\frac{\\partial^2 f}{\\partial x^2}(x,y) &amp;= 2\\\\ \\frac{\\partial^2 f}{\\partial x \\partial y}(x,y) &amp;= 0 \\end{aligned} \\] Let \\(f(x,y)=x^3 y^4 +e^x -\\log y\\). What are the following partial derivaitves? \\[ \\begin{aligned} \\frac{\\partial f}{\\partial x}(x,y) &amp;= 3x^2y^4 + e^x\\\\ \\frac{\\partial f}{\\partial y}(x,y) &amp;=4x^3y^3 - \\frac{1}{y}\\\\ \\frac{\\partial^2 f}{\\partial x^2}(x,y) &amp;= 6xy^4 + e^x\\\\ \\frac{\\partial^2 f}{\\partial x \\partial y}(x,y) &amp;= 12x^2y^3 \\end{aligned} \\] Example 5.5 (Rate of change, linear regression) Suppose we regress \\(\\text{Approval}_{i}\\) rate for Trump in month \\(i\\) on \\(\\text{Employ}_{i}\\) and \\(\\text{Gas}_{i}\\). We obtain the following model: \\[\\text{Approval}_{i} = 0.8 -0.5 \\text{Employ}_{i} -0.25 \\text{Gas}_{i}\\] We are modeling \\(\\text{Approval}_{i} = f(\\text{Employ}_{i}, \\text{Gas}_{i} )\\). What is the partial derivative with respect to employment? \\[\\frac{\\partial f(\\text{Employ}_{i}, \\text{Gas}_{i} ) }{\\partial \\text{Employ}_{i} } = -0.5\\] 5.4 Multivariate optimization Just as we want to optimize functions with a single variable, we often wish to opimize functions with multiple variables. Parameters \\(\\mathbf{\\beta} = (\\beta_{1}, \\beta_{2}, \\ldots, \\beta_{n} )\\) such that \\(f(\\mathbf{\\beta}| \\mathbf{X}, \\mathbf{Y})\\) is maximized Policy \\(\\mathbf{x} \\in \\Re^{n}\\) that maximizes \\(U(\\mathbf{x})\\) Weights \\(\\mathbf{\\pi} = (\\pi_{1}, \\pi_{2}, \\ldots, \\pi_{K})\\) such that a weighted average of forecasts \\(\\mathbf{f} = (f_{1} , f_{2}, \\ldots, f_{k})\\) have minimum loss \\[\\min_{\\mathbf{\\pi}} = - (\\sum_{j=1}^{K} \\pi_{j} f_{j} - y ) ^ 2\\] As before, we will consider both analytic and computational approaches. 5.4.1 Differences from single variable optimization procedure It is the same basic approach, except we have multiple parameters of interest. This requires more explicit knowledge of linear algebra to track all the components and optimize over the multidimensional space Let \\(\\mathbf{x} \\in \\Re^{n}\\) and let \\(\\delta &gt;0\\). Define a neighborhood of \\(\\mathbf{x}\\), \\(B(\\mathbf{x}, \\delta)\\), as the set of points such that, \\[B(\\mathbf{x}, \\delta) = \\{ \\mathbf{y} \\in \\Re^{n} : ||\\mathbf{x} - \\mathbf{y}||&lt; \\delta \\}\\] That is, \\(B(\\mathbf{x}, \\delta)\\) is the set of points where the vector \\(\\mathbf{y}\\) is a vector in n-dimensional space such that vector norm of \\(\\mathbf{x} - \\mathbf{y}\\) is less than \\(\\delta\\) So the neighborhood is at most \\(\\delta\\) big Now suppose \\(f:X \\rightarrow \\Re\\) with \\(X \\subset \\Re^{n}\\). A vector \\(\\mathbf{x}^{*} \\in X\\) is a global maximum if , for all other \\(\\mathbf{x} \\in X\\) \\[f(\\mathbf{x}^{*}) &gt; f(\\mathbf{x} )\\] A vector \\(\\mathbf{x}^{\\text{local}}\\) is a local maximum if there is a neighborhood around \\(\\mathbf{x}^{\\text{local}}\\), \\(Q \\subset X\\) such that, for all \\(x \\in Q\\), \\[f(\\mathbf{x}^{\\text{local} }) &gt; f(\\mathbf{x} )\\] The maximum and minimum values of a function \\(f:X \\rightarrow \\Re\\) on the real number line (in n-dimensional space) will fall somewhere along \\(X\\). This is the same as we saw previously, except now \\(X\\) is not a scalar value - it is a vector \\(\\mathbf{X}\\). 5.4.2 First derivative test: Gradient Suppose \\(f:X \\rightarrow \\Re^{n}\\) with \\(X \\subset \\Re^{1}\\) is a differentiable function. Define the gradient vector of \\(f\\) at \\(\\mathbf{x}_{0}\\), \\(\\nabla f(\\mathbf{x}_{0})\\) as \\[\\nabla f (\\mathbf{x}_{0}) = \\left(\\frac{\\partial f (\\mathbf{x}_{0}) }{\\partial x_{1} }, \\frac{\\partial f (\\mathbf{x}_{0}) }{\\partial x_{2} }, \\frac{\\partial f (\\mathbf{x}_{0}) }{\\partial x_{3} }, \\ldots, \\frac{\\partial f (\\mathbf{x}_{0}) }{\\partial x_{n} } \\right)\\] It is the first partial derivatives for each variable \\(x_n\\) stored in a vector Gradient points in the direction that the function is increasing in the fastest direction So if \\(\\mathbf{a} \\in X\\) is a local extremum, then, \\[ \\begin{aligned} \\nabla f(\\mathbf{a}) &amp;= \\mathbf{0} \\\\ &amp;= (0, 0, \\ldots, 0) \\end{aligned} \\] That is, the root(s) of the gradient are where \\(f(\\mathbf{a})\\) equals \\(\\mathbf{0}\\) in \\(n\\)-dimensional space. Example 5.6 \\[ \\begin{aligned} f(x,y) &amp;= x^2+y^2 \\\\ \\nabla f(x,y) &amp;= (2x, \\, 2y) \\end{aligned} \\] Example 5.7 \\[ \\begin{aligned} f(x,y) &amp;= x^3 y^4 +e^x -\\log y \\\\ \\nabla f(x,y) &amp;= (3x^2 y^4 + e^x, \\, 4x^3y^3 - \\frac{1}{y}) \\end{aligned} \\] 5.4.2.1 Critical points We can have critical points: Maximum Minimum Saddle point In order to know if we are at a maximum/minimum/saddle point, we need to perform the second derivative test. 5.4.3 Second derivative test: Hessian Suppose \\(f:X \\rightarrow \\Re^{1}\\) , \\(X \\subset \\Re^{n}\\), with \\(f\\) a twice differentiable function. We will define the Hessian matrix as the matrix of second derivatives at \\(\\mathbf{x}^{*} \\in X\\), \\[ \\mathbf{H}(f)(\\mathbf{x}^{*} ) = \\begin{bmatrix} \\frac{\\partial^{2} f }{\\partial x_{1} \\partial x_{1} } (\\mathbf{x}^{*} ) &amp; \\frac{\\partial^{2} f }{\\partial x_{1} \\partial x_{2} } (\\mathbf{x}^{*} ) &amp; \\ldots &amp; \\frac{\\partial^{2} f }{\\partial x_{1} \\partial x_{n} } (\\mathbf{x}^{*} ) \\\\ \\frac{\\partial^{2} f }{\\partial x_{2} \\partial x_{1} } (\\mathbf{x}^{*} ) &amp; \\frac{\\partial^{2} f }{\\partial x_{2} \\partial x_{2} } (\\mathbf{x}^{*} ) &amp; \\ldots &amp; \\frac{\\partial^{2} f }{\\partial x_{2} \\partial x_{n} } (\\mathbf{x}^{*} ) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^{2} f }{\\partial x_{n} \\partial x_{1} } (\\mathbf{x}^{*} ) &amp; \\frac{\\partial^{2} f }{\\partial x_{n} \\partial x_{2} } (\\mathbf{x}^{*} ) &amp; \\ldots &amp; \\frac{\\partial^{2} f }{\\partial x_{n} \\partial x_{n} } (\\mathbf{x}^{*} ) \\\\ \\end{bmatrix} \\] Hessians are symmetric, and they describe the curvature of the function (think, how bended). To calculate the hessian, you must differentiate on the entire gradient with respect to each \\(x_n\\). Example 5.8 \\[ \\begin{aligned} f(x,y) &amp;= x^2+y^2 \\\\ \\nabla f(x,y) &amp;= (2x, \\, 2y) \\\\ \\mathbf{H}(f)(x,y) &amp;= \\begin{bmatrix} 2 &amp; 0 \\\\ 0 &amp; 2 \\end{bmatrix} \\end{aligned} \\] Example 5.9 \\[ \\begin{aligned} f(x,y) &amp;= x^3 y^4 +e^x -\\log y \\\\ \\nabla f(x,y) &amp;= (3x^2 y^4 + e^x, \\, 4x^3y^3 - \\frac{1}{y}) \\\\ \\mathbf{H}(f)(x,y) &amp;= \\begin{bmatrix} 6xy^4 + e^x &amp; 12x^2y^3 \\\\ 12x^2y^3 &amp; 12x^3y^2 + \\frac{1}{y^2} \\end{bmatrix} \\end{aligned} \\] 5.4.3.1 Definiteness of a matrix Consider \\(n \\times n\\) matrix \\(\\mathbf{A}\\). If, for all \\(\\mathbf{x} \\in \\Re^{n}\\) where \\(\\mathbf{x} \\neq \\mathbf{0}\\): \\[ \\begin{aligned} \\mathbf{x}^{&#39;} \\mathbf{A} \\mathbf{x} &amp;&gt; 0, \\quad \\mathbf{A} \\text{ is positive definite} \\\\ \\mathbf{x}^{&#39;} \\mathbf{A} \\mathbf{x} &amp;&lt; 0, \\quad \\mathbf{A} \\text{ is negative definite } \\end{aligned} \\] If \\(\\mathbf{x}^{&#39;} \\mathbf{A} \\mathbf{x} &gt;0\\) for some \\(\\mathbf{x}\\) and \\(\\mathbf{x}^{&#39;} \\mathbf{A} \\mathbf{x}&lt;0\\) for other \\(\\mathbf{x}\\), then we say \\(\\mathbf{A}\\) is indefinite. \\(\\mathbf{x}\\) is a vector of the appropriate length (can be any vector drawn from \\(\\Re^n\\) space), so a transposed vector times a matrix times a vector will result in a scalar value 5.4.3.2 Second derivative test If \\(\\mathbf{H}(f)(\\mathbf{a})\\) is positive definite then \\(\\mathbf{a}\\) is a local minimum If \\(\\mathbf{H}(f)(\\mathbf{a})\\) is negative definite then \\(\\mathbf{a}\\) is a local maximum If \\(\\mathbf{H}(f)(\\mathbf{a})\\) is indefinite then \\(\\mathbf{a}\\) is a saddle point 5.4.3.3 Use the determinant to assess definiteness How do we measure definiteness when up until now \\(\\mathbf{x}\\) could be any vector? We can use the determinant of the Hessian of \\(f\\) at the critical value \\(\\mathbf{a}\\): \\[ \\mathbf{H}(f)(\\mathbf{a}) = \\begin{bmatrix} A &amp; B \\\\ B &amp; C \\\\ \\end{bmatrix} \\] The determinant for a \\(2 \\times 2\\) matrix can easily be calculated using the known formula \\(AC - B^2\\). \\(AC - B^2&gt; 0\\) and \\(A&gt;0\\) \\(\\leadsto\\) positive definite \\(\\leadsto\\) \\(\\mathbf{a}\\) is a local minimum \\(AC - B^2&gt; 0\\) and \\(A&lt;0\\) \\(\\leadsto\\) negative definite \\(\\leadsto\\) \\(\\mathbf{a}\\) is a local maximum \\(AC - B^2&lt;0\\) \\(\\leadsto\\) indefinite \\(\\leadsto\\) saddle point \\(AC- B^2 = 0\\) inconclusive 5.4.4 Basic procedure summarized Calculate gradient Set equal to zero, solve system of equations Calculate Hessian Assess Hessian at critical values Boundary values? (if relevant) 5.5 A simple optimization example Suppose \\(f:\\Re^{2} \\rightarrow \\Re\\) with \\[f(x_{1}, x_{2}) = 3(x_1 + 2)^2 + 4(x_{2} + 4)^2 \\] Calculate gradient: \\[ \\begin{aligned} \\nabla f(\\mathbf{x}) &amp;= (6 x_{1} + 12 , 8x_{2} + 32 ) \\\\ \\mathbf{0} &amp;= (6 x_{1}^{*} + 12 , 8x_{2}^{*} + 32 ) \\end{aligned} \\] We now solve the system of equations to yield \\[x_{1}^{*} = - 2, \\quad x_{2}^{*} = -4\\] \\[ \\textbf{H}(f)(\\mathbf{x}^{*}) = \\begin{bmatrix} 6 &amp; 0 \\\\ 0 &amp; 8 \\\\ \\end{bmatrix} \\] \\(\\det(\\textbf{H}(f)(\\mathbf{x}^{*}))\\) = 48 and \\(6&gt;0\\) so \\(\\textbf{H}(f)(\\mathbf{x}^{*})\\) is positive definite. \\(\\mathbf{x^{*}}\\) is a local minimum. 5.6 Maximum likelihood estimation for a normal distribution Suppose that we draw an independent and identically distributed random sample of \\(n\\) observations from a normal distribution, \\[ \\begin{aligned} Y_{i} &amp;\\sim \\text{Normal}(\\mu, \\sigma^2) \\\\ \\mathbf{Y} &amp;= (Y_{1}, Y_{2}, \\ldots, Y_{n} ) \\end{aligned} \\] Our task: Obtain likelihood (summary estimator) Derive maximum likelihood estimators for \\(\\mu\\) and \\(\\sigma^2\\) \\[ \\begin{aligned} L(\\mu, \\sigma^2 | \\mathbf{Y} ) &amp;\\propto \\prod_{i=1}^{n} f(Y_{i}|\\mu, \\sigma^2) \\\\ &amp;\\propto \\prod_{i=1}^{N} \\frac{\\exp[ - \\frac{ (Y_{i} - \\mu)^2 }{2\\sigma^2} ]}{\\sqrt{2 \\pi \\sigma^2}} \\\\ &amp;\\propto \\frac{\\exp[ -\\sum_{i=1}^{n} \\frac{(Y_{i} - \\mu)^2}{2\\sigma^2} ]}{ (2\\pi)^{n/2} \\sigma^{2n/2} } \\end{aligned} \\] Taking the logarithm, we have \\[l(\\mu, \\sigma^2|\\mathbf{Y} ) = -\\sum_{i=1}^{n} \\frac{(Y_{i} - \\mu)^2}{2\\sigma^2} - \\frac{n}{2} \\log(2 \\pi) - \\frac{n}{2} \\log (\\sigma^2)\\] Let’s find \\(\\widehat{\\mu}\\) and \\(\\widehat{\\sigma}^{2}\\) that maximizes log-likelihood. \\[ \\begin{aligned} l(\\mu, \\sigma^2|\\mathbf{Y} ) &amp;= -\\sum_{i=1}^{n} \\frac{(Y_{i} - \\mu)^2}{2\\sigma^2} - \\frac{n}{2} \\log (\\sigma^2) \\\\ \\frac{\\partial l(\\mu, \\sigma^2)|\\mathbf{Y} )}{\\partial \\mu } &amp;= \\sum_{i=1}^{n} \\frac{2(Y_{i} - \\mu)}{2\\sigma^2} \\\\ \\frac{\\partial l(\\mu, \\sigma^2)|\\mathbf{Y})}{\\partial \\sigma^2} &amp;= -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i=1}^{n} (Y_{i} - \\mu)^2 \\end{aligned} \\] \\[ \\begin{aligned} 0 &amp;= -\\sum_{i=1}^{n} \\frac{2(Y_{i} - \\widehat{\\mu})}{2\\widehat{\\sigma}^2} \\\\ 0 &amp;= -\\frac{n}{2\\widehat{\\sigma}^2 } + \\frac{1}{2\\widehat{\\sigma}^4} \\sum_{i=1}^{n} (Y_{i} - \\mu^{*})^2 \\end{aligned} \\] Solving for \\(\\widehat{\\mu}\\) and \\(\\widehat{\\sigma}^2\\) yields, \\[ \\begin{aligned} \\widehat{\\mu} &amp;= \\frac{\\sum_{i=1}^{n} Y_{i} }{n} \\\\ \\widehat{\\sigma}^{2} &amp;= \\frac{1}{n} \\sum_{i=1}^{n} (Y_{i} - \\overline{Y})^2 \\end{aligned} \\] \\[ \\textbf{H}(f)(\\widehat{\\mu}, \\widehat{\\sigma}^2) = \\begin{bmatrix} \\frac{\\partial^{2} l(\\mu, \\sigma^2|\\mathbf{Y} )}{\\partial \\mu^{2}} &amp; \\frac{\\partial^{2} l(\\mu, \\sigma^2|\\mathbf{Y} )}{\\partial \\sigma^{2} \\partial \\mu} \\\\ \\frac{\\partial^{2} l(\\mu, \\sigma^2|\\mathbf{Y} )}{\\partial \\sigma^{2} \\partial \\mu} &amp; \\frac{\\partial^{2} l(\\mu, \\sigma^2|\\mathbf{Y} )}{\\partial^{2} \\sigma^{2}} \\end{bmatrix} \\] Taking derivatives and evaluating at MLE’s yields, \\[ \\textbf{H}(f)(\\widehat{\\mu}, \\widehat{\\sigma}^2) = \\begin{bmatrix} \\frac{-n}{\\widehat{\\sigma}^2} &amp; 0 \\\\ 0 &amp; \\frac{-n}{2(\\widehat{\\sigma}^2)^2} \\\\ \\end{bmatrix} \\] \\(\\text{det}(\\textbf{H}(f)(\\widehat{\\mu}, \\widehat{\\sigma}^2)) = \\dfrac{n^2}{2(\\widehat{\\sigma}^2)^3} &gt; 0\\) and \\(A = \\dfrac{-n}{\\widehat{\\sigma}^2} &lt; 0\\) \\(\\leadsto\\) maximum Determinant is greater than 0 and \\(A\\) is less than zero - local maximum 5.7 Computational optimization procedures As the previous example suggests, analytical approaches can be difficult or impossible for many multivariate functions. Computational approaches simplify the problem. 5.7.1 Multivariate Newton-Raphson Suppose \\(f:\\Re^{n} \\rightarrow \\Re\\). Suppose we have guess \\(\\mathbf{x}_{t}\\). Then our update is: \\[\\mathbf{x}_{t+1} = \\mathbf{x}_{t} - [\\textbf{H}(f)(\\mathbf{x}_{t})]^{-1} \\nabla f(\\mathbf{x}_{t})\\] Approximate function with tangent plane Find value of \\(x_{t+1}\\) that makes the plane equal to zero Update again 5.7.1.1 Drawbacks Expensive to calculate (requires inverting Hessian) Very sensitive to starting points 5.7.2 Grid search Example: MLE for a normal distribution In R, I drew 10,000 realizations from \\(Y_{i} \\sim \\text{Normal}(0.25, 100)\\) Used realized values \\(y_{i}\\) to evaluate \\(l(\\mu, \\sigma^2| \\mathbf{y} )\\) across a range of values Computationally inefficient - have to try a large number of combinations of parameters 5.7.3 Gradient descent Same approach as before, but now the derivative is a vector (i.e. gradient, hence the name of the approach “gradient” descent). \\[f(x, y) = x^2 + 2y^2\\] References "],["integral-calculus.html", "Lecture 6 Integration and integral calculus Learning objectives Supplemental readings 6.1 Prepare for the journey 6.2 Indefinite integration 6.3 The definite integral: area under the curve 6.4 Infinite integrals 6.5 Monte Carlo and integration 6.6 Multivariate integration Acknowledgments", " Lecture 6 Integration and integral calculus Learning objectives Summarize areas, slices, and integrals Apply common rules to calculate definite integrals Define the fundamental theorem of calculus Calculate antiderivatives and integrals using integration by substitution and integration by parts Evaluate improper integrals Evaluate multivariate integrals Supplemental readings Chapters 19-20, Pemberton and Rau (2011) OpenStax Calculus: Volume 2, ch 1-2, 3.1, 3.7 6.1 Prepare for the journey Figure 6.1: Differentiation and Integration. Source: xkcd 6.2 Indefinite integration So far, we’ve been interested in finding the derivative \\(f=F&#39;\\) of a function \\(F\\). However, sometimes we’re interested in exactly the reverse: finding the function \\(F\\) for which \\(f\\) is its derivative. We refer to \\(F\\) as the antiderivative of \\(f\\). For example what is the antiderivative of \\(f(x) = \\frac{1}{x^2}\\)? \\[ \\begin{aligned} \\int \\frac{1}{x^2} \\,dx &amp;= -\\frac{1}{x} + c \\\\ \\frac{d}{dx} \\left[ -\\frac{1}{x} + c \\right] &amp;= \\frac{1}{x^2} \\end{aligned} \\] We know from derivatives how to manipulate \\(F\\) to get \\(f\\). But how do you express the procedure to manipulate \\(f\\) to get \\(F\\)? For that, we need a new symbol, which we will call indefinite integration. The indefinite integral of \\(f(x)\\) is written \\[F(x) = \\int f(x) \\,dx \\] 6.2.1 Many possible antiderivatives Draw the function \\(f(x)\\) and its indefinite integral, \\(\\int\\limits f(x) \\,dx\\) \\[f(x) = (x^2-4)\\] The indefinite integral of the function \\(f(x) = (x^2-4)\\) can, for example, be \\(F(x) = \\frac{1}{3}x^3 - 4x\\). But it can also be \\(F(x) = \\frac{1}{3}x^3 - 4x + 1\\), because the constant 1 disappears when taking the derivative. Figure 6.2: The Many Indefinite Integrals of a Function Notice from these examples that while there is only a single derivative for any function, there are multiple antiderivatives: one for any arbitrary constant \\(c\\). \\(c\\) just shifts the curve up or down on the \\(y\\)-axis. If more information is present about the antiderivative — e.g., that it passes through a particular point — then we can solve for a specific value of \\(c\\). 6.2.2 Common rules of integration Some common rules of integrals follow by virtue of being the inverse of a derivative. Constants are allowed to slip out: \\(\\int a f(x)\\,dx = a\\int f(x)\\,dx\\) Integration of the sum is sum of integrations: \\(\\int [f(x)+g(x)]\\,dx=\\int f(x)\\,dx + \\int g(x)\\,dx\\) Reverse Power-rule: \\(\\int x^n \\,dx = \\frac{1}{n+1} x^{n+1} + c\\) Exponents are still exponents: \\(\\int e^x \\,dx = e^x +c\\) Recall the derivative of \\(\\log(x)\\) is one over \\(x\\), and so: \\(\\int \\frac{1}{x} \\,dx = \\log x + c\\) Reverse chain-rule: \\(\\int e^{f(x)}f^\\prime(x)\\,dx = e^{f(x)}+c\\) More generally: \\(\\int [f(x)]^n f&#39;(x)\\,dx = \\frac{1}{n+1}[f(x)]^{n+1}+c\\) Remember the derivative of a log of a function: \\(\\int \\frac{f^\\prime(x)}{f(x)}\\,dx=\\log f(x) + c\\) 6.2.3 Practice integrating functions Simplify the following indefinite integrals: \\(\\int 3x^2 \\,dx\\) Click for the solution Factor out the constant \\(3\\) and reverse power rule. \\[ \\begin{aligned} \\int 3x^2 \\,dx &amp;= 3 \\int x^2 \\,dx \\\\ &amp;= 3 \\left( \\frac{x^3}{3} + c \\right) \\\\ &amp;= x^3 + c \\end{aligned} \\] \\(\\int (2x+1) \\,dx\\) Click for the solution Integrate the sum term by term and factor out constants. \\[ \\begin{aligned} \\int (2x+1) \\,dx &amp;= 2 \\int x \\,dx + \\int 1 \\,dx \\\\ &amp;= x^2 + c + \\int 1 \\,dx \\\\ &amp;= x^2 + x + c \\end{aligned} \\] \\(\\int e^x e^{e^x} \\,dx\\) Click for the solution Reverse chain-rule and knowledge of the antiderivative of \\(e^x\\). \\[\\int e^x e^{e^x} \\,dx = e^{e^x} + c\\] 6.3 The definite integral: area under the curve If there is an indefinite integral, there must be a definite integral. Indeed there is, but the notion of definite integrals comes from a different objective: finding the area under a function. Suppose we want to determine the area \\(A(R)\\) of a region \\(R\\) defined by a curve \\(f(x)\\) and some interval \\(a\\le x \\le b\\). Figure 6.3: The Riemann Integral as a Sum of Evaluations One way to calculate the area would be to divide the interval \\(a\\le x\\le b\\) into \\(n\\) subintervals of length \\(\\Delta x\\) and then approximate the region with a series of rectangles, where the base of each rectangle is \\(\\Delta x\\) and the height is \\(f(x)\\) at the midpoint of that interval. \\(A(R)\\) would then be approximated by the area of the union of the rectangles, which is given by \\[S(f,\\Delta x)=\\sum\\limits_{i=1}^n f(x_i)\\Delta x\\] and is called a Riemann sum. As we decrease the size of the subintervals \\(\\Delta x\\), making the rectangles “thinner,” we would expect our approximation of the area of the region to become closer to the true area. This allows us to express the area as a limit of a series: \\[A(R)=\\lim\\limits_{\\Delta x\\to 0}\\sum\\limits_{i=1}^n f(x_i)\\Delta x\\] This is how we define the “Definite” Integral. 6.3.1 The definite integral (Riemann) If for a given function \\(f\\) the Riemann sum approaches a limit as \\(\\Delta x \\to 0\\), then that limit is called the Riemann integral of \\(f\\) from \\(a\\) to \\(b\\). We express this with the \\(\\int\\), symbol, and write \\[\\lim\\limits_{\\Delta x\\to 0} \\sum\\limits_{i=1}^n f(x_i)\\Delta x = \\int\\limits_a^b f(x) \\,dx\\] We read \\[\\int\\limits_a^b f(x) \\,dx\\] as the definite integral of \\(f\\) from \\(a\\) to \\(b\\), which is the area under the “curve” \\(f(x)\\) from point \\(x=a\\) to \\(x=b\\). Theorem 6.1 (Continuous functions) Suppose \\(f:[a,b] \\rightarrow \\Re\\) is a continuous function. Then \\(f\\) is integrable. Theorem 6.2 (Monotonic functions) Suppose \\(f:[a,b]\\rightarrow \\Re\\) is a monotonic function. Then \\(f\\) is integrable. 6.3.2 Counterexamples Example 6.1 Suppose \\(f:[0,1]\\rightarrow \\frac{1}{x}\\) \\[\\int_{0}^{1} \\frac{1}{x} \\,dx\\] Then \\(\\frac{1}{x}\\) is not integrable on \\([a,b]\\) because the area that the integral would represent is infinite. Example 6.2 \\[ \\begin{aligned} f(x) &amp;= 1 \\text{ if } x \\text{ rational} \\\\ &amp;= 0 \\text{ if } x \\text{ irrational} \\end{aligned} \\] Not integrable, because every interval will contain a discontinuous jump. 6.3.3 Fundamental theorem of calculus The fundamental theorem of calculus shows us that this Riemann sum is, in fact, the antiderivative. Let the function \\(f\\) be bounded on \\([a,b]\\) and continuous on \\((a,b)\\). Then, use the symbol \\(F(x)\\) to denote the definite integral from \\(a\\) to \\(x\\): \\[F(x)=\\int\\limits_a^x f(t) \\,dt, \\quad a\\le x\\le b\\] Then \\(F(x)\\) has a derivative at each point in \\((a,b)\\) and \\[F^\\prime(x)=f(x), \\quad a&lt;x&lt;b\\] That is, the definite integral function of \\(f\\) is one of the antiderivatives of some \\(f\\). This is again a long way of saying that differentiation is the inverse of integration. But now, we’ve covered definite integrals. The second theorem gives us a simple way of computing a definite integral as a function of indefinite integrals. Let the function \\(f\\) be bounded on \\([a,b]\\) and continuous on \\((a,b)\\). Let \\(F\\) be any function that is continuous on \\([a,b]\\) such that \\(F&#39;(x)=f(x)\\) on \\((a,b)\\). Then \\[\\int\\limits_a^bf(x)\\,dx = F(b)-F(a)\\] So the procedure to calculate a simple definite integral \\(\\int\\limits_a^b f(x)\\,dx\\) is then Find the indefinite integral \\(F(x)\\). Evaluate \\(F(b)-F(a)\\). 6.3.3.1 Uniform distribution Suppose \\(f:\\Re \\rightarrow \\Re\\), with \\[ \\begin{aligned} f(x) &amp;= 1 \\text{ if } x \\in [0,1] \\\\ f(x) &amp;= 0 \\text{ otherwise } \\end{aligned} \\] What is the area under \\(f(x)\\) from \\([0, 1/2]\\)? \\[ \\begin{aligned} \\int_{0}^{1/2} f(x)\\,dx &amp;= \\int_{0}^{1/2} 1 \\,dx \\\\ &amp;= x|_{0}^{1/2} \\\\ &amp;= (1/2) - (0 ) \\\\ &amp;= 1/2 \\end{aligned} \\] We will call \\(f(x) = 1\\) the uniform distribution. 6.3.3.2 Area under a line Suppose \\(f:\\Re \\rightarrow \\Re\\), with \\[f(x) = x\\] Evaluate the \\(\\int_{2}^{t}f(x)\\,dx\\). \\[ \\begin{aligned} \\int_{2}^{t}f(x)\\,dx &amp;= \\int_{2}^{t} x \\,dx \\\\ &amp;= \\frac{x^{2} }{2} |_{2}^{t} \\\\ &amp;= \\frac{t^2}{2} - \\frac{2^2}{2} \\\\ &amp;= \\frac{t^2}{2} - \\frac{4}{2} = \\frac{t^2}{2} - 2 \\end{aligned} \\] 6.3.3.3 Area under a curve Solve \\(\\int\\limits_1^3 3x^2 \\,dx\\) \\[ \\begin{aligned} f(x) &amp;= 3x^2 \\\\ F(x) &amp;= x^3 + c \\\\ \\int\\limits_1^3 3x^2 \\,dx &amp;= F(3) - F(1) \\\\ &amp;= (3^3 + c) - (1^3 + c) \\\\ &amp;= 27 + c - 1 - c \\\\ &amp;= 26 \\end{aligned} \\] 6.3.4 Common rules for definite integrals The area-interpretation of the definite integral provides some rules for simplification. There is no area below a point: \\[\\int\\limits_a^a f(x)\\,dx=0\\] Reversing the limits changes the sign of the integral: \\[\\int\\limits_a^b f(x)\\,dx=-\\int\\limits_b^a f(x)\\,dx\\] Sums can be separated into their own integrals: \\[\\int\\limits_a^b [\\alpha f(x)+\\beta g(x)]\\,dx = \\alpha \\int\\limits_a^b f(x)\\,dx + \\beta \\int\\limits_a^b g(x)\\,dx\\] Areas can be combined as long as limits are linked: \\[\\int\\limits_a^b f(x) \\,dx +\\int\\limits_b^c f(x)\\,dx = \\int\\limits_a^c f(x)\\,dx\\] 6.3.5 Practice solving definite integrals Simplify the following definite intergrals. \\(\\int\\limits_1^1 3x^2 \\,dx\\) Click for the solution \\(0\\). Area under a point is 0. \\(\\int\\limits_0^4 (2x+1) \\,dx\\) Click for the solution Integrate the sum by term and factor out constants. \\[ \\begin{aligned} \\int\\limits_0^4 (2x+1) \\,dx &amp;= 2 \\int_0^4 2x \\,dx + \\int_0^4 1 \\,dx \\\\ &amp;= x^2|_0^4 + x |_0^4 \\\\ &amp;= (4^2 - 0^2) + (4 - 0) \\\\ &amp;= 16 + 4 \\\\ &amp;= 20 \\end{aligned} \\] \\(\\int\\limits_{-2}^0 e^x e^{e^x} dx + \\int\\limits_0^2 e^x e^{e^x} \\,dx\\) Click for the solution Limits are linked, so combine together and use definition of antiderivative we calculated earlier. \\[ \\begin{aligned} \\int\\limits_{-2}^0 e^x e^{e^x} dx + \\int\\limits_0^2 e^x e^{e^x} \\,dx &amp;= \\int\\limits_{-2}^2 e^x e^{e^x} \\,dx \\\\ &amp;= e^{e^{x}} |_{-2}^2 \\\\ &amp;= e^{e^{2}} - e^{e^{-2}} \\\\ &amp;\\approx 1617 \\end{aligned} \\] 6.3.6 Integration by substitution From the second fundamental theorem of calculus, we know that a quick way to get a definite integral is to first find the indefinite integral, and then just plug in the bounds. Sometimes the integrand (the thing that we are trying to take an integral of) doesn’t appear integrable using common rules and antiderivatives. A method one might try is integration by substitution, which is related to the Chain Rule. Suppose we want to find the indefinite integral \\[\\int g(x)\\,dx\\] but \\(g(x)\\) is complex and none of the formulas we have seen so far seem to apply immediately. The trick is to come up with a new function \\(u(x)\\) such that \\[g(x)=f[u(x)]u&#39;(x).\\] Why does an introduction of yet another function end of simplifying things? Let’s refer to the antiderivative of \\(f\\) as \\(F\\). Then the chain rule tells us that \\[\\frac{d}{dx} F[u(x)]=f[u(x)]u&#39;(x)\\] So, \\(F[u(x)]\\) is the antiderivative of \\(g\\). We can then write \\[\\int g(x) \\,dx= \\int f[u(x)]u&#39;(x)\\,dx = \\int \\frac{d}{dx} F[u(x)]\\,dx = F[u(x)]+c\\] To summarize, the procedure to determine the indefinite integral \\(\\int g(x)\\,dx\\) by the method of substitution: Identify some part of \\(g(x)\\) that might be simplified by substituting in a single variable \\(u\\) (which will then be a function of \\(x\\)). Determine if \\(g(x)\\,dx\\) can be reformulated in terms of \\(u\\) and \\(du\\). Solve the indefinite integral. Substitute back in for \\(x\\) Example 6.3 Consider \\[\\int \\frac{x}{x^2 + 1} \\,dx\\] This is particularly nasty to solve given just the rules we learned above. However, if we multiply the expression by \\(\\frac{2}{2}\\) and pull the constant from the denominator outside the integral: \\[\\frac{1}{2} \\int \\frac{2x}{x^2 + 1} \\,dx\\] we see that the derivative of \\(x^2 + 1\\) is \\(2x\\), which is contained in the numerator. So \\[ \\begin{aligned} u &amp;= x^2 + 1 \\\\ du &amp;= 2x \\,dx \\\\ \\int \\frac{x}{x^2 + 1} \\,dx &amp;= \\frac{1}{2} \\int \\frac{2x}{x^2 + 1} \\,dx \\\\ &amp;= \\frac{1}{2} \\int \\frac{1}{u} \\,du \\\\ &amp;= \\frac{1}{2} \\log(u) + c \\\\ &amp;= \\frac{1}{2} \\log(x^2 + 1) + c \\\\ \\end{aligned} \\] Antiderivative of \\(\\frac{1}{x}\\) is \\(\\log(x)\\) - this is known After integrating the \\(u\\) function, substitute back in the original \\(x\\) based function 6.3.7 Integration by parts Another useful integration technique is integration by parts, which is related to the product rule of differentiation. The product rule states that \\[\\frac{d}{dx} f(x) g(x) = f(x) g&#39;(x) + g(x) f&#39;(x)\\] Another way of framing this is that if the function \\(g\\) has a continuous derivative, \\[\\int_a^b g&#39;(x) dx = g(b) - g(a)\\] Suppose that \\(g(x)\\) can be expressed as the product of two functions \\(g(x) = p(x)q(x)\\). Rewriting the above integral and applying the product rule to expand \\(g&#39;(x)\\) we get \\[\\int_a^b (p&#39;(x)q(x) + p(x)q&#39;(x)) dx = p(b)q(b) - p(a)q(a)\\] We can rearrange this equation to read as \\[\\int_a^b p&#39;(x)q(x) dx = p(b)q(b) - p(a)q(a) - \\int_a^b p(x)q&#39;(x) dx\\] The goal is to decompose \\(g(x)\\) into the product of two functions, one of which is easy to integrate and the other becomes simpler once differentiated. Denote the first function by \\(p&#39;(x)\\) where \\(p(x)\\) is easily found, and the second function as \\(q(x)\\). For indefinite integrals, integration by parts is \\[\\int p&#39;(x)q(x) dx = p(x)q(x) - \\int p(x)q&#39;(x) dx\\] Example 6.4 Simplify the following integral: \\[\\int x \\log(x) \\,dx\\] \\[ \\begin{aligned} p(x) &amp;= \\frac{1}{2}x^2 \\\\ p&#39;(x) &amp;= x \\, dx \\\\ q(x) &amp;= \\log(x) \\\\ q&#39;(x) &amp;= \\frac{1}{x} \\,dx \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\int p&#39;(x)q(x) dx &amp;= p(x)q(x) - \\int p(x)q&#39;(x) dx \\\\ &amp;= \\frac{1}{2}x^2 \\times \\log(x) - \\int \\frac{1}{2}x^2 \\times \\frac{1}{x} \\,dx \\\\ &amp;= \\frac{1}{2}x^2\\log(x) - \\int \\frac{1}{2}x \\,dx \\\\ &amp;= \\frac{1}{2}x^2\\log(x) - \\frac{1}{2} \\int x \\,dx \\\\ &amp;= \\frac{1}{2}x^2\\log(x) - \\frac{x^2}{4} + c \\end{aligned} \\] Pull out the constant from the integral \\(\\int x \\,dx = \\frac{1}{2}x^2\\) - simple application of the power rule 6.4 Infinite integrals Consider the definite integral \\[ \\begin{aligned} \\int_1^X x^{-2} \\,dx &amp;= -\\frac{1}{x} |_1^X \\\\ &amp;= -\\frac{1}{X} - -\\frac{1}{1} \\\\ &amp;= -\\frac{1}{X} + 1 \\\\ &amp;= 1 - X^{-1} \\end{aligned} \\] As \\(X \\rightarrow \\infty, \\frac{1}{X} \\rightarrow 0\\). Hence \\[\\lim_{X \\rightarrow \\infty} \\int_1^X x^{-2} \\,dx = 1\\] which can be written compactly as \\[\\int_1^\\infty x^{-2} \\,dx = 1\\] This is an example of an infinite integral. Infinite refers not to the value of the integral (which is in fact the finite number 1) but to the fact that integration is taking place over an infinite integral. The crucial point is that the shaded region extends infinitely far to the right, but tapers sufficiently sharply for its area to be the finite number 1. Definition 6.1 Suppose we have a continuous function \\(f(x)\\) defined for all \\(x \\geq a\\), and suppose the integral \\(\\int_a^X f(x)\\,dx\\) approaches a finite limit \\(L\\) as \\(X \\rightarrow \\infty\\). Then we write \\[\\int_a^\\infty f(x)\\,dx = L\\] In this case, we say that the indefinite integral exists or converges. By contrast, if \\(\\int_a^X f(x)\\,dx\\) does not approach a finite limit as \\(X \\rightarrow \\infty\\), then we say that the infinite integral does not exist or diverges. Example 6.5 \\[f(x) = \\frac{1}{x}\\] \\[ \\begin{aligned} \\int_{1}^{\\infty} \\frac{1}{x} \\,dx &amp;= \\lim_{t \\rightarrow \\infty} \\int_{1}^{t} \\frac{1}{x} \\,dx \\\\ &amp;= \\lim_{t \\rightarrow \\infty} (\\log x)|_{1}^{t} \\\\ &amp;= \\lim_{t \\rightarrow \\infty} (\\log t) - \\lim_{t \\rightarrow \\infty} (\\log 1) \\end{aligned} \\] Does not converge. 6.4.1 Two-sided infinite integrals If \\(f(x)\\) is a continuous function defined for all \\(x \\leq a\\), and if the definite integral \\(\\int_Y^a f(x) \\,dx\\) approaches a finite limit as \\(Y \\rightarrow -\\infty\\), we denote the limit by \\[\\int_{-\\infty}^a f(x) \\,dx\\] If the integrals \\(\\int_{-\\infty}^a f(x) \\,dx\\) and \\(\\int_a^\\infty f(x) \\,dx\\) both exist, we denote their sum by \\[\\int_{-\\infty}^\\infty f(x) \\,dx\\] This property is extremely important when we discuss probability distributions, since a major property of a probability density function is that for a given PDF \\(f(x)\\), \\[\\int_{-\\infty}^\\infty f(x) \\,dx = 1\\] 6.4.2 Improper integrals An improper integral is an integral where the integrand is not defined at one of the limits of integration. Consider the example \\[I = \\int_0^1 \\frac{1}{\\sqrt{x}} \\,dx\\] In this case, the lower limit causes the problem since \\(\\frac{1}{\\sqrt{x}} \\rightarrow \\infty\\) as \\(x \\rightarrow 0\\). Nevertheless, we can define and evaluate \\(I\\) as \\[I = \\lim_{\\delta \\downarrow0} \\int_\\delta^1 \\frac{1}{\\sqrt{x}} \\,dx\\] where \\(\\downarrow\\) means “tends from above”. In fact, \\[ \\begin{aligned} I &amp;= \\lim_{\\delta \\downarrow0} \\int_\\delta^1 \\frac{1}{\\sqrt{x}} \\,dx \\\\ &amp;= \\lim_{\\delta \\downarrow0} \\left[ 2\\sqrt{x} \\right]_\\delta^1 \\\\ &amp;= \\lim_{\\delta \\downarrow0} (2 - 2\\sqrt{\\delta}) \\\\ &amp;= 2 \\end{aligned} \\] More generally, suppose the function \\(f(x)\\) is defined and continuous for \\(a &lt; x \\leq b\\), but is not defined for \\(x = a\\). If the integral \\[\\int_{a + \\delta}^b f(x) \\,dx\\] tends to a finite limit \\(I\\) as \\(\\delta \\downarrow 0\\), then we say that the integral \\(\\int_a^b f(x) \\,dx\\) exists and has the value \\(I\\). Otherwise, we say that the integral \\(\\int_a^b f(x) \\,dx\\) diverges. 6.5 Monte Carlo and integration Suppose that we want to compute the expected value of a function \\(g\\) of \\(X\\) where \\[\\E[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f_X(x) \\,dx\\] but \\(f(x)\\) is complicated. \\[f(x) = \\frac{\\exp\\left(- \\frac{(x- \\mu)^2}{2\\sigma^2} \\right) }{\\sqrt{2\\pi}}\\] Substituting into \\(\\E[g(X)]\\) we have the definite integral \\[\\int_{-\\infty}^{\\infty} x \\times \\frac{\\exp\\left(- \\frac{(x- \\mu)^2}{2\\sigma^2} \\right) }{\\sqrt{2\\pi}} \\,dx\\] \\(f(x)\\) is a probability density function and \\(\\E[g(X)]\\) is its expected value. We’ll learn more about this next week, but for now the important thing to focus on is that \\[\\int_{-\\infty}^{\\infty} x \\times \\frac{\\exp\\left(- \\frac{(x- \\mu)^2}{2\\sigma^2} \\right) }{\\sqrt{2\\pi}} \\,dx\\] is going to be incredibly difficult to calculate analytically. Suppose we can generate random draws of \\(X\\) \\(x_1, \\ldots, x_n\\) and we computed the arithmetic mean of \\(g(x)\\) over the sample, then we would have the Monte Carlo estimate \\[\\tilde{g_n}(x) = \\frac{1}{n} \\sum_{i=1}^n g(x_i)\\] which is the Monte Carlo estimator14 of \\(\\E[g(x)]\\). As \\(n \\rightarrow \\infty\\), \\(\\tilde{g_n}(x) \\leadsto \\E[g(x)]\\).15 We can demonstrate this using the above example where \\(\\mu = 0, \\sigma^2 = 1\\). Analytically \\(\\E[g(x)] = 0\\).16 If we simulate this repeatedly, we can see that as we increase the number of draws our estimate \\(\\tilde{g_n}(x)\\) converges towards 0. 6.6 Multivariate integration Suppose we have a function \\(f:X \\rightarrow \\Re^{1}\\), with \\(X \\subset \\Re^{2}\\). We will integrate a function over an area. Suppose that area, \\(A\\), is in 2-dimensions \\[A = \\{x, y : x \\in[0,1], y \\in [0,1] \\}\\] \\[A = \\{x, y: x^2 + y^2 \\leq 1 \\}\\] \\[A = \\{ x, y: x&lt; y, x, y \\in (0,2) \\}\\] How do calculate the area under the function over these regions? Definition 6.2 (Multivariate integration) Suppose \\(f:X \\rightarrow \\Re\\) where \\(X \\subset \\Re^{n}\\). We will say that \\(f\\) is integrable over \\(A \\subset X\\) if we are able to calculate its area with refined partitions of \\(A\\) and we will write the integral \\(I =\\int_{A} f(\\boldsymbol{x}) d\\boldsymbol{A}\\). That’s horribly abstract. There is an extremely helpful theorem that makes this manageable. Theorem 6.3 (Fubini's theorem) Suppose \\(A = [a_{1}, b_{1}] \\times [a_{2}, b_{2} ] \\times \\ldots \\times [a_{n}, b_{n}]\\) and that \\(f:A \\rightarrow \\Re\\) is integrable. Then \\[\\int_{A} f(\\boldsymbol{x}) d\\boldsymbol{A} = \\int_{a_{n}}^{b_{n}} \\int_{a_{n-1}}^{b_{n-1}} \\ldots \\int_{a_{2}}^{b_{2}} \\int_{a_{1}}^{b_{1}} f(\\boldsymbol{x})\\,dx_{1} \\,dx_{2} \\ldots \\,dx_{n-1} \\,dx_{n}\\] Start with the inside integral \\(x_{1}\\) is the variable, everything else a constant Work inside to out, iterating At the last step, we should arrive at a number 6.6.0.1 Multivariate uniform distribution Suppose \\(f:[0,1] \\times [0,1] \\rightarrow \\Re\\) and \\(f(x_{1}, x_{2}) = 1\\) for all \\(x_{1}, x_{2} \\in [0,1]\\times[0,1]\\). What is \\(\\int_{0}^{1}\\int_{0}^{1} f(x) \\,dx_{1} \\,dx_{2}\\)? \\[ \\begin{aligned} \\int_{0}^{1}\\int_{0}^{1} f(x) \\,dx_{1} \\,dx_{2} &amp;= \\int_{0}^{1} \\int_{0}^{1} 1 \\,dx_{1} \\,dx_{2} \\nonumber \\\\ &amp;= \\int_{0}^{1} x_{1}|_{0}^{1} \\,dx_{2} \\\\ &amp;= \\int_{0}^{1} (1 - 0) \\,dx_{2} \\\\ &amp;= \\int_{0}^{1} 1 \\,dx_{2} \\\\ &amp;= x_{2}|_{0}^{1} \\\\ &amp;= 1 \\end{aligned} \\] 6.6.0.2 Another example Suppose \\(f:[a_{1}, b_{1} ] \\times [a_{2}, b_{2} ] \\rightarrow \\Re\\) is given by \\[f(x_{1}, x_{2} ) = x_{1} x_{2}\\] Find \\(\\int_{a_{2}}^{b_{2}} \\int_{a_{1}}^{b_{1}} f(x_{1}, x_{2} )\\,dx_{1} \\,dx_{2}\\) \\[ \\begin{aligned} \\int_{a_{2}}^{b_{2}} \\int_{a_{1}}^{b_{1}} f(x_{1}, x_{2} )\\,dx_{1} \\,dx_{2} &amp;= \\int_{a_{2}}^{b_{2}} \\int_{a_{1}}^{b_{1}} x_{1} x_{2} \\,dx_{1} \\,dx_{2} \\\\ &amp;= \\int_{a_{2}}^{b_{2}} \\frac{x_{1}^2}{2} x_{2} |_{a_{1}}^{b_{1}} \\,dx_{2} \\\\ &amp;= \\frac{b_{1}^{2} - a_{1}^{2} }{2} \\int_{a_{2}}^{b_{2}} x_{2} \\,dx_{2} \\\\ &amp;= \\frac{b_{1}^{2} - a_{1}^{2} }{2} \\left( \\frac{x_{2}^{2} }{2} |_{a_{2}}^{b_{2}} \\right ) \\\\ &amp;= \\frac{b_{1}^{2} - a_{1}^{2} }{2} \\frac{b_{2}^{2} - a_{2}^{2} }{2} \\end{aligned} \\] 6.6.0.3 Exponential distributions Suppose \\(f:\\Re^{2}_{+} \\rightarrow \\Re\\) and that \\[f(x_{1}, x_{2}) = 2 \\exp(-x_{1}) \\exp(-2 x_{2} )\\] Find \\(\\int_{0}^{\\infty} \\int_{0}^{\\infty} f(x_{1}, x_{2})\\) \\[ \\begin{aligned} \\int_{0}^{\\infty} \\int_{0}^{\\infty} f(x_{1}, x_{2}) &amp;= 2 \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\exp(-x_{1}) \\exp(-2x_{2}) \\,dx_{1} \\,dx_{2} \\\\ &amp;= 2 \\int_{0}^{\\infty}\\exp(-x_{1}) \\,dx_{1}\\int_{0}^{\\infty} \\exp(-2x_{2}) \\,dx_{2} \\\\ &amp;= 2 ( - \\exp(-x)|^{\\infty}_{0} ) (-\\frac{1}{2} \\exp(-2x_{2} ) |^{\\infty}_{0} ) \\\\ &amp;= 2 \\left[ (- \\lim_{x_{1} \\rightarrow \\infty} \\exp(-x_{1} ) + 1 ) (-\\frac{1}{2} \\lim_{x_{2} \\rightarrow \\infty} \\exp(-2x_{2}) + \\frac{1}{2} ) \\right] \\\\ &amp;= 2 [ \\frac{1}{2} ] \\\\ &amp;= 1 \\end{aligned} \\] 6.6.1 More complicated bounds of integration So far, we have integrated over rectangles. But often, we are interested in more complicated regions. How do we do this? 6.6.1.1 Example: more complicated region Suppose \\(f:[0,1] \\times [0,1] \\rightarrow \\Re\\), \\(f(x_{1}, x_{2}) = x_{1} + x_{2}\\). Find area of function where \\(x_{1} &lt; x_{2}\\). Trick: we need to determine bound. If \\(x_{1}&lt; x_{2}\\), \\(x_{1}\\) can take on any value from \\(0\\) to \\(x_{2}\\). \\[ \\begin{aligned} \\iint_{x_{1}&lt; x_{2}} f(\\boldsymbol{x}) &amp;= \\int_{0}^{1} \\int_{0}^{x_{2}} x_{1} + x_{2} \\,dx_{1} \\,dx_{2} \\\\ &amp;= \\int_{0}^{1} x_{2} x_{1} |_{0}^{x_{2}} \\,dx_{2} + \\int_{0}^{1} \\frac{x_{1}^{2} }{2} |_{0}^{x_{2} } \\\\ &amp;= \\int_{0}^{1} x_{2}^{2} \\,dx_{2} + \\int_{0}^{1} \\frac{x_{2}^2}{2} \\\\ &amp;= \\frac{x_{2}^{3} }{3}|_{0}^{1} + \\frac{x_{2}^{3}}{6}|_{0}^{1} \\\\ &amp;= \\frac{1}{3} + \\frac{1}{6} \\\\ &amp;= \\frac{3}{6} = \\frac{1}{2} \\end{aligned} \\] Consider the same function and let’s switch the bounds. \\[ \\begin{aligned} \\iint_{x_{1}&lt;x_{2}} f(\\boldsymbol{x}) &amp;= \\int_{0}^{1} \\int_{x_{1}}^{1} x_{1} + x_{2} \\,dx_{2} \\,dx_{1} \\\\ &amp;= \\int_{0}^{1} x_{1}x_{2}|_{x_{1}}^{1} + \\int_{0}^{1} \\frac{x_{2}^{2}}{2} |_{x_{1}}^{1}\\,dx_{1} \\\\ &amp;= \\int_{0}^{1} x_{1} - x_{1}^2 + \\int_{0}^{1} \\frac{1}{2} - \\frac{x_{1}^2}{2} \\,dx_{1} \\\\ &amp;= \\frac{x_{1}^2}{2}|_{0}^{1} - \\frac{x_{1}^{3}}{3}|_{0}^{1} + \\frac{x_{1}}{2}|_{0}^{1} - \\frac{x_{1}^{3}}{6}|_{0}^{1} \\\\ &amp;= \\frac{1}{2} - \\frac{1}{3} + \\frac{1}{2} - \\frac{1}{6} \\\\ &amp;= 1 - \\frac{3}{6} \\\\ &amp;= \\frac{1}{2} \\end{aligned} \\] Acknowledgments Some materials drawn from Harvard Government Math Prefresher References "],["sample-space-probability.html", "Lecture 7 Sample space and probability Learning objectives Supplemental readings 7.1 Model of probability 7.2 Sample space 7.3 Events 7.4 Probability 7.5 Conditional probability 7.6 Law of total probability 7.7 Bayes’ Rule 7.8 Independence of probabilities 7.9 Counting", " Lecture 7 Sample space and probability Learning objectives Review set notation and operations Define probabilistic models Describe conditional probability Define total probability theorem Implement Bayes’ Rule Define and evaluate independence of events Identify the importance of counting possible events Supplemental readings Chapter 1, Bertsekas and Tsitsiklis (2008) Equivalent reading from Bertsekas and Tsitsiklis lecture notes 7.1 Model of probability Sample space - set of all things that could happen Events - subsets of the sample space Probability - chance of an event 7.2 Sample space The sample space is the set of all things that can occur. We will collect all distinct outcomes into the set \\(\\Omega\\). Example 7.1 (Congressional elections) Members of the U.S. House of Representatives are elected every 2 years. If we consider the outcomes for an incumbent running for reelection, there are two possible outcomes: Win (\\(W\\)) or Not win (\\(N\\)) (i.e. lose) One incumbent: \\(\\Omega = \\{W, N\\}\\) Two incumbents: \\(\\Omega = \\{(W,W), (W,N), (N,W), (N,N)\\}\\) 435 incumbents: \\(\\Omega = 2^{435}\\) possible outcomes (permutations) Example 7.2 (Number of countries signing treaties) \\[\\Omega = \\{0, 1, 2, \\ldots, 194\\}\\] Example 7.3 (Duration of parliamentary governments) In a parliamentary democracy, the government is defined as the ruling political part in parliament. It remains in power until the government collapses under a vote of no confidence.17 All non-negative real numbers: \\([0, \\infty)\\) \\(\\Omega = \\{x : 0 \\leq x &lt; \\infty\\}\\) All possible \\(x\\) such that \\(x\\) is between 0 and infinity The sample space must define all possible realizations. 7.3 Events Events are a subset of the sample space: \\[E \\subset \\Omega\\] Plain English: outcomes from the sample space, collected in a set Example 7.4 (Congressional elections) Consider one incumbent. Possible events are \\[ \\begin{aligned} E &amp;= W \\\\ F &amp;= N \\end{aligned} \\] Now consider two incumbents. Possible events include \\[ \\begin{aligned} E &amp;= \\{(W, N), (W, W) \\} \\\\ F &amp;= \\{(N, N)\\} \\end{aligned} \\] Now consider all 435 incumbents running for election. There are an extraordinarily large possible events that could occur. Consider just two possible examples: Outcome of 2016 election - one event All outcomes where Dems retake control of the House - one event Notation: \\(x\\) is an element of a set \\(E\\) \\[ \\begin{aligned} x &amp;\\in E \\\\ \\{N, N\\} &amp;\\in E \\end{aligned} \\] 7.3.1 Event operations \\(E\\) is a set, or a collection of distinct objects. We can perform operations on sets to create new sets. Consider two example sets: \\[ \\begin{aligned} E &amp;= \\{ (W,W), (W,N) \\} \\\\ F &amp;= \\{ (N, N), (W,N) \\} \\\\ \\Omega &amp;= \\{(W,W), (W,N), (N,W), (N,N) \\} \\end{aligned} \\] Operations determine what lies in the new set \\(E^{\\text{new}}\\). Union: \\(\\cup\\) All objects that appear in either set (OR) \\(E^{\\text{new}} = E \\cup F = \\{(W,W), (W,N), (N,N) \\}\\) Intersection: \\(\\cap\\) All objects that appear in both sets (AND) \\(E^{\\text{new}} = E \\cap F = \\{(W,N)\\}\\) Complement of set \\(E\\): \\(E^{c}\\) All objects in \\(S\\) that are not in \\(E\\) \\(E^{c} = \\{(N, W) , (N, N) \\}\\) \\(F^{c} = \\{(N, W) , (W, W) \\}\\) What is \\(\\Omega^{c}\\)? - an empty set \\(\\emptyset\\). Suppose \\(E = {W}\\), \\(F = {N}\\). Then \\(E \\cap F = \\emptyset\\) (there is nothing that lies in both sets). Definition 7.1 (Mutually exclusive) Suppose \\(E\\) and \\(F\\) are events. If \\(E \\cap F = \\emptyset\\) then we’ll say \\(E\\) and \\(F\\) are mutually exclusive. Mutual exclusivity \\(\\neq\\) independence \\(E\\) and \\(E^{c}\\) are mutually exclusive events Example 7.5 Consider the act of flipping a coin where it can land on either \\[ \\begin{aligned} H &amp;= \\text{heads} \\\\ T &amp;= \\text{tails} \\end{aligned} \\] Suppose \\(S = \\{H, T\\}\\). Then \\(E = H\\) and \\(F = T\\), then \\(E \\cap F = \\emptyset\\) Example 7.6 Suppose \\[ \\begin{aligned} S &amp;= \\{(H, H), (H,T), (T, H), (T,T) \\} \\\\ E &amp;= \\{(H,H)\\} \\\\ F &amp;= \\{(H, H), (T,H)\\} \\\\ G &amp;= \\{(H, T), (T, T) \\} \\end{aligned} \\] \\(E \\cap F = (H, H)\\) \\(E \\cap G = \\emptyset\\) \\(F \\cap G = \\emptyset\\) Example 7.7 Suppose \\[ \\begin{aligned} S &amp;= \\Re_{+} \\quad \\text{(A real positive number)} \\\\ E &amp;= \\{x: x&gt; 10\\} \\\\ F &amp;= \\{x: x &lt; 5\\} \\end{aligned} \\] Then \\(E \\cap F = \\emptyset\\). Definition 7.2 Suppose we have events \\(E_{1}, E_{2}, \\ldots, E_{N}\\). \\[\\cup_{i=1}^{N} E_{i} = E_{1} \\cup E_{2} \\cup E_{3} \\cup \\ldots \\cup E_{N}\\] \\(\\cup_{i=1}^{N} E_{i}\\) is the set of outcomes that occur at least once in \\(E_{1} , \\ldots, E_{N}\\). \\[\\cap_{i=1}^{N} E_{i} = E_{1} \\cap E_{2} \\cap \\ldots \\cap E_{N}\\] \\(\\cap_{i=1}^{N} E_{i}\\) is the set of outcomes that occur in each \\(E_{i}\\). 7.4 Probability Probability is the chance of an event occurring. \\(\\Pr\\) is a function, and the domain contains all events \\(E\\). 7.4.1 Three axioms All probability functions \\(\\Pr\\) satisfy three axioms: Nonnegativity: For all events \\(E\\), \\(0 \\leq \\Pr(E) \\leq 1\\) Normalization: \\(\\Pr(S) = 1\\) Additivity: For all sequences of mutually exclusive events \\(E_{1}, E_{2}, \\ldots,E_{N}\\) (where \\(N\\) can go to infinity): \\[\\Pr\\left(\\cup_{i=1}^{N} E_{i} \\right) = \\sum_{i=1}^{N} \\Pr(E_{i} )\\] Any countable sequence of mutually exclusive events can be added together to generate the probability of any of the mutually exclusive events occurring 7.4.2 Basic examples Example 7.8 (Coin flipping) Suppose we are flipping a fair coin. Then \\(\\Pr(H) = \\Pr(T) = 1/2\\). Suppose we are rolling a six-sided die. Then \\(\\Pr(1) = 1/6\\). Suppose we are flipping a pair of fair coins. Then \\(\\Pr(H, H) = 1/4\\). Example 7.9 (Congressional incumbents) One candidate example: \\(\\Pr(W)\\): probability incumbent wins \\(\\Pr(N)\\): probability incumbent loses (does not win) Two candidate example: \\(\\Pr(\\{W,W\\})\\): probability both incumbents win \\(\\Pr( \\{W,W\\}, \\{W, N\\} )\\): probability incumbent \\(1\\) wins Full House example: \\(\\Pr( \\{ \\text{All Democrats Win}\\} )\\) We’ll use data to infer these things. 7.4.2.1 Rolling the dice Consider the experiment of rolling a pair of 4-sided dice. We assume the dice are fair, and we interpret this assumption to mean that each of the sixteen possible outcomes [pairs \\((i,j)\\) with \\(i,j = 1,2,3,4\\)] has the same probability of \\(1/16\\). To calculate the probablity of an event, we must count the number of elements of the event and divide by 16 (the total number of possible outcomes). Here are some event probabilities calculated this way: \\[ \\begin{aligned} \\Omega &amp;= \\{(1,1), (1,2), (1,3), (1,4), (2,1), (2,2), (2,3), (2,4), \\\\ &amp;\\quad (3,1), (3,2), (3,3), (3,4), (4,1), (4,2), (4,3), (4,4) \\} \\end{aligned} \\] \\[ \\begin{aligned} \\Pr (\\text{the sum of the rolls is even}) &amp;= 8/16 = 1/2 \\\\ \\Pr (\\text{the sum of the rolls is odd}) &amp;= 8/16 = 1/2 \\\\ \\Pr (\\text{the first roll is equal to the second}) &amp;= 4/16 = 1/4 \\\\ \\Pr (\\text{the first roll is larger than the second}) &amp;= 6/16 = 3/8 \\\\ \\Pr (\\text{at least one roll is equal to 4}) &amp;= 7/16 \\end{aligned} \\] 7.4.3 Surprising probability facts Formalized probabilistic reasoning helps us to avoid silly reasoning. “What are the odds this could have happened?!?!?” - perhaps they are not great, but neither are all the other non-patterns that are missed. “There is no way a candidate has a 80% chance of winning, the forecasted vote share is only 55%” - this confuses different events. Chance of winning \\(\\neq\\) vote share. “Group A has a higher rate of some behavior, therefore most of the behavior is from group A” - confuses two different problems (see Simpson’s Paradox). “This is a low probability event, therefore God designed it” Even if we stipulate to a low probability event, intelligent design is an assumption Low probability obviously doesn’t imply divine intervention. Take 100 balls and let them sort into 2 bins. You’ll get a result, but the probability of that result \\(= 1/(10^{29} \\times \\text{Number of Atoms in Universe})\\). That is, there are a vast number of possible combinations of the 100 balls such that any one specific combination is highly unlikely. 7.4.3.1 Birthday problem Suppose we have a room full of \\(N\\) people. What is the probability at least 2 people have the same birthday? Assuming leap year counts, \\(N = 367\\) guarantees at least two people with same birthday. For \\(N&lt; 367?\\)? Actually not that low of a probability. We only need 23 people to have at least a \\(.5\\) probability of two individuals possessing the same birthday. 7.4.3.2 E-Harmony problem Also known as the curse of dimensionality and online dating. eHarmony matches you based on compatibility in the most important areas of life – like values, character, intellect, sense of humor, and 25 other dimensions Suppose (for example) 29 dimensions are binary (0,1) and independent. \\[\\Pr(\\text{2 people agree}) = 0.5\\] \\[ \\begin{aligned} \\Pr\\text{(Exact)} &amp;= \\Pr\\text{(Agree)}_{1} \\times \\Pr\\text{(Agree)}_{2}\\times \\ldots \\times \\Pr\\text{(Agree)}_{29} \\\\ &amp;= 0.5 \\times 0.5 \\times \\ldots \\times 0.5 \\\\ &amp;= 0.5^{29} \\\\ &amp;\\approx 1.8 \\times 10^{-9} \\end{aligned} \\] The probability of an exact match across all 29 dimensions is 1 in 536,870,912. Across many “variables” (events) agreement is harder. Many approaches therefore approximate or match on a handful of dimensions. For instance, \\(k\\)-nearest neighbors is a machine learning algorithm that relaxes the requirement for agreement across all variables and instead looks for observations which are closest on as many dimensions as possible. 7.5 Conditional probability Social scientists almost always examine conditional relationships Given low-interest rates, what is the probability of high inflation? Given “economic anxiety”, what is the probability of voting for Donald Trump? Given opposite political party identification, what is the probability of obtaining a date on Tinder? Our intuition is that some event has occurred: an outcome was realized. With our knowledge that this outcome has already happened, what is the probability that something in another set happens? Definition 7.3 (Conditional probability) Suppose we have two events, \\(E\\) and \\(F\\), and that \\(\\Pr(F)&gt;0\\). Then, \\[\\Pr(E|F) = \\frac{\\Pr(E\\cap F ) } {\\Pr(F) }\\] \\(\\Pr(E \\cap F)\\): Both \\(E\\) and \\(F\\) must occur \\(\\Pr(F)\\) normalize: we know \\(\\Pr(F)\\) already occurred 7.5.1 Examples Example 7.10 Suppose \\(F = \\{\\text{All Democrats Win} \\}\\) \\(E = \\{\\text{Nancy Pelosi Wins (D-CA)} \\}\\) If \\(F\\) occurs then \\(E\\) most occur, \\(\\Pr(E|F) = 1\\) Example 7.11 Suppose \\(F = \\{\\text{All Democrats Win} \\}\\) \\(E = \\{ \\text{Louie Gohmert Wins (R-TX)} \\}\\) \\(F \\cap E = \\emptyset \\Rightarrow \\Pr(E|F) = \\frac{\\Pr(F \\cap E) }{\\Pr(F)} = \\frac{\\Pr(\\emptyset)}{\\Pr(F)} = 0\\) Example 7.12 (Incumbency advantage) Suppose \\(I = \\{ \\text{Candidate is an incumbent} \\}\\) \\(D = \\{ \\text{Candidate Defeated} \\}\\) \\(\\Pr(D|I) = \\frac{\\Pr(D \\cap I)}{\\Pr(I) }\\) The probability that a candidate is defeated given that the candidate is an incumbent is equal to the probability of being defeated AND being an incumbent divided by the probability of being an incumbent 7.5.2 Difference between \\(\\Pr(A|B)\\) and \\(\\Pr(B|A)\\) \\[ \\begin{aligned} \\Pr(A|B) &amp; = \\frac{\\Pr(A\\cap B)}{\\Pr(B)} \\\\ \\Pr(B|A) &amp; = \\frac{\\Pr(A \\cap B) } {\\Pr(A)} \\end{aligned} \\] These are not the same values. Consider a Less Serious Example \\(\\leadsto\\) type of person who attends football games: Figure 7.1: This football. Figure 7.2: Or that football. \\[ \\begin{aligned} \\Pr(\\text{Attending a football game}| \\text{Drunk}) &amp; = 0.01 \\\\ \\Pr(\\text{Drunk}| \\text{Attending a football game}) &amp; \\approx 1 \\end{aligned} \\] 7.6 Law of total probability Suppose that we have a set of events \\(F_{1}, F_{2}, \\ldots, F_{N}\\) such that the events are mutually exclusive and together comprise the entire sample space \\(\\cup_{i=1}^{N} F_{i} = \\Omega\\). Then, for any event \\(E\\) \\[\\Pr(E) = \\sum_{i=1}^{N} \\Pr(E | F_{i} ) \\times \\Pr(F_{i})\\] Example 7.13 (Voter mobilization) Infer \\(\\Pr(\\text{vote})\\) after mobilization campaign. \\(\\Pr(\\text{vote}|\\text{mobilized} ) = 0.75\\) \\(\\Pr(\\text{vote}| \\text{not mobilized} ) = 0.25\\) \\(\\Pr(\\text{mobilized}) = 0.6 ; \\Pr(\\text{not mobilized} ) = 0.4\\) What is \\(\\Pr(\\text{vote})\\)? Sample space (one person) = \\(\\{\\) (mobilized, vote), (mobilized, not vote), (not mobilized, vote) , (not mobilized, not vote) \\(\\}\\) Mobilization partitions the space (mutually exclusive and exhaustive), so we can use the law of total probability: \\[ \\begin{aligned} \\Pr(\\text{vote} ) &amp; = \\Pr(\\text{vote}| \\text{mob.} ) \\times \\Pr(\\text{mob.} ) + \\Pr(\\text{vote} | \\text{not mob} ) \\times \\Pr(\\text{not mob}) \\\\ &amp; = 0.75 \\times 0.6 + 0.25 \\times 0.4 \\\\ &amp; = 0.55 \\end{aligned} \\] Example 7.14 (Chess tournament) You enter a chess tournament where your probability of winning a game is \\(0.3\\) against half the players (type 1), \\(0.4\\) against a quarter of the players (type 2), and \\(0.5\\) against the remaining quarter of the players (type 3). You play a game against a randomly chosen opponent. What is the probability of winning? Let \\(A_i\\) be the event of playing with an opponent of type \\(i\\). We have \\[\\Pr (A_1) = 0.5, \\quad \\Pr (A_2) = 0.25, \\quad \\Pr (A_3) = 0.25\\] Also, let \\(B\\) be the event of winning. We have \\[\\Pr (B | A_1) = 0.3, \\quad \\Pr (B | A_2) = 0.4, \\quad \\Pr (B | A_3) = 0.5\\] Thus, by the total probability theorem, the probability of winning is \\[ \\begin{aligned} \\Pr (B) &amp;= \\Pr (A_1) \\Pr (B | A_1) + \\Pr (A_2) \\Pr (B | A_2) + \\Pr (A_3) \\Pr (B | A_3) \\\\ &amp;= 0.5 \\times 0.3 + 0.25 \\times 0.4 + 0.25 \\times 0.5 \\\\ &amp;= 0.375 \\end{aligned} \\] 7.7 Bayes’ Rule Figure 7.3: Modified Bayes’ Theorem \\(\\Pr(B|A)\\) may be easy to obtain, whereas \\(\\Pr(A|B)\\) may be harder to determine. Bayes’ rule provides a method to move from \\(\\Pr(B|A)\\) to \\(\\Pr(A|B)\\) Definition 7.4 (Bayes' Rule) For two events \\(A\\) and \\(B\\), \\[\\Pr(A|B) = \\frac{\\Pr(A)\\times \\Pr(B|A)}{\\Pr(B)} \\] The proof is: \\[ \\begin{aligned} \\Pr(A|B) &amp; = \\frac{\\Pr(A \\cap B) }{\\Pr(B) } \\\\ &amp; = \\frac{\\Pr(B|A)\\Pr(A) } {\\Pr(B) } \\end{aligned} \\] Conditional probability allows us to replace the joint probability of \\(A\\) and \\(B\\) with the alternative expression Example 7.15 (Chess tournament redux) Let \\(A_i\\) be the event of playing with an opponent of type \\(i\\). We have \\[\\Pr (A_1) = 0.5, \\quad \\Pr (A_2) = 0.25, \\quad \\Pr (A_3) = 0.25\\] Also, let \\(B\\) be the event of winning. We have \\[\\Pr (B | A_1) = 0.3, \\quad \\Pr (B | A_2) = 0.4, \\quad \\Pr (B | A_3) = 0.5\\] Suppose that you win. What is the probability \\(\\Pr (A_1 | B)\\) that you had an opponent of type 1? Using Bayes’ rule, we have \\[ \\begin{aligned} \\Pr (A_1 | B) &amp;= \\frac{\\Pr (A_1) \\Pr (B | A_1)}{\\Pr (A_1) \\Pr (B | A_1) + \\Pr (A_2) \\Pr (B | A_2) + \\Pr (A_3) \\Pr (B | A_3)} \\\\ &amp;= \\frac{0.5 \\times 0.3}{0.5 \\times 0.3 + 0.25 \\times 0.4 + 0.25 \\times 0.5} \\\\ &amp;= \\frac{0.15}{0.375} \\\\ &amp;= 0.4 \\end{aligned} \\] Example 7.16 (Identifying racial groups by name) How do we identify racial groups from lists of names? The Census Bureau collects information on distribution of names by race. For example, Washington is the “blackest” name in America. \\(\\Pr (\\text{black}) = 0.126\\) \\(\\Pr (\\text{not black}) = 1 - \\Pr (\\text{black}) = 0.874\\) \\(\\Pr (\\text{Washington} | \\text{black}) = 0.00378\\) \\(\\Pr (\\text{Washington} | \\text{not black}) = 0.000060615\\) What is the probability of being black conditional on having the name “Washington”? Using Bayes’ rule, we have \\[ \\begin{aligned} \\Pr(\\text{black}|\\text{Wash} ) &amp; = \\frac{\\Pr(\\text{black}) \\Pr(\\text{Wash}| \\text{black}) }{\\Pr(\\text{Wash} ) } \\\\ &amp; = \\frac{\\Pr(\\text{black}) \\Pr(\\text{Wash}| \\text{black}) }{\\Pr(\\text{black})\\Pr(\\text{Wash}|\\text{black}) + \\Pr(\\text{nb})\\Pr(\\text{Wash}| \\text{nb}) } \\\\ &amp; = \\frac{0.126 \\times 0.00378}{0.126\\times 0.00378 + 0.874 \\times 0.000060616} \\\\ &amp; \\approx 0.9 \\end{aligned} \\] Example 7.17 (Let's Make a Deal (aka the Monty Hall problem)) Suppose you’re on a game show, and you’re given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what’s behind the doors, opens another door, say No. 3, which has a goat. He then says to you, “Do you want to pick door No. 2?” Is it to your advantage to switch your choice? Suppose we have three doors: \\(A\\) - event where the first door contains the car \\(B\\) - event where the second door contains the car \\(C\\) - event where the third door contains the car The contestant guesses the first door. \\(\\Pr(A) = 1/3 \\leadsto\\) a chance of winning without switch. If the third door is revealed to not have a car: \\[ \\begin{aligned} \\Pr(B| \\text{door 3 revealed} ) &amp; = \\frac{\\Pr(B)\\Pr(\\text{door 3 revealed} | B)}{\\Pr(B)\\Pr(\\text{door 3 revealed} | B) + \\Pr(A) \\Pr(\\text{door 3 revealed} | A) } \\\\ &amp; = \\frac{1/3 \\times 1}{1/3 \\times 1 + 1/3 \\times 1/2 } = \\frac{1/3}{1/2} = \\frac{2}{3} \\\\ \\Pr(A| \\text{door 3 revealed} ) &amp; = \\frac{\\Pr(A) \\Pr(\\text{door 3 revealed} | A)}{ \\Pr(B)\\Pr(\\text{door 3 revealed} | B) + \\Pr(A) \\Pr(\\text{door 3 revealed} | A) } \\\\ &amp; = \\frac{1/3 \\times 1/2}{1/3 \\times 1 + 1/3 \\times 1/2} = \\frac{1}{3} \\end{aligned} \\] The contestant doubles her chances of winning by switching her door. Example 7.18 (False-positive puzzle) A test for a certain rare disease is assumed to be correct 95% of the time: if a person has the disease, the test results are positive with probability \\(0.95\\), and if the person does not have the disease, the test results are negative with probability \\(0.95\\). A random person drawn from a certain population has probability \\(0.001\\) of having the disease. Given that the person just tested positive, what is the probability of having the disease? If \\(A\\) is the event that the person has the disease, and \\(B\\) is the event that the test results are positive \\[ \\begin{aligned} \\Pr (A) &amp;= 0.001 \\\\ \\Pr (A^c) &amp;= 0.999 \\\\ \\Pr (B | A) &amp;= 0.95 \\\\ \\Pr (B | A^c) &amp;= 0.05 \\end{aligned} \\] The desired probability \\(\\Pr (A|B)\\) is \\[ \\begin{aligned} \\Pr (A|B) &amp;= \\frac{\\Pr (A) \\Pr (B|A)}{\\Pr (A) \\Pr (B|A) + \\Pr (A^c) \\Pr (B | A^c)} \\\\ &amp;= \\frac{0.001 \\times 0.95}{0.001 \\times 0.95 + 0.999 \\times 0.05} \\\\ &amp;= 0.0187 \\end{aligned} \\] 7.8 Independence of probabilities Does one event provide information about another event? Two events \\(E\\) and \\(F\\) are independent if \\[\\Pr(E\\cap F ) = \\Pr(E)\\Pr(F) \\] If \\(E\\) and \\(F\\) are not independent, we’ll say they are dependent. Independence is symmetric: if \\(F\\) is independent of \\(E\\), then \\(E\\) is indepenent of \\(F\\). Suppose \\(E\\) and \\(F\\) are independent. Then, \\[ \\begin{aligned} \\Pr(E|F) &amp; = \\frac{\\Pr(E \\cap F) }{\\Pr(F) } \\\\ &amp; = \\frac{\\Pr(E)\\Pr(F)}{\\Pr(F)} \\\\ &amp; = \\Pr(E) \\end{aligned} \\] Conditioning on the event \\(F\\) does not modify the probability of \\(E\\). There is no information about \\(E\\) in \\(F\\). 7.8.1 Rolling a 4-sided die Consider an experiment involving two successive rolls of a 4-sided die in which all 16 possible outcomes are equally likely and have probability \\(1/16\\). 7.8.1.1 Part A Are the events \\[A_i = \\{ \\text{1st roll results in } i \\}, \\quad B_j = \\{ \\text{2nd roll results in } j \\}\\] independent? We have \\[ \\begin{aligned} \\Pr (A_i \\cap B_j) &amp;= \\Pr (\\text{the outcome of the two rolls is } (i,j)) = \\frac{1}{16} \\\\ \\Pr (A_i) &amp;= \\frac{\\text{number of elements in } A_i}{\\text{total number of possible outcomes}} = \\frac{4}{16} \\\\ \\Pr (B_j) &amp;= \\frac{\\text{number of elements in } B_j}{\\text{total number of possible outcomes}} = \\frac{4}{16} \\end{aligned} \\] We observe that \\(\\Pr (A_i \\cap B_j) = \\Pr (A_i) \\Pr (B_j)\\), and the independence of \\(A_i\\) and \\(B_j\\) is verified. 7.8.1.2 Part B Are the events \\[A = \\{ \\text{1st roll is a 1} \\}, \\quad B = \\{ \\text{sum of the two rolls is a 5} \\}\\] independent? The answer here is not quite obvious. We have \\[\\Pr (A \\cap B) = \\Pr (\\text{the result of the two rolls is } (1,4)) = \\frac{1}{16}\\] and also \\[\\Pr (A) = \\frac{\\text{number of elements of } A}{\\text{total number of possible outcomes}} = \\frac{4}{16}\\] The event \\(B\\) consists of the outcomes \\((1,4), (2,3), (3,2), (4,1)\\), and \\[\\Pr (B) = \\frac{\\text{number of elements of } B}{\\text{total number of possible outcomes}} = \\frac{4}{16}\\] Thus, we see that \\(\\Pr (A \\cap B) = \\Pr (A) \\Pr (B)\\), and the independence of \\(A_i\\) and \\(B_j\\) is verified. 7.8.1.3 Part C Are the events \\[A = \\{ \\text{maximum of the two rolls is 2} \\}, \\quad B = \\{ \\text{minimum of the two rolls is 2} \\}\\] independent? Intuitively, the answer is “no” because the minimum of the two rolls conveys some information about the maximum. For example, if the minimum is \\(2\\) then the maximum cannot be \\(1\\). More precisely, to verify that \\(A\\) and \\(B\\) are not indpendent, we calculate \\[\\Pr (A \\cap B) = \\Pr (\\text{the result of the two rolls is } (2,2)) = \\frac{1}{16}\\] and also \\[ \\begin{aligned} \\Pr (A) &amp;= \\frac{\\text{number of elements in } A_i}{\\text{total number of possible outcomes}} = \\frac{3}{16} \\\\ \\Pr (B) &amp;= \\frac{\\text{number of elements in } B_j}{\\text{total number of possible outcomes}} = \\frac{5}{16} \\end{aligned} \\] We have \\(\\Pr (A) \\Pr (B) = \\frac{15}{16^2}\\), so that \\(\\Pr (A \\cap B) \\neq \\Pr (A) \\Pr (B)\\), and \\(A\\) and \\(B\\) are not independent 7.8.2 Independence and causal inference Independence matters a great deal when conducting observational studies. We often want to infer the effect of some treatment: Incumbency on vote return College education and job earnings In observational studies we observe what we see to make an inference. The problem is that units (e.g. people) self-select into receiving the treatment or not. A simple example is evaluating the effectiveness of job training programs. In an observational study, we could compare people who enroll in job training and people who do not, and measure whether or not they obtain a job within six months. The problem is that people who choose to enroll in job training can be systematically different from people who do not. That is, \\[\\Pr (\\text{job} | \\text{training in study}) \\neq \\Pr(\\text{job} | \\text{forced training})\\] Selection into job training programs is not independent from the effect of the job training program itself. Background characteristics are any difference between treatment and control groups besides the act of treatment itself. Observational studies cannot control for all possible background characteristics. Instead, experiments make background characteristics and treatment status independent. 7.8.3 Independence of a collection of events We say that the events \\(A_1, A_2, \\ldots, A_n\\) are independent if \\[\\Pr \\left( \\bigcap_{i \\in S} A_i \\right) = \\prod_{i \\in S} \\Pr (A_i),\\quad \\text{for every subset } S \\text{ of } \\{1,2,\\ldots,n \\}\\] For the case of three events, \\(A_1, A_2, A_3\\), independence amounts to satisfying the four conditions \\[ \\begin{aligned} \\Pr (A_1 \\cap A_2) &amp;= \\Pr (A_1) \\Pr (A_2) \\\\ \\Pr (A_1 \\cap A_3) &amp;= \\Pr (A_1) \\Pr (A_3) \\\\ \\Pr (A_2 \\cap A_3) &amp;= \\Pr (A_2) \\Pr (A_3) \\\\ \\Pr (A_1 \\cap A_2 \\cap A_3) &amp;= \\Pr (A_1) \\Pr (A_2) \\Pr (A_3) \\end{aligned} \\] 7.8.4 Independent trials and the binomial probabilities If an experiment involves a sequence of independent but identical stages, we say that we have a sequence of independent trials. In the case where there are only two possible results of each stage, we say that we have a sequence of independent Bernoulli trials. Heads or tails Success or fail Rains or does not rain Consider an experiment that consists of \\(n\\) independent tosses of a coin, in which the probability of heads is \\(p\\), where \\(p\\) is some number between 0 and 1. In this context, independence means that the events \\(A_1, A_2, \\ldots, A_n\\) are independent where \\(A_i = i \\text{th toss is a heads}\\). Let us consider the probability \\[p(k) = \\Pr(k \\text{ heads come up in an } n \\text{-toss sequence})\\] The probability of any given sequence that contains \\(k\\) heads is \\(p^k (1-p)^{n-k}\\), so we have \\[p(k) = \\binom{n}{k} p^k (1-p)^{n-k}\\] where we use the notation \\[\\binom{n}{k} = \\text{number of distinct } n \\text{-toss sequences that contain } k \\text{ heads}\\] The numbers \\(\\binom{n}{k}\\) (read as “\\(n\\) choose \\(k\\)”) are known as the binomial coefficients, while the probabilities \\(p(k)\\) are known as the binomial probabilities. Using a counting argument, we can show that \\[\\binom{n}{k} = \\frac{n!}{k! (n-k)!}, \\quad k=0,1,\\ldots,n\\] where for any positive integer \\(i\\) we have \\[i! = 1 \\times 2 \\times \\cdots \\times (i-1) \\times i\\] and, by convention, \\(0! = 1\\). Note that the binomial probabilities \\(p(k)\\) must sum to 1, thus showing the binomial formula \\[\\sum_{k=0}^n \\binom{n}{k} p^k (1-p)^{n-k} = 1\\] 7.8.4.1 Reliability of an \\(k\\)-out-of-\\(n\\) system A system consists of \\(n\\) identical components, each of which is operational with probability \\(p\\), independent of other components. The system is operational if at least \\(k\\) out of the \\(n\\) components are operational. What is the probability that the system is operational? Let \\(A_i\\) be the event that exactly \\(i\\) components are operational. The probability that the system is operational is the probability of the union \\(\\bigcup_{i=k}^n A_i\\), and since the \\(A_i\\) are disjoint, it is equal to \\[\\sum_{i=k}^n \\Pr (A_i) = \\sum_{i=k}^n p(i)\\] where \\(p(i)\\) are the binomial probabilities. Thus, the probability of an operational system is \\[\\sum_{i=k}^n \\binom{n}{i} p^i (1-p)^{n-i}\\] For instance, if \\(n=100, k=60, p=0.7\\), the probability of an operational system is 0.979. 7.9 Counting Frequently we need to calculate the total number of possible outcomes in a sample space. For example, when we want to calculate the probability of an event \\(A\\) with a finite number of equally likely outcomes, each of which has an already known probability \\(p\\), then the probability of \\(A\\) is given by \\[\\Pr (A) = p \\times (\\text{number of elements of } A)\\] 7.9.1 Counting principle Consider a process that consists of \\(r\\) stages. Suppose that: There are \\(n_1\\) possible results at the first stage. For every possible result at the first stage, there are \\(n_2\\) possible results at the second stage. More generally, for any sequence of possible results at the first \\(i-1\\) stages, there are \\(n_i\\) possible results at the \\(i\\)th stage. Then, the total number of possible results of the \\(r\\)-stage process is \\[n_1, n_2, \\cdots, n_r\\] 7.9.1.1 Example - telephone numbers A local telephone number is a 7-digit sequence, but the first digit has to be different from 0 or 1. How many distinct telephone numbers are there? We can visualize the choice of a sequence as a sequential process, where we select one digit at a time. We have a total of 7 stages, and a choice of one out of 10 elements at each stage, except for the first stage where we have only 8 choices. Therefore, the answer is \\[8 \\times 10 \\times 10 \\times 10 \\times 10 \\times 10 \\times 10 = 8 \\times 10^6\\] 7.9.2 Permutations We start with \\(n\\) distinct objects, and let \\(k\\) be some positive integer such that \\(k \\leq n\\). We wish to count the number of different ways that we can pick \\(k\\) out of these \\(n\\) objects and arrange them in a sequence (i.e. the number of distinct \\(k\\)-object sequences). The number of possible sequences, called \\(k\\)-permutations, is \\[ \\begin{aligned} n(n-1) \\cdots (n-k-1) &amp;= \\frac{n(n-1) \\cdots (n-k+1) (n-k) \\cdots 2 \\times 1}{(n-k) \\cdots 2 \\times 1} \\\\ &amp;= \\frac{n!}{(n-k)!} \\end{aligned} \\] 7.9.2.1 Examples Example 7.19 (Counting letters) Let us count the number of words that consist of four distinct letters. This is the problem of counting the number of 4-permutations of the 26 letters in the alphabet. The desired number is \\[\\frac{n!}{(n-k)!} = \\frac{26!}{22!} = 26 \\times 25 \\times 24 \\times 23 = 358,800\\] Example 7.20 (De Méré's puzzle) A six-sided die is rolled three times independently. Which is more likely: a sum of 11 or a sum of 12? A sum of 11 is obtained with the following 6 combinations: \\[(6,4,1) (6,3,2) (5,5,1) (5,4,2) (5,3,3) (4,4,3)\\] A sum of 12 is obtained with the following 6 combinations: \\[(6,5,1) (6,4,2) (6,3,3) (5,5,2) (5,4,3) (4,4,4)\\] Each combination of 3 distinct numbers corresponds to 6 permutations, where \\(k=n\\): \\[3! = 3 \\times 2 \\times 1 = 6\\] while each combination of 3 numbers, two of which are equal, corresponds to 3 permutations. Counting the number of permutations in the 6 combinations corresponding to a sum of 11, we obtain \\(6+6+3+6+3+3 = 27\\) permutations. Counting the number of permutations in the 6 combinations corresponding to a sum of 12, we obtain \\(6 + 6 + 3 + 3 + 6 + 1 = 25\\) permutations. Since all permutations are equally likely, a sum of 11 is more likely than a sum of 12. 7.9.3 Combinations There are \\(n\\) people and we are interested in forming a committee of \\(k\\). How many different committees are possible? Notice that this is a counting problem inside of a counting problem: we need to count the number of \\(k\\)-element subsets of a given \\(n\\)-element set. In a combination, there is no ordering of the selected elements. For example, whereas the 2-permutations of the letters \\(A, B, C, D\\) are \\[AB, BA, AC, CA, AD, DA, BC, CB, BD, DB, CD, DC\\] the combinations of two out of these four letters are \\[AB, AC, AD, BC, BD, CD\\] In this example, we group together duplicates that are not distinct and tabulate their frequency. More generally, we can view each combination as associated with \\(k!\\) duplicate \\(k\\)-permutation. Hence, the number of possible combinations is equal to \\[\\frac{n!}{k!(n-k)!}\\] 7.9.3.1 Examples Example 7.21 (Counting letters redux) The number of combinations of two out of the four letters \\(A, B, C, D\\) is found by letting \\(n=4\\) and \\(k=2\\). It is \\[\\binom{n}{k} = \\binom{4}{2} = \\frac{4!}{2!2!} = 6\\] Example 7.22 (Parking cars) Twenty distinct cars park in the same parking lot every day. Ten of these cars are US-made, while the other ten are foreign-made. The parking lot has exactly twenty spaces, all in a row, so the cars park side by side. However, the drivers have varying schedules, so the position any car might take on a certain day is random. In how many different ways can the cars line up? Since the cars are all distinct, there are \\(n! = 20!\\) ways to line them up. What is the probability that on a given day, the cars will park in such a way that they alternate (no two US-made cars are adjacent and no two foreign-made are adjacent?) To find the probability that the cars will be parked so that they alternate, we count the number of “favorable” outcomes, and divide by the total number of possible outcomes found in part (a). We count in the following manner. We first arrange the US cars in an ordered sequence (permutation). We can do this in \\(10!\\) ways, since there are \\(10\\) distinct cars. Similarly, arrange the foreign cars in an ordered sequence, which can also be done in \\(10!\\) ways. Finally, interleave the two sequences. This can be done in two different ways, since we can let the first car be either US-made or foreign. Thus, we have a total of \\(2 \\times 10! \\times 10!\\) possibilities, and the desired probability is \\[\\frac{2 \\times 10! \\times 10!}{20!}\\] Note that we could have solved the second part of the problem by neglecting the fact that the cars are distinct. Suppose the foreign cars are indistinguishable, and also that the US cars are indistinguishable. Out of the 20 available spaces, we need to choose 10 spaces in which to place the US cars, and thus there are \\(\\binom{20}{10}\\) possible outcomes. Out of these outcomes, there are only two in which the cars alternate, depending on whether we start with a US or a foreign car. Thus, the desired probability is \\(2 / \\binom{20}{10}\\), which coincides with our earlier answer. References "],["discrete-random-variables.html", "Lecture 8 Discrete random variables Learning objectives Supplemental readings 8.1 Random variable 8.2 Probability mass functions 8.3 Cumulative mass function 8.4 Famous discrete random variables 8.5 Functions of random variables 8.6 Expectation, mean, and variance 8.7 Cumulative mass function, redux", " Lecture 8 Discrete random variables Learning objectives Define random variables Distinguish between discrete and interval variables Identify discrete random variable distributions relevant to social science Review measures of central tendency and dispersion Define expected value and variance Define cumulative mass functions (CMFs) for discrete random variables Supplemental readings Chapter 2.1-.4, Bertsekas and Tsitsiklis (2008) Equivalent reading from Bertsekas and Tsitsiklis lecture notes 8.1 Random variable A random variable is a random process or variable with a numerical outcome. More formally, it is a random variable \\(X\\) that is a function of the sample space Number of incumbents who win An indicator whether a country defaults on a loan (1 if a default, 0 otherwise) Number of casualties in a war (rather than all possible outcomes) \\[X:\\text{Sample Space} \\rightarrow \\Re\\] Example 8.1 (Treatment assignment) Suppose we have \\(3\\) units, flipping fair coin (\\(\\frac{1}{2}\\)) to assign each unit. Assign to \\(T=\\)Treatment or \\(C=\\)control, with \\(X\\) = Number of units received treatment. The function is \\[ X = \\left \\{ \\begin{array} {ll} 0 \\text{ if } (C, C, C) \\\\ 1 \\text{ if } (T, C, C) \\text{ or } (C, T, C) \\text{ or } (C, C, T) \\\\ 2 \\text{ if } (T, T, C) \\text{ or } (T, C, T) \\text{ or } (C, T, T) \\\\ 3 \\text{ if } (T, T, T) \\end{array} \\right. \\] In other words: \\[ \\begin{aligned} X( (C, C, C) ) &amp; = 0 \\\\ X( (T, C, C)) &amp; = 1 \\\\ X((T, C, T)) &amp; = 2 \\\\ X((T, T, T)) &amp; = 3 \\end{aligned} \\] Example 8.2 (Legislative calls) \\(X\\) = Number of Calls into congressional office in some period \\(p\\) \\[X(c) = c\\] Example 8.3 (Electoral outcome) Define \\(v\\) as the proportion of vote the candidate receives. \\(X = 1\\) if \\(v&gt;0.50\\), while \\(X = 0\\) if \\(v&lt;0.50\\). For example, if \\(v = 0.48\\), then \\(X(v) = 0\\). Example 8.4 (Loan default) An indicator whether a country defaults on a loan (1 if a default, 0 otherwise) Not all random variables are the result of an experiment - most are observational. 8.1.1 Discrete random variables Discrete random variables are a random variable with a finite or countably infinite range. A random variable with an uncountably infinite number of values is continuous. 8.2 Probability mass functions A probability mass function (PMF) defines the probability of the values that a discrete random variable can take. 8.2.1 Intuition Go back to our experiment example – probability comes from probability of outcomes \\[\\Pr(C, T, C) = \\Pr(C)\\Pr(T)\\Pr(C) = \\frac{1}{2}\\frac{1}{2}\\frac{1}{2} = \\frac{1}{8}\\] This applies to all outcomes: \\[ \\begin{aligned} p(X = 0) &amp; = \\Pr(C, C, C) = \\frac{1}{8}\\\\ p(X = 1) &amp; = \\Pr(T, C, C) + \\Pr(C, T, C) + \\Pr(C, C, T) = \\frac{3}{8} \\\\ p(X = 2) &amp; = \\Pr(T, T, C) + \\Pr(T, C, T) + \\Pr(C, T, T) = \\frac{3}{8} \\\\ p(X = 3) &amp; = \\Pr(T, T, T) = \\frac{1}{8} \\\\ p(X = a) &amp;= 0 \\, \\forall \\, a \\notin (0, 1, 2, 3) \\end{aligned} \\] 8.2.2 Definition Definition 8.1 (Probability mass function) For a discrete random variable \\(X\\), define the probability mass function \\(p_X(x)\\) as \\[p_X(x) = \\Pr(X = x)\\] Use upper-case letters to denote random variables, and lower-case letters to denote real numbers such as the numerical values of a random variable. Note that \\[\\sum_x p_{X}(x) = 1\\] \\(x\\) ranges over all possible values of \\(X\\) which makes sense - probability must sum to 1. We can also add probabilities for smaller sets \\(S\\) of possible values of \\(X \\in S\\). \\[\\Pr(X \\in S) = \\sum_{x \\in S} p_X (x)\\] For example, if \\(X\\) is the number of heads obtained in two independent tosses of a fair coin, the probability of at least one head is \\[\\Pr (X &gt; 0) = \\sum_{x=1}^2 p_X (x) = \\frac{1}{2} + \\frac{1}{4} = \\frac{3}{4}\\] To calculate the PMF of a random variable \\(X\\) For each possible value \\(x\\) of \\(X\\) Collect all possible outcomes that give rise to the event \\(\\{ X=x \\}\\) Add their probabilities to obtain \\(p_X (x)\\) Example 8.5 (Topic models) Topics are distinct concepts (war in Afghanistan, national debt, fire department grants) found in a corpus of text documents. Mathematically they are probability mass functions on Words (probability of using word, when discussing a topic). Suppose we have a set of words: (afghanistan, fire, department, soldier, troop, war, grant) Topic 1 (war) \\(\\Pr(\\text{afghanistan}) = 0.3\\); \\(\\Pr(\\text{fire}) = 0.0001\\); \\(\\Pr(\\text{department}) = 0.0001\\); \\(\\Pr(\\text{soldier}) = 0.2\\); \\(\\Pr(\\text{troop}) = 0.2\\); \\(\\Pr(\\text{war})=0.2997\\); \\(\\Pr(\\text{grant})=0.0001\\) Topic 2 (fire departments): \\(\\Pr(\\text{afghanistan}) = 0.0001\\); \\(\\Pr(\\text{fire}) = 0.3\\); \\(\\Pr(\\text{department}) = 0.2\\); \\(\\Pr(\\text{soldier}) = 0.0001\\); \\(\\Pr(\\text{troop}) = 0.0001\\); \\(\\Pr(\\text{war})=0.0001\\); \\(\\Pr(\\text{grant})=0.2997\\) Topic models take a set of documents and estimate topics. 8.3 Cumulative mass function Definition 8.2 (Cumulative mass function) For a random variable \\(X\\), define the cumulative mass function \\(F(x)\\) as, \\[F(x) = \\Pr(X \\leq x)\\] The CMF characterizes how probability cumulates as \\(X\\) gets larger. \\(F(x) \\in [0,1]\\), and \\(F(x)\\) is non-decreasing. 8.3.1 Three person experiment Consider the three person experiment: \\(\\Pr(T) = \\Pr(C) = 1/2\\). What is \\(F(2)\\)? \\[ \\begin{aligned} F(2) &amp; = \\Pr(X = 0) + \\Pr(X = 1) + \\Pr(X = 2) \\\\ &amp; = \\frac{1}{8} + \\frac{3}{8} + \\frac{3}{8} \\\\ &amp; = \\frac{7}{8} \\end{aligned} \\] What is \\(F(2) - F(1)\\)? \\[ \\begin{aligned} F(2) - F(1) &amp; = [\\Pr(X = 0) + \\Pr(X = 1) + \\Pr(X = 2)] \\nonumber \\\\ &amp; \\quad -[\\Pr(X = 0) + \\Pr(X = 1)] \\\\ F(2) - F(1) &amp; = \\Pr(X = 2) \\end{aligned} \\] There is a close relationship between PMFs and CMFs. Cumulative functions are similar to integration, but over a discrete set of values. 8.4 Famous discrete random variables 8.4.1 Bernoulli Definition 8.3 (Bernoulli random variable) Suppose \\(X\\) is a random variable, with \\(X \\in \\{0, 1\\}\\) and \\(\\Pr(X = 1) = \\pi\\). Then we will say that \\(X\\) is Bernoulli random variable, \\[p_X(k)= \\pi^{k} (1- \\pi)^{1 - k}\\] for \\(k \\in \\{0,1\\}\\) and \\(p_X(k) = 0\\) otherwise. \\(\\pi\\) does not refer to \\(3.14159\\) in this instance, it is a variable. Terrible choice of symbol, but it is convention. We will (equivalently) say that \\[Y \\sim \\text{Bernoulli}(\\pi)\\] Suppose we flip a fair coin and \\(Y = 1\\) if the outcome is Heads. \\[ \\begin{aligned} Y &amp; \\sim \\text{Bernoulli}(1/2) \\nonumber \\\\ p_X(1) &amp; = (1/2)^{1} (1- 1/2)^{ 1- 1} = 1/2 \\nonumber \\\\ p_X(0) &amp; = (1/2)^{0} (1- 1/2)^{1 - 0} = (1- 1/2) \\nonumber \\end{aligned} \\] Figure 8.1: Example Bernoulli probability mass functions Other examples include: Person is healthy or sick Person votes or does not vote 8.4.2 Binomial Definition 8.4 (Binomial random variable) Suppose \\(X\\) is a random variable that counts the number of successes in \\(N\\) independent and identically distributed Bernoulli trials. Then \\(X\\) is a Binomial random variable, \\[p_X(k) = {{N}\\choose{k}}\\pi^{k} (1- \\pi)^{n-k}\\] for \\(k \\in \\{0, 1, 2, \\ldots, N\\}\\) and \\(p_X(k) = 0\\) otherwise, and \\(\\binom{N}{k} = \\frac{N!}{(N-k)! k!}\\). Equivalently, \\[Y \\sim \\text{Binomial}(N, \\pi)\\] Binomial random variables can be used as a model to count the number of successes across \\(N\\) trials. 8.4.2.1 Example Figure 8.2: Example Binomial probability mass functions Recall our experiment example: \\(\\Pr(T) = \\Pr(C) = 1/2\\) \\(Z =\\) number of units assigned to treatment \\[ \\begin{aligned} Z &amp; \\sim \\text{Binomial}(1/2)\\\\ p_Z(0) &amp; = {{3}\\choose{0}} (1/2)^{0} (1- 1/2)^{3-0} = 1 \\times \\frac{1}{8}\\\\ p_Z(1) &amp; = {{3}\\choose{1}} (1/2)^{1} (1 - 1/2)^{2} = 3 \\times \\frac{1}{8} \\\\ p_Z(2) &amp; = {{3}\\choose{2}} (1/2)^{2} (1- 1/2)^1 = 3 \\times \\frac{1}{8} \\\\ p_Z(3) &amp; = {{3}\\choose{3}} (1/2)^{3} (1 - 1/2)^{0} = 1 \\times \\frac{1}{8} \\end{aligned} \\] 8.4.3 Geometric A model to count the number of trials of a Bernoulli outcome before success occurs the first time Definition 8.5 (Geometric random variable) Suppose \\(X\\) is a random variable that counts the number of tosses needed for a head to come up the first time. Its PMF is \\[p_X(k) = (1 - p)^{k-1}p, \\quad k = 1, 2, \\ldots\\] \\((1 - p)^{k-1}p\\) is the probability of the sequence consisting of \\(k-1\\) successive trials followed by a head. This is a valid PMF because \\[ \\begin{aligned} \\sum_{k=1}^{\\infty} p_X(k) &amp;= \\sum_{k=1}^{\\infty} (1 - p)^{k-1}p \\\\ &amp;= p \\sum_{k=1}^{\\infty} (1 - p)^{k-1} \\\\&amp; = p \\times \\frac{1}{1 - (1-p)} \\\\ &amp;= 1 \\end{aligned} \\] Geometric random variables can be used to count the number of trials of a Bernoulli outcome before success occurs the first time. Figure 8.3: Example Geometric probability mass functions Examples include: Number of attempts before passing a test Finding a missing item in a search 8.4.4 Poisson Often social scientists are interested in counting the number of events that occur: Number of wars started Number of speeches made Number of bribes offered Number of people waiting for license These are generally referred to as event counts. Definition 8.6 (Poisson random variable) Suppose \\(X\\) is a random variable that takes on values \\(X \\in \\{0, 1, 2, \\ldots, \\}\\) and that \\(\\Pr(X = k) = p_X(k)\\) is, \\[p_X(k) = e^{-\\lambda} \\frac{\\lambda^{k}}{k!}, \\quad k = 0,1,2,\\ldots\\] for \\(k \\in \\{0, 1, \\ldots, \\}\\) and \\(0\\) otherwise. Then we will say that \\(X\\) follows a Poisson distribution with rate parameter \\(\\lambda\\) \\[X \\sim \\text{Poisson}(\\lambda)\\] Figure 8.4: Example Poisson probability mass functions Example 8.6 (Presidential threats) Suppose the number of threats a president makes in a term is given by \\(X \\sim \\text{Poisson}(5)\\)18 What is the probability the president will make ten threats? \\[p_X(10) = e^{-\\lambda} \\frac{5^{10}}{10!}\\] 8.4.4.1 Approximating a binomial random variable The Poisson PMF with parameter \\(\\lambda\\) is a good approximation for a binomial PMF with parameters \\(n\\) and \\(p\\) \\[e^{-\\lambda} \\frac{\\lambda^{k}}{k!} \\approx {{N}\\choose{k}}\\pi^{k} (1- \\pi)^{n-k}, \\quad \\text{if } k \\ll n\\] provided \\(\\lambda = np\\), \\(n\\) is very large, and \\(p\\) is very small. Sometimes using the Poisson PMF results in simpler models and easier calculations. For instance, \\(n = 100\\) and \\(p = 0.01\\). Using the binomial PMF \\[\\frac{100!}{95! 5!} \\times 0.01^5 (1 - 0.01)^{95} = 0.00290\\] Versus using the Poisson PMF with \\(\\lambda = np = 100 \\times 0.01 = 1\\) \\[e^{-1} \\frac{1}{5!} = 0.00306\\] 8.5 Functions of random variables Given a random variable \\(X\\), you may wish to create a new random variable \\(Y\\) using transformations of \\(X\\). This could be a linear function of the form \\[Y = g(X) = aX + b\\] where \\(a\\) and \\(b\\) are scalars. It could be a logarithmic transformation \\[g(X) = \\log(X)\\] If \\(Y = g(X)\\) is a function of a random variable \\(X\\), then \\(Y\\) is also a random variable. All outcomes in the sample space defined with a numerical value \\(x\\) for \\(X\\) also have a numerical value \\(y = g(x)\\) for \\(Y\\). 8.6 Expectation, mean, and variance The PMF of a random variable \\(X\\) provides several numbers – the probabilities of all possible values of \\(X\\). Often it is desirable to summarize this information into a single representative number: the expectation of \\(X\\) – weighted average of the possible values of \\(X\\). 8.6.1 Motivation Consider spinning a wheel of fortune many times. At each spin, one of the numbers \\(m_1, m_2, \\ldots, m_n\\) comes up with corresponding probability \\(p_1, p_2, \\ldots, p_n\\), and this is your monetary reward from that spin. What is the amount of money that you “expect” to get “per spin”? The terms “expect” and “per spin” are a little ambiguous, but here is a reasonable interpretation. Suppose you spin the wheel \\(k\\) times, and that \\(k_i\\) is the number of times that the outcome is \\(m_i\\). Then, the total amount received is \\(m_1 k_1 + m_2 k_2 + \\ldots + m_n k_n\\). The amount received per spin is a simple average: \\[M = \\frac{m_1 k_1 + m_2 k_2 + \\ldots + m_n k_n}{k}\\] If the number of spins \\(k\\) is very large and we interpret probabilities as relative frequencies, we could anticipate that \\(m_i\\) comes up a fraction of times roughly equal to \\(p_i\\): \\[\\frac{k_i}{k} \\approx p_i, i = 1, \\ldots,n\\] Thus, the amount of money you “expect” to receive is \\[M = \\frac{m_1 k_1 + m_2 k_2 + \\ldots + m_n k_n}{k} \\approx m_1p_1 + m_2p_2 + \\ldots + m_np_n\\] 8.6.2 Expectation Definition 8.7 (Expected value) Define expected value (known as expectation or the mean) of a random variable \\(X\\), with PMF \\(p_X\\) as \\[\\E[X] = \\sum_{x:p(x)&gt;0} x p(x)\\] where \\(\\sum_{x:p(x)&gt;0}\\) is all values of \\(X\\) with probability greater than 0. In words: for all values of \\(x\\) with \\(p(x)\\) greater than zero, take the weighted average of the values. Example 8.7 Suppose \\(X\\) is number of units assigned to treatment, in one of our previous example. \\[ X = \\left \\{ \\begin{array} {ll} 0 \\text{ if } (C, C, C) \\\\ 1 \\text{ if } (T, C, C) \\text{ or } (C, T, C) \\text{ or } (C, C, T) \\\\ 2 \\text{ if } (T, T, C) \\text{ or } (T, C, T) \\text{ or } (C, T, T) \\\\ 3 \\text{ if } (T, T, T) \\end{array} \\right. \\] What is \\(\\E[X]\\)? \\[ \\begin{aligned} \\E[X] &amp; = 0\\times \\frac{1}{8} + 1 \\times \\frac{3}{8} + 2 \\times \\frac{3}{8} + 3 \\times \\frac{1}{8} \\\\ &amp; = 1.5 \\end{aligned} \\] The expected value is essentially a weighted average, or the average outcome (value in the middle of the range). It gives us a measure of central tendency. Example 8.8 (A single person poll) Suppose that there is a group of \\(N\\) people. Suppose \\(M&lt; N\\) people approve of Donald Trump’s performance as president, and \\(N - M\\) disapprove of his performance. Draw one person \\(i\\), with \\(\\Pr(\\text{Draw } i ) = \\frac{1}{N}\\) \\[ X = \\left \\{ \\begin{array} {ll} 1 \\text{ if person Approves} \\\\ 0 \\text{ if Disapproves} \\\\ \\end{array} \\right. \\] What is \\(\\E[X]\\)? \\[ \\begin{aligned} \\E[X] &amp; = 1 \\times \\Pr(\\text{Approve}) + 0 \\times \\Pr(\\text{Disapprove}) \\\\ &amp; = 1 \\times \\frac{M}{N} \\\\ &amp; = \\frac{M}{N} \\end{aligned} \\] 8.6.2.1 Indicator variables and probabilities Proposition 8.1 Suppose \\(A\\) is an event. Define random variable \\(I\\) such that \\(I= 1\\) if an outcome in \\(A\\) occurs and \\(I =0\\) if an outcome in \\(A^{c}\\) occurs. Then, \\[\\E[I] = \\Pr(A)\\] Proof. \\[ \\begin{aligned} \\E[I] &amp; = 1 \\times \\Pr(A) + 0 \\times \\Pr(A^{c}) \\\\ &amp; = \\Pr(A) \\end{aligned} \\] 8.6.3 Variance, moments, and the expected value rule Other quantities we care about include variance and moments. 8.6.3.1 Moments 1st moment: \\(\\E[X^1] = \\E[X]\\) - that is, the mean 2nd moment: \\(\\E[X^2]\\) \\(n\\)th moment: \\(\\E[X^n]\\) 8.6.3.2 Variance Expected value is a measure of central tendency. What about spread or dispersion? For each value, we might measure distance from the center. For example, Euclidean distance is squared \\(d(x, E[X])^{2} = (x - E[X])^2\\) Then, we might take weighted average of these distances \\[ \\begin{aligned} \\E[(X - \\E[X])^2] &amp; = \\sum_{x:p_X(x)&gt;0} (x - \\E[X])^2p_X(x) \\\\ &amp; = \\sum_{x:p_X(x)&gt;0} \\left(x^2 p_X(x)\\right) - 2 \\E[X]\\sum_{x:p_X(x)&gt;0} \\left(x p_X(x)\\right) + \\E[X]^2\\sum_{x:p_X(x)&gt;0} p_X(x) \\\\ &amp; = \\E[X^2] - 2\\E[X]^2 + \\E[X]^2 \\\\ &amp; = \\E[X^2] - \\E[X]^2 \\\\ &amp; = \\text{Var}(X) \\end{aligned} \\] Defined as the expected value of the random variable \\((X - \\E[X])^2\\) \\[ \\begin{aligned} \\Var(X) &amp;= \\E[(X - \\E[X])^2] \\\\ &amp;= \\E[X^2] - \\E[X]^2 \\end{aligned} \\] Since \\((X - \\E[X])^2\\) can only take non-negative values, \\(\\Var(X) \\geq 0\\) This is a measure of dispersion of \\(X\\) around its mean. We will define the standard deviation of \\(X\\), \\(\\sigma_X = \\sqrt{\\Var(X)}\\). Standard deviation is easier to interpret sometimes since it is in the same units as \\(X\\). 8.6.3.2.1 Calculating variance of a random variable We could generate the PMF of the random variable \\((X - \\E[X])^2\\), then calculate the expectation of this function – it is just a linear function. Instead, we can take a shortcut: the expected value rule for functions of random variables. Let \\(X\\) be a random variable with PMF \\(p_X\\), and let \\(g(X)\\) be a function of \\(X\\). Then, the expected value of the random variable \\(g(X)\\) is given by \\[\\E[g(X)] = \\sum_{x} g(x) p_X(x)\\] This allows us to rewrite our variance formula: \\[ \\begin{align} \\Var(X) &amp;= \\E[(X - \\E[X])^2] \\\\ \\Var(X) &amp;= \\E[X^2] - \\E[X]^2 \\end{align} \\] We just need the first and second moments to calculate variance. 8.6.4 Practice calculating expectation and variance 8.6.4.1 Bernoulli variable Suppose \\(Y \\sim \\text{Bernoulli}(\\pi)\\) \\[ \\begin{aligned} \\E[Y] &amp; = 1 \\times \\Pr(Y = 1) + 0 \\times \\Pr(Y = 0) \\nonumber \\\\ &amp; = \\pi + 0 (1 - \\pi) \\nonumber = \\pi \\\\ \\Var(Y) &amp; = \\E[Y^2] - \\E[Y]^2 \\nonumber \\\\ \\E[Y^2] &amp; = 1^{2} \\Pr(Y = 1) + 0^{2} \\Pr(Y = 0) \\nonumber \\\\ &amp; = \\pi \\nonumber \\\\ \\Var(Y) &amp; = \\pi - \\pi^{2} \\nonumber \\\\ &amp; = \\pi(1 - \\pi ) \\nonumber \\end{aligned} \\] \\(\\E[Y] = \\pi\\) \\(\\Var(Y) = \\pi(1- \\pi)\\) What is the maximum variance? \\[ \\begin{aligned} \\Var(Y) &amp; = \\pi - \\pi^{2} \\nonumber \\\\ &amp; = 0.5(1 - 0.5 ) \\\\ &amp; = 0.25 \\end{aligned} \\] 8.6.4.2 Binomial \\[Z = \\sum_{i=1}^{N} Y_{i} \\text{ where } Y_{i} \\sim \\text{Bernoulli}(\\pi)\\] \\[ \\begin{aligned} \\E[Z] &amp; = \\E[Y_{1} + Y_{2} + Y_{3} + \\ldots + Y_{N} ] \\\\ &amp; = \\sum_{i=1}^{N} \\E[Y_{i} ] \\\\ &amp; = N \\pi \\\\ \\Var(Z) &amp; = \\sum_{i=1}^{N} \\Var(Y_{i}) \\\\ &amp; = N \\pi (1-\\pi) \\end{aligned} \\] 8.6.5 Decision making using expected values We can use expected values to optimizes the choice between several candidate decisions that result in random rewards. View the expected reward of a decision as its average payoff over a large number of trials, and choose a decision with maximum expected reward. Example 8.9 (Going to war) Suppose country \\(1\\) is engaged in a conflict and can either win or lose. Define \\(Y = 1\\) if the country wins and \\(Y = 0\\) otherwise. Then, \\[Y \\sim \\text{Bernoulli}(\\pi)\\] Suppose country \\(1\\) is deciding whether to fight a war. Engaging in the war will cost the country \\(c\\). If they win, country \\(1\\) receives \\(B\\). What is \\(1\\)’s expected utility from fighting a war? \\[ \\begin{aligned} \\E[U(\\text{war})] &amp; = U(\\text{war} | \\text{win}) \\times \\Pr(\\text{win}) + U(\\text{war} | \\text{lose}) \\times \\Pr(\\text{lose}) \\\\ &amp; = (B - c) \\Pr(Y = 1) + (- c) \\Pr(Y = 0 ) \\\\ &amp; = B \\times \\Pr(Y = 1) - c(\\Pr(Y = 1) + \\Pr(Y = 0)) \\\\ &amp; = B \\times \\pi - c \\end{aligned} \\] Based on your beliefs about the appropriate values for \\(B, \\pi, c\\), you can decide whether to engage in the war If expected utility is greater than 0, you should decide to go to war If expected utility is less than 0, you should decide not to go to war 8.7 Cumulative mass function, redux The cumulative mass function (CMF) defines the the cumulative probability \\(F_X(x)\\) up to the value of \\(x\\). For a discrete random variable \\(X\\), we define the CMF \\(F_X\\) which provides the probability \\(\\Pr (X \\leq x)\\). For every \\(x\\) \\[F_X(x) = \\Pr (X \\leq x) = \\sum_{k \\leq x} p_X(k)\\] Any random variable associated with a given probability model has a CMF. Basic properties of CMFs for discrete random variables are: \\(F_X\\) is monotonically non-decreasing – if \\(x \\leq y\\), then \\(F_X(x) \\leq F_X(y)\\) \\(F_X(x)\\) tends to \\(0\\) as \\(x \\rightarrow -\\infty\\), and to \\(1\\) as \\(x \\rightarrow \\infty\\) \\(F_X(x)\\) is a piecewise constant function of \\(x\\) If \\(X\\) is discrete and takes integer values, the PMF and the CMF can be obtained from each other by summing or differencing: \\[F_X(k) = \\sum_{i = -\\infty}^k p_X(i),\\] \\[p_X(k) = \\Pr (X \\leq k) - \\Pr (X \\leq k-1) = F_X(k) - F_X(k-1)\\] for all integers \\(k\\) 8.7.1 Common CMFs Figure 8.5: Example Bernoulli cumulative mass functions Figure 8.6: Example Binomial cumulative mass functions Figure 8.7: Example Geometric cumulative mass functions Figure 8.8: Example Poisson cumulative mass functions References "],["general-random-variables.html", "Lecture 9 General random variables Learning objectives Supplemental readings 9.1 Continuous random variables 9.2 Probability density function 9.3 Cumulative distribution function 9.4 Normal distribution 9.5 Gamma distribution 9.6 \\(\\chi^2\\) distribution 9.7 Student’s \\(t\\) distribution", " Lecture 9 General random variables Learning objectives Define a continuous random variable Identify continuous random variable distributions relevant to social science Define expected value and variance of continuous random variables Relate continuous random variables to discrete random variables Define cumulative distribution functions (CDFs) for continuous random variables and compare to discrete random variables Estimate probability of events using probability density functions (PDFs) and cumulative distribution functions (CDFs) Supplemental readings Chapter 3.1-.3 Bertsekas and Tsitsiklis (2008) Equivalent reading from Bertsekas and Tsitsiklis lecture notes 9.1 Continuous random variables Continuous random variables are random variables that are not discrete. Examples include: Approval ratings GDP Wait time between wars: \\(X(t) = t\\) for all \\(t\\) Proportion of vote received: \\(X(v) = v\\) for all \\(v\\) Continuous random variables have many analogues to discrete probability distributions, but instead we need calculus to answer questions about probability. 9.2 Probability density function What is the area under the curve under \\(f(x)\\) between \\(.5\\) and \\(2\\)? \\[\\int_{1/2}^{2} f(x)\\,dx = F(2) - F(1/2)\\] 9.2.1 Definition ::: {.definition echo=TRUE name=“Probability density function”} \\(X\\) is a continuous random variable if there exists a nonnegative function defined for all \\(x \\in \\Re\\) having the property for any (measurable) set of real numbers \\(B\\), \\[\\Pr(X \\in B) = \\int_{B} f_X(x)\\,dx\\] Non-negative meaning \\(f(x)\\) is never negative \\(\\Pr(X \\in B)\\): probability that \\(X\\) is is an element of \\(B\\) We’ll call \\(f(\\cdot)\\) the probability density function (PDF) for \\(X\\) ::: The probability that the value of \\(X\\) falls within an interval is \\[\\Pr (a \\leq X \\leq b) = \\int_a^b f_X(x) \\,dx\\] and can be interpreted as the area under the graph of the PDF. For any single value \\(a\\), we have \\(\\Pr (X = a) = \\int_a^a f_X(x) \\,dx = 0\\). Note that to qualify as a PDF, a function \\(f_X\\) must be nonnegative, i.e. \\(f_X(x) \\geq 0\\) for every \\(x\\), and must also have the normalization property \\[\\int_{-\\infty}^{\\infty} f_X(x) \\,dx = \\Pr (-\\infty \\leq X \\leq \\infty) = 1\\] 9.2.2 Example: Uniform Random Variable \\[X \\sim \\text{Uniform}(0,1)\\] \\[ f_X(x) = \\left\\{ \\begin{array}{ll} c &amp; \\quad \\text{if } 0 \\leq x \\leq 1 \\\\ 0 &amp; \\quad \\text{otherwise} \\end{array} \\right. \\] for some constant \\(c\\). The constant can be determined from the normalization property \\[1 = \\int_{-\\infty}^{\\infty} f_X(x)\\,dx = \\int_0^1 c \\,dx = c \\int_0^1 \\,dx = c\\] so that \\(c=1\\). Example 9.1 \\[ \\begin{aligned} \\Pr(X \\in [0.2, 0.5]) &amp; = \\int_{0.2}^{0.5} 1 \\,dx \\\\ &amp; = X |^{0.5}_{0.2} \\\\ &amp; = 0.5 - 0.2 \\\\ &amp; = 0.3 \\end{aligned} \\] Example 9.2 \\[ \\begin{aligned} \\Pr(X \\in [0, 1] ) &amp; = \\int_{0}^{1} 1 \\,dx \\\\ &amp; = X |^{1}_{0} \\\\ &amp; = 1 - 0 \\\\ &amp; = 1 \\end{aligned} \\] Example 9.3 \\[ \\begin{aligned} \\Pr(X \\in [0.5, 0.5]) &amp; = \\int_{0.5}^{0.5} 1\\,dx \\\\ &amp; = X|^{0.5}_{0.5} \\\\ &amp; = 0.5 - 0.5 \\\\ &amp; = 0 \\end{aligned} \\] Example 9.4 \\[ \\begin{aligned} \\Pr(X \\in \\{[0, 0.2]\\cup[0.5, 1]\\}) &amp; = \\int_{0}^{0.2} 1\\,dx + \\int_{0.5}^{1} 1\\,dx \\\\ &amp; = X_{0}^{0.2} + X_{0.5}^{1} \\\\ &amp; = 0.2 - 0 + 1 - 0.5 \\\\ &amp; = 0.7 \\end{aligned} \\] More generally, the PDF of a uniform random variable has the form \\[ f_X(x) = \\left\\{ \\begin{array}{ll} \\frac{1}{b-a} &amp; \\quad \\text{if } a \\leq x \\leq b \\\\ 0 &amp; \\quad \\text{otherwise} \\end{array} \\right. \\] To summarize \\(\\Pr(X = a) = 0\\) \\(\\Pr(X \\in (-\\infty, \\infty) ) = 1\\) If \\(F\\) is antiderivative of \\(f\\), then \\(\\Pr(X \\in [c,d]) = F(d) - F(c)\\)19 9.2.3 Expectation The expected value of a continuous random variable \\(X\\) is defined as \\[\\E[X] = \\int_{-\\infty}^{\\infty} x f_X(x) \\,dx\\] This is similar to the discrete case except instead of a summation operation, we use integration to calculate the expected value. If \\(X\\) is a continuous random variable with a given PDF, any real-valued function \\(Y = g(X)\\) is also a random variable. The mean of \\(g(X)\\) satisfies the expected value rule: \\[\\E[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f_X(x) \\,dx\\] The \\(n\\)th moment is defined as \\(\\E[X^n]\\), the expected value of the random variable \\(X^n\\) The variance, denoted by \\(\\Var(X)\\) is defined as the expected value of the random variable \\((X - \\E[X])^2 = \\E[X^2] - (\\E[X])^2\\) Example 9.5 (Uniform random variable) Consider a uniform PDF over an interval \\([a,b]\\): \\[ \\begin{align} \\E[X] = \\int_{-\\infty}^{\\infty} x f_X(x) \\,dx &amp;= \\int_a^b x \\times \\frac{1}{b-a} \\,dx \\\\ &amp;= \\frac{1}{b-a} \\times \\frac{1}{2}x^2 \\Big|_a^b \\\\ &amp;= \\frac{1}{b-a} \\times \\frac{b^2 - a^2}{2} \\\\ &amp;= \\frac{a+b}{2} \\end{align} \\] To obtain the variance, we first calculate the second moment. We have \\[ \\begin{align} \\E[X^2] = \\int_a^b x^2 \\times \\frac{1}{b-a} \\,dx &amp;= \\frac{1}{b-a} \\int_a^b x^2 \\,dx \\\\ &amp;= \\frac{1}{b-a} \\times \\frac{1}{3}x^3 \\Big|_a^b \\\\ &amp;= \\frac{b^3 - a^3}{3(b-a)} \\\\ &amp;= \\frac{a^2 + ab + b^2}{3} \\end{align} \\] Thus, the variance is \\[ \\begin{align} \\Var(X) = \\E[X^2] - (\\E[X])^2 = \\frac{a^2 + ab + b^2}{3} - \\left( \\frac{a+b}{2} \\right)^2 = \\frac{(b-a)^2}{12} \\end{align} \\] 9.2.4 Exponential random variable An exponential random variable has a PDF of the form \\[ f_X(x) = \\left\\{ \\begin{array}{ll} \\lambda e^{-\\lambda x} &amp; \\quad \\text{if } x \\geq 0 \\\\ 0 &amp; \\quad \\text{otherwise} \\end{array} \\right. \\] where \\(\\lambda\\) is a positive parameter characterizing the PDF. \\[\\E[X] = \\frac{1}{\\lambda}, \\quad \\Var(X) = \\frac{1}{\\lambda^2}\\] It is frequently used to model phenomena of a continuous nature such as the time between arrivals (e.g. time between customers arriving at a restaraunt) and the distance between occurrences (e.g. distance between defects in a plate glass window). It is closely associated with the Poisson discrete random variable, which we will return to later. 9.3 Cumulative distribution function The probability density function (\\(f\\)) characterizes distribution of continuous random variable. Equivalently, the cumulative distribution function characterizes continuous random variables. For a continuous random variable \\(X\\) define its cumulative distribution function (CDF) \\(F_X(x)\\) as, \\[F_X(x) = \\Pr(X \\leq x) = \\int_{-\\infty} ^{x} f_X(t) \\,dt\\] Example 9.6 (Uniform distribution) Suppose \\(X \\sim \\text{Uniform}(0,1)\\), then \\[ \\begin{aligned} F_X(x) &amp; = \\Pr(X\\leq x) \\\\ &amp; = 0 \\text{, if }x&lt; 0 \\\\ &amp; = 1 \\text{, if }x &gt;1 \\\\ &amp; = x \\text{, if } x \\in [0,1] \\end{aligned} \\] 9.3.1 Properties of CDFs \\(F_X\\) is monotonically nodecreasing – if \\(x \\leq y\\), then \\(F_X(x) \\leq F_X(y)\\) \\(F_X(x)\\) tends to \\(0\\) as \\(x \\rightarrow -\\infty\\), and to \\(1\\) as \\(x \\rightarrow \\infty\\) \\(F_X(x)\\) is a continuous function of \\(x\\) If \\(X\\) is continuous, the PDF and CDF can be obtained from each other by integration or differentiation \\[F_X(x) = \\int_{-\\infty}^x f_X(t) \\,dt, \\quad f_X(x) = \\frac{dF_X}{dx} (x)\\] 9.4 Normal distribution Suppose \\(X\\) is a random variable with \\(X \\in \\Re\\) and density \\[f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\\] where \\(\\mu\\) and \\(\\sigma\\) are two scalar parameters characterizing the PDF, with \\(\\sigma\\) assumed positive. Then \\(X\\) is a normally distributed random variable with parameters \\(\\mu\\) and \\(\\sigma^2\\). Equivalently, we’ll write \\[X \\sim \\text{Normal}(\\mu, \\sigma^2)\\] 9.4.1 Expected value/variance of normal distribution \\(Z\\) is a standard normal distribution if \\[Z \\sim \\text{Normal}(0,1)\\] We’ll call the cumulative distribution function of \\(Z\\), \\[F_{Z}(x) = \\frac{1}{\\sqrt{2\\pi} }\\int_{-\\infty}^{x} \\exp(-z^2/2) \\,dz\\] Suppose \\(Z \\sim \\text{Normal}(0,1)\\) \\[ \\begin{aligned} Y &amp;= 2Z + 6 \\\\ Y &amp;\\sim \\text{Normal}(6, 4) \\end{aligned} \\] Scale/Location: If \\(Z \\sim N(0,1)\\), then \\(X = aZ + b\\) is, \\[X \\sim \\text{Normal} (b, a^2)\\] Assume we know: \\[ \\begin{aligned} \\E[Z] &amp; = 0 \\\\ \\Var(Z) &amp; = 1 \\end{aligned} \\] This implies that, for \\(Y \\sim \\text{Normal}(\\mu, \\sigma^2)\\) \\[ \\begin{aligned} \\E[Y] &amp; = \\E[\\sigma Z + \\mu] \\\\ &amp; = \\sigma \\E[Z] + \\mu \\\\ &amp; = \\mu \\\\ \\Var(Y) &amp; = \\Var(\\sigma Z + \\mu) \\\\ &amp; = \\sigma^2 \\Var(Z) + \\Var(\\mu) \\\\ &amp; = \\sigma^2 + 0 \\\\ &amp; = \\sigma^2 \\end{aligned} \\] This illustrates a key property of normal random variables. If \\(X\\) is a normal random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\), and if \\(a \\neq 0, b\\) are scalars, then the random variable \\[Y = aX + b\\] is also normal, with mean and variance \\[\\E[Y] = a\\mu + b, \\quad \\Var(Y) = a^2 \\sigma^2\\] 9.4.2 Why rely on the standard normal distribution The normal distribution is commonly used in statistical analysis. Many random variables can be approximated with the normal distribution. Standardizing this makes it easier to make comparisons across variables with different ranges/variances Example 9.7 (GRE scores) Consider GRE scores. The verbal and quantitative scores have different variability, so one cannot easily determine if a 159 on the verbal is better or worse than a 159 on the quantitative section. The standard normal distribution is unitless, so any random variable can be compared with another random variable. This also saves time on the calculus as we don’t need to recalculate the integral when calculating cumulative probability based on the unique \\(\\mu\\) and \\(\\sigma^2\\). Instead, do this once and store the info in a lookup table. This made calculating probabilities (relatively) easy before computers. ::: {.example name=“Support for President Trump”} Suppose we are interested in modeling presidential approval. Let \\(Y\\) represent the random variable “proportion of population who approves the job the president is doing”. Individual responses (that constitute proportion) are independent and identically distributed (sufficient, but not necessary) and we take the average of those individual responses. If we observe many responses (\\(N\\rightarrow \\infty\\)), then by the Central Limit Theorem20 \\(Y\\) is Normally distributed, or \\[ \\begin{aligned} Y &amp; \\sim \\text{Normal}(\\mu, \\sigma^2) \\\\ f_Y(y) &amp; = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2} \\right) \\end{aligned} \\] Suppose \\(\\mu = 0.39\\) and \\(\\sigma^2 = 0.0025\\). What is the probability it isn’t that bad? That is, \\(\\Pr(Y\\geq 0.45)\\)? \\[ \\begin{aligned} \\Pr(Y \\geq 0.45) &amp; = 1 - \\Pr(Y \\leq 0.45 ) \\\\ &amp; = 1 - \\Pr(0.05 Z + 0.39 \\leq 0.45) \\\\ &amp; = 1 - \\Pr(Z \\leq \\frac{0.45-0.39 }{0.05} ) \\\\ &amp; = 1 - \\frac{1}{\\sqrt{2\\pi} } \\int_{-\\infty}^{6/5} \\exp(-z^2/2) \\,dz \\\\ &amp; = 1 - F_{Z} (\\frac{6}{5} ) \\\\ &amp; = 0.1150697 \\end{aligned} \\] 9.5 Gamma distribution Definition 9.1 (Gamma function) Suppose \\(\\alpha&gt;0\\). Then define \\(\\Gamma(\\alpha)\\) as \\[ \\begin{aligned} \\Gamma(\\alpha) &amp;= \\int_{0}^{\\infty} y^{\\alpha- 1} e^{-y} \\,dy \\\\ &amp;= (\\alpha- 1)! \\, \\forall \\alpha \\in \\{1, 2, 3, \\ldots\\} \\end{aligned} \\] \\[\\Gamma(\\frac{1}{2}) = \\sqrt{\\pi}\\] Suppose we have \\(\\Gamma(\\alpha)\\), \\[ \\begin{aligned} \\frac{\\Gamma(\\alpha)}{\\Gamma(\\alpha)} &amp; = \\frac{\\int_{0}^{\\infty} y^{\\alpha-1} e^{-y} dy}{\\Gamma(\\alpha)} \\\\ 1 &amp; = \\int_{0}^{\\infty} \\frac{1}{\\Gamma(\\alpha)} y^{\\alpha-1} e^{-y} \\,dy \\end{aligned} \\] Because \\(\\frac{\\Gamma(\\alpha)}{\\Gamma(\\alpha)}\\) integrates to 1 over its full domain (i.e. non-negative real numbers), it is also a PDF. Set \\(X = Y/\\beta\\) \\[ \\begin{aligned} F(x) = \\Pr(X \\leq x) &amp; = \\Pr(Y/\\beta \\leq x ) \\\\ &amp; = \\Pr(Y \\leq x \\beta ) \\\\ &amp; = F_{Y} (x \\beta) \\\\ \\frac{\\partial F_{Y} (x \\beta) }{\\partial x} &amp; = f_{Y} (x \\beta) \\beta \\end{aligned} \\] The result is: \\[f(x|\\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha - 1} e^{-x\\beta}\\] Definition 9.2 (Gamma distribution) Suppose \\(X\\) is a continuous random variable, with \\(X \\geq 0\\). Then if the pdf of \\(X\\) is \\[f(x|\\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha - 1} e^{-x\\beta}\\] if \\(x\\geq 0\\) and \\(0\\) otherwise, we will say \\(X\\) is a Gamma distribution. \\[X \\sim \\text{Gamma}(\\alpha, \\beta)\\] Suppose \\(X \\sim \\text{Gamma}(\\alpha, \\beta)\\) \\[ \\begin{aligned} \\E[X] &amp; = \\frac{\\alpha}{\\beta} \\\\ \\Var(X) &amp; = \\frac{\\alpha}{\\beta^2} \\end{aligned} \\] Suppose \\(\\alpha = 1\\) and \\(\\beta = \\lambda\\). If \\[ \\begin{aligned} X &amp; \\sim \\text{Gamma}(1, \\lambda) \\\\ f(x|1, \\lambda ) &amp; = \\lambda e^{- x \\lambda} \\end{aligned} \\] We will say \\[X \\sim \\text{Exponential}(\\lambda)\\] 9.5.1 Properties of Gamma distributions Proposition 9.1 Suppose we have a sequence of independent random variables, with \\[X_{i} \\sim \\text{Gamma}(\\alpha_{i}, \\beta)\\] Then \\[Y = \\sum_{i=1}^{N} X_{i}\\] \\[Y \\sim \\text{Gamma}(\\sum_{i=1}^{N} \\alpha_{i} , \\beta)\\] 9.5.2 Importance of the Gamma distribution The exponential and \\(\\chi^2\\) distributions are special cases of the Gamma distribution. The Gamma distribution is also commonly used in Bayesian statistics as it is the conjugate prior for various types of inverse scale (rate) parameters. \\(\\lambda\\) for exponential or Poisson distributions \\(\\beta\\) for the Gamma distribution itself 9.6 \\(\\chi^2\\) distribution Suppose \\(Z \\sim \\text{Normal}(0,1)\\). Consider \\(X = Z^2\\) \\[ \\begin{aligned} F_{X}(x) &amp; = \\Pr(X \\leq x) \\\\ &amp; = \\Pr(Z^2 \\leq x ) \\\\ &amp; = \\Pr(-\\sqrt{x} \\leq Z \\leq \\sqrt{x}) \\\\ &amp; = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\sqrt{x}}^{\\sqrt{x} } e^{-\\frac{z^2}{2}} \\,dz\\\\ &amp; = F_{Z} (\\sqrt{x}) - F_{Z} (-\\sqrt{x}) \\end{aligned} \\] The PDF then is \\[ \\begin{aligned} \\frac{\\partial F_{X}(x) }{\\partial x } &amp; = f_{Z} (\\sqrt{x}) \\frac{1}{2\\sqrt{x}} + f_{Z}(-\\sqrt{x}) \\frac{1}{2\\sqrt{x}} \\\\ &amp; = \\frac{1}{\\sqrt{x}}\\frac{1}{2 \\sqrt{2\\pi}} ( 2e^{-\\frac{x}{2}}) \\\\ &amp; = \\frac{1}{\\sqrt{x}}\\frac{1}{\\sqrt{2\\pi}} ( e^{-\\frac{x}{2}}) \\\\ &amp; = \\frac{(\\frac{1}{2})^{1/2}}{\\Gamma(\\frac{1}{2})}\\left(x^{1/2 - 1} e^{-\\frac{x}{2}}\\right) \\end{aligned} \\] \\(X \\sim \\text{Gamma}(1/2, 1/2)\\) Then if \\(X = \\sum_{i=1}^{N} Z^2\\), \\(X \\sim \\text{Gamma}(n/2, 1/2)\\) Definition 9.3 (Chi-Squared distribution) Suppose \\(X\\) is a continuous random variable with \\(X\\geq 0\\), with PDF \\[f(x) = \\frac{1}{2^{n/2} \\Gamma(n/2) } x^{n/2 - 1} e^{-x/2}\\] Then we will say \\(X\\) is a \\(\\chi^2\\) distribution with \\(n\\) degrees of freedom. Equivalently, \\[X \\sim \\chi^{2}(n)\\] 9.6.1 \\(\\chi^2\\) properties Suppose \\(X \\sim \\chi^2(n)\\) \\[ \\begin{aligned} \\E[X] &amp; = \\E\\left[\\sum_{i=1}^{N} Z_{i}^2\\right] \\\\ &amp; = \\sum_{i=1}^{N} \\E[Z_{i}^{2} ] \\\\ \\Var(Z_{i} ) &amp; = \\E[Z_{i}^2] - \\E[Z_{i}]^2\\\\ 1 &amp; = \\E[Z_{i}^2]- 0 \\\\ \\E[X] &amp; = n \\end{aligned} \\] \\[ \\begin{aligned} \\Var(X) &amp; = \\sum_{i=1}^{N} \\Var(Z_{i}^2) \\\\ &amp; = \\sum_{i=1}^{N} \\left(\\E[Z_{i}^{4} ] - \\E[Z_{i}]^{2} \\right) \\\\ &amp; = \\sum_{i=1}^{N} \\left(3 - 1\\right ) = 2n \\end{aligned} \\] We will use the \\(\\chi^2\\) across statistics. 9.7 Student’s \\(t\\) distribution Definition 9.4 (Student's t-distribution) Suppose \\(Z \\sim \\text{Normal}(0, 1)\\) and \\(U \\sim \\chi^2(n)\\). Define the random variable \\(Y\\) as, \\[Y = \\frac{Z}{\\sqrt{\\frac{U}{n}}}\\] If \\(Z\\) and \\(U\\) are independent then \\(Y \\sim t(n)\\), with PDF \\[t(n) = \\frac{\\Gamma(\\frac{n+1}{2})}{\\sqrt{\\pi n } \\Gamma(\\frac{n}{2})}\\left(1 + \\frac{x^2}{n}\\right)^{-\\frac{n+1}{2}}\\] We will use the t-distribution extensively for test statistics. \\(n\\) is the number of degrees of freedom \\(\\Gamma\\) is the Gamma function \\(\\Gamma(n) = (n-1)!\\) 9.7.1 History of Student’s \\(t\\) Figure 9.1: William Sealy Gosset, creator of the Student’s \\(t\\) distribution. Source: Wikipedia Figure 9.2: A pint of Guinness. Not really my preferred type of beer, but a favorite of my brother-in-law. Source: Wikipedia William Sealy Gosset was a researcher who worked for the Guinness brewery. Gosset was part of a revolution applying statistics to beermaking. How do the chemical properties of barley effect beer taste? What fertilizer produces the best crop yield? Gosset’s problem was that his experiments typically had an \\(N\\) as low as 3. The properties of the normal distribution break down at these low sample sizes - how could Gosset determine if the estimated mean was statistically distinguishable from zero? In order to solve the problem, Gosset applied a new distribution which accounts for sample size. Because he worked for Guinness, Gosset could not publish this distribution under his real name. The resulting paper was published under the pseudonym “Student”, and it became known as Student’s \\(t\\)-distribution 9.7.2 Differences from the Normal Distribution The normal distribution always has the same shape. The \\(Z\\)-scores of \\(-1.96\\) and \\(+1.96\\) always mark the boundaries of the 95% confidence interval. However the shape of the student’s \\(t\\)-distribution changes depending on the sample size. When sample sizes are low, the student’s \\(t\\)-distribution expands the boundaries on random sampling error, creating a larger confidence interval. This avoids being overconfident in our sample statistic. As sample size increases, the confidence bounds shrink. As sample size approaches infinite size, student’s \\(t\\)-distribution takes on the same shape as the normal distribution. References "],["multivar-distribs.html", "Lecture 10 Multivariate distributions Learning objectives Supplemental readings 10.1 Multivariate distribution 10.2 Examples of joint PDFs 10.3 Multivariate cumulative density function 10.4 Marginalization 10.5 Conditional distribution 10.6 Expectation 10.7 Covariance and correlation 10.8 Sums of random variables 10.9 Multivariate normal distribution", " Lecture 10 Multivariate distributions Learning objectives Define a joint probability density function Condition PDFs on other random variables Identify independence between two random variables Define covariance and correlation Examine sums of random variables Define the multivariate normal distribution Supplemental readings Chapters 2.5-.7, 3.4-5, 4.2, 4.5, Bertsekas and Tsitsiklis (2008) Equivalent reading from Bertsekas and Tsitsiklis lecture notes 10.1 Multivariate distribution Definition 10.1 (Multivariate distribution) We will say that \\(X\\) and \\(Y\\) are jointly continuous if, for all \\(x\\in\\Re\\) and \\(y\\in \\Re\\), there exists a function \\(f(x,y)\\) such that \\(C \\subset \\Re^{2}\\), \\[\\Pr\\{(X, Y) \\in C \\} = \\iint_{(x,y)\\in C} f(x,y)\\, dx\\, dy\\] What is \\(C \\subset R^{2}\\)? \\[R^{2} = R \\underbrace{\\times}_{\\text{Cartesian Product}} R\\] This is the 2-d plane (your piece of paper). \\(C\\) is a subset of the 2-d plane. \\[C = \\{x, y: x \\in [0,1] , y\\in [0,1] \\}\\] \\[C = \\{x, y: x^2 + y^2 \\leq 1 \\}\\] \\[C = \\{ x, y: x&gt; y, x,y\\in(0,2)\\}\\] More over, by letting \\(C\\) be the entire two-dimensional plane, we obtain the normalization property \\[\\iint_{(x,y)\\in C} f(x,y)\\, dx\\, dy = 1\\] More generally, \\[ \\begin{aligned} C &amp;= \\{ x, y: x \\in A, y \\in B \\} \\\\ \\Pr\\{(X,Y) \\in C \\} &amp;= \\int_{B} \\int_{A} f(x,y) \\, dx\\, dy \\end{aligned} \\] 10.2 Examples of joint PDFs We’re going to focus (initially) on pdfs of two random variables. Consider a function \\(f:\\Re \\times \\Re \\rightarrow \\Re\\): Input: an \\(x\\) value and a \\(y\\) value. Output: a number from the real line \\(f(x,y) = a\\) 10.2.0.0.1 Multivariate normal distribution \\[ f(x,y) = \\frac{1}{2 \\pi \\sigma_X \\sigma_Y \\sqrt{1-\\rho^2}} \\mathrm{e}^{ -\\frac{1}{2(1-\\rho^2)}\\left[ \\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)^2 - 2\\rho\\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)\\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right) + \\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right)^2 \\right] } \\] where \\(\\rho\\) is the Pearson product-moment correlation coefficient between \\(X\\) and \\(Y\\) and where \\(\\sigma_X&gt;0\\) and \\(\\sigma_Y&gt;0\\). In this case, \\[ \\boldsymbol\\mu = \\begin{pmatrix} \\mu_X \\\\ \\mu_Y \\end{pmatrix}, \\quad \\boldsymbol\\Sigma = \\begin{pmatrix} \\sigma_X^2 &amp; \\rho \\sigma_X \\sigma_Y \\\\ \\rho \\sigma_X \\sigma_Y &amp; \\sigma_Y^2 \\end{pmatrix}. \\] 3D plot: Contour plot: 10.2.0.0.2 Multivariate uniform distribution \\(f(x,y) = 1\\) if \\(x \\in [0,1], y \\in [0,1]\\), \\(f(x,y) = 0\\) 10.2.0.0.3 Joint exponential \\(\\times\\) standard normal distribution \\(f(x,y) = \\frac{2 \\exp(-2x)}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{(x)^2}{2}\\right)\\) if \\(x \\in [0,\\infty), y \\in \\Re\\), \\(f(x,y) = 0\\) otherwise 10.3 Multivariate cumulative density function Definition 10.2 (Multivariate cumulative density function) For jointly continuous random variables \\(X\\) and \\(Y\\) define, \\(F(b,a)\\) as \\[F(b,a) = \\Pr\\{ X \\leq b , Y \\leq a\\} = \\int_{-\\infty}^{a} \\int_{-\\infty}^{b} f(x,y) \\, dx\\, dy\\] 10.3.0.0.1 Multivariate normal \\[ f(x,y) = \\frac{1}{2 \\pi \\sigma_X \\sigma_Y \\sqrt{1-\\rho^2}} \\mathrm{e}^{ -\\frac{1}{2(1-\\rho^2)}\\left[ \\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)^2 - 2\\rho\\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)\\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right) + \\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right)^2 \\right] } \\] where \\(\\rho\\) is the Pearson product-moment correlation coefficient between \\(X\\) and \\(Y\\) and where \\(\\sigma_X&gt;0\\) and \\(\\sigma_Y&gt;0\\). In this case, \\[ \\boldsymbol\\mu = \\begin{pmatrix} \\mu_X \\\\ \\mu_Y \\end{pmatrix}, \\quad \\boldsymbol\\Sigma = \\begin{pmatrix} \\sigma_X^2 &amp; \\rho \\sigma_X \\sigma_Y \\\\ \\rho \\sigma_X \\sigma_Y &amp; \\sigma_Y^2 \\end{pmatrix}. \\] 10.3.0.0.2 Multivariate uniform distribution \\(f(x,y) = 1\\) if \\(x \\in [0,1], y \\in [0,1]\\), \\(f(x,y) = 0\\) 10.3.0.0.3 Joint exponential \\(\\times\\) standard normal distribution \\(f(x,y) = \\frac{2 \\exp(-2x)}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{(x)^2}{2}\\right)\\) if \\(x \\in [0,\\infty), y \\in \\Re\\), \\(f(x,y) = 0\\) otherwise 10.4 Marginalization Definition 10.3 (Moving from joint distributions to univariate PDFs) Define \\(f_{X}(x)\\) as the marginal pdf for \\(X\\), \\[f_{X}(x) = \\int_{-\\infty}^{\\infty} f(x,y) \\, dy\\] Similarly, define \\(f_{Y}(y)\\) as the marginal pdf for \\(Y\\), \\[f_{Y}(y) = \\int_{-\\infty}^{\\infty} f(x,y)\\, dx\\] Definition 10.4 (Conditional probability distribution function) Suppose \\(X\\) and \\(Y\\) are continuous random variables with joint pdf \\(f(x,y)\\). Then define the conditional probability function \\(f(x|y)\\) as \\[f(x|y) = \\frac{f(x, y) }{f_{Y}(y) }\\] 10.4.1 Joint vs. conditional PDF An example using two standard normal variables \\(x\\) and \\(y\\): \\[f(x,y) = x \\times y\\] As \\(y\\) increases, the marginal probability of \\(x\\) changes. Note that \\(f(x | y)\\) is the same regardless of the value of \\(y\\). That is, the conditional probability of \\(x\\) given \\(y\\) does not depend on the specific value of \\(y\\). Why is that? 10.4.2 Why does marginalization work? Begin with the discrete case. Consider jointly distributed discrete random variables, \\(X\\) and \\(Y\\). We’ll suppose they have a joint PMF, \\[\\Pr(X =x, Y = y) = p(x, y)\\] Suppose that the distribution allocates its mass at \\(x_{1}, x_{2}, \\ldots, x_{M}\\) and \\(y_{1}, y_{2}, \\ldots, y_{N}\\). Define the conditional mass function \\(\\Pr(X= x| Y= y)\\) as, \\[ \\begin{aligned} \\Pr(X=x|Y=y) &amp;\\equiv p(x|y) \\\\ &amp; = \\frac{p(x,y)}{p(y)} \\end{aligned} \\] Then it follows that: \\[p(x,y) = p(x|y)p(y)\\] Marginalizing over \\(y\\) to get \\(p(x)\\) is then, \\[p(x_{j}) = \\sum_{i=1}^{N} p(x_{j} |y_{i})p(y_{i} )\\] 10.4.2.0.1 Table setup \\(Y = 0\\) \\(Y = 1\\) \\(X = 0\\) \\(p(0,0)\\) \\(p(0, 1)\\) \\(p_{X}(0)\\) \\(X = 1\\) \\(p(1,0)\\) \\(p(1,1)\\) \\(p_{X}(1)\\) \\(p_{Y} (0)\\) \\(p_{Y} (1)\\) 10.4.2.0.2 Example probabilities \\(Y = 0\\) \\(Y = 1\\) \\(X = 0\\) \\(0.01\\) \\(0.05\\) \\(p_{X}(0)\\)? \\(X = 1\\) \\(0.25\\) \\(0.69\\) \\(p_{X}(1)\\)? \\(0.26\\) \\(0.74\\) 10.4.2.0.3 Marginalize over columns \\[ \\begin{aligned} p_{X}(0) &amp; = \\Pr(0|y = 0) \\Pr(y= 0) + \\Pr(0|y=1) \\Pr(y=1) \\\\ &amp; = \\frac{0.01}{0.26} \\times 0.26 + \\frac{0.05}{0.74} \\times 0.74 \\\\ &amp; = 0.06 \\end{aligned} \\] \\[ \\begin{aligned} p_{X}(1) &amp; = \\Pr(1|y = 0) \\Pr(y= 0) + \\Pr(1|y=1) \\Pr(y=1) \\\\ &amp; = \\frac{0.25}{0.26} \\times 0.26 + \\frac{0.69}{0.74} \\times 0.74 \\\\ &amp; = 0.94 \\end{aligned} \\] 10.4.3 Move to the continuous case For jointly distributed continuous random variables \\(X\\) and \\(Y\\) define, \\[f_{X|Y}(x|y) = \\frac{f(x,y)}{f_{Y}(y) }\\] Then, analogously, we can define \\[f_{X}(x) = \\int_{-\\infty }^{\\infty} f_{X|Y}(x|y)f_{Y}(y) \\, dy\\] Think of \\(f_{X|Y}(x|y)\\) as the pdf for \\(X\\) at a value of \\(Y\\). We average over those pdfs to get the final pdf for \\(X\\) (we want densities where there is lots of area of \\(Y\\) to receive lots of weight, whereas the densities without much area from \\(Y\\) should receive little weight). 10.4.4 A (simple) example Suppose \\(X\\) and \\(Y\\) are jointly continuous and that \\[ \\begin{aligned} f_{XY}(x,y) &amp; = x + \\frac{3}{2}y^2 \\text{ , if } x \\in [0,1], y \\in [0,1] \\\\ &amp; = 0 \\text{ , otherwise } \\end{aligned} \\] We can show this function is a joint PDF since the area under the multivariate curve is 1. \\[ \\begin{aligned} \\iint_{-\\infty}^{+ \\infty} \\, dy \\, dx &amp;= \\iint_0^1 x + \\frac{3}{2}y^2 \\, dy \\, dx \\\\ &amp;= \\int_0^1 \\left[ \\frac{1}{2} x^2 + \\frac{3}{2}y^2x \\right]_0^1 \\, dy \\\\ &amp;= \\int_0^1 \\frac{1}{2} + \\frac{3}{2}y^2 \\, dy \\\\ &amp;= \\frac{1}{2} \\int_0^1 1 \\, dy + \\frac{3}{2} \\int_0^1 y^2 \\, dy \\\\ &amp;= \\left[ \\frac{y}{2} \\right]_0^1 + \\left[\\frac{y^3}{2} \\right]_0^1 \\\\ &amp;= \\frac{1}{2} + \\frac{1}{2} \\\\ &amp;= 1 \\end{aligned} \\] We want \\(f_{X}(x)\\). Assume we have \\[f_{Y}(y) = y\\] Then \\[f_{X|Y}(x|y) = \\frac{x + \\frac{3}{2}y^2}{y}\\] \\[ \\begin{aligned} f_X(x) &amp;= \\int_{0}^{1} f(x|y)f(y)\\, dy \\\\ &amp;= \\int_0^1 \\frac{x + \\frac{3}{2}y^2}{y} (y) \\, dy \\\\ &amp;= \\int_0^1 x + \\frac{3}{2}y^2\\, dy \\\\ &amp;= x \\int_0^1 1 \\, dy + \\frac{3}{2} \\int_0^1 y^2 \\, dy \\\\ &amp;= x \\left[y \\right]_0^1 + \\frac{3}{2} \\left[\\frac{y^3}{3} \\right]_0^1 \\\\ &amp;= \\left[xy \\right]_0^1 + \\left[\\frac{y^3}{2} \\right]_0^1 \\\\ &amp;= x + \\frac{1}{2} \\end{aligned} \\] 10.4.5 More complex example Suppose \\(X\\) and \\(Y\\) are jointly distributed with PDF \\[f(x,y) = 2 \\exp(-x) \\exp(-2y), \\forall \\, x&gt;0, y&gt;0)\\] 10.4.5.1 Verify this is a PDF \\[ \\begin{aligned} \\int_{0}^{\\infty} \\int_{0}^{\\infty} f(x, y) &amp; = 2\\int_{0}^{\\infty} \\int_{0}^{\\infty} \\exp(-x) \\exp(-2y) \\, dx\\, dy \\\\ &amp; = 2 \\int_{0}^{\\infty}\\exp(-2y) \\, dy \\int_{0}^{\\infty} \\exp(-x) \\, dx \\\\ &amp; = 2 (-\\frac{1}{2} \\exp(-2y)|_{0}^{\\infty} ) ( - \\exp(-x)|_{0}^{\\infty} ) \\\\ &amp; = 2\\left[ (-\\frac{1}{2}(\\lim_{y\\rightarrow\\infty} \\exp(-2y) - 1))(- (\\lim_{x\\rightarrow \\infty} \\exp(-x) - 1) ) \\right] \\\\ &amp; = 2 \\left[ -\\frac{1}{2} (-1) \\times -1 (-1) \\right] \\\\ &amp; = 1 \\end{aligned} \\] 10.4.5.2 Calculate CDF \\[ \\begin{aligned} F(x,y) \\equiv \\Pr\\{X \\leq b, Y \\leq a\\} &amp; = 2 \\int_{0}^{a} \\int_{0}^{b} \\exp(-x) \\exp(-2y) \\, dx\\, dy \\\\ &amp; = 2 (\\int_{0}^{a} \\exp(-2y) \\, dy) (\\int_{0}^{b} \\exp(-x) \\, dx) \\\\ &amp; = 2 \\left[-\\frac{1}{2} (\\exp(-2a) -1 )\\right]\\left[ - (\\exp(-b) - 1) \\right] \\\\ &amp; = \\left[1 - \\exp(-2a) \\right] \\left[ 1- \\exp(-b) \\right] \\end{aligned} \\] 10.4.5.3 Calculate \\(f_{X}(x)\\) and \\(f_{Y}(y)\\) \\[ \\begin{aligned} f_{X}(x) &amp; = \\int_{0}^{\\infty} 2\\exp(-x) \\exp(-2y) \\, dy \\\\ &amp; = 2 \\exp(-x) \\int_{0}^{\\infty} \\exp(-2y) \\, dy \\\\ &amp; = 2 \\exp(-x) \\left[ -\\frac{1}{2}(0 - 1) \\right] \\\\ &amp; = \\exp(-x) \\end{aligned} \\] \\[ \\begin{aligned} f_{Y}(y) &amp; = \\int_{0}^{\\infty} 2 \\exp(-x) \\exp(-2y) \\, dx \\\\ &amp; = 2 \\exp(-2y) \\int_{0}^{\\infty} \\exp(-x) \\, dx \\\\ &amp; = 2 \\exp(-2y) \\left[-(0 -1) \\right] \\\\ &amp; = 2 \\exp(-2y) \\end{aligned} \\] 10.5 Conditional distribution Definition 10.5 Two random variables \\(X\\) and \\(Y\\) are independent if for any two sets of real numbers \\(A\\) and \\(B\\), \\[\\Pr\\{ X \\in A , Y \\in B \\} = \\Pr\\{X \\in A\\} \\Pr\\{Y \\in B\\}\\] Equivalently we will say \\(X\\) and \\(Y\\) are independent if, \\[f(x,y) = f_{X}(x) f_{Y}(y)\\] If \\(X\\) and \\(Y\\) are not independent, we will say they are dependent. If \\(X\\) and \\(Y\\) are independent, then \\[ \\begin{aligned} f_{X|Y} (x|y) &amp; = \\frac{f(x,y)}{f_{Y}(y)} \\\\ &amp; = \\frac{f_{X}(x)f_{Y}(y)}{f_{Y}(y) } \\\\ &amp; = f_{X}(x) \\end{aligned} \\] In words: the distribution of \\(X\\) does not change as levels of \\(Y\\) change. 10.5.1 A (simple) example of dependence Suppose \\(X\\) and \\(Y\\) are jointly continuous and that \\[ \\begin{eqnarray} f(x,y) &amp; = &amp; x + y \\text{ , if } x \\in [0,1], y \\in [0,1] \\\\ &amp; = &amp; 0 \\text{ , otherwise } \\end{eqnarray} \\] \\[ \\begin{eqnarray} f_{X}(x) &amp; = &amp; \\int_{0}^{1} \\left(x + y \\right) \\, dy \\\\ &amp; = &amp; xy + \\frac{y^2}{2} |^{1}_{0} \\\\ &amp; = &amp; x + \\frac{1}{2} \\\\ f_{Y}(y) &amp; = &amp; \\frac{1}{2} + y \\end{eqnarray} \\] \\[ \\begin{eqnarray} f(x, y) &amp;= &amp; x + y \\\\ f_{X}(x) f_{Y}(y) &amp; = &amp; (\\frac{1}{2} + x) (\\frac{1}{2} + y) \\\\ &amp; = &amp; \\frac{1}{4} + \\frac{x + y}{2} + xy \\end{eqnarray} \\] Intuition: at different levels of \\(X\\) the distribution on \\(Y\\) behaves differently. \\(X\\) provides information about \\(Y\\). 10.6 Expectation Definition 10.6 For jointly continuous random variables \\(X\\) and \\(Y\\) define, \\[ \\begin{eqnarray} \\E[X] &amp; = &amp; \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} x f(x,y) \\, dx\\, dy \\\\ \\E[Y] &amp; = &amp; \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} y f(x,y) \\, dx\\, dy \\\\ \\E[XY] &amp; = &amp; \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} x y f(x,y) \\, dx\\, dy \\end{eqnarray} \\] Proposition 10.1 Suppose \\(g:\\Re^{2} \\rightarrow \\Re\\). Then \\[\\E[g(X, Y)] = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x, y) f(x,y) \\, dx\\, dy\\] 10.7 Covariance and correlation Definition 10.7 (Covariance) For jointly continous random variables \\(X\\) and \\(Y\\) define, the covariance of \\(X\\) and \\(Y\\) as, \\[ \\begin{align} \\Cov(X, Y) &amp;= \\E\\left[\\left(X - \\E\\left[X\\right]\\right) \\left(Y - \\E\\left[Y\\right]\\right)\\right] \\\\ &amp;= \\E\\left[X Y - X \\E\\left[Y\\right] - \\E\\left[X\\right] Y + \\E\\left[X\\right] \\E\\left[Y\\right]\\right] \\\\ &amp;= \\E\\left[X Y\\right] - \\E\\left[X\\right] \\E\\left[Y\\right] - \\E\\left[X\\right] \\E\\left[Y\\right] + \\E\\left[X\\right] \\E\\left[Y\\right] \\\\ &amp;= \\E\\left[X Y\\right] - \\E\\left[X\\right] \\E\\left[Y\\right] \\end{align} \\] Definition 10.8 (Correlation) Define the correlation of \\(X\\) and \\(Y\\) as, \\[\\Cor(X,Y) = \\frac{\\Cov(X,Y) }{\\sqrt{\\Var(X) \\Var(Y) } }\\] 10.7.1 Some observations 10.7.1.1 Variance is the covariance of a random variable with itself \\[ \\begin{eqnarray} \\Cov(X,X) &amp; = &amp; \\E[X X] - \\E[X]\\E[X] \\\\ &amp; = &amp; \\E[X^2] - \\E[X]^2 \\end{eqnarray} \\] 10.7.1.2 Correlation measures the linear relationship between two random variables Suppose \\(X = Y\\): \\[ \\begin{eqnarray} \\Cor(X,Y) &amp; = &amp; \\frac{\\Cov(X,Y)}{\\sqrt{\\Var(X)\\Var(Y)} } \\\\ &amp; = &amp; \\frac{\\Var(X)}{\\Var(X)} \\\\ &amp; = &amp; 1 \\end{eqnarray} \\] Suppose \\(X = -Y\\): \\[ \\begin{eqnarray} \\Cor(X,Y) &amp; = &amp; \\frac{\\Cov(X,Y)}{\\sqrt{\\Var(X)\\Var(Y)} } \\\\ &amp; = &amp; \\frac{- \\Var(X)}{\\Var(X)} \\\\ &amp; = &amp; -1 \\end{eqnarray} \\] 10.7.1.3 Correlation is between -1 and 1 \\[|\\Cor(X,Y)| \\leq 1\\] 10.8 Sums of random variables Suppose we have a sequence of random variables \\(X_{i}\\) , \\(i = 1, 2, \\ldots, N\\) and that they have joint pdf \\[f(\\boldsymbol{x}) = f(x_{1}, x_{2}, \\ldots, x_{n})\\] \\(\\E[\\sum_{i=1}^{N}X_{i} ] = \\sum_{i=1}^{N} \\E[X_{i}]\\) \\(\\Var(\\sum_{i=1}^{N} X_{i} ) = \\sum_{i=1}^{N} \\Var(X_{i} ) + 2 \\sum_{i&lt;j} \\Cov(X_{i}, X_{j})\\) Proposition 10.2 Suppose we have a sequence of random variables \\(X_{i}\\) , \\(i = 1, 2, \\ldots, N\\). Suppose that they have joint pdf, \\[f(\\boldsymbol{x}) = f(x_{1}, x_{2}, \\ldots, x_{n})\\] Then \\[\\E[\\sum_{i=1}^{N} X_{i} ] = \\sum_{i=1}^{N} \\E[X_{i} ]\\] Proof. \\[ \\begin{eqnarray} \\E[\\sum_{i=1}^{N} X_{i} ] &amp; = &amp; \\E[X_{1} + X_{2} + \\ldots + X_{N}] \\\\ &amp; = &amp; \\int_{-\\infty}^{\\infty} \\cdot \\cdot \\cdot \\iint_{-\\infty}^{\\infty} (x_{1} + x_{2} + \\ldots + x_{N}) f(x_{1}, x_{2}, \\ldots, x_{N}) \\, dx_{1}\\, dx_{2}\\ldots \\, dx_{N} \\\\ &amp; = &amp; \\int_{-\\infty}^{\\infty}x_{1} f_{X_{1}}(x_{1}) \\, dx_{1} + \\int_{-\\infty}^{\\infty}x_{2} f_{X_{2}}(x_{2}) \\, dx_{2} + \\ldots + \\int_{-\\infty}^{\\infty}x_{N} f_{X_{N}}(x_{N}) \\, dx_{N} \\\\ &amp; = &amp; \\E[X_{1} ] + \\E[X_{2}] + \\ldots + \\E[X_{N}] \\end{eqnarray} \\] Proposition 10.3 Suppose \\(X_{i}\\) is a sequence of random variables. Then \\[ \\begin{eqnarray} \\Var(\\sum_{i=1}^{N} X_{i} ) &amp; = &amp; \\sum_{i=1}^{N} \\Var(X_{i} ) + 2 \\sum_{i&lt;j} \\Cov(X_{i}, X_{j} ) \\end{eqnarray} \\] Proof. Consider two random variables, \\(X_{1}\\) and \\(X_{2}\\). Then, \\[ \\begin{eqnarray} \\Var(X_{1} + X_{2} ) &amp; = &amp; \\E[(X_{1} + X_{2})^2] - \\left(\\E[X_{1}] + \\E[X_{2}] \\right)^2 \\\\ &amp; = &amp; \\E[X_{1}^2] + 2 \\E[X_{1}X_{2}] + \\E[X_{2}^2] \\\\ &amp;&amp; - (\\E[X_{1}])^2 - 2 \\E[X_{1}] \\E[X_{2}] - 2 \\E[X_{2}]^2 \\\\ &amp; = &amp; \\underbrace{\\E[X_{1}^2] - (\\E[X_{1}])^2}_{\\Var(X_{1}) } + \\underbrace{\\E[X_{2}^2] - \\E[X_{2}]^{2}}_{\\Var(X_{2})} \\\\ &amp;&amp; + 2 \\underbrace{(\\E[X_{1} X_{2} ] - \\E[X_{1}] \\E[X_{2} ] )}_{\\Cov(X_{1}, X_{2} ) } \\\\ &amp; = &amp; \\Var(X_{1} ) + \\Var(X_{2} ) + 2 \\Cov(X_{1}, X_{2}) \\end{eqnarray} \\] 10.9 Multivariate normal distribution Definition 10.9 (Multivariate normal distribution) Suppose \\(\\boldsymbol{X} = (X_{1}, X_{2}, \\ldots, X_{N})\\) is a vector of random variables. If \\(\\boldsymbol{X}\\) has pdf \\[f(\\boldsymbol{x}) = (2 \\pi)^{-N/2} \\text{det}\\left(\\boldsymbol{\\Sigma}\\right)^{-1/2} \\exp\\left(-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu})^{&#39;}\\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu} ) \\right)\\] Then we will say \\(\\boldsymbol{X}\\) is a Multivariate Normal Distribution, \\[\\boldsymbol{X} \\sim \\text{Multivariate Normal} (\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\] This is regularly used for likelihood, Bayesian, and other parametric inferences. 10.9.1 Bivariate example Consider the (bivariate) special case where \\(\\boldsymbol{\\mu} = (0, 0)\\) and \\[ \\boldsymbol{\\Sigma} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{pmatrix} \\] Then \\[ \\begin{eqnarray} f(x_{1}, x_{2} ) &amp; = &amp; (2\\pi)^{-2/2} 1^{-1/2} \\exp\\left(-\\frac{1}{2}\\left( (\\boldsymbol{x} - \\boldsymbol{0} ) ^{&#39;} \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{pmatrix} (\\boldsymbol{x} - \\boldsymbol{0} ) \\right) \\right) \\\\ &amp; = &amp; \\frac{1}{2\\pi} \\exp\\left(-\\frac{1}{2} (x_{1}^{2} + x_{2} ^ 2 ) \\right) \\\\ &amp; = &amp; \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x_{1}^{2}}{2} \\right) \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x_{2}^{2}}{2} \\right) \\end{eqnarray} \\] \\(\\leadsto\\) product of univariate standard normally distributed random variables Definition 10.10 (Standard multivariate normal distribution) Suppose \\(\\boldsymbol{Z} = (Z_{1}, Z_{2}, \\ldots, Z_{N})\\) is \\[\\boldsymbol{Z} \\sim \\text{Multivariate Normal}(\\boldsymbol{0}, \\boldsymbol{I}_{N} )\\] Then we will call \\(\\boldsymbol{Z}\\) the standard multivariate normal. 10.9.2 Properties of the multivariate normal distribution Suppose \\(\\boldsymbol{X} = (X_{1}, X_{2}, \\ldots, X_{N} )\\) \\[ \\begin{eqnarray} \\E[\\boldsymbol{X} ] &amp; = &amp; \\boldsymbol{\\mu} \\\\ \\Cov(\\boldsymbol{X} ) &amp; = &amp; \\boldsymbol{\\Sigma} \\end{eqnarray} \\] So that, \\[ \\begin{eqnarray} \\boldsymbol{\\Sigma} &amp; = &amp; \\begin{pmatrix} \\Var(X_{1}) &amp; \\Cov(X_{1}, X_{2}) &amp; \\ldots &amp; \\Cov(X_{1}, X_{N}) \\\\ \\Cov(X_{2}, X_{1}) &amp; \\Var(X_{2}) &amp; \\ldots &amp; \\Cov(X_{2}, X_{N} ) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\Cov(X_{N}, X_{1} ) &amp; \\Cov(X_{N}, X_{2} ) &amp; \\ldots &amp; \\Var(X_{N} ) \\\\ \\end{pmatrix} \\end{eqnarray} \\] 10.9.3 Independence and multivariate normal Proposition 10.4 Suppose \\(X\\) and \\(Y\\) are independent. Then \\[\\Cov(X, Y) = 0\\] Proof. Suppose \\(X\\) and \\(Y\\) are independent. \\[\\Cov(X, Y) = \\E[XY] - \\E[X]\\E[Y]\\] Calculating \\(\\E[XY]\\) \\[ \\begin{eqnarray} \\E[XY] &amp; = &amp; \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} x y f(x,y)\\, dx\\, dy \\\\ &amp; =&amp; \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} x y f_{X}(x) f_{Y}(y)\\, dx\\, dy \\\\ &amp; = &amp; \\int_{-\\infty}^{\\infty} x f_{X}(x) \\, dx \\int_{-\\infty}^{\\infty} y f_{Y}(y) \\, dy \\\\ &amp; = &amp; \\E[X] \\E[Y] \\end{eqnarray} \\] Then \\(\\Cov(X,Y) = 0\\). Zero covariance does not generally imply independence! Example 10.1 (Dependent variables with zero covariance) Suppose \\(X \\in \\{-1, 1\\}\\) with \\(\\Pr(X = 1) = \\Pr(X = -1) = 1/2\\). Suppose \\(Y \\in \\{-1, 0,1\\}\\) with \\(Y = 0\\) if \\(X = -1\\) and \\(\\Pr(Y = 1) = \\Pr(Y= -1)\\) if \\(X = 1\\). \\[ \\begin{eqnarray} \\E[XY] &amp; = &amp; \\sum_{i \\in \\{-1, 1\\} } \\sum_{j \\in \\{-1, 0, 1\\}} i j \\Pr(X = i, Y = j) \\\\ &amp; = &amp; -1 \\times 0 \\times \\Pr(X = -1, Y = 0) + 1 \\times 1 \\times \\Pr(X = 1, Y = 1) \\\\ &amp;&amp; - 1 \\times 1 \\times \\Pr(X = 1, Y = -1) \\\\ &amp;= &amp; 0 + \\Pr(X = 1, Y = 1) - \\Pr(X = 1, Y = -1 ) \\\\ &amp; = &amp; 0.25 - 0.25 = 0 \\\\ \\E[X] &amp; = &amp; 0 \\\\ \\E[Y] &amp; = &amp; 0 \\end{eqnarray} \\] Proposition 10.5 Suppose \\(\\boldsymbol{X} \\sim \\text{Multivariate Normal}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), where \\(\\boldsymbol{X}= (X_{1}, X_{2}, \\ldots, X_{N})\\). If \\(\\Cov(X_{i}, X_{j}) = 0\\), then \\(X_{i}\\) and \\(X_{j}\\) are independent. References "],["limits.html", "Lecture 11 Properties of random variables and limit theorems Learning objectives Supplemental readings 11.1 Iterated Expectations 11.2 Change of coordinates 11.3 Moment generating functions 11.4 Sequences of independent random variables 11.5 Inequalities and limit theorems 11.6 Sequence of random variables 11.7 Sequences and convergence", " Lecture 11 Properties of random variables and limit theorems Learning objectives Define iterated expectations Demonstrate how to change coordinates for a probability density function Define moment generating functions and transformations Consider the impact of limit theorems on core statistical techniques Prove the weak law of large numbers Connect principles of convergence to estimators through the central limit theorem Supplemental readings Chapter 5 Bertsekas and Tsitsiklis (2008) Equivalent reading from Bertsekas and Tsitsiklis lecture notes 11.1 Iterated Expectations Suppose \\(X\\) and \\(Y\\) are random variables. Then \\[ \\E[X] = \\E[\\E[X|Y]] \\] Inner Expectation is \\(E[X|Y] = \\int_{-\\infty}^{\\infty} x f_{X|Y} (x|y) dx\\). Outer expectation is over \\(y\\). Proof (Iterated expectations). \\[ \\begin{aligned} \\E[\\E[X|Y]] &amp; = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} x f_{X|Y} (x|y) f_{Y}(y) dx dy \\nonumber \\\\ &amp;= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} x f_{X|Y} (x|y) f_{Y}(y) dy dx \\nonumber \\\\ &amp; = \\int_{-\\infty}^{\\infty} x \\int_{-\\infty}^{\\infty} f(x, y) dy dx \\nonumber \\\\ &amp; = \\int_{-\\infty}^{\\infty} x f_{X}(x) dx \\nonumber \\\\ &amp; = \\E[X] \\nonumber \\end{aligned} \\] Definition 11.1 (Beta distribution) Suppose \\(Y\\) is a continuous random variable with \\(Y \\in [0,1]\\) and pdf of \\(Y\\) given by \\[ f(y) = \\frac{\\Gamma(\\alpha_1 + \\alpha_2)}{\\Gamma(\\alpha_{1} ) \\Gamma(\\alpha_{2})} y^{\\alpha_{1} - 1} (1- y)^{\\alpha_{2} - 1 } \\nonumber \\] Then we will say \\(Y\\) is a Beta distribution with parameters \\(\\alpha_{1}\\) and \\(\\alpha_{2}\\). Equivalently, \\[ Y \\sim \\text{Beta}(\\alpha_{1}, \\alpha_{2} ) \\nonumber \\] Beta is a distribution on proportions Beta is a special case of the Dirichlet distribution \\(\\E[Y] = \\frac{\\alpha_{1}}{\\alpha_{1} + \\alpha_{2}}\\) Suppose \\[ \\begin{aligned} \\pi &amp; \\sim \\text{Beta}(\\alpha_{1}, \\alpha_{2}) \\nonumber \\\\ Y|\\pi, n &amp; \\sim \\text{Binomial}(n, \\pi)\\nonumber \\end{aligned} \\] What is \\(\\E[Y]\\)? \\[ \\begin{aligned} \\E[Y] &amp; = \\E[\\E[Y| \\pi]] \\nonumber \\\\ &amp; = \\int_{-\\infty}^{\\infty} \\sum_{j = 0}^{N} {{N}\\choose{j}} j p(j|\\pi) f(\\pi) d\\pi \\nonumber \\\\ &amp; = \\int_{-\\infty}^{\\infty} N \\pi f(\\pi) d\\pi \\nonumber \\\\ &amp; = N \\frac{\\alpha_{1}}{\\alpha_{1} + \\alpha_{2}} \\nonumber \\end{aligned} \\] 11.2 Change of coordinates Suppose \\(X\\) is a random variable and \\(Y = g(X)\\), where \\(g:\\Re \\rightarrow \\Re\\) that is a \\(monotonic\\) function. Define \\(g^{-1}:\\Re \\rightarrow \\Re\\) such that \\(g^{-1}(g(X)) = X\\) and is differentiable. Then, \\[ \\begin{aligned} f_{Y}(y) &amp; = f_{X}(g^{-1}(y))\\left|\\frac{\\partial g^{-1}(y)}{\\partial y} \\right| \\text{ if } y = g(x) \\text{ for some } x \\nonumber \\\\ &amp; = 0 \\text{ otherwise } \\nonumber \\end{aligned} \\] Proof (Change of coordinates). Suppose \\(g(\\cdot)\\) is monotonically increasing (WLOG) \\[ \\begin{aligned} F_{Y}(y) &amp; = \\Pr(Y \\leq y) \\nonumber \\\\ &amp; = \\Pr(g(X) \\leq y) \\nonumber \\\\ &amp; = \\Pr(X \\leq g^{-1}(y) ) \\nonumber \\\\ &amp; = F_{X}(g^{-1}(y) ) \\nonumber \\end{aligned} \\] Now differentiating to get the pdf \\[ \\begin{aligned} \\frac{\\partial F_{Y}(y)}{\\partial y} &amp; = \\frac{\\partial F_{X}(g^{-1}(y) )} {\\partial y } \\nonumber \\\\ &amp; = f_{X}(g^{-1}(y) ) \\frac{\\partial g^{-1}(y)}{\\partial y } \\nonumber \\end{aligned} \\] Then this is a pdf because \\(\\frac{\\partial g^{-1}(Y)}{\\partial y } &gt; 0\\). Suppose \\(X\\) is a random variable with pdf \\(f_{X}(x)\\). Suppose \\(Y = X^{n}\\). Find \\(f_{Y}(y)\\). Then \\(g^{-1} (x) = x^{1/n}\\). \\[ \\begin{aligned} f_{Y}(y) &amp; = f_{X}(g^{-1}(y)) \\left| \\frac{\\partial g^{-1}(Y)}{\\partial y } \\right| \\nonumber \\\\ &amp; = f_{X} (y^{1/n} ) \\frac{y^{\\frac{1}{n} - 1}}{n} \\nonumber \\end{aligned} \\] We’ve used this to derive many of the pdfs. Normal distribution Chi-Squared Distribution 11.3 Moment generating functions Definition 11.2 (Moment) Suppose \\(X\\) is a random variable with pdf \\(f\\). Define, \\[ \\E[X^{n}] = \\int_{-\\infty}^{\\infty} x^{n} f(x) dx \\nonumber \\] We will call \\(X^{n}\\) the \\(n^{\\text{th}}\\) moment of \\(X\\). By this definition \\(\\Var(X) = \\text{Second Moment} - \\text{First Moment}^{2}\\). We are assuming that the integral converges. Proposition 11.1 (Moment generating function) Suppose \\(X\\) is a random variable with pdf \\(f(x)\\). Call \\(M(t) = E[e^{tX}]\\), \\[ \\begin{aligned} M(t) &amp; = \\E[e^{tX}] \\nonumber \\\\ &amp; = \\int_{-\\infty}^{\\infty} e^{tx} f(x) dx \\nonumber \\end{aligned} \\] We will call \\(M(t)\\) the moment generating function, because:21 \\[ \\frac{\\partial^{n} M (t) }{\\partial^{n} t}|_{0} = E[X^{n}] \\nonumber \\] Proof (Moment generating function). Recall the Taylor Expansion of \\(e^{tX}\\) at \\(0\\), \\[ e^{tX} = 1 + tx + \\frac{t^2 x^2}{2!} + \\frac{t^3 x^3}{3!} + \\ldots \\nonumber \\] Then, \\[ \\E[e^{tX} ] = 1 + t\\E[X] + \\frac{t^2}{2!} \\E[X^2] + \\frac{t^3}{3!} \\E[X^3] + \\ldots \\nonumber \\] Differentiate once: \\[ \\begin{aligned} \\frac{\\partial M(t)}{\\partial t} &amp; = 0 + \\E[X] + \\frac{2t}{2!} \\E[X^2] + \\ldots \\nonumber \\\\ M^{&#39;}(0) &amp; = 0 + \\E[X] + 0 + 0 \\ldots \\nonumber \\end{aligned} \\] Differentiate \\(n\\) times: \\[ \\begin{aligned} \\frac{\\partial^{n} M(t)}{\\partial^{n} t } &amp; = 0 + 0 + 0 + \\ldots + \\frac{n \\times n-1 \\times \\ldots 2 \\times t^{0} \\E[X^{n}] }{n!} + \\frac{n!t \\E[X^{n+1}] }{(n+ 1)! } + \\ldots \\nonumber \\\\ &amp; = \\frac{n! \\E[X^{n}] }{n!} + \\frac{n!t \\E[X^{n+1}] }{(n+ 1)! } + \\ldots \\nonumber \\end{aligned} \\] Evaluated at 0, yields \\(M^{n}(0) = \\E[X^{n}]\\). If two random variables, \\(X\\) and \\(Y\\) have the same moment generating functions, then \\(F_{X}(x) = F_{Y}(x)\\) for almost all \\(x\\). 11.3.1 The moments of the normal distribution Suppose \\(Z \\sim N(0,1)\\). \\[ \\E[e^{tX}] = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{tx} e^{-x^2/2} dx \\nonumber \\] \\[ tx - \\frac{1}{2} x^2 = -\\frac{1}{2}\\left( ( x - t)^2 - t^2 \\right) \\] \\[ \\begin{aligned} \\E[e^{tX}] &amp; = \\frac{1}{\\sqrt{2\\pi}} e^{\\frac{t^{2}}{2}}\\int_{-\\infty}^{\\infty} e^{-(x- t)^2/2} dx \\nonumber \\\\ &amp; = e^{\\frac{t^{2}}{2}} \\nonumber \\end{aligned} \\] 11.3.2 Extracting moments of the normal distribution \\[ \\begin{aligned} M^{&#39;}(0) &amp; = \\E[X] = e^{t^2/2} t |_{0} = 0 \\nonumber \\\\ M^{&#39;&#39;} (0 ) &amp; = \\E[X^2] = e^{t^2/2} (t^2 + 1) |_{0} = 1 \\nonumber \\\\ M^{&#39;&#39;&#39;} (0) &amp; = \\E[X^3] = e^{t^2/2} t (t^2 + 3) |_{0} = 0 \\nonumber \\\\ M^{&#39;&#39;&#39;&#39;} (0) &amp; = \\E[X^4] = e^{t^2/2} (t^4 + 6t^2 + 3)|_{0} = 3 \\nonumber \\\\ M^{5} (0) &amp; = \\E[X^5] = e^{t^2/2} t (t^4 + 10t^2 + 15)|_{0} = 0 \\nonumber \\\\ M^{6} (0) &amp; = \\E[X^6] = e^{t^2/2} (t^6 + 15t^4 + 45t^2 + 15 )|_{0} = 15 \\nonumber \\end{aligned} \\] 11.4 Sequences of independent random variables Suppose \\(X_{i}\\) are a sequence of independent random variables. Define \\[ Y = \\sum_{i=1}^{N} X_{i} \\nonumber \\] Then \\[ M_{Y}(t) = \\prod_{i=1}^{N} M_{X_{i}}(t) \\nonumber \\] Proof. \\[ \\begin{aligned} M_{Y}(t) &amp; = \\E[e^{tY}] \\nonumber \\\\ &amp; = \\E[e^{t\\sum_{i=1}^{N} X_{i} } ] \\nonumber \\\\ &amp; = \\E[e^{tX_{1} + tX_{2} + \\ldots tX_{N} } ] \\nonumber \\\\ &amp; = \\E[e^{tX_{1} }]E[e^{tX_{2} }] \\ldots \\E[e^{tX_{N} }] \\text{ (by independence) } \\nonumber \\\\ &amp; = \\prod_{i=1}^{N} \\E[e^{tX_{i}}] \\nonumber \\end{aligned} \\] 11.5 Inequalities and limit theorems 11.5.1 Limit theorems What happens when we consider a long sequence of random variables? What can we reasonably infer from data? Laws of large numbers: averages of random variables converge on expected value? Central Limit Theorems: sum of random variables have normal distribution? We’ll focus on intuition for both, but we’ll prove some stuff too. 11.5.2 Weak law of large numbers Proof plan: Markov’s Inequality Chebyshev’s Inequality Weak Law of Large Numbers 11.5.2.1 Markov’s inequality Suppose \\(X\\) is a random variable that takes on non-negative values. Then, for all \\(a&gt;0\\), \\[ \\Pr(X\\geq a) \\leq \\frac{\\E[X]}{a} \\nonumber \\] Proof (Markov's inequality). For \\(a&gt;0\\), \\[ \\begin{aligned} \\E[X] &amp; = \\int_{0}^{\\infty} x f(x) dx \\nonumber \\\\ &amp; = \\int_{0}^{a} x f(x) dx + \\int_{a}^{\\infty} x f(x) dx \\nonumber \\end{aligned} \\] Because \\(X\\geq 0\\), \\[ \\begin{aligned} \\E[X] \\geq \\int_{a}^{\\infty} x f(x) dx \\geq \\int_{a}^{\\infty} a f(x)dx = a P(X \\geq a )\\nonumber \\\\ \\frac{\\E[X]}{a} \\geq P(X \\geq a )\\nonumber \\end{aligned} \\] 11.5.2.2 Chebyshev’s inequality If \\(X\\) is a random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then, for any value \\(k&gt;0\\), \\[ \\Pr(|X - \\mu| \\geq k) \\leq \\frac{\\sigma^2}{k^2} \\nonumber \\] Proof (Chebyshev's inequality). Define the random variable \\[ Y = (X - \\mu)^2 \\nonumber \\] Where \\(\\mu = E[X]\\). Then we know \\(Y\\) is a non-negative random variable. Set \\(a = k^2\\). Applying the inequality: \\[ \\begin{aligned} \\Pr(Y \\geq k^2) \\leq \\frac{\\E[Y]}{k^2} \\nonumber \\\\ \\Pr( (X- \\mu)^2 \\geq k^2) \\leq \\frac{\\E[(X - \\mu)^2]}{k^2} \\nonumber \\\\ \\Pr( (X- \\mu)^2 \\geq k^2) \\leq \\frac{\\sigma^2}{k^2}\\nonumber \\end{aligned} \\] Further we know that \\[ (X - \\mu)^2 \\geq k^2 \\nonumber \\] Implies that \\[ |X - \\mu| \\geq k \\nonumber \\] Thus, we have shown \\[ \\Pr( |X- \\mu| \\geq k) \\leq \\frac{\\sigma^2}{k^2}\\nonumber \\] 11.6 Sequence of random variables Sequence of independent and identically distributed (i.i.d.) random variables. Sequence: \\(X_{1}, X_{2}, \\ldots, X_{n}, \\ldots\\) Think of a sequence as sampled data: Suppose we are drawing a sample of \\(N\\) observations Each observation will be a random variable, say \\(X_{i}\\) With realization \\(x_{i}\\) 11.6.1 Mean/variance of sample mean Let \\(X_{1}, X_{2}, \\ldots, X_{n}\\) be a random sample from a distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Let \\(\\bar{X}_{n}\\) be the sample mean. Then \\[ \\E[\\bar{X}_{n}] = \\mu \\text{ and } \\Var(\\bar{X}_{n}) = \\frac{\\sigma^2}{n} \\] Proof. \\[ \\begin{aligned} \\E[\\bar{X}_{n}] &amp; = \\frac{1}{n}\\sum_{i=1}^{n} \\E[X_{i}] \\nonumber \\\\ &amp; = \\frac{1}{n} n \\mu = \\mu \\nonumber \\end{aligned} \\] \\[ \\begin{aligned} \\Var(\\bar{X}_{n}) &amp; = \\frac{1}{n^2} \\Var(\\sum_{i=1}^{n} X_{i}) \\nonumber \\\\ &amp;= \\frac{1}{n^2} \\sum_{i=1}^{n} \\Var(X_{i}) \\nonumber \\\\ &amp; = \\frac{1}{n^2} n \\sigma^2 = \\frac{\\sigma^2}{n} \\nonumber \\end{aligned} \\] 11.6.2 Weak law of large numbers Suppose \\(X_{1}, X_{2}, \\ldots, X_{n}\\) is a random sample from a distribution with mean \\(\\mu\\) and \\(Var(X_{i})= \\sigma^2\\). Then, for all \\(\\epsilon &gt;0\\), \\[ \\Pr\\left\\{ \\left| \\frac{X_{1} + X_{2} + \\ldots + X_{n} }{n} -\\mu \\right| \\geq \\epsilon \\right\\} \\rightarrow 0 \\text{ as } n \\rightarrow \\infty \\nonumber \\] Proof. From our previous proposition \\[ \\frac{\\text{E}[X_{1} + X_{2} + \\cdots + X_{n} ]}{n} = \\frac{\\sum_{i=1}^{n} E[X_{i}] }{n} = \\mu \\nonumber \\] Further, \\[ \\E[ (\\frac{\\sum_{i=1}^{n} X_{i} - \\mu}{n} )^2] = \\frac{\\Var(X_{1} + X_{2} + \\cdots + X_{n} )}{n^2} = \\frac{ \\sum_{i=1}^{n} \\Var(X_{i}) }{n^2} = \\frac{\\sigma^2}{n} \\nonumber \\] Apply Chebyshev’s Inequality: \\[ \\Pr\\left\\{ \\left| \\frac{X_{1} + X_{2} + \\ldots + X_{n} }{n} -\\mu \\right| \\geq \\epsilon \\right\\}\\leq \\frac{\\sigma^2}{n \\epsilon^2} \\nonumber \\] Suppose \\(X_{1}, X_{2}, \\ldots\\) are i.i.d. normal distributions, \\[ X_{i} \\sim \\text{Normal}(0, 10) \\nonumber \\] \\[ \\Pr\\left\\{ \\left| \\frac{X_{1} + X_{2} + \\ldots + X_{n} }{n} -\\mu \\right| \\geq 0.1 \\right\\} \\text{ as } n \\rightarrow \\infty\\nonumber \\] Suppose we want to guarantee that we have at most a \\(0.01\\) probability of being more than \\(0.1\\) away from the true \\(\\mu\\). How big do we need \\(n\\)? \\[ \\begin{aligned} 0.01 &amp; = \\frac{10}{n (0.1^2) } \\nonumber \\\\ n &amp; = \\frac{1000}{0.01} \\nonumber \\\\ n &amp; = 100,000 \\end{aligned} \\] 11.7 Sequences and convergence Sequence (refresher): \\[ \\left\\{a_{i} \\right\\}_{i=1}^{\\infty} = \\left\\{a_{1}, a_{2}, a_{3}, \\ldots, a_{n}, \\ldots, \\right\\} \\nonumber \\] Definition 11.3 We say that the sequence \\(\\left\\{a_{i} \\right\\}_{i=1}^{\\infty}\\) converges to real number \\(A\\) if for each \\(\\epsilon&gt;0\\) there is a positive integer \\(N\\) such that for \\(n\\geq N\\), \\(|a_{n} - A| &lt; \\epsilon\\) Sequence of functions: \\[ \\left\\{ f_{i} \\right\\}_{i=1}^{\\infty} = \\left\\{f_{1}, f_{2}, f_{3}, \\ldots, f_{n}, \\ldots, \\right\\}\\nonumber \\] Definition 11.4 (Pointwise convergence) Suppose \\(f_{i}: X \\rightarrow \\Re\\) for all \\(i\\). Then \\(\\left\\{ f_{i} \\right\\}_{i=1}^{\\infty}\\) converges pointwise to \\(f\\) if, for all \\(x \\in X\\) and \\(\\epsilon&gt; 0\\), there is an \\(N\\) such that for all \\(n\\geq N\\), \\[ |f_{n} (x) - f(x)|&lt;\\epsilon \\nonumber \\] This is as strong of a statement as we’re likely to make in statistics. 11.7.1 Convergence definitions Define \\(\\widehat{\\theta}_{n}\\) to be estimator for \\(\\theta\\) based on \\(n\\) observations. Sequence of estimators: increasing sample size \\[ \\left\\{\\widehat{\\theta}_{i}\\right\\}_{i=1}^{n} = \\left\\{\\widehat{\\theta}_{1}, \\widehat{\\theta}_{2}, \\widehat{\\theta}_{3}, \\ldots, \\widehat{\\theta}_{n} \\right\\} \\nonumber \\] Question: What can we say about \\(\\left\\{\\widehat{\\theta}_{i}\\right\\}_{i=1}^{n}\\) as \\(n\\rightarrow \\infty\\)? What is the probability \\(\\widehat{\\theta}_{n}\\) differs from \\(\\theta\\)? What is the probability \\(\\left\\{\\widehat{\\theta}_{i}\\right\\}_{i=1}^{n}\\) converges to \\(\\theta\\)? What is sampling distribution of \\(\\widehat{\\theta}_{n}\\) as \\(n \\rightarrow \\infty\\) ? 11.7.2 Convergence in probability Definition 11.5 (Convergence in probability) We will say the sequence \\(\\widehat{\\theta}_{n}\\) converges in probability to \\(\\theta\\) (perhaps a non-degenerate RV) if, \\[ \\lim_{n\\rightarrow\\infty} \\Pr(|\\widehat{\\theta}_{n} - \\theta | &gt; \\epsilon ) = 0 \\nonumber \\] For any \\(\\epsilon&gt;0\\) \\(\\epsilon\\) is a tolerance parameter: how much error around \\(\\theta\\)? In the limit, convergence in probability implies sampling distribution collapses on a spike at \\(\\theta\\) \\(\\left\\{\\widehat{\\theta}_{i}\\right\\}\\) need not actually converge to \\(\\theta\\), only (\\(\\Pr|\\theta_{n}\\) - \\(\\theta| &gt; \\epsilon) = 0\\) Example 11.1 Suppose \\(S \\sim\\) Uniform(0,1). Define \\(X(s) = s\\). Suppose \\(X_{n}\\) is defined as follows: \\[ \\begin{aligned} X_{1}(s) = s + I(s \\in [0,1]) &amp; , X_{2} (s) = s + I(s \\in [0,1/2]) \\nonumber \\\\ X_{3}(s) = s + I(s \\in [1/2, 1])&amp; , X_{4}(s) = s + I(s \\in [0,1/3]) \\nonumber \\\\ X_{5} (s) = s + I(s \\in [1/3,2/3]) &amp; , X_{6}(s) = s + I(s \\in [2/3, 1]) \\nonumber \\end{aligned} \\] Does \\(X_{n}(s)\\) pointwise converge to \\(X(s)\\)? Does \\(X_{n}(s)\\) converge in probability to \\(X(s)\\)? \\[ \\Pr(|X_{n} - X | &gt; \\epsilon) = \\Pr ( s \\in [l_{n}, u_{n} ] ) \\nonumber \\] Length of \\([l_{n}, u_{n}] \\rightarrow 0 \\Rightarrow P ( s \\in [L_{n}, U_{n} ] ) = 0\\). 11.7.3 Almost sure convergence Definition 11.6 We will say the sequence \\(\\widehat{\\theta}_{n}\\) converges almost surely to \\(\\theta\\) if, \\[ \\Pr(\\lim_{n \\rightarrow \\infty} |\\widehat{\\theta}_{n} - \\theta| &gt; \\epsilon ) = 0 \\nonumber \\] Stronger: says that sequence converges to \\(\\theta\\) (almost everywhere) ) Think about definition of random variable: \\(\\widehat{\\theta}_{n}\\) is a function from sample space to real line. Almost sure says that, for all outcomes (\\(s\\)) in sample space (\\(S\\)) \\(s \\in S\\), \\[ \\widehat{\\theta}_{n}(s) \\rightarrow \\theta(s) \\nonumber \\] Except for a subset \\(\\mathcal{N} \\subset S\\) such that \\(\\Pr(\\mathcal{N}) = 0\\). 11.7.3.1 Example Suppose \\(S \\sim\\) Uniform(0,1). Suppose \\(X_{n}\\) is defined as follows: \\[ \\begin{aligned} X_{1}(s) = s + I(s \\in [0,1]) &amp; , X_{2} (s) = s + I(s \\in [0,1/2]) \\nonumber \\\\ X_{3}(s) = s + I(s \\in [1/2, 1])&amp; , X_{4}(s) = s + I(s \\in [0,1/3]) \\nonumber \\\\ X_{5} (s) = s + I(s \\in [1/3,2/3]) &amp; , X_{6}(s) = s + I(s \\in [2/3, 1]) \\nonumber \\end{aligned} \\] Does \\(X_{n}(s)\\) converge almost surely to \\(X(s) = s\\)? No! – the sequence doesn’t converge for each \\(s\\). For each value of \\(s\\) the sequence varies between \\(s\\) and \\(s + 1\\) infinitely often. 11.7.4 Convergence in distribution We’ve talked about \\(\\widehat{\\theta}_{n}\\)’s sampling distribution converging to a normal distribution. This is convergence in distribution. Definition 11.7 (Convergence in distribution) \\(\\widehat{\\theta}_{n}\\), with cdf \\(F_{n}(x)\\), converges in distribution to random variable \\(Y\\) with cdf \\(F(x)\\) if \\[ \\lim_{n\\rightarrow \\infty} |F_{n} (x) - F(x) | = 0 \\nonumber \\] For all \\(x \\in \\Re\\) where \\(F(x)\\) is continuous. Weakest form of convergence almost sure \\(\\rightarrow\\) probability \\(\\rightarrow\\) distribution Says that cdfs are equal, says nothing about convergence of underlying random variable Useful for justifying use of some sampling distributions 11.7.5 Convergence in distribution \\(\\not \\Rightarrow\\) convergence in probability Define \\(X \\sim N(0,1)\\) and each \\(X_{n} = - X\\). Then: \\(X_{n} \\sim N(0,1)\\) for all \\(n\\) so \\(X_{n}\\) trivially converges to \\(X\\). But, \\[ \\begin{aligned} \\Pr(|X_{n} - X| &gt; \\epsilon ) &amp; = \\Pr(|X + X| &gt; \\epsilon) \\nonumber \\\\ &amp; = \\Pr( |2X| &gt; \\epsilon) \\nonumber \\\\ &amp; = \\Pr(|X| &gt; \\epsilon/2) \\not \\leadsto 0 \\end{aligned} \\] 11.7.6 Central limit theorem Let \\(X_{1}\\), \\(X_{2}, \\ldots\\) be a sequence of independent random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Let \\(X_{i}\\) have a cdf \\(P(X_{i} \\leq x) = F(x)\\) and moment generating function \\(M(t) = \\E[e^{tX_{i}}]\\) . Let \\(S_{n} = \\sum_{i=1}^{n} X_{i}\\). Then \\[ \\lim_{n\\rightarrow \\infty} P\\left( \\frac{S_{n} - \\mu n }{\\sigma\\sqrt{n}} \\leq x \\right) = \\frac{1}{\\sqrt{2\\pi} } \\int_{-\\infty}^{x} \\exp\\left( -\\frac{z^{2} }{2} \\right) dz \\nonumber \\] Proof plan: Rely on fact that convergence of MGFs \\(\\leadsto\\) convergence in CDFs Show that MGFs, in limit, converge on normal MGF Let \\(F_{n}\\) be a sequence of cumulative distribution functions with the corresponding moment generating functions \\(M_{n}\\). \\(F\\) be a cdf with the moment generating functions \\(M\\). If \\(\\lim_{n\\rightarrow \\infty} M_{n}(t) \\rightarrow M(t)\\) for all \\(t\\) in some interval, then \\(F_{n}(x) \\leadsto F(x)\\) for all \\(x\\) (when \\(F\\) is continuous). Suppose \\(\\lim_{n\\rightarrow \\infty} a_{n} \\rightarrow a\\), then \\[ \\lim_{n\\rightarrow \\infty} \\left( 1 + \\frac{a_{n}}{n}\\right)^{n} = e^{a} \\nonumber \\] Suppose \\(M(t)\\) is a moment generating function some random variable \\(X\\). Then \\(M(0) = 1\\). Proof (Central limit theorem). Suppose \\(X_{1}, \\ldots, X_{n}\\) are i.i.d. variables with \\(\\E[X] = 0\\), variance \\(\\sigma^{2}_{x}\\), Moment Generating Function (MGF) \\(M_{x}(t)\\). Let \\(S_{n} = \\sum_{i=1}^{n} X_{i}\\) and \\(Z_{n} = \\frac{S_{n}}{\\sigma_{x} \\sqrt{n}}\\). \\(M_{S_{n}} = (M_{x}(t))^{n}\\) and \\(M_{Z_{n}} (t) = \\left(M_{x} \\left(\\frac{t}{\\sigma_{x} \\sqrt{n}} \\right) \\right)^{n}\\) Using Taylor’s Theorem we can write \\[ M_{x} (s) = M_{x} (0) + s M^{&#39;}_{x}(0) + \\frac{1}{2} s^2 M_{x}^{&#39;&#39;}(0) + e_{s} \\nonumber \\] \\(e_{s}/s^2 \\rightarrow 0\\) as \\(s\\rightarrow 0\\). Filling in the values we have \\[ M_{x} (s) = 1 + 0 + \\frac{\\sigma_{x}^{2}}{2} s^2 + \\underbrace{e_{s}}_{\\text{Goes to zero}} \\nonumber \\] Set \\(s = \\frac{t}{\\sigma_{x} \\sqrt{n}}\\) \\(\\lim_{n\\rightarrow \\infty} s \\rightarrow 0\\). Then \\[ \\begin{aligned} M_{Z_{n}}(t) &amp; = \\left(1 + \\frac{\\sigma_{x}^{2}}{2}\\left( \\frac{t}{\\sigma_{x} \\sqrt{n}} \\right)^{2} \\right)^{n} \\nonumber \\\\ &amp; = \\left( 1 + \\frac{t^{2}/2}{n} \\right)^{n} \\nonumber \\\\ \\lim_{n\\rightarrow \\infty} M_{Z_{n}}(t) &amp; = e^{\\frac{t^2}{2}}\\nonumber \\end{aligned} \\] 11.7.6.1 Why this matters The central limit theorem has a wide range of applications in statistics. Most importantly, the central limit theorem eliminates the need for detailed probabilistic model and extended calculations of PMFs and PDFs. Instead, many probabilities can be calculated using a normal CDF table based purely on our knowledge of means and variances. Most of the core techniques for classical statistical inference are derived from this theorem. References "],["classic-inference.html", "Lecture 12 Classical statistical inference Learning objectives Supplemental readings 12.1 Statistical inference 12.2 Parametric models 12.3 Point estimates 12.4 Confidence sets 12.5 Hypothesis testing 12.6 \\(p\\)-values", " Lecture 12 Classical statistical inference Learning objectives Define classical statistical inference Summarize core concepts of point estimates, confidence sets, and hypothesis testing Define parametric inference and identify use cases Summarize point estimates Define hypothesis testing and \\(p\\)-value Define the Wald test Summarize the \\(\\chi^2\\) test of significance Supplemental readings Chapter 9 Bertsekas and Tsitsiklis (2008) Wasserman (2013) Ch 6 - Models, Statistical Inference and Learning Ch 10 - Hypothesis Testing and p-values 12.1 Statistical inference Statistical inference is the process of using data to infer the probability distribution/random variable that generated the data. Given a sample \\(X_1, \\ldots, X_n \\sim F\\), how do we infer \\(F\\)? Sometimes we want to infer all the features/parameters of \\(F\\), and sometimes we only need a subset of those features/parameters. 12.2 Parametric models A statistical model \\(\\xi\\) is a set of distributions (or densities or regression functions). A parametric model is a set \\(\\xi\\) that can be parameterized by a finite number of parameters. We have seen many examples of parametric models - all the major types of random variables we’ve explored are defined in terms of a fixed number of parameters. For instance, if we assume that the data is generated by a Normal distribution, then the model is \\[\\xi \\equiv f(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left[ -\\frac{1}{2\\sigma^2} (x - \\mu)^2 \\right], \\quad \\mu \\in \\Re, \\sigma &gt; 0\\] This is an example of a two-parameter model. The density \\(f(x; \\mu, \\sigma)\\) indicates that \\(x\\) is a value of the random variable \\(X\\), whereas \\(\\mu\\) and \\(\\sigma\\) are parameters that define the model. In general, a parametric model takes the form \\[\\xi \\equiv f(x; \\theta) : \\theta \\in \\Theta\\] where \\(\\theta\\) is an unknown parameter (or vector of parameters) that can only take values in the parameter space \\(\\Theta\\). If \\(\\theta\\) is a vector but we are only interested in one component of \\(\\theta\\), then we call the remaining parameters nuisance parameters. 12.2.1 Examples of parametric models Example 12.1 (One-dimensional parametric estimation) Let \\(X_1, \\ldots, X_n\\) be independent observations drawn from a Bernoulli random variable with probability \\(\\pi\\) of success. The problem is to estimate the parameter \\(\\pi\\). Example 12.2 (Two-dimensional parametric estimation) Suppose that \\(X_1, \\ldots, X_n \\sim F\\) and we assume that the PDF \\(f \\in \\xi\\) where \\[\\xi \\equiv f(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left[ -\\frac{1}{2\\sigma^2} (x - \\mu)^2 \\right], \\quad \\mu \\in \\Re, \\sigma &gt; 0\\] In this case, there are two parameters, \\(\\mu\\) and \\(\\sigma\\). The goal is to estimate the parameters from the data. If we are only interested in estimating \\(\\mu\\) (which is generally the case for inferential methods such as linear regression), then \\(\\mu\\) is the parameter of interest and \\(\\sigma\\) is a nuisance parameter. 12.3 Point estimates Point estimation refers to providing a single “best guess” of some quantity of interest. This quantity of interest could be a parameter in a parametric model, a CDF \\(F\\), a PDF \\(f\\), a regression function \\(r\\), or a prediction for a future value \\(Y\\) of some random variable. We denote a point estimate of \\(\\theta\\) by \\(\\hat{\\theta}\\) or \\(\\hat{\\theta}_n\\). Remember that \\(\\theta\\) is a fixed, unknown quantity. The estimate \\(\\hat{\\theta}\\) depends on the data, so \\(\\hat{\\theta}\\) is a random variable. More formally, let \\(X_1, \\ldots, X_n\\) be \\(n\\) IID (independently and identically drawn) data points from some distribution \\(F\\). A point estimator \\(\\hat{\\theta}_n\\) of a parameter \\(\\theta\\) is some function of \\(X_1, \\ldots, X_n\\): \\[\\hat{\\theta}_n = g(X_1, \\ldots, X_n)\\] 12.3.1 Properties of point estimates The bias of an estimator is defined as \\[\\text{bias}(\\hat{\\theta}_n) = \\E_\\theta (\\hat{\\theta_n}) - \\theta\\] When \\(\\E (\\hat{\\theta_n}) - \\theta = 0\\), we say that \\(\\hat{\\theta_n}\\) is unbiased. Many estimators in statistical inference are not unbiased – with modern approaches, this is sometimes justified. We will see examples of this in Perspectives on Computational Modeling. A more preferable requirement for an estimator is consistency: as the number of observations \\(n\\) increases, the estimator should converge towards the true parameter \\(\\theta\\). The distribution of \\(\\hat{\\theta}_n\\) is called the sampling distribution. The standard deviation of \\(\\hat{\\theta}_n\\) is called the standard error: \\[\\se = \\sd(\\hat{\\theta}_n) = \\sqrt{\\Var (\\hat{\\theta}_n)}\\] Frequently the standard error depends on the unknown \\(F\\). In those cases, we usually estimate it. The estimated standard error is denoted by \\(\\widehat{\\se}\\). The quality of the point estimate is sometimes assessed by the mean squared error (MSE) defined by \\[ \\begin{align} \\text{MSE} &amp;= \\E_\\theta [(\\hat{\\theta}_n - \\theta)^2] \\\\ &amp;= \\text{bias}^2(\\hat{\\theta}_n) + \\Var_\\theta (\\hat{\\theta}_n) \\end{align} \\] Remember that \\(\\E_\\theta (\\cdot)\\) refers to expectation with respect to the distribution \\(f(x_1, \\ldots, x_n; \\theta)\\) that generated the data. \\(\\theta\\) does not have a distribution - it is a fixed, but unknown, value. Many estimators turn out to have, approximately, a Normal distribution – another reason why this continuous distribution is so important to statistical inference. \\[\\frac{\\hat{\\theta}_n - \\theta}{\\se} \\leadsto N(0,1)\\] Example 12.3 (Bernoulli distributed random variable) Let \\(X_1, \\ldots, X_n ~ \\text{Bernoulli}(\\pi)\\) and let \\(\\hat{\\pi}_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\). Then \\[\\E(\\hat{\\pi}_n) = \\frac{1}{n} \\sum_{i=1}^n \\E(X_i) = \\pi\\] so \\(\\hat{\\pi}_n\\) is unbiased. The standard error is \\[\\se = \\sqrt{\\Var (\\hat{\\pi}_n)} = \\sqrt{\\frac{\\pi (1 - \\pi)}{n}}\\] which can be estimated as \\[\\widehat{\\se} = \\sqrt{\\frac{\\hat{\\pi} (1 - \\hat{\\pi})}{n}}\\] Additionally, we have that \\(\\E_\\pi (\\hat{\\pi}_n) = \\pi\\) so \\(\\text{bias} = \\pi - \\pi = 0\\) \\[ \\begin{align} \\text{bias}(\\hat{\\pi}_n) &amp;= \\E_\\pi (\\hat{\\pi}) - \\pi \\\\ &amp;= \\pi - \\pi \\\\ &amp;= 0 \\end{align} \\] and \\[\\se = \\sqrt{\\frac{\\pi (1 - \\pi)}{n}} \\rightarrow 0\\] as \\(n\\) increases. Hence, \\(\\hat{\\pi}_n\\) is a consistent estimator of \\(\\pi\\). 12.4 Confidence sets A \\(1 - \\alpha\\) confidence interval for a parameter \\(\\theta\\) is an interval \\(C_n = (a,b)\\) where \\(a = a(X_1, \\ldots, X_n)\\) and \\(b = b(X_1, \\ldots, X_n)\\) are functions of the data such that \\[\\Pr_{\\theta} (\\theta \\in C_n) \\geq 1 - \\alpha, \\quad \\forall \\, \\theta \\in \\Theta\\] In other words, \\((a,b)\\) traps \\(\\theta\\) with probability \\(1- \\alpha\\). We call \\(1 - \\alpha\\) the coverage of the confidence interval. 12.4.1 Caution interpreting confidence intervals \\(C_n\\) is random and \\(\\theta\\) is fixed. This is a core assumption of statistical inference and especially critical for frequentist inference. Commonly people use 95% confidence intervals corresponding to \\(\\alpha = 0.05\\). If \\(\\theta\\) is a vector then we use a confidence set (such as a sphere or an ellipse) instead of an interval. A confidence interval is not a probability statement about \\(\\theta\\) since \\(\\theta\\) is a fixed quantity, not a random variable. Either \\(\\theta\\) is or is not in the interval with probability \\(1\\). A better definition is: Definition 12.1 (Confidence interval) On day 1, you collect data and construct a 95% confidence interval for a parameter \\(\\theta_1\\). On day 2, you collect new data and construct a 95% confidence interval for a parameter \\(\\theta_2\\). You continue this way constructing confidence intervals for a sequence of unrelated parameters \\(\\theta_1, \\theta_2, \\ldots\\). Then 95% of your intervals will trap the true parameter value. 12.4.2 Constructing confidence intervals Because point estimators have an approximate Normal distribution, we can use the Normal distribution to construct confidence intervals relatively easily for point estimates by relying directly on the Normal distribution. Suppose that \\(\\hat{\\theta}_n \\approx N(\\theta, \\widehat{\\se}^2)\\). Let \\(\\Phi\\) be the CDF of a standard Normal distribution and let \\[z_{\\frac{\\alpha}{2}} = \\Phi^{-1} \\left(1 - \\frac{\\alpha}{2} \\right)\\] That is, \\[\\Pr (Z &gt; \\frac{\\alpha}{2}) = \\frac{\\alpha}{2}\\] and \\[\\Pr (-z_{\\frac{\\alpha}{2}} \\leq Z \\leq z_{\\frac{\\alpha}{2}}) = 1 - \\alpha\\] where \\(Z \\sim N(0,1)\\). Let \\[C_n = (\\hat{\\theta}_n - z_{\\frac{\\alpha}{2}} \\widehat{\\se}, \\hat{\\theta}_n + z_{\\frac{\\alpha}{2}} \\widehat{\\se})\\] Then \\[ \\begin{align} \\Pr_\\theta (\\theta \\in C_n) &amp;= \\Pr_\\theta (\\hat{\\theta}_n - z_{\\frac{\\alpha}{2}} \\widehat{\\se} &lt; \\theta &lt; \\hat{\\theta}_n + z_{\\frac{\\alpha}{2}} \\widehat{\\se}) \\\\ &amp;= \\Pr_\\theta (- z_{\\frac{\\alpha}{2}} &lt; \\frac{\\hat{\\theta}_n - \\theta}{\\widehat{\\se}} &lt; z_{\\frac{\\alpha}{2}}) \\\\ &amp;\\rightarrow \\Pr ( - z_{\\frac{\\alpha}{2}} &lt; Z &lt; z_{\\frac{\\alpha}{2}}) \\\\ &amp;= 1 - \\alpha \\end{align} \\] For 95% confidence intervals, \\(\\alpha = 0.05\\) and \\(z_{\\frac{\\alpha}{2}} = 1.96 \\approx 2\\) leading to the approximate 95% confidence interval \\(\\hat{\\theta}_n \\pm 2 \\widehat{\\se}\\). 12.5 Hypothesis testing In hypothesis testing, we start with some default theory – called a null hypothesis – and we ask if the data provide sufficient evidence to reject the theory. If not, we fail to reject the null hypothesis. Formally, suppose we partition the parameter space \\(\\Theta\\) into two disjoint sets \\(\\Theta_0\\) and \\(\\Theta_1\\) and that we wish to test \\[H_0: \\theta \\in \\Theta_0 \\quad \\text{versus} \\quad H_1: \\theta \\in \\Theta_1\\] \\(H_0\\) - null hypothesis \\(H_1\\) - alternative hypothesis Let \\(X\\) be a random variable and let \\(\\chi\\) be the range of \\(X\\). We test a hypothesis by finding an appropriate subset of outcomes \\(R \\subset \\chi\\) called the rejection region. If \\(X \\subset R\\) we reject the null hypothesis, otherwise we do not reject the null hypothesis. Usually the rejection region \\(R\\) is of the form \\[R = \\left\\{ x: T(x) &gt; c \\right\\}\\] where \\(T\\) is a test statistic and \\(c\\) is a critical value. Hypothesis testing requires us to find an appropriate test statistic \\(T\\) and an appropriate critical value \\(c\\) to test a given hypothesis. Different hypotheses require different test statistics. 12.5.1 Types of errors Figure 12.1: Stereotypical example of hypothesis testing errors. Hypothesis testing is not error-proof. We start from the assumption that \\(H_0\\) is true unless there is strong evidence to reject \\(H_0\\). Rejecting \\(H_0\\) when \\(H_0\\) is true is a type I error (false positive), while retaining \\(H_0\\) when \\(H_1\\) is true is called a type II error (false negative). 12.5.2 Power function The power function of a test with rejection region \\(R\\) is defined by \\[\\beta(\\theta) = \\Pr_\\theta (X \\in R)\\] The size of a test is defined to be \\[\\alpha = \\text{sup}_{\\theta \\in \\Theta_0} \\beta(\\theta)\\] \\(\\text{sup}\\) - supremum, or the largest value that \\(\\beta(\\theta)\\) could take on in the given \\(\\theta \\in \\Theta_0\\). A test is said to have level \\(\\alpha\\) if its size is less than or equal to \\(\\alpha\\). 12.5.3 Sided tests A test of the form \\[H_0: \\theta = \\theta_0 \\quad \\text{versus} \\quad H_1: \\theta \\neq \\theta_0\\] is called a two-sided test, or a simple hypothesis. A test of the form \\[H_0: \\theta \\leq \\theta_0 \\quad \\text{versus} \\quad H_1: \\theta &gt; \\theta_0\\] or \\[H_0: \\theta \\geq \\theta_0 \\quad \\text{versus} \\quad H_1: \\theta &lt; \\theta_0\\] is called a one-sided test, or a composite hypothesis. 12.5.4 Example hypothesis test Let \\(X_1, \\ldots, X_n \\sim N(\\mu, \\sigma^2)\\) where \\(\\sigma\\) is known. We want to test \\(H_0: \\mu \\leq 0\\) versus \\(H_1: \\mu &gt; 0\\). Hence, \\(\\Theta_0 = (-\\infty, 0]\\) and \\(\\Theta_1 = (0, \\infty]\\). Consider the test \\[\\text{reject } H_0 \\text{ if } T&gt;c\\] where \\(T = \\bar{X}\\). The rejection region is \\[R = \\left\\{(x_1, \\ldots, x_n): T(x_1, \\ldots, x_n) &gt; c \\right\\}\\] Let \\(Z\\) denote the standard Normal random variable. The power function is \\[ \\begin{align} \\beta(\\mu) &amp;= \\Pr_\\mu (\\bar{X} &gt; c) \\\\ &amp;= \\Pr_\\mu \\left(\\frac{\\sqrt{n} (\\bar{X} - \\mu)}{\\sigma} &gt; \\frac{\\sqrt{n} (c - \\mu)}{\\sigma} \\right) \\\\ &amp;= \\Pr_\\mu \\left(Z &gt; \\frac{\\sqrt{n} (c - \\mu)}{\\sigma} \\right) \\\\ &amp;= 1 - \\Phi \\left( \\frac{\\sqrt{n} (c - \\mu)}{\\sigma} \\right) \\end{align} \\] This function is increasing in \\(\\mu\\): Hence \\[\\alpha = \\text{sup}_{\\mu \\leq 0} \\beta(\\mu) = \\beta(0) = 1 - \\Phi \\left( \\frac{\\sqrt{n} (c)}{\\sigma} \\right)\\] For a size \\(\\alpha\\) test, we set this equal to \\(\\alpha\\) and solve for \\(c\\) to get \\[c = \\frac{\\sigma \\Phi^{-1} (1 - \\alpha)}{\\sqrt{n}}\\] We reject \\(H_0\\) when \\[\\bar{X} &gt; \\frac{\\sigma \\Phi^{-1} (1 - \\alpha)}{\\sqrt{n}}\\] Equivalently, we reject when \\[\\frac{\\sqrt{n}(\\bar{X} - 0)}{\\sigma} &gt; z_\\alpha\\] where \\(z_\\alpha = \\Phi^{-1} (1 - \\alpha)\\). Ideally we find the test with the highest power under \\(H_1\\) among all size \\(\\alpha\\) tests. In practice, we use many of the same commonly used tests. 12.5.5 Wald test Let \\(\\theta\\) be a scalar parameter, let \\(\\hat{\\theta}\\) be an estimate of \\(\\theta\\), and let \\(\\widehat{\\se}\\) be the estimated standard error of \\(\\hat{\\theta}\\). Consider testing \\[H_0: \\theta = \\theta_0 \\quad \\text{versus} \\quad H_1: \\theta \\neq \\theta_0\\] Assume that \\(\\hat{\\theta}\\) is asymptotically Normal: \\[\\frac{\\hat{\\theta} - \\theta_0}{\\widehat{\\se}} \\leadsto N(0,1)\\] The size \\(\\alpha\\) Wald test is: reject \\(H_0\\) when \\(|W| &gt; z_{\\alpha / 2}\\) where \\[W = \\frac{\\hat{\\theta} - \\theta_0}{\\widehat{\\se}}\\] This test statistic follows the Normal distribution. 12.5.5.1 Power of the Wald test Suppose the true value of \\(\\theta\\) is \\(\\theta_* \\neq \\theta_0\\). The power \\(\\beta(\\theta_*)\\) – the probability of correctly rejecting the null hypothesis – is given (approximately) by \\[1 - \\Phi \\left( \\frac{\\hat{\\theta} - \\theta_0}{\\widehat{\\se}} + z_{\\alpha/2} \\right) + \\Phi \\left( \\frac{\\hat{\\theta} - \\theta_0}{\\widehat{\\se}} - z_{\\alpha/2} \\right)\\] Remember this is a two-tailed test. Essentially we are collecting the probability mass in the center of the standard normal distribution and subtracting that from 1, to get the area in the tails of the distribution. Hence, two-tailed test. Recall that \\(\\widehat{\\se}\\) tends to 0 as the sample size increases. So we can note that: The power is large if \\(\\theta_*\\) is far from \\(\\theta_0\\) The power is large if the sample size is large Example 12.4 (Comparing two means) Let \\(X_1, \\ldots, X_m\\) and \\(Y_1, \\ldots, Y_n\\) be two independent samples from populations with means \\(\\mu_1, \\mu_2\\) respectively. Let’s test the null hypothesis that \\(\\mu_1 = \\mu_2\\). Write this as \\[H_0: \\delta = 0 \\quad \\text{versus} \\quad H_1: \\delta \\neq 0\\] where \\(\\delta = \\mu_1 - \\mu_2\\). The estimate of \\(\\delta\\) is \\(\\hat{\\delta} = \\bar{X} - \\bar{Y}\\) with estimated standard error \\[\\widehat{\\se} = \\sqrt{\\frac{s_1^2}{m} + \\frac{s_2^2}{n}}\\] where \\(s_1^2\\) and \\(s_2^2\\) are the sample variances. The size \\(\\alpha\\) Wald test rejects \\(H_0\\) when \\(|W| &gt; z_{\\alpha / 2}\\) where \\[W = \\frac{\\hat{\\delta} - 0}{\\widehat{\\se}} = \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\frac{s_1^2}{m} + \\frac{s_2^2}{n}}}\\] 12.5.6 Wald or \\(t\\)-test? To test \\(H_0: \\mu = \\mu_0\\) where \\(\\mu = \\E[X_i]\\) is the mean, we can use the Wald test. When the data are assumed to be Normal and the sample size is small, it is common to use the \\(t\\)-test based on the Student’s \\(t\\) distribution. Definition 12.2 (Degrees of freedom) Generally defined as the number of observations minus the number of estimated parameters. 12.5.7 Relationship to confidence intervals There is a relationship between the Wald test and the \\(1 - \\alpha\\) asymptotic confidence interval \\(\\hat{\\theta} \\pm \\widehat{\\se} z_{\\alpha/2}\\). The size \\(\\alpha\\) Wald test rejects \\(H_0: \\theta = \\theta_0 \\quad \\text{versus} \\quad \\theta \\neq \\theta_0\\) if and only if \\(\\theta_0 \\notin C\\) where \\[C = (\\hat{\\theta} - \\widehat{\\se}z_{\\alpha / 2}, \\hat{\\theta} + \\widehat{\\se}z_{\\alpha / 2})\\] Thus, testing the hypothesis is equivalent to checking whether the null value is in the confidence interval. 12.5.8 Statistical vs. scientific significance Figure 12.2: Difference between statistical and scientific significance. Courtesy of Carl Sagan. Rejecting \\(H_0\\) indicates the result is statistically significant. That is, we have strong evidence to reject \\(H_0\\). The result or effect size can still be small if our test is powerful. In that situation, we have statistical significance but not necessarily scientific/substantive/practical significance. You should always be concerned with both of these types of significance. Statistical significance alone is not necessarily a useful or informative finding. 12.6 \\(p\\)-values I hesitate to delve too deeply into \\(p\\)-values in this camp. \\(p\\)-values are increasingly problematic due to their improper usage. That said, you will see them all the time in published research so you need to understand how to properly use and interpret them. We could use a more fine-grained measure of the evidence against the null hypothesis. Generally, if the test rejects at level \\(\\alpha\\) it will also reject at level \\(\\alpha&#39; &gt; \\alpha\\). Hence, there is the smallest \\(\\alpha\\) at which the test rejects and we call this number the \\(p\\)-value. Informally, the smaller the \\(p\\)-value, the stronger the evidence against \\(H_0\\). Remember that this \\(\\alpha\\) will be a function of the power of the test, so both the magnitude of the difference between \\(\\theta_*\\) and \\(\\theta_0\\) and the sample size will influence this value. 12.6.1 Interpreting \\(p\\)-values Again - this is not really a great usage of \\(p\\)-values, but they are extremely common thresholds that you will see researchers use. \\(p\\)-value evidence \\(&lt; .01\\) very strong evidence against \\(H_0\\) \\(.01 - .05\\) strong evidence against \\(H_0\\) \\(.05 - .10\\) weak evidence against \\(H_0\\) \\(&gt; .1\\) little or no evidence against \\(H_0\\) These values are informal standards. There is no rhyme or reason they have to be so A large \\(p\\)-value is not strong evidence in favor of \\(H_0\\) \\(H_0\\) could be true \\(H_0\\) is false but the test has low power \\(p\\)-value is not \\(\\Pr (H_0 | \\text{Data})\\). The \\(p\\)-value is not the probability that the null hypothesis is true 12.6.2 Calculating \\(p\\)-values Suppose that the size \\(\\alpha\\) test is of the form \\[\\text{reject } H_0 \\text{ if and only if } T(X_n) \\geq c_\\alpha\\] Then, \\[\\text{p-value} = \\text{sup}_{\\theta \\in \\Theta_0} \\Pr_\\theta (T(X^n) \\geq T(x^n))\\] where \\(x^n\\) is the observed value of \\(X^n\\). If \\(\\Theta_0 = \\{ \\theta_0 \\}\\) then \\[\\text{p-value} = \\Pr_{\\theta_0} (T(X^n) \\geq T(x^n))\\] Informally, the \\(p\\)-value is the probability under \\(H_0\\) of observing a value of the test statistic the same as or more extreme then what was actually observed. 12.6.2.1 \\(p\\)-value for Wald test Let \\[w = \\frac{\\hat{\\theta} - \\theta_0}{\\widehat{\\se}}\\] denote the observed value of the Wald statistic \\(W\\). The \\(p\\)-value is given by \\[\\text{p-value} = \\Pr_{\\theta_0} (|W| &gt; |w|) \\approx \\Pr (|Z| &gt; |w| = 2 \\Phi(-|w|)\\] where \\(Z \\sim N(0,1)\\). Example 12.5 (Cholesterol data) Consider a set of 371 individuals in a health study examining cholesterol levels (in mg/dl). 320 individuals have narrowing of the arteries, while 51 patients have no evidence of heart disease. Is the mean cholesterol different in the two groups? Let the estimated mean cholesterol levels for the first group be \\(\\bar{X} = 216.2\\) and for the second group \\(\\bar{Y} = 195.3\\). Let the estimated standard error for each group be \\(\\widehat{\\se}(\\hat{\\mu}_1) = 5.0\\) and \\(\\widehat{\\se}(\\hat{\\mu}_2) = 2.4\\). The Wald test statistic is \\[W = \\frac{\\hat{\\delta} - 0}{\\widehat{\\se}} = \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\widehat{\\se}_1^2 + \\widehat{\\se}_2^2}} = \\frac{216.2 - 195.3}{\\sqrt{5^2 + 2.4^2}} = 3.78\\] To compute the \\(p\\)-value, let \\(Z \\sim N(0,1)\\) denote a standard Normal random variable. Then \\[\\text{p-value} = \\Pr (|Z| &gt; 3.78) = 2 \\Pr(Z &lt; -3.78) = 0.0002\\] which is very strong evidence against the null hypothesis. 12.6.3 Pearson’s \\(\\chi^2\\) test for multinomial data Pearson’s \\(\\chi^2\\) test is used for multinomial data. Recall that if \\(X = (X_1, \\ldots, X_k)\\) has a multinomial \\((n,p)\\) distribution, then the MLE of \\(p\\) is \\(\\hat{p} = (\\hat{p}_1, \\ldots, \\hat{p}_k) = (x_1 / n, \\ldots, x_k / n)\\). Let \\(p_0 = (p_{01}, \\ldots, p_{0k})\\) be some fixed vector and suppose we want to test \\[H_0: p = p_0 \\quad \\text{versus} \\quad H_1: p \\neq p_0\\] Pearson’s \\(\\chi^2\\) statistic is \\[T = \\sum_{j=1}^k \\frac{(X_j - np_{0j})^2}{np_{0j}} = \\sum_{j=1}^k \\frac{(X_j - \\E[X_j])^2}{\\E[X_j]}\\] where \\(\\E[X_j] = \\E[X_j] = np_{0j}\\) is the expected value under \\(H_0\\). 12.6.3.1 Example: Attitudes towards abortion \\(H_A\\) - In a comparison of individuals, liberals are more likely to favor allowing a woman to obtain an abortion for any reason than conservatives \\(H_0\\) - There is no difference in support between liberals and conservatives for allowing a woman to obtain an abortion for any reason. Any difference is the result of random sampling error. Say the null hypothesis is correct - there are no differences between ideological groups and attitudes towards abortion. What would the table look like?22 Right to Abortion Liberal Moderate Conservative Total Yes 40.8% 40.8% 40.8% 40.8% (206.45) (289.68) (271.32) (768) No 59.2% 59.2% 59.2% 59.2% (299.55) (420.32) (393.68) (1113) Total 26.9% 37.7% 35.4% 100% (506) (710) (665) (1881) In truth, what does our table actually look like? Right to Abortion Liberal Moderate Conservative Total Yes 62.6% 36.6% 28.7% 40.8% (317) (260) (191) (768) No 37.4% 63.4% 71.28% 59.2% (189) (450) (474) (1113) Total 26.9% 37.7% 35.4% 100% (506) (710) (665) (1881) How can we test if these differences are statistically significant? That is, how do we test to see if we can reject the null hypothesis? Right to Abortion Liberal Moderate Conservative Yes Observed Frequency (\\(X_j\\)) 317.0 260.0 191.0 Expected Frequency (\\(\\E[X_j]\\)) 206.6 289.9 271.5 \\(X_j - \\E[X_j]\\) 110.4 -29.9 -80.5 \\((X_j - \\E[X_j])^2\\) 12188.9 893.3 6482.7 \\(\\frac{(X_j - \\E[X_j])^2}{\\E[X_j]}\\) 59.0 4.1 23.9 No Observed Frequency (\\(X_j\\)) 189.0 450.0 474.0 Expected Frequency (\\(\\E[X_j]\\)) 299.4 420.1 393.5 \\(X_j - \\E[X_j]\\) -110.4 29.9 80.5 \\((X_j - \\E[X_j])^2\\) 12188.9 893.3 6482.7 \\(\\frac{(X_j - \\E[X_j])^2}{\\E[X_j]}\\) 40.7 2.1 16.5 Calculating test statistic \\(\\chi^2=\\sum{\\frac{(X_j - \\E[X_j])^2}{\\E[X_j]}}=145.27\\) \\(\\text{Degrees of freedom} = (\\text{number of rows}-1)(\\text{number of columns}-1)=2\\) Calculating \\(p\\)-value \\(\\text{p-value} = \\Pr (\\chi_2^2 &gt; 145.27) = 0\\) The probability that the null hypothesis is true and the observed frequencies are the result of random sampling error is less than 1 in a quintillion. Extremely extremely unlikely the null hypothesis is true. References "],["mle-ols.html", "Lecture 13 Maximum likelihood estimation and linear regression Learning objectives Supplemental readings 13.1 Maximum likelihood 13.2 Least squares regression 13.3 Assumptions of linear regression models 13.4 Unusual and influential data 13.5 Non-normally distributed errors 13.6 Non-constant error variance 13.7 Non-linearity in the data 13.8 Collinearity", " Lecture 13 Maximum likelihood estimation and linear regression Learning objectives Define maximum likelihood estimation (MLE) Review the properties of the maximum likelihood estimator Demonstrate MLE for basic estimators Define ordinary least squares (OLS) estimation Identify why OLS estimator is the best linear unbiased estimator Identify key assumptions of OLS models Evaluate methods to test for violations of assumptions Consider how to alleviate violations Supplemental readings Chapter 9 Bertsekas and Tsitsiklis (2008) Wasserman (2013) Ch 9 - Parametric Inference Ch 13 - Linear and Logistic Regression 13.1 Maximum likelihood The most common method for estimating parameters in a parametric model is the maximum likelihood method. Let \\(X_1, \\ldots, X_n\\) be IID with PDF \\(f(x; \\theta)\\). The likelihood function is defined by \\[\\Lagr_n(\\theta) = \\prod_{i=1}^n f(X_i; \\theta)\\] The log-likelihood function is defined by \\(\\lagr_n (\\theta) = \\log \\Lagr_n(\\theta)\\). The likelihood function is the joint density of the data, except we treat it as a function of the parameter \\(\\theta\\). However the likelihood function is not a density function – it is a likelihood function. In general, it is not true that \\(\\Lagr_n(\\theta)\\) integrates to 1 (with respect to \\(\\theta\\)). The maximum likelihood estimator (MLE), denoted by \\(\\hat{\\theta}_n\\), is the value of \\(\\theta\\) that maximizes \\(\\Lagr_n(\\theta)\\). The maximum of \\(\\lagr_n(\\theta)\\) occurs at the same place as the maximum of \\(\\Lagr_n(\\theta)\\), so maximizing the log-likelihood leads to the same answer as maximizing the likelihood. Often, it is just easier to work with the log-likelihood. If we multiply \\(\\Lagr_n(\\theta)\\) by any positive constant \\(c\\) (not depending on \\(\\theta\\)) then this will not change the MLE. Thus we shall drop constants in the likelihood function. Example 13.1 (Bernoulli distribution) Suppose that \\(X_1, \\ldots, X_n \\sim \\text{Bernoulli} (\\pi)\\). The probability function is \\[f(x; \\pi) = \\pi^x (1 - \\pi)^{1 - x}\\] for \\(x = 0,1\\). The unknown parameter is \\(\\pi\\). Then, \\[ \\begin{align} \\Lagr_n(\\pi) &amp;= \\prod_{i=1}^n f(X_i; \\pi) \\\\ &amp;= \\prod_{i=1}^n \\pi^{X_i} (1 - \\pi)^{1 - X_i} \\\\ &amp;= \\pi^S (1 - \\pi)^{n - S} \\end{align} \\] where \\(S = \\sum_{i} X_i\\). The log-likelihood function is therefore \\[\\lagr_n (\\pi) = S \\log(\\pi) + (n - S) \\log(1 - \\pi)\\] To analytically solve for \\(\\hat{\\pi}_n\\), take the derivative of \\(\\lagr_n (\\pi)\\), set it equal to 0, and solve for \\(\\hat{\\pi}_n = \\frac{S}{n}\\). Example 13.2 (Normal distribution) Let \\(X_1, \\ldots, X_n \\sim N(\\mu, \\sigma^2)\\). The parameter is \\(\\theta = (\\mu, \\sigma)\\) and the likelihood function (ignoring some constants) is: \\[ \\begin{align} \\Lagr_n (\\mu, \\sigma) &amp;= \\prod_i \\frac{1}{\\sigma} \\exp \\left[ - \\frac{1}{2\\sigma^2} (X_i - \\mu)^2 \\right] \\\\ &amp;= \\frac{1}{\\sigma^n} \\exp \\left[ - \\frac{1}{2\\sigma^2} \\sum_i (X_i - \\mu)^2 \\right] \\\\ &amp;= \\frac{1}{\\sigma^n} \\exp \\left[ \\frac{n S^2}{2 \\sigma^2} \\right] \\exp \\left[ - \\frac{n (\\bar{X} - \\mu)^2}{2 \\sigma^2} \\right] \\end{align} \\] where \\(\\bar{X} = \\frac{1}{n} \\sum_i X_i\\) is the sample mean and \\(S^2 = \\frac{1}{n} \\sum_i (X_i - \\bar{X})^2\\). The log-likelihood is \\[\\lagr_n (\\mu, \\sigma) = -n \\log \\sigma - \\frac{nS^2}{2\\sigma^2} - \\frac{n(\\bar{X} - \\mu)^2}{2\\sigma^2}\\] Calculating the first derivatives with respect to \\(\\mu\\) and \\(\\sigma\\), setting them equal to 0, and solving for \\(\\hat{\\mu}, \\hat{\\sigma}\\) leads to \\(\\hat{\\mu} = \\bar{X} = \\E [X]\\) and \\(\\hat{\\sigma} = S = \\sqrt{\\Var[X]}\\). So the mean and variance/standard deviation of a normal distribution are also maximum likelihood estimators. 13.1.1 Properties of maximum likelihood estimators Under certain conditions, the maximum likelihood estimator \\(\\hat{\\theta}_n\\) possesses many properties that make it an appealing choice of estimatory. The main properties are: Consistency Equivariant Asymptotically Normal Asymptotically optimal or efficient These properties generally hold true for random variables with large sample sizes and smooth conditions for \\(f(x; \\theta)\\). If these requirements are not met, then MLE may not be a good estimator for the parameter of interest. 13.1.1.1 Consistency The MLE is consistent, in that \\(\\hat{\\theta}_n \\xrightarrow{P} \\theta_*\\), where \\(\\theta_*\\) denotes the true value of the parameter \\(\\theta\\). Consistency means that the MLE converges in probability to the true value as the number of observations increases. 13.1.1.2 Equivariance Equivariance indicates that if \\(\\hat{\\theta}_n\\) is the MLE of \\(\\theta\\), then \\(g(\\hat{\\theta}_n)\\) is the MLE of \\(g(\\theta)\\). Basically, the MLE estimator for a random variable transformed by a function \\(g(x)\\) is also the MLE estimator for the new random variable \\(g(x)\\). For example, let \\(X_1, \\ldots, X_n \\sim N(\\theta,1)\\). The MLE for \\(\\theta\\) is \\(\\hat{\\theta}_n = \\bar{X}_n\\). Let \\(\\tau = e^\\theta\\). Then the MLE for \\(\\tau\\) is \\(\\hat{\\tau} = e^{\\hat{\\theta}} = e^{\\bar{X}}\\). 13.1.1.3 Asymptotic normality Asymptotic normality indicates that the distribution of the MLE estimator is asymptotically normal. That is, let \\(\\se = \\sqrt{\\Var (\\hat{\\sigma}_n)}\\). \\[\\frac{\\hat{\\theta}_n - \\theta_*}{\\se} \\leadsto N(0,1)\\] The distribution of the true standard error of \\(\\hat{\\theta}_n\\) is approximately a standard Normal distribution. Since we typically have to estimate the standard error from the data, it also holds true that \\[\\frac{\\hat{\\theta}_n - \\theta_*}{\\widehat{\\se}} \\leadsto N(0,1)\\] The proof of this property is in the book. Informally, this means that the distribution of the MLE can be approximated with \\(N(\\theta, \\widehat{\\se}^2)\\). This is what allows us to construct confidence intervals for point estimates like we saw previously. As long as the sample size is sufficiently large and \\(f(x; \\theta)\\) is sufficiently smooth, this property holds true and we do not need to estimate actual confidence intervals for the MLE – the Normal approximation is sufficient. 13.1.1.4 Optimality Suppose that \\(X_1, \\ldots, X_n \\sim N(\\theta, \\sigma^2)\\). The MLE is \\(\\hat{\\sigma}_n = \\bar{X}_n\\). Another reasonable estimator of \\(\\theta\\) is the sample median \\(\\tilde{\\theta}_n\\). The MLE satisfies \\[\\sqrt{n} (\\hat{\\theta}_n - \\theta) \\leadsto N(0, \\sigma^2)\\] It can be shown that the median satisfies \\[\\sqrt{n} (\\tilde{\\theta}_n - \\theta) \\leadsto N \\left(0, \\sigma^2 \\frac{\\pi}{2} \\right)\\] This means the median converges to the right value but has a larger variance than the MLE. More generally, consider two estimators \\(T_n\\) and \\(U_n\\), and suppose that \\[ \\begin{align} \\sqrt{n} (T_n - \\theta) &amp;\\leadsto N(0, t^2) \\\\ \\sqrt{n} (U_n - \\theta) &amp;\\leadsto N(0, u^2) \\\\ \\end{align} \\] We define the asymptotic relative efficiency of \\(U\\) to \\(T\\) by \\(\\text{ARE}(U, T) = \\frac{t^2}{u^2}\\). In the Normal example, \\(\\text{ARE}(\\tilde{\\theta}_n, \\hat{\\theta}_n) = \\frac{2}{\\pi} \\approx .63\\). The interpretation is that if you use the median, you are effectively using only a fraction of the data and your estimate is not as optimal as the mean. Fundamentally, if \\(\\hat{\\theta}_n\\) is the MLE and \\(\\tilde{\\theta}_n\\) is any other estimator, then \\[\\text{ARE} (\\tilde{\\theta}_n, \\hat{\\theta}_n) \\leq 1\\] Thus, the MLE has the smallest (asymptotic) variance and we say the MLE is efficient or asymptotically optimal. Example 13.3 (Calculating the MLE of the mean of the Normal variable) We presume the response variable \\(Y\\) is drawn from a Gaussian (normal) distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\): \\[\\Pr(X_i = x_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left[\\frac{(X_i - \\mu)^2}{2\\sigma^2}\\right]\\] This is the density, or probability density function (PDF) of the variable \\(Y\\). The probability that, for any one observation \\(i\\), \\(Y\\) will take on the particular value \\(y\\). This is a function of \\(\\mu\\), the expected value of the distribution, and \\(\\sigma^2\\), the variability of the distribution around the mean. We want to generate estimates of the parameters \\(\\hat{\\mu}_n\\) and \\(\\hat{\\sigma}_n^2\\) based on the data. For the normal distribution, the log-likelihood function is: \\[ \\begin{align} \\lagr_n(\\mu, \\sigma^2 | X) &amp;= \\log \\prod_{i = 1}^{N}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left[\\frac{(X_i - \\mu)^2}{2\\sigma^2}\\right]} \\\\ &amp;= \\sum_{i=1}^{N}{\\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left[\\frac{(X_i - \\mu)^2}{2\\sigma^2}\\right]\\right)} \\\\ &amp;= -\\frac{N}{2} \\log(2\\pi) - \\left[ \\sum_{i = 1}^{N} \\log{\\sigma^2 - \\frac{1}{2\\sigma^2}} (X_i - \\mu)^2 \\right] \\end{align} \\] Suppose we had a sample of assistant professor salaries: Table 13.1: Salaries of assistant professors id salary 1 60 2 55 3 65 4 50 5 70 If we want to explain the distribution of possible assistant professor salaries given these data points, we could use maximum-likelihood estimation to find the \\(\\hat{\\mu}\\) that maximizes the likelihood of the data. We are testing different values for \\(\\mu\\) to see what optimizes the function. If we have no regressors or predictors, \\(\\hat{\\mu}\\) is a constant. Furthermore, we treat \\(\\sigma^2\\) as a nuisance parameter and hold it constant at \\(\\sigma^2 = 1\\). The log-likelihood curve would look like this: And the maximum is 60, which is the mean of the 5 sample observations. Notice our choice of value for \\(\\sigma^2\\) doesn’t change our estimate \\(\\hat{\\mu}_n\\). 13.2 Least squares regression Regression is a method for studying the relationship between a response variable \\(Y\\) and a covariate \\(X\\) (also known as the predictor variable or a feature). One way to summarize this relationship between \\(X\\) and \\(Y\\) is through a regression function: \\[r(x) = \\E (Y | X = x) = \\int y f(y|x) dy\\] Our goal is to estimate the regression function \\(r(x)\\) from the data of the form \\[(Y_1, X_1), \\ldots, (Y_n, X_n) \\sim F_{X,Y}\\] 13.2.1 Simple linear regression The simplest form of regression is when \\(X_i\\) is simple (one-dimensional) and \\(r(x)\\) is assumed to be linear: \\[r(x) = \\beta_0 + \\beta_1 x\\] This model is called the simple linear regression model. We make the further assumption that \\(\\Var (\\epsilon_i | X = x) = \\sigma^2\\) does not depend on \\(x\\). Thus the linear regression model is: \\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\] where \\(\\E (\\epsilon_i | X_i) = 0\\) and \\(\\Var (\\epsilon_i | X_i) = \\sigma^2\\). The unknown parameters in the model are the intercept \\(\\beta_0\\) and the slope \\(\\beta_1\\) and the variance \\(\\sigma^2\\). Let \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) denote estimates of \\(\\beta_0\\) and \\(\\beta_1\\). The fitted line is \\[\\hat{r}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\] The predicted values or fitted values are \\[\\hat{Y}_i = \\hat{r}(X_i)\\] and the residuals are defined as \\[\\hat{\\epsilon}_i = Y_i - \\hat{Y}_i = Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x)\\] The residual sum of squares or RSS measures how well the line fits the data. It is defined by \\[RSS = \\sum_{i=1}^n \\hat{\\epsilon}_i^2\\] 13.2.2 Estimation strategy What is an appropriate way to estimate the \\(\\beta\\)s? We could fit many lines to this data, some better than others. We should seek estimators with some set of desired qualities. Classically, two desired qualities for an estimator are unbiasedness and efficiency. Definition 13.1 (Unbiasedness) \\(\\E(\\hat{\\beta}) = \\beta\\), or an estimator that “gets it right” vis-a-vis \\(\\beta\\). Definition 13.2 (Efficiency) \\(\\min(\\Var(\\hat{\\beta}))\\). Not only do we get it right, but for any given sample used to generate the model we never want to be too far off from “right”. 13.2.3 Least squares estimator The least squares estimates are the values \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\) that minimize the RSS. \\[\\min(RSS)\\] This requires a bit of calculus to solve. \\[ \\begin{aligned} RSS &amp;= \\sum_{i=1}^n \\hat{\\epsilon}_i^2 \\\\ \\sum_{i=1}^n (\\hat{\\epsilon}_i)^2 &amp;= \\sum_{i=1}^n (Y_i - (\\beta_0 + \\beta_1 X_i))^2\\\\ f(\\beta_0, \\beta_1 | x_i, y_i) &amp; = \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i )^2\\\\ \\dfrac{\\partial{ f(\\beta_0, \\beta_1 | x_i, y_i)}}{\\partial \\beta_0} &amp; = -2 (\\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 X_i))\\\\ &amp; = \\sum_{i=1}^n -2Y_i + 2\\beta_0 + 2\\beta_1 X_i\\\\ 0 &amp; = \\sum_{i=1}^n -2Y_{i} + 2\\beta_0 + 2\\beta_1 X_i\\\\ 0 &amp; = -2 \\sum_{i=1}^n Y_{i} + 2\\sum_{i=1}^n \\beta_0 + 2\\beta_1 \\sum_{i=1}^n X_i\\\\ 0 &amp; = -2 \\sum_{i=1}^n Y_{i} + (n \\times 2\\beta_0) + 2\\beta_1 \\sum_{i=1}^n X_i\\\\ n \\times 2\\beta_0 &amp; = 2 \\sum_{i=1}^n Y_i - 2\\beta_1 \\sum_{i=1}^n X_i\\\\ \\hat \\beta_0 &amp; = \\dfrac{2 \\sum_{i=1}^n Y_i}{2n} - \\dfrac{2\\beta_1 \\sum_{i=1}^n X_i}{2n}\\\\ &amp; = \\dfrac{\\sum_{i=1}^n Y_i}{n} - \\beta_1\\dfrac{ \\sum_{i=1}^n X_i}{n}\\\\ \\hat \\beta_0 &amp; = \\bar{Y}_n - \\beta_1 \\bar{X}_n \\end{aligned} \\] \\[ \\begin{aligned} \\dfrac{\\partial{ f(\\beta_0, \\beta_1 | x_i, y_i)}}{\\partial \\beta_1} &amp; = \\sum_{i=1}^n -2X_i(Y_i - \\beta_0 - \\beta_1 X_i) \\\\ &amp; = \\sum_{i=1}^n -2Y_iX_i + 2\\beta_0X_i + 2\\beta_1 X_i^2\\\\ 0 &amp; = \\sum_{i=1}^n -2Y_iX_i + 2\\beta_0 \\sum_{i=1}^nX_i + 2\\beta_1 \\sum_{i=1}^n X_i^2\\\\ &amp; = \\sum_{i=1}^n -2Y_iX_i + 2 (\\bar{Y}_n - \\beta_1 \\bar{X}_n) \\sum_{i=1}^nX_i + 2\\beta_1 \\sum_{i=1}^n X_i^2\\\\ &amp; = \\sum_{i=1}^n -2Y_iX_i + 2\\bar{Y}_n \\sum_{i=1}^nX_i - 2\\beta_1 \\bar{X}_n\\sum_{i=1}^nX_i + 2\\beta_1 \\sum_{i=1}^n X_i^2\\\\ 2\\beta_1 \\sum_{i=1}^n X_i^2 - 2\\beta_1 \\bar{X}_n\\sum_{i=1}^nX_i &amp; = \\sum_{i=1}^n 2Y_iX_i - 2\\bar{Y}_n \\sum_{i=1}^nX_i\\\\ \\beta_1 ( \\sum_{i=1}^n X_i^2 - \\bar{X}_n\\sum_{i=1}^nX_i ) &amp; = \\sum_{i=1}^n Y_iX_i - \\bar{Y}_n \\sum_{i=1}^nX_i\\\\ \\hat \\beta_1 &amp; = \\dfrac{ \\sum_{i=1}^n Y_iX_i - \\bar{Y}_n \\sum_{i=1}^nX_i}{ \\sum_{i=1}^n X_i^2 - \\bar{X}_n\\sum_{i=1}^nX_i}\\\\ \\hat \\beta_0 &amp; = \\bar{Y}_n - \\hat{\\beta}_1 \\bar{X}_n \\end{aligned} \\] Recall that we also need an estimate for \\(\\sigma^2\\). An unbiased estimate turns out to be \\[\\hat{\\sigma}^2 = \\left( \\frac{1}{n - 2} \\right) \\sum_{i=1}^n \\hat{\\epsilon}_i^2\\] 13.2.4 Maximum likelihood estimation Suppose we add the assumption that \\(\\epsilon_i | X_i \\sim N(0, \\sigma^2)\\), that is, \\[Y_i | X_i \\sim N(\\mu_i, \\sigma^2)\\] where \\(\\mu_i = \\beta_0 + \\beta_1 X_i\\). This means each \\(i\\)th observation has a systematic mean that varies based on the value of \\(X_i\\). The likelihood function is \\[ \\begin{align} \\prod_{i=1}^n f(X_i, Y_i) &amp;= \\prod_{i=1}^n f_X(X_i) f_{Y | X} (Y_i | X_i) \\\\ &amp;= \\prod_{i=1}^n f_X(X_i) \\times \\prod_{i=1}^n f_{Y | X} (Y_i | X_i) \\\\ &amp;= \\Lagr_1 \\times \\Lagr_2 \\end{align} \\] where \\[ \\begin{align} \\Lagr_1 &amp;= \\prod_{i=1}^n f_X(X_i) \\\\ \\Lagr_2 &amp;= \\prod_{i=1}^n f_{Y | X} (Y_i | X_i) \\end{align} \\] \\(\\Lagr_1\\) does not involve the parameters \\(\\beta_0, \\beta_1\\). Instead we can focus on the second term \\(\\Lagr_2\\) which is called the conditional likelihood, given by \\[ \\begin{align} \\Lagr_2 &amp;\\equiv \\Lagr(\\beta_0, \\beta_1, \\sigma^2) \\\\ &amp;= \\prod_{i=1}^n f_{Y | X}(Y_i | X_i) \\\\ &amp;\\propto \\frac{1}{\\sigma} \\exp \\left\\{ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Y_i - \\mu_i)^2 \\right\\} \\end{align} \\] The conditional log-likelihood is \\[\\lagr(\\beta_0, \\beta_1, \\sigma^2) = -n \\log(\\sigma) - \\frac{1}{2\\sigma^2} \\left( Y_i - (\\beta_0 + \\beta_1 X_i) \\right)^2\\] To find the MLE of \\((\\beta_0, \\beta_1)\\), we maximize \\(\\lagr(\\beta_0, \\beta_1, \\sigma^2)\\). This is equivalent to minimizing the RSS \\[RSS = \\sum_{i=1}^n \\hat{\\epsilon}_i^2 = \\sum_{i=1}^n \\left( Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x) \\right)\\] Therefore, under the assumption that the residuals are distributed normally, the least squares estimator is also the maximum likelihood estimator. 13.2.5 Properties of the least squares estimator In regression problems, we usually focus on the properties of the estimators conditional on \\(X^n = (X_1, \\ldots, X_n)\\). Thus we state the means and variances as conditional means and variances. Let \\(\\hat{\\beta}^T = (\\hat{\\beta}_0, \\hat{\\beta}_1)^T\\) denote the least squares estimators (\\(^T\\)) simply indicates the vector is transposed to be a column vector. Then \\[ \\begin{align} \\E (\\hat{\\beta} | X^n) &amp;= \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix} \\\\ \\Var (\\hat{\\beta} | X^n) &amp;= \\frac{\\sigma^2}{n s_X^2} \\begin{pmatrix} \\frac{1}{n} \\sum_{i=1}^n X_i^2 &amp; -\\bar{X}^n \\\\ -\\bar{X}^n &amp; 1 \\end{pmatrix} \\end{align} \\] where \\[s_X^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X}_n)^2\\] The estimated standard errors of \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\) are obtained by taking the square roots of the corresponding diagonal terms of \\(\\Var (\\hat{\\beta} | X^n)\\) and inserting the estimate \\(\\hat{\\sigma}\\) for \\(\\sigma\\). Thus, \\[ \\begin{align} \\widehat{\\se} (\\hat{\\beta}_0) &amp;= \\frac{\\hat{\\sigma}}{s_X \\sqrt{n}} \\sqrt{\\frac{ \\sum_{i=1}^n X_i^2}{n}} \\\\ \\widehat{\\se} (\\hat{\\beta}_1) &amp;= \\frac{\\hat{\\sigma}}{s_X \\sqrt{n}} \\end{align} \\] Under appropriate conditions, these estimators meet the criteria for maximum likelihood estimators. 13.3 Assumptions of linear regression models Basic linear regression follows the functional form: \\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\] where \\(Y_i\\) is the value of the response variable \\(Y\\) for the \\(i\\)th observation, \\(X_i\\) is the value for the explanatory variable \\(X\\) for the \\(i\\)th observation. The coefficients \\(\\beta_0\\) and \\(\\beta_1\\) are population regression coefficients - our goal is to estimate these population parameters given the observed data. \\(\\epsilon_i\\) is the error representing the aggregated omitted causes of \\(Y\\), other explanatory variables that could be included in the model, measurement error in \\(Y\\), and any inherently random component of \\(Y\\). The key assumptions of linear regression concern the behavior of the errors. 13.3.1 Linearity The expectation of the error is 0: \\[\\E(\\epsilon_i) \\equiv E(\\epsilon_i | X_i) = 0\\] This allows us to recover the expected value of the response variable as a linear function of the explanatory variable: \\[ \\begin{aligned} \\mu_i \\equiv E(Y_i) \\equiv E(Y | X_i) &amp;= E(\\beta_0 + \\beta_1 X_i + \\epsilon_i) \\\\ \\mu_i &amp;= \\beta_0 + \\beta_1 X_i + E(\\epsilon_i) \\\\ \\mu_i &amp;= \\beta_0 + \\beta_1 X_i + 0 \\\\ \\mu_i &amp;= \\beta_0 + \\beta_1 X_i \\end{aligned} \\] Because \\(\\beta_0\\) and \\(\\beta_1\\) are fixed parameters in the population, we can remove them from the expectation operator. 13.3.2 Constant variance The variance of the errors is the same regardless of the values of \\(X\\): \\[\\Var(\\epsilon_i | X_i) = \\sigma^2\\] 13.3.3 Normality The errors are assumed to be normally distributed: \\[\\epsilon_i \\mid X_i \\sim N(0, \\sigma^2)\\] 13.3.4 Independence Observations are sampled independently from one another. Any pair of errors \\(\\epsilon_i\\) and \\(\\epsilon_j\\) are independent for \\(i \\neq j\\). Simple random sampling from a large population will ensure this assumption is met. However data collection procedures frequently (and explicitly) violate this assumption (e.g. time series data, panel survey data). 13.3.5 Fixed \\(X\\), or \\(X\\) measured without error and independent of the error \\(X\\) is assumed to be fixed or measured without error and independent of the error. With a fixed \\(X\\), the researcher controls the precise value of \\(X\\) for a given observation (think experimental design with treatment/control). In observational study, we assume \\(X\\) is measured without error and that the explanatory variable and the error are independent in the population from which the sample is drawn. \\[\\epsilon_i \\sim N(0, \\sigma^2), \\text{for } i = 1, \\dots, n\\] 13.3.6 \\(X\\) is not invariant If \\(X\\) is fixed, it must vary (i.e. it’s values cannot all be the same). If \\(X\\) is random, then in the population \\(X\\) must vary. You cannot estimate a regression line for an invariant \\(X\\). 13.3.7 Handling violations of assumptions If these assumptions are violated, conducting inference from linear regression becomes tricky, biased, inefficient, and/or error prone. You could move to a more robust inferential method such as nonparametric regression, decision trees, support vector machines, etc., but these methods are more tricky to generate inference about the explanatory variables. Instead, we can attempt to diagnose assumption violations and impose solutions while still constraining ourselves to a linear regression framework. 13.4 Unusual and influential data Outliers are observations that are somehow unusual, either in their value of \\(Y_i\\), of one or more \\(X_i\\)s, or some combination thereof. Outliers have the potential to have a disproportionate influence on a regression model. 13.4.1 Terms Outlier - an observation that has an unusual value on the dependent variable \\(Y\\) given its particular combination of values on \\(X\\) Leverage - degree of potential influence on the coefficient estimates that a given observation can (but not necessarily does) have Discrepancy - extent to which an observation is “unusual” or “different” from the rest of the data Influence - how much effect a particular observation’s value(s) on \\(Y\\) and \\(X\\) have on the coefficient estimates. Influence is a function of leverage and discrepancy: \\[\\text{Influence} = \\text{Leverage} \\times \\text{Discrepancy}\\] Dino is an observation with high leverage but low discrepancy (close to the regression line defined by Betty, Fred, and Wilma). Therefore he has little impact on the regression line (long dashed line); his influence is low because his discrepancy is low. Barney has high leverage (though lower than Dino) and high discrepancy, so he substantially influences the regression results (short-dashed line). 13.4.2 Measuring leverage Leverage is typically assessed using the leverage (hat) statistic: \\[h_i = \\frac{1}{n} + \\frac{(X_i - \\bar{X})^2}{\\sum_{j=1}^{n} (X_{j} - \\bar{X})^2}\\] Measures the contribution of observation \\(Y_i\\) to the fitted value \\(\\hat{Y}_j\\) (the other values in the dataset) It is solely a function of \\(X\\) Larger values indicate higher leverage \\(\\frac{1}{n} \\leq h_i \\leq 1\\) \\(\\bar{h} = \\frac{(p + 1)}{n}\\) Observations with a leverage statistic greater than the average could have high leverage. 13.4.3 Measuring discrepancy Residuals are a natural way to look for discrepant or outlying observations (discrepant observations typically have large residuals, or differences between actual and fitted values for \\(y_i\\).) The problem is that variability of the errors \\(\\hat{\\epsilon}_i\\) do not have equal variances, even if the actual errors \\(\\epsilon_i\\) do have equal variances: \\[\\Var(\\hat{\\epsilon}_i) = \\sigma^2 (1 - h_i)\\] High leverage observations tend to have small residuals, which makes sense because they pull the regression line towards them. Alternatively we can calculate a standardized residual which parses out the variability in \\(X_i\\) for \\(\\hat{\\epsilon}_i\\): \\[\\hat{\\epsilon}_i &#39; \\equiv \\frac{\\hat{\\epsilon}_i}{S_{E} \\sqrt{1 - h_i}}\\] where \\(S_E\\) is the standard error of the regression: \\[S_E = \\sqrt{\\frac{\\hat{\\epsilon}_i^2}{(n - k - 1)}}\\] The problem is that the numerator and the denominator are not independent - they both contain \\(\\hat{\\epsilon}_i\\), so \\(\\hat{\\epsilon}_i &#39;\\) does not follow a \\(t\\)-distribution. Instead, we can modify this measure by calculating \\(S_{E(-i)}\\); that is, refit the model deleting each \\(i\\)th observation, estimating the standard error of the regression \\(S_{E(-i)}\\) based on the remaining \\(i-1\\) observations. We then calculate the studentized residual: \\[\\hat{\\epsilon}_i^{\\ast} \\equiv \\frac{\\hat{\\epsilon}_i}{S_{E(-i)} \\sqrt{1 - h_i}}\\] which now has an independent numerator and denominator and follows a \\(t\\)-distribution with \\(n-k-2\\) degrees of freedom. They are on a common scale and we should expect roughly 95% of the studentized residuals to fall within the interval \\([-2,2]\\). 13.4.4 Measuring influence As described previously, influence is the a combination of an observation’s leverage and discrepancy. In other words, influence is the effect of a particular observation on the coefficient estimates. A simple measure of that influence is the difference between the coefficient estimate with and without the observation in question: \\[D_{ij} = \\hat{\\beta_1j} - \\hat{\\beta}_{1j(-i)}, \\text{for } i=1, \\dots, n \\text{ and } j = 0, \\dots, k\\] This measure is called \\(\\text{DFBETA}_{ij}\\). Since coefficient estimates are scaled differently depending on how the variables are scaled, we can rescale \\(\\text{DFBETA}_{ij}\\) by the coefficient’s standard error to account for this fact: \\[D^{\\ast}_{ij} = \\frac{D_{ij}}{SE_{-i}(\\beta_{1j})}\\] This measure is called \\(\\text{DFBETAS}_{ij}\\). Positive values of \\(\\text{DFBETAS}_{ij}\\) correspond to observations which decrease the estimate of \\(\\hat{\\beta}_{1j}\\) Negative values of \\(\\text{DFBETAS}_{ij}\\) correspond to observations which increase the estimate of \\(\\hat{\\beta}_{1j}\\) Frequently \\(\\text{DFBETA}\\)s are used to construct summary statistics of each observation’s influence on the regression model. Cook’s D is based on the theory that one could conduct an \\(F\\)-test on each observation for the hypothesis that \\(\\beta_{1j} = \\hat{\\beta}_{1k(-i)} \\forall j \\in J\\). The formula for this measure is: \\[D_i = \\frac{\\hat{\\epsilon}^{&#39;2}_i}{k + 1} \\times \\frac{h_i}{1 - h_i}\\] where \\(\\hat{\\epsilon}^{&#39;2}_i\\) is the squared standardized residual, \\(k\\) is the number of parameters in the model, and \\(\\frac{h_i}{1 - h_i}\\) is the hat value. We look for values of \\(D_i\\) that stand out from the rest. 13.4.5 Visualizing leverage, discrepancy, and influence For example, here are the results of a basic model of the number of federal laws struck down by the U.S. Supreme Court in each Congress, based on: Age - the mean age of the members of the Supreme Court Tenure - mean tenure of the members of the Court Unified - a dummy variable indicating whether or not the Congress was controlled by the same party in that period ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -12.1 2.54 -4.76 0.00000657 ## 2 age 0.219 0.0448 4.88 0.00000401 ## 3 tenure -0.0669 0.0643 -1.04 0.300 ## 4 unified 0.718 0.458 1.57 0.121 A major concern with regression analysis of this data is that the results are being driven by outliers in the data. During the 74th Congress (1935-36), the New Deal/Court-packing crisis was associated with an abnormally large number of laws struck down by the court. We should determine whether or not this observation is driving our results. By combining all three variables into a “bubble plot”, we can visualize all three variables simultaneously. Each observation’s leverage (\\(h_i\\)) is plotted on the \\(x\\) axis Each observation’s discrepancy (i.e. Studentized residual) is plotted on the \\(y\\) axis Each symbol is drawn proportional to the observation’s Cook’s \\(D_i\\) The bubble plot tells us several things: The size/color of the symbols is proportional to Cook’s D, which is in turn a multiplicative function of the square of the Studentized residuals (Y axis) and the leverage (X axis), so observations farther away from \\(Y=0\\) and/or have higher values of \\(X\\) will have larger symbols. The plot tells us whether the large influence of an observation is due to high discrepancy, high leverage, or both The 104th Congress has relatively low leverage but is very discrepant The 74th and 98th Congresses demonstrate both high discrepancy and high leverage 13.4.6 Numerical rules of thumb These are not hard and fast rules rigorously defended by mathematical proofs; they are simply potential rules of thumb to follow when interpreting the above statistics. 13.4.6.1 Hat-values Anything exceeding twice the average \\(\\bar{h} = \\frac{k + 1}{n}\\) is noteworthy. In our example that would be the following observations: ## # A tibble: 9 × 10 ## Congress congress nulls age tenure unified year hat student cooksd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1st 1 0 49.8 0.800 1 1789 0.0974 0.330 0.00296 ## 2 3rd 3 0 52.8 4.20 0 1793 0.113 0.511 0.00841 ## 3 12th 12 0 49 6.60 1 1811 0.0802 0.669 0.00980 ## 4 17th 17 0 59 16.6 1 1821 0.0887 -0.253 0.00157 ## 5 20th 20 0 61.7 17.4 1 1827 0.0790 -0.577 0.00719 ## 6 23rd 23 0 64 18.4 1 1833 0.0819 -0.844 0.0159 ## 7 34th 34 0 64 14.6 0 1855 0.0782 -0.561 0.00671 ## 8 36th 36 0 68.7 17.8 0 1859 0.102 -1.07 0.0326 ## 9 99th 99 3 71.9 16.7 0 1985 0.0912 0.295 0.00221 13.4.6.2 Studentized residuals Anything outside of the range \\([-2,2]\\) is discrepant. ## # A tibble: 7 × 10 ## Congress congress nulls age tenure unified year hat student cooksd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 67th 67 6 66 9 1 1921 0.0361 2.14 0.0415 ## 2 74th 74 10 71.1 14.2 1 1935 0.0514 4.42 0.223 ## 3 90th 90 6 64.7 13.3 1 1967 0.0195 2.49 0.0292 ## 4 91st 91 6 65.1 13 1 1969 0.0189 2.42 0.0269 ## 5 92nd 92 5 62 9.20 1 1971 0.0146 2.05 0.0150 ## 6 98th 98 7 69.9 14.7 0 1983 0.0730 3.02 0.165 ## 7 104th 104 8 60.6 12.5 1 1995 0.0208 4.48 0.0897 13.4.6.3 Influence \\[D_i &gt; \\frac{4}{n - k - 1}\\] where \\(n\\) is the number of observations and \\(k\\) is the number of coefficients in the regression model. ## # A tibble: 4 × 10 ## Congress congress nulls age tenure unified year hat student cooksd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 67th 67 6 66 9 1 1921 0.0361 2.14 0.0415 ## 2 74th 74 10 71.1 14.2 1 1935 0.0514 4.42 0.223 ## 3 98th 98 7 69.9 14.7 0 1983 0.0730 3.02 0.165 ## 4 104th 104 8 60.6 12.5 1 1995 0.0208 4.48 0.0897 13.4.7 How to treat unusual observations 13.4.7.1 Mistakes If the data is just wrong (miscoded, mismeasured, misentered, etc.), then either fix the error, impute a plausible value for the observation, or omit the offending observation. 13.4.7.2 Weird observations If the data for a particular observation is just strange, then you may want to ask “why is it so strange?” The data are strange because something unusual/weird/singular happened to that data point If that “something” is important to the theory being tested, then you may want to respecify your model If the answer is no, then you can drop the offending observation from the analysis The data are strange for no apparent reason Not really a good answer here. Try digging into the history of the observation to find out what is going on. Dropping the observation is a judgment call You could always rerun the model omitting the observation and including the results as a footnote (i.e. a robustness check) For example, let’s re-estimate the SCOTUS model and omit observations that were commonly identified as outliers:23 ## [1] 0.232 ## [1] 0.258 ## [1] 1.68 ## [1] 1.29 Not much has changed from the original model Estimate for age is a bit smaller, as well as a smaller standard error Tenure is also smaller, but only fractionally Unified is a bit larger and with a smaller standard error \\(R^2\\) is larger for the omitted observation model, and the RMSE is smaller These three observations mostly influenced the precision of the estimates (i.e. standard errors), not the accuracy of them 13.5 Non-normally distributed errors Recall that OLS assumes errors are distributed normally: \\[\\epsilon_i | X_i \\sim N(0, \\sigma^2)\\] However according to the central limit theorem, inference based on the least-squares estimator is approximately valid under broad conditions.24 So while the validity of the estimates is robust to violating this assumption, the efficiency of the estimates is not robust. Recall that efficiency guarantees us the smallest possible sampling variance and therefore the smallest possible mean squared error (MSE). Heavy-tailed or skewed distributions of the errors will therefore give rise to outliers (which we just recognized as a problem). Alternatively, we interpret the least-squares fit as a conditional mean \\(Y | X\\). But arithmetic means are not good measures of the center of a highly skewed distribution. 13.5.1 Detecting non-normally distributed errors Graphical interpretations are easiest to detect non-normality in the errors. Consider a regression model using survey data from the 1994 wave of Statistics Canada’s Survey of Labour and Income Dynamics (SLID), explaining hourly wages as an outcome of sex, education, and age: ## # A tibble: 3,997 × 4 ## age sex compositeHourlyWages yearsEducation ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 40 Male 10.6 15 ## 2 19 Male 11 13 ## 3 46 Male 17.8 14 ## 4 50 Female 14 16 ## 5 31 Male 8.2 15 ## 6 30 Female 17.0 13 ## 7 61 Female 6.7 12 ## 8 46 Female 14 14 ## 9 43 Male 19.2 18 ## 10 17 Male 7.25 11 ## # … with 3,987 more rows ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -8.12 0.599 -13.6 5.27e- 41 ## 2 sexMale 3.47 0.207 16.8 4.04e- 61 ## 3 yearsEducation 0.930 0.0343 27.1 5.47e-149 ## 4 age 0.261 0.00866 30.2 3.42e-180 ## [1] 967 3911 The above figure is a quantile-comparison plot, graphing for each observation its studentized residual on the \\(y\\) axis and the corresponding quantile in the \\(t\\)-distribution on the \\(x\\) axis. The dashed lines indicate 95% confidence intervals calculated under the assumption that the errors are normally distributed. If any observations fall outside this range, this is an indication that the assumption has been violated. Clearly, here that is the case. From the density plot of the studentized residuals, we can also see that the residuals are positively skewed. 13.5.2 Fixing non-normally distributed errors Power and log transformations are typically used to correct this problem. Here, trial and error reveals that by log transforming the wage variable, the distribution of the residuals becomes much more symmetric: ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 1.10 0.0380 28.9 1.97e-167 ## 2 sexMale 0.224 0.0131 17.1 2.16e- 63 ## 3 yearsEducation 0.0559 0.00217 25.7 2.95e-135 ## 4 age 0.0182 0.000549 33.1 4.50e-212 ## [1] 2760 3267 13.6 Non-constant error variance Recall that linear regression assumes the error terms \\(\\epsilon_i\\) have a constant variance, \\(\\text{Var}(\\epsilon_i) = \\sigma^2\\). This is called homoscedasticity. Remember that the standard errors directly rely upon the estimate of this value: \\[\\widehat{\\se}(\\hat{\\beta}_{1j}) = \\sqrt{\\hat{\\sigma}^{2} (X&#39;X)^{-1}_{jj}}\\] If the variances of the error terms are non-constant (aka heteroscedastic), our estimates of the parameters \\(\\hat{\\beta}_1\\) will still be unbiased because they do not depend on \\(\\sigma^2\\). However our estimates of the standard errors will be inaccurate - they will either be inflated or deflated, leading to incorrect inferences about the statistical significance of predictor variables. 13.6.1 Detecting heteroscedasticity 13.6.1.1 Graphically We can uncover homo- or heteroscedasticity through the use of the residual plot. Below is data generated from the process: \\[Y_i = 2 + 3X_i + \\epsilon\\] where \\(\\epsilon_i\\) is random error distributed normally \\(N(0,1)\\). Compare this to a linear model fit to the data generating process: \\[Y_i = 2 + 3X_i + \\epsilon_i\\] where \\(\\epsilon_i\\) is random error distributed normally \\(N(0,\\frac{X}{2})\\). Note that the variance for the error term of each observation \\(\\epsilon_i\\) is not constant, and is itself a function of \\(X\\). We see a distinct funnel-shape to the relationship between the predicted values and the residuals. This is because by assuming the variance is constant, we substantially over or underestimate the actual response \\(Y_i\\) as \\(X_i\\) increases. 13.6.1.2 Statistical tests There are formal statistical tests to check for heteroscedasticity. One such test is the Breusch-Pagan test. The procedure is: Estimate an OLS model and obtain the squared residuals \\(\\hat{\\epsilon}^2\\) Regress \\(\\hat{\\epsilon}^2\\) against: All the \\(k\\) variables you think might be causing the heteroscedasticity By default, include the same explanatory variables as the original model Calculate the coefficient of determination (\\(R^2_{\\hat{\\epsilon}^2}\\)) for the residual model and multiply it by the number of observations \\(n\\) The resulting statistic follows a \\(\\chi^2_{(k-1)}\\) distribution Rejecting the null hypothesis indicates heteroscedasticity is present The lmtest library contains a function for the Breusch-Pagan test: ## ## studentized Breusch-Pagan test ## ## data: sim_homo_mod ## BP = 1e-04, df = 1, p-value = 1 ## ## studentized Breusch-Pagan test ## ## data: sim_hetero_mod ## BP = 168, df = 1, p-value &lt;2e-16 13.6.2 Accounting for heteroscedasticity 13.6.2.1 Weighted least squares regression Instead of assuming the errors have a constant variance \\(\\text{Var}(\\epsilon_i) = \\sigma^2\\), instead we can assume that the errors are independent and normally distributed with mean zero and different variances \\(\\epsilon_i \\sim N(0, \\sigma_i^2)\\): \\[ \\begin{bmatrix} \\sigma_1^2 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\ddots &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma_n^2 \\\\ \\end{bmatrix} \\] We can define the reciprocal of each variance \\(\\sigma_i^2\\) as the weight \\(w_i = \\frac{1}{\\sigma_i^2}\\), then let matrix \\(\\mathbf{W}\\) be a diagonal matrix containing these weights: \\[ \\mathbf{W} = \\begin{bmatrix} \\frac{1}{\\sigma_1^2} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\frac{1}{\\sigma_2^2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\ddots &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\frac{1}{\\sigma_n^2} \\\\ \\end{bmatrix} \\] So rather than following the traditional linear regression estimator \\[\\hat{\\mathbf{\\beta_1}} = (\\mathbf{X}&#39;\\mathbf{X})^{-1} \\mathbf{X}&#39;\\mathbf{y}\\] we can substitute in the weighting matrix \\(\\mathbf{W}\\): \\[\\hat{\\mathbf{\\beta_1}} = (\\mathbf{X}&#39; \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}&#39; \\mathbf{W} \\mathbf{y}\\] \\[\\sigma_{i}^2 = \\frac{\\sum(w_i \\hat{\\epsilon}_i^2)}{n}\\] This is equivalent to minimizing the weighted sum of squares, according greater weight to observations with smaller variance. How do we estimate the weights \\(W_i\\)? Use the residuals from a preliminary OLS regression to obtain estimates of the error variance within different subsets of observations. Model the weights as a function of observable variables in the model. For example, using the first approach on our original SLID model: ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -8.12 0.599 -13.6 5.27e- 41 ## 2 sexMale 3.47 0.207 16.8 4.04e- 61 ## 3 yearsEducation 0.930 0.0343 27.1 5.47e-149 ## 4 age 0.261 0.00866 30.2 3.42e-180 ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -8.10 0.0160 -506. 0 ## 2 sexMale 3.47 0.00977 356. 0 ## 3 yearsEducation 0.927 0.00142 653. 0 ## 4 age 0.261 0.000170 1534. 0 We see some mild changes in the estimated parameters, but drastic reductions in the standard errors. The problem is that this reduction is potentially biased through the estimated covariance matrix because the sampling error in the estimates should reflect the additional source of uncertainty, which is not explicitly accounted for just basing it on the original residuals. Instead, it would be better to model the weights as a function of relevant explanatory variables.25 13.6.2.2 Corrections for the variance-covariance estimates Alternatively, we could instead attempt to correct for heteroscedasticity only in the standard error estimates. This produces the same estimated parameters, but adjusts the standard errors to account for the violation of the constant error variance assumption (we won’t falsely believe our estimates are more precise than they really are.) One major estimation procedure are Huber-White standard errors (also called robust standard errors) which can be recovered using the car::hccm() function:26 ## # A tibble: 4 × 6 ## term estimate std.error statistic p.value std.error.rob ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -8.12 0.599 -13.6 5.27e- 41 0.636 ## 2 sexMale 3.47 0.207 16.8 4.04e- 61 0.207 ## 3 yearsEducation 0.930 0.0343 27.1 5.47e-149 0.0385 ## 4 age 0.261 0.00866 30.2 3.42e-180 0.00881 Notice that these new standard errors are a bit larger than the original model, accounting for the increased uncertainty of our parameter estimates due to heteroscedasticity. 13.7 Non-linearity in the data By assuming the average error \\(\\E (\\epsilon_i)\\) is 0 everywhere implies that the regression line (surface) accurately reflects the relationship between \\(X\\) and \\(Y\\). Violating this assumption means that the model fails to capture the systematic relationship between the response and explanatory variables. Therefore here, the term nonlinearity could mean a couple different things: The relationship between \\(X_1\\) and \\(Y\\) is nonlinear - that is, it is not constant and monotonic The relationship between \\(X_1\\) and \\(Y\\) is conditional on \\(X_2\\) - that is, the relationship is interactive rather than purely additive Detecting nonlinearity can be tricky in higher-dimensional regression models with multiple explanatory variables. 13.7.1 Partial residual plots Define the partial residual for the \\(j\\)th explanatory variable: \\[\\hat{\\epsilon}_i^{(j)} = \\hat{\\epsilon}_i + \\hat{\\beta}_j X_{ij}\\] In essence, calculate the least-squares residual (\\(\\hat{\\epsilon}_i\\)) and add to it the linear component of the partial relationship between \\(Y\\) and \\(X_j\\). Finally, we can plot \\(X_j\\) versus \\(\\hat{\\epsilon}_i^{(j)}\\) and assess the relationship. For instance, consider the results of the logged wage model from earlier: The solid lines are generalized additive models (GAMs), while the dashed lines are linear least-squares fits. For age, the partial relationship with logged wages is not linear - some transformation of age is necessary to correct this. For education, the relationship is more approximately linear except for the discrepancy for individual with very low education levels. We can correct this by adding a squared polynomial term for age, and square the education term. The resulting regression model is: \\[\\log(\\text{Wage}) = \\beta_0 + \\beta_1(\\text{Male}) + \\beta_2 \\text{Age} + \\beta_3 \\text{Age}^2 + \\beta_4 \\text{Education}^2\\] ## # A tibble: 5 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.397 0.0578 6.87 7.62e- 12 ## 2 sexMale 0.221 0.0124 17.8 3.21e- 68 ## 3 I(yearsEducation^2) 0.00181 0.0000786 23.0 1.19e-109 ## 4 age 0.0830 0.00319 26.0 2.93e-138 ## 5 I(age^2) -0.000852 0.0000410 -20.8 3.85e- 91 Because the model is now nonlinear in both age and education, we need to rethink how to draw the partial residuals plot. The easiest approach is to plot the partial residuals for both age and education against the original explanatory variable. For age, that is \\[\\hat{\\epsilon}_i^{\\text{Age}} = 0.083 \\times \\text{Age}_i -0.0008524 \\times \\text{Age}^2_i + \\hat{\\epsilon}_i\\] and for education, \\[\\hat{\\epsilon}_i^{\\text{Education}} = 0.002 \\times \\text{Education}^2_i + \\hat{\\epsilon}_i\\] On the same graph, we also plot the partial fits for the two explanatory variables: \\[\\hat{Y}_i^{(\\text{Age})} = 0.083 \\times \\text{Age}_i -0.0008524 \\times \\text{Age}^2_i\\] and for education, \\[\\hat{Y}_i^{(\\text{Education})} = 0.002 \\times \\text{Education}^2_i\\] On the graphs, the solid lines represent the partial fits and the dashed lines represent the partial residuals. If the two lines overlap significantly, then the revised model does a good job accounting for the nonlinearity. 13.8 Collinearity Collinearity (or multicollinearity) is a state of a model where explanatory variables are correlated with one another. 13.8.1 Perfect collinearity Perfect collinearity is incredibly rare, and typically involves using transformed versions of a variable in the model along with the original variable. For example, let’s estimate a regression model explaining mpg as a function of displ, wt, and cyl: ## ## Call: ## lm(formula = mpg ~ disp + wt + cyl, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.403 -1.403 -0.495 1.339 6.072 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 41.10768 2.84243 14.46 1.6e-14 *** ## disp 0.00747 0.01184 0.63 0.5332 ## wt -3.63568 1.04014 -3.50 0.0016 ** ## cyl -1.78494 0.60711 -2.94 0.0065 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.59 on 28 degrees of freedom ## Multiple R-squared: 0.833, Adjusted R-squared: 0.815 ## F-statistic: 46.4 on 3 and 28 DF, p-value: 5.4e-11 Now let’s say we want to recode displ so it is centered around it’s mean and re-estimate the model: ## ## Call: ## lm(formula = mpg ~ disp + wt + cyl + disp_mean, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.403 -1.403 -0.495 1.339 6.072 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 41.10768 2.84243 14.46 1.6e-14 *** ## disp 0.00747 0.01184 0.63 0.5332 ## wt -3.63568 1.04014 -3.50 0.0016 ** ## cyl -1.78494 0.60711 -2.94 0.0065 ** ## disp_mean NA NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.59 on 28 degrees of freedom ## Multiple R-squared: 0.833, Adjusted R-squared: 0.815 ## F-statistic: 46.4 on 3 and 28 DF, p-value: 5.4e-11 Oops. What’s the problem? disp and disp_mean are perfectly correlated with each other: Because they perfectly explain each other, we cannot estimate a linear regression model that contains both variables.27 Fortunately R automatically drops the second variable so it can estimate the model. Because of this, perfect multicollinearity is rarely problematic in social science. 13.8.2 Less-than-perfect collinearity Instead consider the credit dataset: Age and limit are not strongly correlated with one another, so estimating a linear regression model to predict an individual’s balance as a function of age and limit is not a problem: ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -173. 43.8 -3.96 9.01e- 5 ## 2 age -2.29 0.672 -3.41 7.23e- 4 ## 3 limit 0.173 0.00503 34.5 1.63e-121 But what about using an individual’s credit card rating instead of age? It is likely a good predictor of balance as well: ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -378. 45.3 -8.34 1.21e-15 ## 2 limit 0.0245 0.0638 0.384 7.01e- 1 ## 3 rating 2.20 0.952 2.31 2.13e- 2 By replacing age with rating, we developed a problem in our model. The problem is that limit and rating are strongly correlated with one another: In the regression model, it is difficult to parse out the independent effects of limit and rating on balance, because limit and rating tend to increase and decrease in association with one another. Because the accuracy of our estimates of the parameters is reduced, the standard errors increase. This is why you can see above that the standard error for limit is much larger in the second model compared to the first model. 13.8.2.1 Detecting collinearity 13.8.2.1.1 Scatterplot matrix A correlation or scatterplot matrix would help to reveal any strongly correlated variables: Here it is very clear that limit and rating are strongly correlated with one another. 13.8.2.1.2 Variance inflation factor (VIF) Unfortunately correlation matrices may not be sufficient to detect collinearity if the correlation exists between three or more variables (aka multicollinearity) while not existing between any two pairs of these variables. Instead, we can calculate the variance inflation factor (VIF) which is the ratio of the variance of \\(\\hat{\\beta}_{1j}\\) when fitting the full model divided by the variance of \\(\\hat{\\beta}_{1j}\\) if fit on its own model. We can use the car::vif() function in R to calculate this statistic for each coefficient. A good rule of thumb is that a VIF statistic greater than 10 indicates potential multicollinearity in the model. Applied to the credit regression models above: ## age limit ## 1.01 1.01 ## limit rating ## 160 160 13.8.3 Fixing multicollinearity 13.8.3.1 What not to do Drop one or more of the collinear variables from the model This is not a good idea, even if it makes your results “significant”. By omitting the variable, you are completely re-specifying your model in direct contradiction to your theory. If your theory suggests that a variable can be dropped, go ahead. But if not, then don’t do it. 13.8.3.2 What you could do instead 13.8.3.2.1 Add data The more observations, the better. It could at least decrease your standard errors and give you more precise estimates. And if you add “odd” or unusual observations, it could also reduce the degree of multicollinearity. 13.8.3.2.2 Transform the covariates If the variables are indicators of the same underlying concept, you can combine them into an index variable. This could be an additive index where you sum up comparable covariates or binary indicators. Alternatively, you could create an index via principal components analysis. 13.8.3.2.3 Shrinkage methods Shrinkage methods involve fitting a model involving all \\(p\\) predictors and shrinking the estimated coefficients towards zero. This shrinkage reduces variance in the model. When multicollinearity is high, the variance of the estimator \\(\\hat{\\beta}_1\\) is also high. By shrinking the estimated coefficient towards zero, we may increase bias in exchange for smaller variance in our estimates. References "],["bayesian-inference.html", "Lecture 14 Bayesian inference Learning objectives Supplemental readings 14.1 Bayesian philosophy 14.2 Bayes’ theorem 14.3 Bayesian method 14.4 Updating your prior beliefs 14.5 Simulation 14.6 Priors 14.7 Multiparameter problems 14.8 Critiques and defenses of Bayesian inference Acknowledgements", " Lecture 14 Bayesian inference Learning objectives Define the Bayesian philosophy and distinguish from frequentist inference Define core concepts for Bayesian methods Discuss the importance of simulation to estimate density functions Assess methods for defining priors Identify the strengths and weaknesses of Bayesian inference Supplemental readings Chapter 8.1 Bertsekas and Tsitsiklis (2008) Wasserman (2013) Ch 11 - Bayesian Inference 14.1 Bayesian philosophy Frequentist methods are the major methods we’ve employed thus far. The frequentist point of view is based on the following postulates: Probability refers to limiting relative frequencies. Probabilities are objective properties of the real world. Parameters are fixed, unknown constants. Because they are not fluctuating, no useful probability statements can be made about parameters. Statistical procedures should be designed to have well-defined long run frequency properties. For example, a 95% confidence interval should trap the true value of the parameter with limiting frequency at least 95 percent. An alternative approach to inference is called Bayesian inference. The Bayesian approach is based on the following postulates: Probability describes degree of belief, not limiting frequency. As such, we can make probability statements about lots of things, not just data which are subject to random variables. For example, I might say “the probability that Donald Trump offended someone on November 25, 2018” is \\(0.99\\). This does not refer to any limiting frequency. It reflects my strength of belief that the proposition is true. We can make probability statements about parameters, even though they are fixed constants. We make inferences about a parameter \\(\\theta\\) by producing a probability distribution for \\(\\theta\\). Inferences, such as point estimates and interval estimates, may then be extracted from this distribution. 14.2 Bayes’ theorem Bayes’ theorem is a fundamental component of both probability and statistics and is central to understanding the differences between frequentist and Bayesian inference. For two events \\(A\\) and \\(B\\), Bayes’ theorem states that: \\[\\Pr(B|A) = \\frac{\\Pr(A|B) \\times \\Pr(B)}{\\Pr(A)}\\] Bayes’ rule tells us how to invert conditional probabilities. That is, to find \\(\\Pr(B|A)\\) from \\(\\Pr(A|B)\\). Example 14.1 (Coin tossing) Toss a coin 5 times. Let \\(H_1 =\\) “first toss is heads” and let \\(H_A =\\) “all 5 tosses are heads”. Therefore \\(\\Pr(H_1 | H_A) = 1\\) (if all five tosses are heads, then the first one must by definition also be heads) and \\(\\Pr(H_A | H_1) = \\frac{1}{16}\\) (\\(\\frac{1}{2^4} = \\frac{1}{16}\\)). However we can also use Bayes’ theorem to calculate \\(\\Pr(H_1 | H_A)\\) using \\(\\Pr(H_A | H_1)\\). The terms we need are: \\(\\Pr(H_A | H_1) = \\frac{1}{16}\\) \\(\\Pr(H_1) = \\frac{1}{2}\\) \\(\\Pr(H_A) = \\frac{1}{32}\\) So, \\[\\Pr(H_A | H_1) = \\frac{\\Pr(H_A | H_1) \\times \\Pr(H_1)}{\\Pr(H_A)} = \\frac{\\frac{1}{16} \\times \\frac{1}{2}}{\\frac{1}{32}} = 1\\] Example 14.2 (False positive fallacy) A test for a certain rare disease is assumed to be correct 95% of the time: If a person has the disease, then the test results are positive with probability \\(0.95\\) If the person does not have the disease, then the test results are negative with probability \\(0.95\\) A random person drawn from a certain population has probability \\(0.001\\) of having the disease. Given that the person just tested positive, what is the probability of having the disease? \\(A = {\\text{person has the disease}}\\) \\(B = {\\text{test result is positive for the disease}}\\) \\(\\Pr(A) = 0.001\\) \\(\\Pr(B | A) = 0.95\\) \\(\\Pr(B | A = 0) = 0.05\\) \\[ \\begin{align} \\Pr(\\text{person has the disease} | \\text{test is positive}) &amp;= \\Pr(A|B) \\\\ &amp; = \\frac{\\Pr(A) \\times \\Pr(B|A)}{\\Pr(B)} \\\\ &amp; = \\frac{\\Pr(A) \\times \\Pr(B|A)}{\\Pr(A) \\times \\Pr(B|A) + \\Pr(A = 0) \\times(B | A = 0)} \\\\ &amp; = \\frac{0.001 \\times 0.95}{0.001 \\times 0.95 + 0.999 \\times 0.05} \\\\ &amp; = 0.0187 \\end{align} \\] Even though the test is fairly accurate, a person who has tested positive is still very unlikely (less than 2%) to have the disease. Because the base rate of the disease in the population is so low, the vast majority of people taking the test are healthy and even with an accurate test most of the positives will be healthy people.28 14.3 Bayesian method Bayesian inference is usually carried out in the following way: Choose a probability density \\(f(\\theta)\\) – called the prior distribution – that expresses our beliefs about a parameter \\(\\theta\\) before we see any data. Choose a statistical model \\(f(x|\\theta)\\) that reflects our beliefs about \\(x\\) given \\(\\theta\\). Note that we now write this as \\(f(x|\\theta)\\), not \\(f(x; \\theta)\\). After observing data \\(X_1, \\ldots, X_n\\), we update our beliefs and calculate the posterior distribution \\(f(\\theta | X_1, \\ldots, X_n)\\). To calculate the posterior, suppose that \\(\\theta\\) is discrete and that there is a single, discrete observation \\(X\\). We should use a capital letter to denote the parameter since we now treat it like a random variable, so let \\(\\Theta\\) denote the parameter. In this discrete setting, \\[ \\begin{align} \\Pr(\\Theta = \\theta | X = x) &amp;= \\frac{\\Pr(X = x, \\Theta = \\theta)}{\\Pr(X = x)} \\\\ &amp;= \\frac{\\Pr(X = x | \\Theta = \\theta) \\Pr(\\Theta = \\theta)}{\\sum_\\theta \\Pr (X = x| \\Theta = \\theta) \\Pr (\\Theta = \\theta)} \\end{align} \\] which is a basic application of Bayes’ theorem. The version for continuous variables is obtained using density functions \\[f(\\theta | x) = \\frac{f(x | \\theta) f(\\theta)}{\\int f(x | \\theta) f(\\theta) d\\theta}\\] If we have \\(n\\) IID observations \\(X_1, \\ldots, X_n\\), we replace \\(f(x | \\theta)\\) with \\[f(x_1, \\ldots, x_n | \\theta) = \\prod_{i = 1}^n f(x_i | \\theta) = \\Lagr_n(\\theta)\\] We will now write \\(X^n\\) to mean \\((X_1, \\ldots, X_n)\\) and \\(x^n\\) to mean \\((x_1, \\ldots, x_n)\\). Now, \\[ \\begin{align} f(\\theta | x^n) &amp;= \\frac{f(x^n | \\theta) f(\\theta)}{\\int f(x^n | \\theta) f(\\theta) d\\theta} \\\\ &amp;= \\frac{\\Lagr_n(\\theta) f(\\theta)}{c_n} \\\\ &amp;\\propto \\Lagr_n(\\theta) f(\\theta) \\end{align} \\] where \\[c_n = \\int f(x^n | \\theta) f(\\theta) d\\theta\\] is called the normalizing constant. We can summarize this by stating the posterior is proportional to Likelihood times Prior: \\[f(\\theta | x^n) \\propto \\Lagr_n(\\theta) f(\\theta)\\] Since \\(c_n\\) does not depend on \\(\\theta\\), we can safely ignore it at this point, and in fact can recover the constant later on if we need it. With the posterior distribution, we can get a point estimate by summarizing the center of the posterior. Typically this is the mean or mode of the posterior. The posterior mean is \\[\\bar{\\theta}_n = \\int \\theta f(\\theta | x^n) d\\theta = \\frac{\\int \\theta \\Lagr_n(\\theta) f(\\theta)}{\\int \\Lagr_n(\\theta) f(\\theta) d\\theta}\\] We can also obtain a Bayesian interval estimate. We find \\(a\\) and \\(b\\) such that \\[\\int_{-\\infty}^a f(\\theta | x^n) d\\theta = \\int_b^\\infty f(\\theta | x^n) d\\theta = \\frac{\\alpha}{2}\\] Let \\(C = (a,b)\\). Then \\[\\Pr (\\theta \\in C | x^n) = \\int_a^b f(\\theta | x^n) d\\theta = 1 - \\alpha\\] So \\(C\\) is a \\(1 - \\alpha\\) posterior (or credible) interval. 14.3.1 Example: coin tossing There are three types of coins with different probabilities of landing heads when tossed. Type \\(A\\) coins are fair, with \\(p = 0.5\\) of heads Type \\(B\\) coins are bent, with \\(p = 0.6\\) of heads Type \\(C\\) coins are bent, with \\(p = 0.9\\) of heads Suppose I have a drawer containing 5 coins: 2 of type \\(A\\), 2 of type \\(B\\), and 1 of type \\(C\\). I reach into the drawer and pick a coin at random. Without showing you the coin I flip it once and get heads. What is the probability it is type \\(A\\)? Type \\(B\\)? Type \\(C\\)? 14.3.1.1 Terminology Let \\(A\\), \\(B\\), and \\(C\\) be the event the chosen coin was of the respective type. Let \\(D\\) be the event that the toss is heads. The problem then asks us to find: \\[\\Pr(A|D), \\Pr(B|D), \\Pr(C|D)\\] Before applying Bayes’ theorem, we need to define a few things: Experiment - pick a coin from the drawer at random, flip it, and record the result Data - the result of the experiment. Here, \\(D = \\text{heads}\\). \\(D\\) is data that provides evidence for or against each hypothesis Hypotheses - we are testing three hypotheses: the coin is type \\(A\\), \\(B\\), or \\(C\\) Prior probability - the probability of each hypothesis prior to tossing the coin (collecting data). Since the drawer has 2 coins of type \\(A\\), 2 of type \\(B\\), and 1 of type \\(C\\), we have: \\[\\Pr(A) = 0.4, \\Pr(B) = 0.4, \\Pr(C) = 0.2\\] Likelihood - the likelihood function is \\(\\Pr(D|H)\\), the probability of the data assuming that the hypothesis is true. Most often we will consider the data as fixed and let the hypothesis vary. For example, \\(\\Pr(D|A) =\\) probability of heads if the coin is type \\(A\\). In our case, the likelihoods are: \\[\\Pr(D|A) = 0.5, \\Pr(D|B) = 0.6, \\Pr(D|C) = 0.9\\] We can think of these as parameters for a series of Bernoulli distributions. Posterior probability - the probability (posterior to) of each hypothesis given the data from tossing the coin: \\[\\Pr(A|D), \\Pr(B|D), \\Pr(C|D)\\] These posterior probabilities are what we want to find. We can now use Bayes’ theorem to compute each of the posterior probabilities. The theorem says: \\[\\Pr(A|D) = \\frac{\\Pr(D|A) \\times \\Pr(A)}{\\Pr(D)}\\] \\[\\Pr(B|D) = \\frac{\\Pr(D|B) \\times \\Pr(B)}{\\Pr(D)}\\] \\[\\Pr(C|D) = \\frac{\\Pr(D|C) \\times \\Pr(C)}{\\Pr(D)}\\] \\(\\Pr(D)\\) can be computed using the law of total probability: \\[ \\begin{align} \\Pr(D) &amp; = \\Pr(D|A) \\times \\Pr(A) + \\Pr(D|B) \\times \\Pr(B) + \\Pr(D|C) \\times \\Pr(C) \\\\ &amp; = 0.5 \\times 0.4 + 0.6 \\times 0.4 + 0.9 \\times 0.2 = 0.62 \\end{align} \\] So each of the posterior probabilities are: \\[\\Pr(A|D) = \\frac{\\Pr(D|A) \\times \\Pr(A)}{\\Pr(D)} = \\frac{0.5 \\times 0.4}{0.62} = \\frac{0.2}{0.62}\\] \\[\\Pr(B|D) = \\frac{\\Pr(D|B) \\times \\Pr(B)}{\\Pr(D)} = \\frac{0.6 \\times 0.4}{0.62} = \\frac{0.24}{0.62}\\] \\[\\Pr(C|D) = \\frac{\\Pr(D|C) \\times \\Pr(C)}{\\Pr(D)} = \\frac{0.9 \\times 0.2}{0.62} = \\frac{0.18}{0.62}\\] Notice that the total probability \\(\\Pr(D)\\) is the same in each of the denominators and is the sum of the three numerators. hypothesis prior likelihood Bayes numerator posterior \\(H\\) \\(\\Pr(H)\\) \\(\\Pr(D\\mid H)\\) \\(\\Pr(D \\mid H) \\times \\Pr(H)\\) \\(\\Pr(H \\mid D)\\) A 0.4 0.5 0.2 0.3226 B 0.4 0.6 0.24 0.3871 C 0.2 0.9 0.18 0.2903 total 1 0.62 1 The Bayes numerator is the product of the prior and the likelihood. The posterior probability is obtained by dividing the Bayes numerator by \\(\\Pr(D) = 0.625\\). The process of going from the prior probability \\(\\Pr(H)\\) to the posterior \\(\\Pr(H|D)\\) is called Bayesian updating. Bayesian updating uses the data to alter our understanding of the probability of each hypothesis. 14.3.1.2 Things to notice The posterior probabilities for each hypothesis are in the last column. Coin \\(B\\) is the most probable, even with the decrease from the prior to the posterior. \\(C\\) has increased from 0.2 to 0.29. The Bayes numerator determines the posterior probability. To compute the posterior probability, simply rescale the Bayes numerator so that it sums to 1. If all we care about is finding the most likely hypothesis, the Bayes numerator works as well as the normalized posterior. The posterior probability represents the outcome of a tug-of-war between the likelihood and the prior. When calculating the posterior, a large prior may be deflated by a small likelihood, and a small prior may be inflated by a large likelihood. Therefore we can express Bayes’ theorem as: \\[\\Pr(\\text{hypothesis}| \\text{data}) = \\frac{\\Pr(\\text{data} | \\text{hypothesis}) \\times \\Pr(\\text{hypothesis})}{\\Pr(\\text{data})}\\] \\[\\Pr(H|D) = \\frac{\\Pr(D | H) \\times \\Pr(H)}{\\Pr(D)}\\] With the data fixed, the denominator \\(\\Pr(D)\\) just serves to normalize the total posterior probability to 1. So we could express Bayes’ theorem as a statement about the proportionality of two functions of \\(H\\): \\[\\Pr(\\text{hypothesis}| \\text{data}) \\propto \\Pr(\\text{data} | \\text{hypothesis}) \\times \\Pr(\\text{hypothesis})\\] \\[\\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior}\\] Example 14.3 (Bernoulli random variable) Let \\(X_1, \\ldots, X_n \\sim \\text{Bernoulli} (p)\\). Suppose we take the uniform distribution \\(f(p) = 1\\) as a prior. By Bayes’ theorem, the posterior has the form \\[ \\begin{align} f(p | x^n) &amp;\\propto f(p) \\Lagr_n(p) \\\\ &amp;= p^s (1 - p)^{n - s} \\\\ &amp;= p^{s + 1 - 1} (1 - p)^{n - s + 1 - 1} \\end{align} \\] where \\(s = \\sum_{i=1}^n x_i\\) is the number of successes. Importantly, a random variable has a Beta distribution with parameters \\(\\alpha\\) and \\(\\beta\\) if its density is \\[f(p; \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}p^{\\alpha - 1} (1 - p)^{\\beta - 1}\\] We can see that the posterior for \\(p\\) is a Beta distribution with parameters \\(s + 1\\) and \\(n - s + 1\\). That is, \\[f(p | x^n) = \\frac{\\Gamma(n + 2)}{\\Gamma(s + 1) \\Gamma(n - s + 1)}p^{(s + 1) - 1} (1 - p)^{(n - s + 1) - 1}\\] We write this as \\[p | x^n \\sim \\text{Beta} (s + 1, n - s + 1)\\] Notice that we have figured out the normalizing constant \\(c_n = \\frac{\\Gamma(n + 2)}{\\Gamma(s + 1) \\Gamma(n - s + 1)}\\) without actually doing the integral \\(\\int \\Lagr_n(p) f(p) dp\\). The mean of a \\(\\text{Beta}(\\alpha, \\beta)\\) distribution is \\(\\frac{\\alpha}{\\alpha + \\beta}\\), so the Bayes estimator is \\[\\bar{p} = \\frac{s + 1}{n + 2}\\] We can rewrite the estimator as \\[\\bar{p} = \\lambda_n \\hat{p} + (1 - \\lambda_n) \\tilde{p}\\] where \\(\\hat{p} = \\frac{s}{n}\\) is the MLE, \\(\\tilde{p} = \\frac{1}{2}\\) is the prior mean, and \\(\\lambda_n = \\frac{n}{n + 2} \\approx 1\\). So we can think of the MLE as the estimate for \\(p\\) with a flat prior. If we have a non-flat prior, then our estimate \\(\\bar{p}\\) is a weighted average between the prior and the MLE. A 95% credible interval can be obtained by numerically finding \\(a\\) and \\(b\\) such that \\(\\int_a^b f(p | x^n) dp = 0.95\\). Suppose that instead of a uniform prior, we use the prior \\(p \\sim \\text{Beta} (\\alpha, \\beta)\\). If we repeat the calculations from before, we see that \\(p | x^n \\sim \\text{Beta} (\\alpha + s, \\beta + n - s)\\). The flat prior is the special case with \\(\\alpha = \\beta = 1\\). The posterior mean is \\[\\bar{p} = \\frac{\\alpha + s}{\\alpha + \\beta + n} = \\left( \\frac{n}{\\alpha + \\beta + n} \\right) \\hat{p} + \\left( \\frac{\\alpha + \\beta}{\\alpha + \\beta + n} \\right) p_0\\] where \\(p_0 = \\frac{\\alpha}{\\alpha + \\beta}\\) is the prior mean. 14.4 Updating your prior beliefs In life we continually update our beliefs with each new experience of the world. In Bayesian inference, today’s posterior is tomorrow’s prior. Example 14.4 (September 11, 2001) Consider the September 11th attacks in New York City.29 Say that before the first plane hit, our estimate of the probability of a terror attack on tall buildings in Manhattan was just 1 in 20,000, or \\(0.00005\\). But we also assign a low probability to a plane hitting the World Trade Center by accident: 1 in 12,500 on any given day.30 Consider the use of Bayes’ theorem in this instance. What is the probability of terrorists crashing planes into Manhattan skyscrapers given the first plane hitting the World Trade Center? Our initial estimate of how likely it is that terrorists would crash planes into Manhattan skyscrapers is \\(\\Pr(\\text{Terror attack}) = 0.00005\\) Probability of plane hitting if terrorists are attacking Manhattan is \\(\\Pr(\\text{Plane hits the WTC} | \\text{Terror attack}) = 1\\) Probability of plane hitting if terrorists are not attacking Manhattan skyscrapers (i.e. an accident) is \\(\\Pr(\\text{Plane hits the WTC} | \\text{No terror attack}) = 0.00008\\) Our posterior probability of a terror attack, given the first plane hitting the world trade center, is: \\(A =\\) terror attack \\(B =\\) plane hitting the World Trade Center \\(\\Pr(A) = 0.00005 =\\) probability that terrorists would crash a plane into the World Trade Center \\(\\Pr(A^C) = 0.99995 =\\) probability that terrorists would not crash a plane into the World Trade Center \\(\\Pr(B|A) = 1 =\\) probability of a plane crashing into the World Trade Center if terrorists are attacking the World Trade Center \\(\\Pr(B|A^C) = 0.00008 =\\) probability of a plane hitting if terrorists are not attacking the World Trade Center (i.e. an accident) \\[ \\begin{align} \\Pr(A|B) &amp;= \\frac{\\Pr(A) \\times \\Pr(B|A)}{\\Pr(B)} \\\\ &amp;= \\frac{\\Pr(A) \\times \\Pr(B|A)}{ \\Pr(A) \\times \\Pr(B|A) + \\Pr(A^C) \\times \\Pr(B| A^C)} \\\\ &amp; = \\frac{0.00005 \\times 1}{0.00005 \\times 1 + 0.99995 \\times 0.00008} \\\\ &amp; = 0.385 \\end{align} \\] We would now estimate a posterior probability of a 38% chance of a terrorist attack on the World Trade Center. But we can continuously update this posterior probability as new data presents itself. \\[ \\begin{align} \\Pr(A|B) &amp;= \\frac{\\Pr(A) \\times \\Pr(B|A)}{\\Pr(B)} \\\\ &amp;= \\frac{\\Pr(A) \\times \\Pr(B|A)}{ \\Pr(A) \\times \\Pr(B|A) + \\Pr(A^C) \\times \\Pr(B| A^C)} \\\\ &amp; = \\frac{0.385 \\times 1}{0.385 \\times 1 + 0.615 \\times 0.00008} \\\\ &amp; \\approx .9998 \\end{align} \\] 14.5 Simulation The posterior can often be approximated by simulation. Suppose we draw \\(\\theta_1, \\ldots, \\theta_B \\sim p(\\theta | x^n)\\). Then a histogram of \\(\\theta_1, \\ldots, \\theta_B\\) approximates the posterior density \\(p(\\theta | x^n)\\). An approximation to the posterior mean \\(\\bar{\\theta}_n = \\E (\\theta | x^n)\\) is \\(\\frac{\\sum_{j=1}^B \\theta_j}{B}\\). The posterior \\(1 - \\alpha\\) interval can be approximated by \\((\\theta_{\\alpha / 2}, \\theta_{1 - \\alpha /2})\\) where \\(\\theta_{\\alpha / 2}\\) is the \\(\\alpha / 2\\) sample quantile of \\(\\theta_1, \\ldots, \\theta_B\\). Once we have a sample \\(\\theta_1, \\ldots, \\theta_B\\) from \\(f(\\theta | x^n)\\), let \\(\\tau_i = g(\\theta_i)\\). Then \\(\\tau_1, \\ldots, \\tau_B\\) is a sample from \\(f(\\tau | x^n)\\). This avoids the need to do any analytic calculations, especially when \\(f(\\theta | x^n)\\) is an especially complex function. Example 14.5 (Bernoulli random variable) Let \\(X_1, \\ldots, X_n \\sim \\text{Bernoulli} (p)\\) and \\(f(p) = 1\\) so that \\(p | X^n \\sim \\text{Beta} (s + 1, n - s + 1)\\) with \\(s = \\sum_{i=1}^n x_i\\). Let \\(\\psi = \\log \\left( \\frac{p}{1 - p} \\right)\\) (i.e. the log-odds). If we wanted to calculate the PMF and CDF of \\(\\psi | x^n\\), we could do a lot of calculus and analytic math to solve for these equations.31 Alternatively, we can approximate the posterior for \\(\\psi\\) without doing any calculus. Draw \\(P_1, \\ldots, P_B \\sim \\text{Beta} (s + 1, n - s + 1)\\). Let \\(\\psi_i = \\log \\left( \\frac{P_i}{1 - P_i} \\right)\\), for \\(i = 1, \\ldots, B\\) Now \\(\\psi_1, \\ldots, \\psi_B\\) are IID draws from \\(h(\\psi | x^n)\\). A histogram of these values provides an estimate of \\(h(\\psi | x^n)\\). 14.6 Priors To employ Bayesian inference, one requires a prior. Where do you get the prior \\(f(\\theta)\\)? One approach is to use a subjective prior based on your subjective opinion about \\(\\theta\\) before you collect any data. This may be possible, but is impractical for many complicated problems (especially when there are many parameters). Some would argue this approach is also not “scientific” because our inferences should be as objective as possible. An alternative approach is to define some sort of noninformative prior. One obvious choice is to use a flat prior \\(f(\\theta) \\propto\\) constant. In the example earlier, taking \\(f(p) = 1\\) leads to \\(p | X^n \\sim \\text{Beta} (s + 1, n - s + 1)\\) which seems reasonable. But unfettered use of flat priors raises some questions. 14.6.1 Improper priors Let \\(X \\sim N(\\theta, \\sigma^2)\\) with \\(\\sigma\\) known. Suppose we adopt a flat prior \\(f(\\theta) \\propto c\\) where \\(c &gt; 0\\) is a constant. Note that \\(\\int f(\\theta) d\\theta = \\infty\\), so this is not a probability density in the usual sense (otherwise it would integrate to 1). Such a prior is called an improper prior. However, we can still carry out Bayes’ theorem and compute the posterior density by multiplying the prior and the likelihood: \\[f(\\theta) \\propto \\Lagr_n(\\theta) f(\\theta) = \\Lagr_n(\\theta)\\] This gives \\(\\theta | X^n \\sim N(\\bar{X}, \\sigma^2 / n)\\) and the resulting point and interval estimators agree exactly with their frequentist counterparts. In general, improper priors are not a problem as long as the resulting posterior is a well-defined probability distribution. 14.6.2 Flat priors are not invariant Let \\(X \\sim \\text{Bernoulli} (p)\\) and suppose we use the flat prior \\(f(p) = 1\\). This flat prior represents our lack of knowledge about \\(p\\) before the experiment. Now let \\(\\psi = \\log(p / (1 - p))\\). This is a transformation of \\(p\\) and we can compute the resulting distribution for \\(\\psi\\) \\[f_\\Psi (\\psi) = \\frac{e^\\psi}{(1 + e^\\psi)^2}\\] which is not flat. But if we are ignorant of \\(p\\), then we are also ignorant about \\(\\psi\\) so we should use a flat prior for \\(\\psi\\). This is a contradiction. In short, the notion of a flat prior is not well defined because a flat prior on a parameter does not imply a flat prior on a transformed version of the parameter. Flat priors are not transformation invariant. 14.7 Multiparameter problems Suppose that \\(\\theta = (\\theta_1, \\ldots, \\theta_p)\\). The posterior density is given by \\[f(\\theta | x^n) \\propto \\Lagr_n(\\theta) f(\\theta)\\] However, now we need to consider how to extract inferences about one parameter. The key is to find the marginal posterior density for the parameter of interest. Suppose we want to make inferences about \\(\\theta_1\\). The marginal posterior for \\(\\theta_1\\) is \\[f(\\theta_1 | x^n) = \\int \\cdots \\int f(\\theta_1, \\ldots, \\theta_p | x^n) d\\theta_2 \\cdots d\\theta_p\\] Essentially, we calculate the integral of the function over all parameters except \\(\\theta_1\\). If there are two parameters \\((\\theta_1, \\theta_2)\\), we integrate with respect to \\(\\theta_2\\). As the number of parameters increase, this operation gets extremely tricky (if not impossible) to solve analytically. Instead, simulation can be used to approximate by drawing randomly from the posterior \\[\\theta^1, \\ldots, \\theta^B \\sim f(\\theta | x^n)\\] where the superscripts index the different draws. Each \\(\\theta^j\\) is a vector \\(\\theta^j = (\\theta_1^j, \\ldots, \\theta_p^j)\\). Now collect together the first component of each draw \\[\\theta_1^1, \\ldots, \\theta_1^B\\] These are a sample from \\(f(\\theta_1 | x^n)\\) and we have avoided doing any integrals. Example 14.6 (Comparing two binomials) Suppose we have \\(n_1\\) control patients and \\(n_2\\) treatment patients and that \\(X_1\\) control patients survive while \\(X_2\\) treatment patients survive. We want to estimate \\(\\tau = g(p_1, p_2) = p_2 - p_1\\). Then, \\[X_1 \\sim \\text{Binomial} (n_1, p_1) \\, \\text{and} \\, X_2 \\sim \\text{Binomial} (n_2, p_2)\\] If \\(f(p_1, p_2) = 1\\), the posterior is \\[f(p_1, p_2 | x_1, x_2) \\propto p_1^{x_1} (1 - p_1)^{n_1 - x_1} p_2^{x_2} (1 - p_2)^{n_2 - x_2}\\] Notice that \\[f(p_1, p_2 | x_1, x_2) = f(p_1 | x_1) f(p_2 | x_2)\\] where \\[f(p_1 | x_1) \\propto p_1^{x_1} (1 - p_1)^{n_1 - x_1} \\, \\text{and} \\, f(p_2 | x_2) \\propto p_2^{x_2} (1 - p_2)^{n_2 - x_2}\\] which implies that \\(p_1\\) and \\(p_2\\) are independent under the posterior. Also \\[ \\begin{align} p_1 | x_1 &amp;\\sim \\text{Beta} (x_1 + 1, n_1 - x_1 + 1) \\\\ p_2 | x_2 &amp;\\sim \\text{Beta} (x_2 + 1, n_2 - x_2 + 1) \\end{align} \\] If we simulate \\[ \\begin{align} P_{1,1}, \\ldots, P_{1,B} &amp;\\sim \\text{Beta} (x_1 + 1, n_1 - x_1 + 1) \\\\ P_{2,1}, \\ldots, P_{2,B} &amp;\\sim \\text{Beta} (x_2 + 1, n_2 - x_2 + 1) \\end{align} \\] Then \\(\\tau_b = P_{2,b} - P_{1,b}, \\, b = 1, \\ldots, B\\) is a sample from \\(f(\\tau | x_1, x_2)\\). 14.8 Critiques and defenses of Bayesian inference 14.8.1 Critique of Bayesian inference The subjective prior is subjective. There is no single method for choosing a prior, so different (well-intentioned) people will produce different priors and therefore arrive at different posteriors and conclusions. If you accept the premise of subjective priors, you still need good information to create a well-defined prior distribution. Philosophically, some object to assigning probabilities to hypotheses as hypotheses do not constitute outcomes of repeatable experiments in which one can measure long-term frequency. Rather, a hypothesis is either true or false, regardless of whether one knows which is the case. A coin is either fair or unfair Treatment 1 is either better or worse than treatment 2 The sun will or will not come up tomorrow I will either win or not win the lottery For many parametric models with large samples, Bayesian and frequentist methods give approximately the same inferences. Since frequentist methods are historically more common and easier to estimate, there is no reason to go through the steps of Bayesian inference. Bayesian inference depends entirely on the likelihood function. In high dimensional and nonparametric methods, the likelihood function may not yield accurate inferences. 14.8.2 Defense of Bayesian inference The probability of hypotheses is exactly what we need to make decisions. When the doctor tells me a screening test came back positive for a disease, what I really want to know is the probability of the hypothesis “I’m sick”. Bayes’ theorem is logically rigorous (once we obtain a prior). By testing different priors we can see how sensitive our results are to the choice of prior. It is easy to communicate a result framed in terms of probabilities of hypotheses (try explaining the result of a null hypothesis test to a layperson). Priors can be defended based on the assumptions made to arrive at it. Evidence derived from the data is independent of notions about “data more extreme” that depend on the exact experimental setup. Data can be used as it comes in. We don’t have to wait for every contingency to be planned for ahead of time. Acknowledgements Material drawn from All of Statistics by Larry Wasserman References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
